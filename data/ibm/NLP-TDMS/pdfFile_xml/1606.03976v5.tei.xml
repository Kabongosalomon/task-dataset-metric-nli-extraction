<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Estimating individual treatment effect: generalization bounds and algorithms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shalit</surname></persName>
							<email>shalit@cs.nyu.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
							<email>fredrikj@mit.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
							<email>dsontag@csail.mit.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CIMS</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
								<address>
									<postCode>10003</postCode>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">IMES, MIT</orgName>
								<address>
									<postCode>02142</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">CSAIL &amp; IMES, MIT</orgName>
								<address>
									<postCode>02139</postCode>
									<settlement>Cambridge</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Estimating individual treatment effect: generalization bounds and algorithms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education. In particular, individual-level causal inference has important applications such as precision medicine. We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability. The algorithms learn a "balanced" representation such that the induced treated and control distributions look similar. We give a novel, simple and intuitive generalization-error bound showing that the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation. We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances. Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Making predictions about causal effects of actions is a central problem in many domains. For example, a doctor deciding which medication will cause better outcomes for a patient; a government deciding who would benefit most from subsidized job training; or a teacher deciding which study program would most benefit a specific student. In this paper we focus on the problem of making these predic-* Equal contribution tions based on observational data. Observational data is data which contains past actions, their outcomes, and possibly more context, but without direct access to the mechanism which gave rise to the action. For example we might have access to records of patients (context), their medications (actions), and outcomes, but we do not have complete knowledge of why a specific action was applied to a patient.</p><p>The hallmark of learning from observational data is that the actions observed in the data depend on variables which might also affect the outcome, resulting in confounding: For example, richer patients might better afford certain medications, and job training might only be given to those motivated enough to seek it. The challenge is how to untangle these confounding factors and make valid predictions. Specifically, we work under the common simplifying assumption of "no-hidden confounding", assuming that all the factors determining which actions were taken are observed. In the examples above, it would mean that we have measured a patient's wealth or an employee's motivation.</p><p>As a learning problem, estimating causal effects from observational data is different from classic learning in that in our training data we never see the individual-level effect. For each unit, we only see their response to one of the possible actions -the one they had actually received. This is close to what is known in the machine learning literature as "learning from logged bandit feedback" <ref type="bibr">(Strehl et al., 2010;</ref><ref type="bibr">Swaminathan &amp; Joachims, 2015)</ref>, with the distinction that we do not have access to the model generating the action.</p><p>Our work differs from much work in causal inference in that we focus on the individual-level causal effect (also known as "c-specific treatment effects" <ref type="bibr">Shpitser &amp; Pearl (2006)</ref>; <ref type="bibr" target="#b44">Pearl (2015)</ref>), rather that the average or population level. Our main contribution is to give what is, to the best of our knowledge, the first generalization-error 1 bound for estimating individual-level causal effect, where each indi-vidual is identified by its features x. The bound leads naturally to a new family of representation-learning based algorithms <ref type="bibr" target="#b13">(Bengio et al., 2013)</ref>, which we show to match or outperform state-of-the-art methods on several causal effect inference tasks.</p><p>We frame our results using the Rubin-Neyman potential outcomes framework <ref type="bibr">(Rubin, 2011)</ref>, as follows. We assume that for a unit with features x ∈ X , and an action (also known as treatment or intervention) t ∈ {0, 1}, there are two potential outcomes: Y 0 and Y 1 . In our data, for each unit we only see one of the potential outcomes, depending on the treatment assignment: if t = 0 we observe y = Y 0 , if t = 1, we observe y = Y 1 ; this is known as the Consistency assumption. For example, x can denote the set of lab tests and demographic factors of a diabetic patient, t = 0 denote the standard medication for controlling blood sugar, t = 1 denotes a new medication, and Y 0 and Y 1 indicate the patient's blood sugar level if they were to be given medications t = 0 and t = 1, respectively.</p><p>We will denote m 1 (</p><formula xml:id="formula_0">x) = E [Y 1 |x], m 0 (x) = E [Y 0 |x].</formula><p>We are interested in learning the function τ (</p><formula xml:id="formula_1">x) := E [Y 1 − Y 0 |x] = m 1 (x) − m 0 (x). τ (x)</formula><p>is the expected treatment effect of t = 1 relative to t = 0 on an individual unit with characteristics x, or the Individual Treatment Effect (ITE) 2 . For example, for a patient with features x, we can use this to predict which of two treatments will have a better outcome. The fundamental problem of causal inference is that for any x in our data we only observe Y 1 or Y 0 , but never both.</p><p>As mentioned above, we make an important "no-hidden confounders" assumption, in order to make the conditional causal effect identifiable. We formalize this assumption by using the standard strong ignorability condition: (Y 1 , Y 0 ) ⊥ ⊥ t|x, and 0 &lt; p(t = 1|x) &lt; 1 for all x. Strong ignorability is a sufficient condition for the ITE function τ (x) to be identifiable <ref type="bibr" target="#b33">(Imbens &amp; Wooldridge, 2009;</ref><ref type="bibr" target="#b44">Pearl, 2015;</ref><ref type="bibr">Rolling, 2014)</ref>: see proof in the supplement. The validity of strong ignorability cannot be assessed from data, and must be determined by domain knowledge and understanding of the causal relationships between the variables.</p><p>One approach to the problem of estimating the function τ (x) is by learning the two functions m 0 (x) and m 1 (x) using samples from p(Y t |x, t). This is similar to a standard machine learning problem of learning from finite samples. However, there is an additional source of variance at work here: For example, if mostly rich patients received treatment t = 1, and mostly poor patients received treatment t = 0, we might have an unreliable estimation of m 1 (x) for poor patients. In this paper we upper bound this 2 Sometimes known as the Conditional Average Treatment Effect, CATE.  <ref type="figure">Figure 1</ref>. Neural network architecture for ITE estimation. L is a loss function, IPMG is an integral probability metric. Note that only one of h0 and h1 is updated for each sample during training.</p><p>additional source of variance using an Integral Probability Metric (IPM) measure of distance between two distributions p(x|t = 0), and p(x|t = 1), also known as the control and treated distributions. In practice we use two specific IPMs: the Maximum Mean Discrepancy <ref type="bibr" target="#b27">(Gretton et al., 2012)</ref>, and the Wasserstein distance <ref type="bibr">(Villani, 2008;</ref><ref type="bibr" target="#b20">Cuturi &amp; Doucet, 2014)</ref>. We show that the expected error in learning the individual treatment effect function τ (x) is upper bounded by the error of learning Y 1 and Y 0 , plus the IPM term. In the randomized controlled trial setting, where t ⊥ ⊥ x, the IPM term is 0, and our bound naturally reduces to a standard learning problem of learning two functions.</p><p>The bound we derive points the way to a family of algorithms based on the idea of representation learning <ref type="bibr" target="#b13">(Bengio et al., 2013)</ref>: Jointly learn hypotheses for both treated and control on top of a representation which minimizes a weighted sum of the factual loss (the standard supervised machine learning objective), and the IPM distance between the control and treated distributions induced by the representation. This can be viewed as learning the functions m 0 and m 1 under a constraint that encourages better generalization across the treated and control populations. In the Experiments section we apply algorithms based on multilayer neural nets as representations and hypotheses, along with MMD or Wasserstein distributional distances over the representation layer; see <ref type="figure">Figure 1</ref> for the basic architecture.</p><p>In his foundational text about causality, Pearl (2009) writes: "Whereas in traditional learning tasks we attempt to generalize from one set of instances to another, the causal modeling task is to generalize from behavior under one set of conditions to behavior under another set. Causal models should therefore be chosen by a criterion that challenges their stability against changing conditions..." [emphasis ours]. We believe our work points the way to one such stability criterion, for causal inference in the strongly ignorable case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Much recent work in machine learning for causal inference focuses on causal discovery, with the goal of discovering the underlying causal graph or causal direction from data <ref type="bibr" target="#b32">(Hoyer et al., 2009;</ref><ref type="bibr" target="#b38">Maathuis et al., 2010;</ref><ref type="bibr">Triantafillou &amp; Tsamardinos, 2015;</ref><ref type="bibr" target="#b40">Mooij et al., 2016)</ref>. We focus on the case when the causal graph is simple and known to be of the form (Y 1 , Y 0 ) ← x → t, with no hidden confounders.</p><p>Under the causal model we assume, the most common goal of causal effect inference as used in the applied sciences is to obtain the average treatment effect:</p><formula xml:id="formula_2">AT E = E x∼p(x) [τ (x)].</formula><p>We will briefly discuss how some standard statistical causal effect inference methods relate to our proposed method. Note that most of these approaches assume some form of ignorability.</p><p>One of the most widely used approaches to estimating ATE is covariate adjustment, also known as back-door adjustment or the G-computation formula <ref type="bibr" target="#b43">(Pearl, 2009;</ref><ref type="bibr">Rubin, 2011)</ref>. In its basic version, covariate adjustment amounts to estimating the functions m 1 (x), m 0 (x). Therefore, covariate adjustment methods are the most natural candidates for estimating ITE as well as ATE, using the estimates of m t (x). However, most previous work on this subject focused on asymptotic consistency <ref type="bibr" target="#b9">(Belloni et al., 2014;</ref><ref type="bibr" target="#b16">Chernozhukov et al., 2016)</ref>, and so far there has not been much work on the generalization-error of such a procedure. One way to view our results is that we point out a previously unaccounted for source of variance when using covariate adjustment to estimate ITE. We suggest a new type of regularization, by learning representations with reduced IPM distance between treated and control, enabling a new type of bias-variance trade-off.</p><p>Another widely used family of statistical methods used in causal effect inference are weighting methods. Methods such as propensity score weighting <ref type="bibr" target="#b4">(Austin, 2011)</ref> reweight the units in the observational data so as to make the treated and control populations more comparable. These methods do not yield themselves immediately to estimating an individual level effect, and adapting them for that purpose is an interesting research question. Doubly robust methods combine re-weighting the samples and covariate adjustment in clever ways to reduce model bias <ref type="bibr" target="#b24">(Funk et al., 2011)</ref>. Again, we believe that finding how to adapt the concept of double robustness to the problem of effectively estimating ITE is an interesting open question.</p><p>Adapting machine learning methods for causal effect inference, and in particular for individual level treatment effect, has gained much interest recently. For example Wager &amp; Athey (2015);  discuss how treebased methods can be adapted to obtain a consistent estimator with semi-parametric asymptotic convergence rate. Recent work has also looked into how machine learning method can help detect heterogeneous treatment effects when some data from randomized experiments is available <ref type="bibr" target="#b30">(Taddy et al., 2016;</ref><ref type="bibr" target="#b45">Peysakhovich &amp; Lada, 2016)</ref>. Neural nets have also been used for this purpose, exemplified in early work by <ref type="bibr" target="#b8">Beck et al. (2000)</ref>, and more recently by <ref type="bibr" target="#b30">Hartford et al. (2016)</ref>'s work on deep instrumental variables. Our work differs from all the above by focusing on the generalization-error aspects of estimating individual treatment effect, as opposed to asymptotic consistency, and by focusing solely on the observational study case, with no randomized components or instrumental variables.</p><p>Another line of work in the causal inference community relates to bounding the estimate of the average treatment effect given an instrumental variable <ref type="bibr" target="#b5">(Balke &amp; Pearl, 1997;</ref><ref type="bibr" target="#b6">Bareinboim &amp; Pearl, 2012)</ref>, or under hidden confounding, for example when the ignorability assumption does not hold <ref type="bibr" target="#b43">(Pearl, 2009;</ref><ref type="bibr" target="#b15">Cai et al., 2008)</ref>. Our work differs, in that we only deal with the ignorable case, and in that we bound a very different quantity: the generalization-error of estimating individual level treatment effect.</p><p>Our work has strong connections with work on domain adaptation. In particular, estimating ITE requires prediction of outcomes over a different distribution from the observed one. Our ITE error upper bound has similarities with generalization bounds in domain adaptation given by Ben-  <ref type="bibr" target="#b19">Cortes &amp; Mohri (2014)</ref>. These bounds employ distribution distance metrics such as the A-distance or the discrepancy metric, which are related to the IPM distance we use. Our algorithm is similar to a recent algorithm for domain adaptation by <ref type="bibr" target="#b25">Ganin et al. (2016)</ref>, and in principle other domain adaptation methods (e.g. Daumé III (2009); <ref type="bibr" target="#b42">Pan et al. (2011);</ref><ref type="bibr">Sun et al. (2016)</ref>) could be adapted for use in ITE estimation as presented here.</p><p>Finally, our paper builds on work by <ref type="bibr" target="#b34">Johansson et al. (2016)</ref>, where the authors show a connection between covariate shift and the task of estimating the counterfactual outcome in a causal inference scenario. They proposed learning a representation of the data that makes the treated and control distributions more similar, and fitting a linear ridge-regression model on top of it. They then bounded the relative error of fitting a ridge-regression using the distribution with reverse treatment assignment versus fitting a ridge-regression using the factual distribution. Unfortunately, the relative error bound is not at all informative regarding the absolute quality of the representation. In this paper we focus on a related but more substantive task: estimating the individual treatment effect, building on top of the counterfactual error term. We further provide an informative bound on the absolute quality of the representation. We also derive a much more flexible family of algorithms, including non-linear hypotheses and much more powerful distribution metrics in the form of IPMs such as the Wasserstein and MMD distances. Finally, we conduct significantly more thorough experiments including a real-world dataset and out-of-sample performance, and show our methods outperform previously proposed ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Estimating ITE: Error bounds</head><p>In this section we prove a bound on the expected error in estimating the individual treatment effect for a given representation, and a hypothesis defined over that representation. The bound is expressed in terms of (1) the expected loss of the model when learning the observed outcomes y as a function of x and t, denoted F , F standing for "Factual";</p><p>(2) an Integral Probability Metric (IPM) distance between the distribution of treated and control units. The term F is the classic machine learning generalization-error, and in turn can be upper bounded using the empirical error and model complexity terms, applying standard machine learning theory (Shalev-Shwartz &amp; Ben-David, 2014).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem setup</head><p>We will employ the following assumptions and notations.</p><p>The most important notations are in the Notation box in the supplement. The space of covariates is a bounded subset Throughout this paper we will discuss representation functions of the form Φ : X → R, where R is the representation space. We make the following assumption about Φ:</p><formula xml:id="formula_3">X ⊂ R d . The outcome space is Y ⊂ R. Treatment t is a binary variable. We assume there exists a joint distribution p(x, t, Y 0 , Y 1 ), such that (Y 1 , Y 0 ) ⊥ ⊥ t|x and 0 &lt; p(t = 1|x) &lt; 1 for all x ∈ X (</formula><p>Assumption 1. The representation Φ is a twicedifferentiable, one-to-one function. Without loss of generality we will assume that R is the image of X under Φ. We then have Ψ : R → X as the inverse of Φ, such that Ψ(Φ(x)) = x for all x ∈ X .</p><p>The representation Φ pushes forward the treated and control distributions into the new space R; we denote the induced distribution by p Φ .</p><formula xml:id="formula_4">Definition 1. Define p t=1 Φ (r) := p Φ (r|t = 1), p t=0 Φ (r) := p Φ (r|t = 0)</formula><p>, to be the treated and control distributions induced over R. For a one-to-one Φ, the distributions p t=1 Φ (r) and p t=0 Φ (r) can be obtained by the standard change of variables formula, using the determinant of the Jacobian of Ψ(r).</p><p>Let Φ : X → R be a representation function, and h : R × {0, 1} → Y be an hypothesis defined over the representation space R. Let L : Y × Y → R + be a loss function. We define two complimentary loss functions: one is the standard machine learning loss, which we will call the factual loss and denote F . The other is the expected loss with respect to the distribution where the treatment assignment is flipped, which we call the counterfactual loss, CF . Definition 2. The expected loss for the unit and treatment pair (x, t) is:</p><formula xml:id="formula_5">h,Φ (x, t) = Y L(Y t , h(Φ(x), t))p(Y t |x)dY t .</formula><p>The expected factual and counterfactual losses of h and Φ are:</p><formula xml:id="formula_6">F (h, Φ) = X ×{0,1} h,Φ (x, t) p(x, t) dxdt, CF (h, Φ) = X ×{0,1} h,Φ (x, t) p(x, 1 − t) dxdt.</formula><p>If x denotes patients' features, t a treatment, and Y t a potential outcome such as mortality, we think of F as measuring how well do h and Φ predict mortality for the patients and doctors' actions sampled from the same distribution as our data sample. CF measures how well our prediction with h and Φ would do in a "topsy-turvy" world where the patients are the same but the doctors are inclined to prescribe exactly the opposite treatment than the one the real-world doctors would prescribe. Definition 3. The expected factual treated and control losses are:</p><formula xml:id="formula_7">t=1 F (h, Φ) = X h,Φ (x, 1) p t=1 (x) dx, t=0 F (h, Φ) = X h,Φ (x, 0) p t=0 (x) dx. For u := p(t = 1), it is immediate to show that F (h, Φ) = u t=1 (h, Φ) + (1 − u) t=0 (h, Φ).</formula><p>Definition 4. The treatment effect (ITE) for unit x is:</p><formula xml:id="formula_8">τ (x) := E [Y 1 − Y 0 |x] .</formula><p>Let f : X × {0, 1} → Y by an hypothesis. For example, we could have that f (x, t) = h(Φ(x), t).</p><p>Definition 5. The treatment effect estimate of the hypothesis f for unit x is:</p><formula xml:id="formula_9">τ f (x) = f (x, 1) − f (x, 0).</formula><p>Definition 6. The expected Precision in Estimation of Heterogeneous Effect (PEHE, Hill <ref type="formula" target="#formula_10">(2011)</ref>) loss of f is:</p><formula xml:id="formula_10">PEHE (f ) = X (τ f (x) − τ (x)) 2 p(x) dx,<label>(1)</label></formula><p>When f (x, t) = h(Φ(x), t), we will also use the notation</p><formula xml:id="formula_11">PEHE (h, Φ) = PEHE (f ).</formula><p>Our proof relies on the notion of an Integral Probability Metric (IPM), which is a class of metrics between probability distributions <ref type="bibr">(Sriperumbudur et al., 2012;</ref><ref type="bibr" target="#b41">Müller, 1997)</ref>. For two probability density functions p, q defined over S ⊆ R d , and for a function family G of functions g : S → R, we have that</p><formula xml:id="formula_12">IPM G (p, q) := sup g∈G S g(s)(p(s) − q(s)) ds .</formula><p>Integral probability metrics are always symmetric and obey the triangle inequality, and trivially satisfy IPM G (p, p) = 0. For rich enough function families G, we also have that IPM G (p, q) = 0 =⇒ p = q, and then IPM G is a true metric over the corresponding set of probabilities. Examples of function families G for which IPM G is a true metric are the family of bounded continuous functions, the family of 1-Lipschitz functions <ref type="bibr">(Sriperumbudur et al., 2012)</ref>, and the unit-ball of functions in a universal reproducing Hilbert kernel space <ref type="bibr" target="#b27">(Gretton et al., 2012)</ref>.</p><formula xml:id="formula_13">Definition 7. Recall that m t (x) = E [Y t |x]. The expected variance of Y t with respect to a distribution p(x, t): σ 2 Yt (p(x, t)) = X ×Y (Y t − m t (x)) 2 p(Y t |x)p(x, t) dY t dx.</formula><p>We define:</p><formula xml:id="formula_14">σ 2 Yt = min{σ 2 Yt (p(x, t)), σ 2 Yt (p(x, 1 − t))}, σ 2 Y = min{σ 2 Y0 , σ 2 Y1 }.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Bounds</head><p>We first state a Lemma bounding the counterfactual loss, a key step in obtaining the bound on the error in estimating individual treatment effect. We then give the main Thoerem. The proofs and details are in the supplement.</p><p>Let u := p(t = 1) be the marginal probability of treatment. By the strong ignorability assumption, 0 &lt; u &lt; 1.</p><formula xml:id="formula_15">Lemma 1. Let Φ : X → R be a one-to-one representation function, with inverse Ψ. Let h : R × {0, 1} → Y be an hypothesis. Let G be a family of functions g : R → Y. As- sume there exists a constant B Φ &gt; 0, such that for fixed t ∈ {0, 1}, the per-unit expected loss functions h,Φ (Ψ(r), t) (Definition 2) obey 1 BΦ · h,Φ (Ψ(r), t) ∈ G.</formula><p>We have:</p><formula xml:id="formula_16">CF (h, Φ) ≤ (1 − u) t=1 F (h, Φ) + u t=0 F (h, Φ) + B Φ · IPM G p t=1 Φ , p t=0 Φ ,</formula><p>where CF , t=0 F and t=1 F are as in Definitions 2 and 3. Theorem 1. Under the conditions of Lemma 1, and assuming the loss L used to define h,Φ in Definitions 2 and 3 is the squared loss, we have:</p><formula xml:id="formula_17">PEHE (h, Φ) ≤ 2 CF (h, Φ) + F (h, Φ) − 2σ 2 Y ≤ (2) 2 t=0 F (h, Φ)+ t=1 F (h, Φ)+B Φ IPM G p t=1 Φ , p t=0 Φ −2σ 2 Y ,</formula><p>where F and CF are defined w.r.t. the squared loss.</p><p>The main idea of the proof is showing that PEHE is upper bounded by the sum of the expected factual loss F and expected counterfactual loss CF . However, we cannot estimate CF , since we only have samples relevant to F . We therefore bound the difference CF − F using an IPM.</p><p>Choosing a small function family G will make the bound tighter. However, choosing too small a family could result in an incomputable bound. For example, for the min-</p><formula xml:id="formula_18">imal choice G = { h,Φ (x, 0), h,Φ (x, 1)}, we will have to evaluate an expectation term of Y 1 over p t=0 Φ , and of Y 0 over p t=1 Φ .</formula><p>We cannot in general evaluate these expectations, since by assumption when t = 0 we only observe Y 0 , and the same for t = 1 and Y 1 . In addition, for some function families there is no known way to efficiently compute the IPM distance or its gradients. In this paper we use two function families for which there are available optimization tools. The first is the family of 1-Lipschitz functions, which leads to IPM being the Wasserstein distance <ref type="bibr">(Villani, 2008;</ref><ref type="bibr">Sriperumbudur et al., 2012)</ref>, denoted Wass(p, q). The second is the family of norm-1 reproducing kernel Hilbert space (RKHS) functions, leading to the MMD metric <ref type="bibr" target="#b27">(Gretton et al., 2012;</ref><ref type="bibr">Sriperumbudur et al., 2012)</ref>, denoted MMD(p, q). Both the Wasserstein and MMD metrics have consistent estimators which can be efficiently computed in the finite sample case <ref type="bibr">(Sriperumbudur et al., 2012)</ref>. Both have been used for various machine learning tasks in recent years <ref type="bibr" target="#b26">(Gretton et al., 2009;</ref><ref type="bibr" target="#b20">Cuturi &amp; Doucet, 2014)</ref>.</p><p>In order to explicitly evaluate the constant B Φ in Theorem 1, we have to make some assumptions about the elements of the problem. For the Wasserstein case these are the loss L, the Lipschitz constants of p(Y t |x) and h, and the condition number of the Jacobian of Φ. For the MMD case, we make assumptions about the RKHS representability and RKHS norms of h , Φ, and the standard deviation of Y t |x. The full details are given in the supplement, with the major results stated in Theorems 2 and 3. In all cases we obtain that making Φ smaller increases the constant B Φ precluding trivial solutions such as making Φ arbitrarily small.</p><p>For an empirical sample, and a family of representations and hypotheses, we can further upper bound t=0 F and t=1 F by their respective empirical losses and a model complexity term using standard arguments (Shalev-Shwartz &amp; Ben-David, 2014). The IPMs we use can be consistently estimated from finite samples <ref type="bibr">(Sriperumbudur et al., 2012)</ref>. The negative variance term σ 2 Y arises from the fact that, following Hill (2011); , we define the error PEHE in terms of the conditional mean functions m t (x), as opposed to fitting the random variables Y t .</p><p>Our results hold for any given h and Φ obeying the The-orem conditions. This immediately suggest an algorithm in which we minimize the upper bound in Eq. (2) with respect to Φ and h and either the Wasserstein or MMD IPM, in order to minimize the error in estimating the individual treatment effect. This leads us to Algorithm 1 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Algorithm for estimating ITE</head><p>We propose a general framework called CFR (for Counterfactual Regression) for ITE estimation based on the theoretical results above. Our algorithm is an end-to-end, regularized minimization procedure which simultaneously fits both a balanced representation of the data and a hypothesis for the outcome. CFR draws on the same intuition as the approach proposed by <ref type="bibr" target="#b34">Johansson et al. (2016)</ref>, but overcomes the following limitations of their method: a) Their theory requires a two-step optimization procedure and is specific to linear hypotheses of the learned representation (and does not support e.g. deep neural networks), b) The treatment indicator might get lost if the learned representation is high-dimensional (see discussion below).</p><p>We assume there exists a distribution p(</p><formula xml:id="formula_19">x, t, Y 0 , Y 1 ) over X × {0, 1} × Y × Y, such that strong ignorability holds.</formula><p>We further assume we have a sample from that distribution</p><formula xml:id="formula_20">(x 1 , t 1 , y 1 ), . . . (x n , t n , y n ), where y i ∼ p(Y 1 |x i ) if t i = 1, y i ∼ p(Y 0 |x i ) if t i = 0.</formula><p>This standard assumption means that the treatment assignment determines which potential outcome we see. Our goal is to find a representation Φ : X → R and hypothesis h :</p><formula xml:id="formula_21">X × {0, 1} → Y that will minimize PEHE (f ) for f (x, t) := h(Φ(x), t).</formula><p>In this work, we let Φ(x) and h(Φ, t) be parameterized by deep neural networks trained jointly in an end-to-end fashion, see <ref type="figure">Figure 1</ref>. This model allows for learning complex non-linear representations and hypotheses with large flexibility. <ref type="bibr" target="#b34">Johansson et al. (2016)</ref> parameterized h(Φ, t) with a single network using the concatenation of Φ and t as input. When the dimension of Φ is high, this risks losing the influence of t on h during training. To combat this, our first contribution is to parameterize h 1 (Φ) and h 0 (Φ) as two separate "heads" of the joint network, the former used to estimate the outcome under treatment, and the latter under control. This means that statistical power is shared in the representation layers of the network, while the effect of treatment is retained in the separate heads. Note that each sample is used to update only the head corresponding to the observed treatment; for example, an observation (x i , t i = 1, y i ) is only used to update h 1 .</p><p>Our second contribution is to excplicitly account and adjust for the bias induced by treatment group imbalance. To this end, we seek a representation Φ and hypothesis h that minimizes a trade-off between predictive accuracy and imbalance in the representation space, using the following ob-jective:</p><formula xml:id="formula_22">min h,Φ Φ =1 1 n n i=1 w i · L (h(Φ(x i ), t i ) , y i ) + λ · R(h) + α · IPM G ({Φ(x i )} i:ti=0 , {Φ(x i )} i:ti=1 ) , with w i = t i 2u + 1 − t i 2(1 − u) , where u = 1 n n i=1 t i ,</formula><p>and R is a model complexity term.</p><p>(3) Note that u = p(t = 1) in the definition of w i is simply the proportion of treated units in the population. The weights w i compensate for the difference in treatment group size in our sample, see Theorem 1. IPM G (·, ·) is the (empirical) integral probability metric defined by the function family G. For most IPMs, we cannot compute the factor B φ in Equation 2, but treat it as part of the hyperparameter α. This makes our objective sensitive to the scaling of Φ, even for a constant α. We therefore normalize Φ through either projection or batch-normalization with fixed scale. We refer to the model minimizing <ref type="formula">(3)</ref> with α &gt; 0 as Counterfactual Regression (CFR) and the variant without balance regularization (α = 0) as Treatment-Agnostic Representation Network (TARNet).</p><p>We train our models by minimizing (3) using stochastic gradient descent, where we backpropagate the error through both the hypothesis and representation networks, as described in Algorithm 1. Both the prediction loss and the penalty term IPM G (·, ·) are computed for one minibatch at a time. Details of how to obtain the gradient g 1 with respect to the empirical IPMs are in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Evaluating causal inference algorithms is more difficult than many machine learning tasks, since for real-world data we rarely have access to the ground truth treatment effect. Existing literature mostly deals with this in two ways. One is by using synthetic or semi-synthetic datasets, where the outcome or treatment assignment are fully known; we use the semi-synthetic IHDP dataset from <ref type="bibr" target="#b31">Hill (2011)</ref>. The other is using real-world data from randomized controlled trials (RCT). The problem in using data from RCTs is that there is no imbalance between the treated and control distributions, making our method redundant. We partially overcome this problem by using the Jobs dataset from LaLonde (1986), which includes both a randomized and a non-randomized component. We use both for training, but can only use the randomized component for evaluation. This alleviates, but does not solve, the issue of a completely balanced dataset being unsuited for our method.</p><p>We evaluate our framework CFR, and its variant without Algorithm 1 CFR: Counterfactual regression with integral probability metrics 1: Input: Factual sample (x 1 , t 1 , y 1 ), . . . , (x n , t n , y n ), scaling parameter α &gt; 0, loss function L (·, ·), representation network Φ W with initial weights W, outcome network h V with initial weights V, function family G for IPM.</p><formula xml:id="formula_23">2: Compute u = 1 n n i=1 t i 3: Compute w i = ti 2u + 1−ti 2(1−u) for i = 1 . . . n 4: while not converged do 5: Sample mini-batch {i 1 , i 2 , . . . , i m } ⊂ {1, 2, . . . , n} 6:</formula><p>Calculate the gradient of the IPM term:</p><formula xml:id="formula_24">g 1 =∇ W IPMG({Φ W (xi j )}t i j =0, {Φ W (xi k )}t i j =1) 7:</formula><p>Calculate the gradients of the empirical loss:</p><formula xml:id="formula_25">g 2 = ∇ V 1 m j w ij · L h V (Φ W (x ij ), t ij ), y ij g 3 = ∇ W 1 m j w ij · L h V (Φ W (x ij ), t ij ), y ij 8:</formula><p>Obtain step size scalar or matrix η with standard neural net methods e.g. Adam <ref type="bibr" target="#b35">(Kingma &amp; Ba, 2014)</ref> 9:</p><formula xml:id="formula_26">[W, V] ← [W − η(αg 1 + g 3 ), V − η(g 2 + 2λV)] 10:</formula><p>Check convergence criterion 11: end while balancing regularization (TARNet), in the task of estimating ITE and ATE. CFR is implemented as a feed-forward neural network with 3 fully-connected exponential-linear layers for the representation and 3 for the hypothesis. Layer sizes were 200 for all layers used for Jobs and 200 and 100 for the representation and hypothesis used for IHDP. The model is trained using Adam <ref type="bibr" target="#b35">(Kingma &amp; Ba, 2014)</ref>. For an overview, see <ref type="figure">Figure 1</ref>. Layers corresponding to the hypothesis are regularized with a small 2 weight decay. For continuous data we use mean squared loss and for binary data, we use log-loss. While our theory does not immediately apply to log-loss, we were curious to see how our model performs with it.</p><p>We compare our method to Ordinary Least Squares with treatment as a feature (OLS-1), OLS with separate regressors for each treatment (OLS-2), k-nearest neighbor (k-NN), Targeted Maximum Likelihood, which is a doubly robust method (TMLE) <ref type="bibr" target="#b28">(Gruber &amp; van der Laan, 2011)</ref>, Bayesian Additive Regression Trees (BART) <ref type="bibr" target="#b18">(Chipman et al., 2010;</ref><ref type="bibr" target="#b17">Chipman &amp; McCulloch, 2016)</ref>, Random Forests (Rand. For.) <ref type="bibr" target="#b14">(Breiman, 2001)</ref>, Causal Forests (Caus. For.) <ref type="bibr">(Wager &amp; Athey, 2015)</ref> as well as the Balancing Linear Regression (BLR) and Balancing Neural Network (BNN) by <ref type="bibr" target="#b34">Johansson et al. (2016)</ref>. For classification tasks we substitute Logistic Regression (LR) for OLS. Choosing hyperparameters for estimating PEHE is nontrivial; we detail our selection procedure, applied to all methods, in subsection C.1 of the supplement. We evaluate our model in two different settings. One is within-sample, where the task is to estimate ITE for all units in a sample for which the (factual) outcome of one treatment is observed. This corresponds to the common scenario in which a cohort is selected once and not changed. This task is non-trivial, as we never observe the ITE for any unit. The other is the out-of-sample setting, where the goal is to estimate ITE for units with no observed outcomes. This corresponds to the case where a new patient arrives and the goal is to select the best possible treatment.</p><p>Within-sample error is computed over both the training and validation sets, and out-of-sample error over the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Simulated outcome: IHDP</head><p>Hill (2011) compiled a dataset for causal effect estimation based on the Infant Health and Development Program (IHDP), in which the covariates come from a randomized experiment studying the effects of specialist home visits on future cognitive test scores. The treatment groups have been made imbalanced by removing a biased subset of the treated population. The dataset comprises 747 units (139 treated, 608 control) and 25 covariates measuring aspects of children and their mothers. We use the simulated outcome implemented as setting "A" in the NPCI package <ref type="bibr" target="#b23">(Dorie, 2016)</ref>. Following Hill <ref type="formula" target="#formula_10">(2011)</ref>, we use the noiseless outcome to compute the true effect. We report the estimated (finite-sample) PEHE loss PEHE (Eq. 1), and the absolute error in average treatment effect</p><formula xml:id="formula_27">ATE = | 1 n n i=1 (f (x i , 1) − f (x i , 0)) − 1 n n i=1 (m 1 (x i ) − m 0 (x i ))|.</formula><p>The results of the experiments on IHDP are presented in Table 1 (left). We average over 1000 realizations of the outcomes with 63/27/10 train/validation/test splits. We investigate the effects of increasing imbalance between the original treatment groups by constructing biased subsamples of the IHDP dataset. A logistic-regression propensity score model is fit to form estimatesp(t = 1|x) of the conditional treatment probability. Then, repeatedly, with probability q we remove the remaining control observation x that hasp(t = 1|x) closest to 1, and with probability 1−q, we remove a random control observation. The higher q, the more imbalance. For each value of q, we remove 347 observations from each set, leaving 400.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Real-world outcome: Jobs</head><p>The study by LaLonde (1986) is a widely used benchmark in the causal inference community, where the treatment is job training and the outcomes are income and employment status after training. This dataset combines a randomized study based on the National Supported Work program with observational data to form a larger dataset <ref type="bibr">(Smith &amp; Todd, 2005)</ref>. The presence of the randomized subgroup gives a way to estimate the "ground truth" causal effect. The study includes 8 covariates such as age and education, as well as previous earnings. We construct a binary classification task, called Jobs, where the goal is to predict unemployment, using the feature set of <ref type="bibr" target="#b22">Dehejia &amp; Wahba (2002</ref>  </p><formula xml:id="formula_28">i∈T y i − |C ∩ E| −1 i∈C∩E y i , where C is the control group. We report the error ATT = |ATT − 1 |T | i∈T (f (x i , 1) − f (x i , 0))|.</formula><p>We cannot evaluate PEHE on this dataset, since there is no ground truth for the ITE. Instead, in order to evaluate the quality of ITE estimation, we use a measure we call policy risk. The policy risk is defined as the average loss in value when treating according to the policy implied by an ITE estimator. In our case, for a model f , we let the policy be to treat, π f (x) = 1, if f (x, 1) − f (x, 0) &gt; λ, and to not treat, π f (x) = 0 otherwise. The policy risk is R Pol (π f ) = 1 − (E[Y 1 |π f (x) = 1] · p(π f = 1) + E[Y 0 |π f (x) = 0] · p(π f = 0)) which we can estimate for the randomized trial subset of Jobs byR Pol (π f = 1 − (E[Y 1 |π f (x) = 1, t = 1] · p(π f = 1) + E[Y 0 |π f (x) = 0, t = 0] · p(π f = 0)). See figure 3 for risk as a function of treatment threshold λ, aligned by proportion of treated, and <ref type="table" target="#tab_0">Table 1</ref> for the risk when λ = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results</head><p>We begin by noting that indeed imbalance confers an advantage to using the IPM regularization term, as our theoretical results indicate, see e.g. the results for CFR Wass (α &gt; 0) and TARNet (α = 0) on IHDP in <ref type="table" target="#tab_0">Table 1</ref>. We also see in <ref type="figure" target="#fig_3">Figure 2</ref> that even for the harder case of increased imbalance (q &gt; 0) between treated and control, the relative gain from using our method remains significant. On Jobs, we see a smaller gain from using IPM penalties than on IHDP. We believe this is the case because, while we are minimizing our bound over observational data and accounting for this bias, we are evaluating the predictions only on a randomized subset, where the treatment groups are distributed identically. For both IHDP, non-linear estimators do significantly better than linear ones in terms of individual effect ( PEHE ). On the Jobs dataset, straightforward logistic regression does remarkably well in estimating the ATT. However, being a linear model, LR can only ascribe a uniform policy -in this case, "treat everyone". The more nuanced policies offered by non-linear methods achieve lower policy risk in the case of Causal Forests and CFR. This emphasizes the fact that estimating average effect and individual effect can require different models. Specifically, while smoothing over many units may yield a good ATE estimate, this might significantly hurt ITE estimation. k-nearest neighbors has very good within-sample results on Jobs, because evaluation is performed over the randomized component, but suffers heavily in generalizing out of sample, as expected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper we give a meaningful and intuitive error bound for the problem of estimating individual treatment effect. Our bound relates ITE estimation to the classic machine learning problem of learning from finite samples, along with methods for measuring distributional distances from finite samples. The bound lends itself naturally to the creation of learning algorithms; we focus on using neural nets as representations and hypotheses. We apply our theoryguided approach to both synthetic and real-world tasks, showing that in every case our method matches or outperforms the state-of-the-art. Important open questions are theoretical considerations in choosing the IPM weight α, how to best derive confidence intervals for our model's predictions, and how to integrate our work with more complicated causal models such as those with hidden confounding or instrumental variables. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Definitions, assumptions, and auxiliary lemmas</head><p>We first define the necessary distributions and prove some simple results about them. We assume a joint distribution function p(x, t, Y 0 , Y 1 ), such that (Y 1 , Y 0 ) ⊥ ⊥ t|x, and 0 &lt; p(t = 1|x) &lt; 1 for all x. Recall that we assume Consistency, that is we assume that we observe y = Y 1 |(t = 1) and y = Y 0 |(t = 0). Definition A1. The treatment effect for unit x is:</p><formula xml:id="formula_29">τ (x) := E [Y 1 − Y 0 |x] .</formula><p>We first show that under consistency and strong ignorability, the ITE function τ (x) is identifiable: Lemma A1. We have:</p><formula xml:id="formula_30">E [Y 1 − Y 0 |x] = E [Y 1 |x] − E [Y 0 |x] = (4) E [Y 1 |x, t = 1] − E [Y 0 |x, t = 0] = (5) E [y|x, t = 1] − E [y|x, t = 0] .</formula><p>Equality <ref type="formula">(4)</ref> is because we assume that Y t and t are independent conditioned on x. Equality (5) follows from the consistency assumption. Finally, the last equation is composed entirely of observable quantities and can be estimated from data since we assume 0 &lt; p(t = 1|x) &lt; 1 for all x.</p><p>Definition A2. Let p t=1 (x) := p(x|t = 1), and p t=0 (x) := p(x|t = 0) denote respectively the treatment and control distributions.</p><p>Let Φ : X → R be a representation function. We will assume that Φ is differentiable. Assumption A1. The representation function Φ is one-toone. Without loss of generality we will assume that R is the image of X under Φ, and define Ψ : R → X to be the inverse of Φ, such that Ψ(Φ(x)) = x for all x ∈ X .</p><p>Definition A3. For a representation function Φ : X → R, and for a distribution p defined over X , let p Φ be the distribution induced by Φ over R. Define p t=1 Φ (r) := p Φ (r|t = 1), p t=0 Φ (r) := p Φ (r|t = 0), to be the treatment and control distributions induced over R. Proof. Let J Ψ (r) be the absolute of the determinant of the Jacobian of Ψ(r).</p><formula xml:id="formula_31">p Φ (t|r) = p Φ (t, r) p Φ (r) (a) = p(t, Ψ(r))J Ψ (r) p(Ψ(r))J Ψ (r) = p(t, Ψ(r)) p(Ψ(r)) = p(t|Ψ(r)),</formula><p>where equality (a) is by the change of variable formula. The proof is identical for p(Y t |r).</p><p>Let L : Y × Y → R + be a loss function, e.g. the absolute loss or squared loss.</p><p>Definition A4. Let Φ : X → R be a representation function. Let h : R×{0, 1} → Y be an hypothesis defined over the representation space R. The expected loss for the unit and treatment pair (x, t) is:</p><formula xml:id="formula_32">h,Φ (x, t) = Y L(Y t , h(Φ(x), t))p(Y t |x)dY t</formula><p>Definition A5. The expected factual loss and counterfactual losses of h and Φ are, respectively:</p><formula xml:id="formula_33">F (h, Φ) = X ×{0,1} h,Φ (x, t) p(x, t) dxdt CF (h, Φ) = X ×{0,1} h,Φ (x, t) p(x, 1 − t) dxdt.</formula><p>When it is clear from the context, we will sometimes use F (f ) and CF (f ) for the expected factual and counterfactual losses of an arbitrary function f :</p><formula xml:id="formula_34">X × {0, 1} → Y.</formula><p>Definition A6. The expected treated and control losses are:</p><formula xml:id="formula_35">t=1 F (h, Φ) = X h,Φ (x, 1) p t=1 (x) dx t=0 F (h, Φ) = X h,Φ (x, 0) p t=0 (x) dx t=1 CF (h, Φ) = X h,Φ (x, 1) p t=0 (x) dx t=0 CF (h, Φ) = X h,Φ (x, 0) p t=1 (x) dx.</formula><p>The four losses above are simply the loss conditioned on either the control or treated set. Let u := p(t = 1) be the proportion of treated in the population. We then have the immediate result:</p><formula xml:id="formula_36">Lemma A3. F (h, Φ) = u · t=1 F (h, Φ) + (1 − u) · t=0 F (h, Φ) CF (h, Φ) = (1 − u) · t=1 CF (h, Φ) + u · t=0 CF (h, Φ).</formula><p>The proof is immediate, noting that p(x, t) = u · p t=1 (x) + (1 − u) · (x), and from the Definitions A4 and A6 of the losses.</p><p>Definition A7. Let G be a function family consisting of functions g : S → R. For a pair of distributions p 1 , p 2 over S, define the Integral Probability Metric: <ref type="figure">·)</ref> defines a pseudo-metric on the space of probability functions over S, and for sufficiently large function families, IPM G (·, ·) is a proper metric <ref type="bibr" target="#b41">(Müller, 1997)</ref>. Examples of sufficiently large functions families includes the set of bounded continuous functions, the set of 1-Lipschitz functions, and the set of unit norm functions in a universal Reproducing Norm Hilbert Space. The latter two give rise to the Wasserstein and Maximum Mean Discrepancy metrics, respectively <ref type="bibr" target="#b27">(Gretton et al., 2012;</ref><ref type="bibr">Sriperumbudur et al., 2012)</ref>. We note that for function families G such as the three mentioned above, for which g ∈ G =⇒ −g ∈ G, the absolute value can be omitted from definition A7.</p><formula xml:id="formula_37">IPM G (p 1 , p 2 ) = sup g∈G S g(s) (p 1 (s) − p 2 (s)) ds IPM G (·,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. General IPM bound</head><p>We now state and prove the most important technical lemma of this section. Lemma A4 (Lemma 1, main text). Let Φ : X → R be an invertible representation with Ψ its inverse. Let p t=1 Φ , p t=0 Φ be defined as in Definition A3. Let u = p(t = 1). Let G be a family of functions g : R → R, and denote by IPM G (·, ·) the integral probability metric induced by G.</p><p>Let h : R×{0, 1} → Y be an hypothesis. Assume there exists a constant B Φ &gt; 0, such that for t = 0, 1, the function g Φ,h (r, t) := 1 BΦ · h,Φ (Ψ(r), t) ∈ G. Then we have:</p><formula xml:id="formula_38">CF (h, Φ) ≤ (1 − u) t=1 F (h, Φ) + u t=0 F (h, Φ)+ B Φ · IPM G p t=1 Φ , p t=0 Φ .<label>(6)</label></formula><p>Proof.</p><formula xml:id="formula_39">CF (h, Φ) − (1 − u) · t=1 F (h, Φ) + u · t=0 F (h, Φ) = (1 − u) · t=1 CF (h, Φ) + u · t=0 CF (h, Φ) − (1 − u) · t=1 F (h, Φ) + u · t=0 F (h, Φ) = (1 − u) · t=1 CF (h, Φ) − t=1 F (h, Φ) + u · t=0 CF (h, Φ) − t=0 F (h, Φ) = (7) (1 − u) X h,Φ (x, 1) p t=0 (x) − p t=1 (x) dx+ u X h,Φ (x, 0) p t=1 (x) − p t=0 (x) dx = (8) (1 − u) R h,Φ (Ψ(r), 1) p t=0 Φ (r) − p t=1 Φ (r) dr+ u R h,Φ (Ψ(r), 0) p t=1 Φ (r) − p t=0 Φ (r) dr = B Φ · (1 − u) R 1 B Φ h,Φ (Ψ(r), 1) p t=0 Φ (r) − p t=1 Φ (r) dr+ B Φ · u R 1 B Φ h,Φ (Ψ(r), 0) p t=1 Φ (r) − p t=0 Φ (r) dr ≤ (9) B Φ · (1 − u) sup g∈G R g(r) p t=0 Φ (r) − p t=1 Φ (r) dr + B Φ · u sup g∈G R g(r) p t=1 Φ (r) − p t=0 Φ (r) dr = (10) B Φ · IPM G (p t=0 Φ , p t=1 Φ ).<label>(11)</label></formula><p>Equality <ref type="formula">(7)</ref> is by Definition A6 of the treated and control loss, equality (8) is by the change of variables formula and Definition A3 of p t=1 Φ and p t=0 Φ , inequality (9) is by the premise that 1 BΦ · h,Φ (Ψ(r), t) ∈ G for t = 0, 1, and (10) is by Definition A7 of an IPM.</p><p>The essential point in the proof of Lemma A4 is inequality 9. Note that on the l.h.s. of the inequality, we need to evaluate the expectations of h,Φ (Ψ(r), 0) over p t=1 Φ and h,Φ (Ψ(r), 1) over p t=0 Φ . Both of these expectations are in general unavailable, since they require us to evaluate treatment outcomes on the control, and control outcomes on the treated. We therefore upper bound these unknowable quantities by taking a supremum over a function family which includes h,Φ (Ψ(r), 0) and h,Φ <ref type="figure">(Ψ(r), 1)</ref>. The upper bound ignores most of the details of the outcome, and amounts to measuring a distance between two distributions we have samples from: the control and treated distribution. Note that for a randomized trial (i.e. when t ⊥ ⊥ x) with we have that IPM(p t=1 Φ , p t=0 Φ ) = 0. Indeed, it is straightforward to show that in that case we actually have an equality:</p><formula xml:id="formula_40">CF (h, Φ) = (1 − u) · t=1 F (h, Φ) + u · t=0 F (h, Φ)</formula><p>. The crucial condition in Lemma A4 is that the function g Φ,h (r) := 1 BΦ h,Φ (Ψ(r), t) is in G. In subsections A.3 and A.4 below we look into two specific function families G, and evaluate what does this inclusion condition entail, and in particular we will derive specific bounds for B Φ .</p><p>Definition A8. For t = 0, 1 define:</p><formula xml:id="formula_41">m t (x) := E [Y t |x] .</formula><p>Obviously for the treatment effect τ (x) we have τ (x) = m 1 (x) − m 0 (x).</p><p>Let f : X × {0, 1} → Y by an hypothesis, such that f (x, t) = h(Φ(x), t) for a representation Φ and hypothesis h defined over the output of Φ.</p><p>Definition A9. The treatment effect estimate for unit x is:</p><formula xml:id="formula_42">τ f (x) = f (x, 1) − f (x, 0).</formula><p>Definition A10. The expected Precision in Estimation of Heterogeneous Effect (PEHE) loss of g is:</p><formula xml:id="formula_43">PEHE (f ) = X (τ f (x) − τ (x)) 2 p(x) dx.</formula><p>Definition A11. The expected variance of Y t with respect to a distribution p(x, t):</p><formula xml:id="formula_44">σ 2 Yt (p(x, t)) = X ×Y (Y t − m t (x)) 2 p(Y t |x)p(x, t) dY t dx.</formula><p>We define:</p><formula xml:id="formula_45">σ 2 Yt = min{σ 2 Yt (p(x, t)), σ 2 Yt (p(x, 1 − t))}, σ 2 Y = min{σ 2 Y0 , σ 2 Y1 }.</formula><p>If Y t are deterministic functions of x, then σ 2 Y = 0. We now show that PEHE (f ) is upper bounded by 2 F + 2 CF − 2σ 2 Y where F and CF are w.r.t. to the squared loss. An analogous result can be obtained for the absolute loss, using mean absolute deviation.</p><p>Lemma A5. For any function f : X × {0, 1} → Y, and distribution p(x, t) over X × {0, 1}:</p><formula xml:id="formula_46">X (f (x, t) − m t (x)) 2 p(x, t) dxdt = F (f ) − σ 2 Yt (p(x, t)), X (f (x, t) − m t (x)) 2 p(x, 1 − t) dxdt = CF (f ) − σ 2 Yt (p(x, 1 − t)),</formula><p>where F (f ) and CF (f ) are w.r.t. to the squared loss.</p><p>Proof. For simplicity we will prove for p(x, t) and F (f ).</p><p>The proof for p(x, 1 − t) and CF is identical.</p><formula xml:id="formula_47">F (f ) = X ×{0,1}×Y (f (x, t) − Y t ) 2 p(Y t |x)p(x, t) dY t dxdt = X ×{0,1}×Y (f (x, t) − m t (x)) 2 p(Y t |x)p(x, t) dY t dxdt+ X ×{0,1}×Y (m t (x) − Y t ) 2 p(Y t |x)p(x, t) dY t dxdt+ (12) X ×{0,1}×Y (f (x, t) − m t (x)) (m t (x) − Y t ) p(Y t |x)p(x, t) dY t dxdt = (13) X ×{0,1} (f (x, t) − m t (x)) 2 p(x, t) dxdt+ σ 2 Y0 (p(x, t)) + σ 2 Y1 (p(x, t)) + 0,</formula><p>where the equality (13) is by the Definition A11 of σ 2 Yt (p), and because the integral in (12) evaluates to zero, since m t (x) = X Y t p(Y t |x) dx.</p><p>Theorem 1. Let Φ : X → R be a one-to-one representation function, with inverse Ψ. Let p t=1 Φ , p t=0 Φ be defined as in Definition A3. Let u = p(t = 1). Let G be a family of functions g : R → R, and denote by IPM G (·, ·) the integral probability metric induced by G. Let h : R × {0, 1} → Y be an hypothesis. Let the loss L(y 1 , y 2 ) = (y 1 − y 2 ) 2 . Assume there exists a constant B Φ &gt; 0, such that for t ∈ {0, 1}, the functions g Φ,h (r, t) := 1 BΦ · h,Φ (Ψ(r), t) ∈ G. We then have:</p><formula xml:id="formula_48">PEHE (h, Φ) ≤ 2 CF (h, Φ) + F (h, Φ) − 2σ 2 Y ≤ 2 t=0 F (h, Φ)+ t=1 F (h, Φ)+B Φ IPM G p t=1 Φ , p t=0 Φ −2σ 2 Y ,</formula><p>where F and CF are with respect to the squared loss.</p><p>Proof. We will prove the first inequality,</p><formula xml:id="formula_49">PEHE (f ) ≤ 2 CF (h, Φ) + 2 F (h, Φ) − 2σ 2 Y .</formula><p>The second inequality is then immediate by Lemma A4. Recall that we denote</p><formula xml:id="formula_50">PEHE (f ) = PEHE (h, Φ) for f (x, t) = h(Φ(x), t)</formula><p>.</p><formula xml:id="formula_51">PEHE (f ) = X (f (x, 1) − f (x, 0)) − (m 1 (x) − m 0 (x)) 2 p(x) dx = X (f (x, 1) − m 1 (x)) + (m 0 (x) − f (x, 0)) 2 p(x) dx ≤ (14) 2 X (f (x, 1) − m 1 (x)) 2 + (m 0 (x) − f (x, 0)) 2 p(x) dx = (15) 2 X (f (x, 1) − m 1 (x)) 2 p(x, t = 1) dx+ 2 X (m 0 (x) − f (x, 0)) 2 p(x, t = 0) dx+ 2 X (f (x, 1) − m 1 (x)) 2 p(x, t = 0) dx+ 2 X (m 0 (x) − f (x, 0)) 2 p(x, t = 1) dx = 2 X (f (x, t) − m t (x)) 2 p(x, t) dxdt+ 2 X (f (x, t) − m t (x)) 2 p(x, 1 − t) dxdt ≤ (16) 2( F − σ 2 Y ) + 2( CF − σ 2 Y ).</formula><p>where <ref type="formula" target="#formula_10">(14)</ref> is because (x + y) 2 ≤ 2(x 2 + y 2 ), (15) is because p(x) = p(x, t = 0) + p(x, t = 1) and (16) is by Lemma A5 and Definition A5 of the losses F , CF and Definition A11 of σ 2 Y . Having established the first inequality in the Theorem statement, we now show the second. We have by Lemma A4 that:</p><formula xml:id="formula_52">CF (h, Φ) ≤ (1 − u) t=1 F (h, Φ) + u t=0 F (h, Φ) + B Φ · IPM G p t=1 Φ , p t=0 Φ .</formula><p>We further have by Lemma A3 that:</p><formula xml:id="formula_53">F (h, Φ) = u t=1 F (h, Φ) + (1 − u) t=0 F (h, Φ). Therefore CF (h, Φ) + F (h, Φ) ≤ t=1 F (h, Φ) + t=0 F (h, Φ) + B Φ IPM G p t=1 Φ , p t=0 Φ .</formula><p>The upper bound is in terms of the standard generalization error on the treated and control distributions separately. Note that in some cases we might have very different sample sizes for treated and control, and that will show up in the finite sample bounds of these generalization errors.</p><p>We also note that the upper bound can be easily adapted to the case of the absolute loss PEHE |τ (x) − τ (x)|. In that case the upper bound in the Theorem will have a factor 1 instead of the 2 stated above, and the standard deviation σ 2 Y replaced by mean absolute deviation. The proof is straightforward where one simply applies the triangle inequality in inequality <ref type="formula" target="#formula_10">(14)</ref>.</p><p>We will now give specific upper bounds for the constant B Φ in Theorem 1, using two function families G in the IPM: the family of 1-Lipschitz functions, and the family of 1-norm reproducing kernel Hilbert space functions. Each one will have different assumptions about the distribution p(x, t, Y 0 , Y 1 ) and about the representation Φ and hypothesis h.</p><p>A.3. The family of 1-Lipschitz functions</p><formula xml:id="formula_54">For S ⊂ R d , a function f : S → R has Lipschitz constant K if for all x, y ∈ S, |f (x) − f (y)| ≤ K x − y .</formula><p>If f is differentiable, then a sufficient condition for K-Lipschitz constant is if ∂f ∂s ≤ K for all s ∈ S. For simplicity's sake we assume throughout this subsection that the true labeling functions the densities p(Y t |x) and the loss L are differentiable. However, this assumption could be relaxed to a mere Lipschitzness assumption. Assumption A2. There exists a constant K &gt; 0 such that for all x ∈ X , t ∈ {0, 1}, p(Yt|x) ∂x ≤ K.</p><p>Assumption A2 entails that each of the potential outcomes change smoothly as a function of the covariates (context) x. Assumption A3. The loss function L is differentiable, and there exists a constant K L &gt; 0 such that dL(y1,y2) dyi ≤ K L for i = 1, 2. Additionally, there exists a constant M such that for all y 2 ∈ Y, M ≥ Y L(y 1 , y 2 ) dy 1 .</p><p>Assuming Y is compact, loss functions which obey Assumption A3 include the log-loss, hinge-loss, absolute loss, and the squared loss.</p><p>When we let G in Definition A7 be the family of 1-Lipschitz functions, we obtain the so-called 1-Wasserstein distance between distributions, which we denote Wass(·, ·). It is well known that Wass(·, ·) is indeed a metric between distributions <ref type="bibr">(Villani, 2008)</ref>. Definition A12. Let <ref type="bibr">∂Φ(x)</ref> ∂x be the Jacobian matrix of Φ at point x, i.e. the matrix of the partial derivatives of Φ. Let σ max (A) and σ min (A) denote respectively the largest and smallest singular values of a matrix A. Define ρ(Φ) = sup x∈X σ max</p><formula xml:id="formula_55">∂Φ(x) ∂x /σ min ∂Φ(x) ∂x .</formula><p>It is an immediate result that ρ(Φ) ≥ 1. Definition A13. We will call a representation function Φ :</p><formula xml:id="formula_56">X → R Jacobian-normalized if sup x∈X σ max ∂Φ(x) ∂x = 1.</formula><p>Note that any non-constant representation function Φ can be Jacobian-normalized by a simple scalar multiplication.</p><p>Lemma A6. Assume that Φ is a Jacobian-normalized representation, and let Ψ be its inverse. For t = 0, 1, the Lipschitz constant of p(Y t |Ψ(r)) is bounded by ρ(Φ)K, where K is from Assumption A2, and ρ(Φ) as in Definition A12.</p><p>Proof. Let Ψ : R → X be the inverse of Φ, which exists by the assumption that Φ is one-to-one. Let <ref type="bibr">∂Φ(x)</ref> ∂x be the Jacobian matrix of Φ evaluated at x, and similarly let ∂Ψ(r) ∂r be the Jacobian matrix of Ψ evaluated at r. Note that <ref type="bibr">∂Ψ(r)</ref> ∂r · ∂Φ(x) ∂x = I for r = Φ(x), since Ψ•Φ is the identity function on X . Therefore for any r ∈ R and x = Ψ(r):</p><formula xml:id="formula_57">σ max ∂Ψ(r) ∂r = 1 σ min ∂Φ(x) ∂x ,<label>(17)</label></formula><p>where σ max (A) and σ min (A) are respectively the largest and smallest singular values of the matrix A, i.e. σ max (A) is the spectral norm of A.</p><p>For x = Ψ(r) and t ∈ {0, 1}, we have by the chain rule:</p><formula xml:id="formula_58">∂p(Y t |Ψ(r)) ∂r = ∂p(Y t |Ψ(r)) ∂Ψ(r) ∂Ψ(r) ∂r ≤ (18) ∂Ψ(r) ∂r ∂p(Y t |Ψ(r)) ∂Ψ(r) = (19) 1 σ min ∂Φ(x) ∂x ∂p(Y t |x) ∂x ≤ (20) K σ min ∂Φ(x) ∂x ≤ ρ(Φ)K,<label>(21)</label></formula><p>where inequality <ref type="formula" target="#formula_10">(18)</ref> is by the matrix norm inequality, equality <ref type="formula" target="#formula_10">(19)</ref> is by <ref type="formula" target="#formula_10">(17)</ref>, inequality <ref type="formula">(20)</ref> is by assumption A2 on the norms of the gradient of p(Y t |x) w.r.t x , and inequality <ref type="formula" target="#formula_10">(21)</ref> is by Definition A12 of ρ(Φ), the assumption that Φ is Jacobian-normalized, and noting that singular values are necessarily non-negative.</p><p>Lemma A7. Under the conditions of Lemma A4, further assume that for t = 0, 1, p(Y t |x) has gradients bounded by K as in A2, that h has bounded gradient norm bK, that the loss L has bounded gradient norm K L , and that Φ is Jacobian-normalized. Then the Lipschitz constant of h,Φ (Ψ(r), t) is upper bounded by K L · K (M ρ(Φ) + b) for t = 0, 1.</p><p>Proof. Using the chain rule, we have that:</p><formula xml:id="formula_59">∂ h,Φ (Ψ(r), t) ∂r = ∂ ∂r Y L(Y t , h(r, t))p(Y t |r)dY t = Y ∂ ∂r [L(Y t , h(r, t))p(Y t |r)] dY t = Y p(Y t |r) ∂ ∂r L(Y t , h(r, t))+L(Y t , h(r, t)) ∂ ∂r p(Y t |r)dY t ≤ Y p(Y t |r) ∂ ∂r L(Y t , h(r, t)) dY t + Y L(Y t , h(r, t)) ∂ ∂r p(Y t |r) dY t ≤ (22) Y p(Y t |r) ∂L(Y t , h(r, t)) ∂h(r, t) ∂h(r, t) ∂r dY t + Y L(Y t , h(r, t)) ∂ ∂r p(Y t |r) dY t ≤ (23) Y p(Y t |r)K L · b · K + M · ρ(Φ) · K,<label>(24)</label></formula><p>where inequality 22 is due to Assumption A3 and inequality 23 is due to Lemma A6.</p><p>Lemma A8. Let u = p(t = 1) be the marginal probability of treatment, and assume 0 &lt; u &lt; 1. Let Φ : X → R be a one-to-one, Jacobian-normalized representation function. Let K be the Lipschitz constant of the functions p(Y t |x) on X . Let K L be the Lipschitz constant of the loss function L, and M be as in Assumption A3. Let h : R × {0, 1} → R be an hypothesis with Lipschitz constant bK. Then:</p><formula xml:id="formula_60">CF (h, Φ) ≤ (1 − u) t=1 F (h, Φ) + u t=0 F (h, Φ)+ 2 (M ρ(Φ) + b) · K · K L · Wass(p t=1 Φ , p t=0 Φ ).<label>(25)</label></formula><p>Proof. We will apply Lemma A4 with G = {g : R → R s.t. f is 1-Lipschitz}. By Lemma A7, we have that for B Φ = (M ρ(Φ) + b) · K · K L , the function 1 BΦ h,Φ (Ψ(r), t) ∈ G. Inequality (25) then holds as a special case of Lemma A4.</p><p>Theorem 2. Under the assumptions of Lemma A8, using the squared loss for F , we have:</p><formula xml:id="formula_61">PEHE (h, Φ) ≤ 2 t=0 F (h, Φ) + 2 t=1 F (h, Φ) − 4σ 2 Y + 2 (M ρ(Φ) + b) · K · K L · Wass(p t=1 Φ , p t=0 Φ ).</formula><p>Proof. Plug in the upper bound of Lemma A8 into the upper bound of Theorem 1.</p><p>We examine the constant (M ρ(Φ) + b)·K ·K L in Theorem A8. K, the Lipschitz constant of m 0 and m 1 , is not under our control and measures an aspect of the complexity of the true underlying functions we wish to approximate. The terms K L and M depend on our choice of loss function and the size of the space Y. The term b comes from our assumption that the hypothesis h has norm bK. Note that smaller b, while reducing the bound, might force the factual loss term F (h, Φ) to be larger since a small b implies a less flexible h. Finally, consider the term ρ(Φ). The assumption that Φ is normalized is rather natural, as we do not expect a certain scale from a representation. Furthermore, below we show that in fact the Wasserstein distance is positively homogeneous with respect to the representation Φ. Therefore, in Lemma A8, we can indeed assume that Φ is normalized. The specific choice of Jacobian-normalized scaling yields what is in our opinion a more interpretable result in terms of the inverse condition number ρ(Φ). For twice-differentiable Φ, ρ(Φ) is minimized if and only if Φ is a linear orthogonal transformation (mat). Lemma A9. The Wasserstein distance is positive homogeneous for scalar transformations of the underlying space. Let p, q be probability density functions defined over X . For α &gt; 0 and the mapping Φ(x) = αx, let p α and q α be the distributions on αX induced by Φ. Then:</p><formula xml:id="formula_62">Wass (p α , q α ) = αWass (p, q) .</formula><p>Proof. Following <ref type="bibr">(Villani, 2008;</ref><ref type="bibr" target="#b36">Kuang &amp; Tabak, 2016)</ref>, we use another characterization of the Wasserstein distance. Let M p,q be the set of mass preserving maps from X to itself which map the distribution p to the distribution q. That is, M p,q = {M : X → X s.t. q(M (S)) = p(S) for all measurable bounded S ⊂ X }. We then have that:</p><formula xml:id="formula_63">Wass(p, q) = inf M ∈Mp,q X M (x) − x p(x) dx. (26)</formula><p>It is known that the infimum in (26) is actually achievable <ref type="bibr">(Villani, 2008, Theorem 5.2)</ref>. Denote by M * : X → X the map achieving the infimum for Wass(p, q) . Define</p><formula xml:id="formula_64">M * α : αX → αX , by M * α (x ) = αM * ( x α )</formula><p>, where x = αx. M * α maps p α to q α , and we have that M * α (x ) − x = α M * (x)−x . Therefore M * α achieves the infimum for the pair (p α , q α ), and we have that Wass (p α , q α ) = αWass (p, q).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. Functions in the unit ball of a RKHS</head><p>Let H x , H r be a reproducing kernel Hilbert space, with corresponding kernels k x (·, ·), k r (·, ·). We have for all x ∈ X that k x (·, x) is its Hilbert space mapping, and similarly k r (·, r) for all r ∈ R.</p><p>Recall that the major condition in Lemma A4 is that 1 BΦ h,Φ (Ψ(r), t) ∈ G. The function space G we use here is G = {g ∈ H r s.t. g Hr ≤ 1}. squared loss. Then:</p><formula xml:id="formula_65">CF (h, Φ) ≤ (1 − u) t=1 F (h, Φ) + u t=0 F (h, Φ)+ 2 K 2 Φ (K 2 + M 2 ) + b 2 · MMD(p t=1 Φ , p t=0 Φ ),<label>(34)</label></formula><p>where CF and F use the squared loss.</p><p>Proof. We will apply Lemma A4 with G = f ∈ H r ⊗ H r s.t. f Hr⊗Hr ≤ 1.</p><p>By Lemma A10, we have that for B Φ = 2 K 2 Φ (K 2 + M 2 ) + b 2 and L being the squared loss, 1 BΦ h,Φ (Ψ(r), t) ∈ G. Inequality (34) then holds as a special case of Lemma A4.</p><p>Theorem 3. Under the assumptions of Lemma A11, using the squared loss for F , we have:</p><formula xml:id="formula_66">PEHE (h, Φ) ≤ 2 t=0 F (h, Φ) + 2 t=1 F (h, Φ) − 4σ 2 Y + 4 K 2 Φ (K 2 + M 2 ) + b 2 · MMD(p t=1 Φ , p t=0 Φ ).</formula><p>Proof. Plug in the upper bound of Lemma A11 into the upper bound of Theorem 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Algorithmic details</head><p>We give details about the algorithms used in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Minimizing the Wasserstein distance</head><p>In general, computing (and minimizing) the Wasserstein distance involves solving a linear program, which may be prohibitively expensive for many practical applications. <ref type="bibr">Cuturi (2013)</ref> showed that an approximation based on entropic regularization can be obtained through the Sinkhorn-Knopp matrix scaling algorithm, at orders of magnitude faster speed. Dubbed Sinkhorn distances, the approximation is computed using a fixed-point iteration involving repeated multiplication with a kernel matrix K. We can use the algorithm of Cuturi (2013) in our framework. See Algorithm 2 for an overview of how to compute the gradient g 1 in Algorithm 1. When computing g 1 , disregarding the gradient ∇ W T * amounts to minimizing an upper bound on the Sinkhorn transport. More advanced ideas for stochastic optimization of this distance have recently proposed by <ref type="bibr" target="#b3">Aude et al. (2016)</ref>, and might be used in future work.</p><p>While our framework is agnostic to the parameterization of Φ, our experiments focus on the case where Φ is a neural network. For convenience of implementation, we may represent the fixed-point iterations of the Sinkhorn algorithm Algorithm 2 Computing the stochastic gradient of the Wasserstein distance 1: Input: Factual (x 1 , t 1 , y 1 ), . . . , (x n , t n , y n ), representation network Φ W with current weights by W 2: Randomly sample a mini-batch with m treated and m control units (x i1 , 0, y i1 ), . . . , (x im , 0, y im ), (x im+1 , 1, y im+1 ), . . . , (x i2m , 1, y i2m ) 3: Calculate the m × m pairwise distance matrix between all treatment and control pairs M (Φ W ): M kl (Φ) = Φ W (x i k ) − Φ W (x i m+l ) 4: Calculate the approximate optimal transport matrix T * using Algorithm 3 of <ref type="bibr" target="#b20">Cuturi &amp; Doucet (2014)</ref>, with input M (Φ W ) 5: Calculate the gradient:</p><formula xml:id="formula_67">g 1 = ∇ W T * , M (Φ W )</formula><p>as a recurrent neural network, where the states u t evolve according to u t+1 = n t ./(n c K(1./(u t K) )) .</p><p>Here, K is a kernel matrix corresponding to a metric such as the euclidean distance, K ij = e −λ Φ(xi)−Φ(xj ) 2 , and n c , n t are the sizes of the control and treatment groups. In this way, we can minimize our entire objective with most of the frameworks commonly used for training neural networks, out of the box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Minimizing the maximum mean discrepancy</head><p>The MMD of treatment populations in the representation Φ, for a kernel k(·, ·) can be written as,</p><formula xml:id="formula_68">MMD k ({Φ W (x ij )} m j=1 , {Φ W (x i k )} m k=m+1 ) = (35) 1 m(m − 1) m j=1 m k=1,k =j k(Φ W (x ij ), Φ W (x i k )) (36) + 2 mm m j=1 m+m k=m k(Φ W (x ij ), Φ W (x i k )) (37) + 1 m (1 − m ) m j=1 m k=m,k =j k(Φ W (x ij ), Φ W (x i k )) (38)</formula><p>The linear maximum-mean discrepancy can be written as a distance between means. In the notation of Algorithm 1,</p><formula xml:id="formula_69">MMD = 2 1 m m j=1 Φ W (x ij ) − 1 m m k=m+1 Φ W (x i k ) 2 Let f (W) = 1 m m j=1 Φ W (x ij ) − 1 m m+m k=m+1 Φ W (x i k )</formula><p>Estimating individual treatment effect: generalization bounds and algorithms Then the gradient of the MMD with respect to W is,</p><formula xml:id="formula_70">g 1 = 2 df (W) dW f (W) f (W) 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Experimental details C.1. Hyperparameter selection</head><p>Standard methods for hyperparameter selection, such as cross-validation, are not generally applicable for estimating the PEHE loss since only one potential outcome is observed (unless the outcome is simulated). For realworld data, we may use the observed outcome y j(i) of the nearest neighbor j(i) to i in the opposite treatment group, t j(i) = 1 − t i as surrogate for the counterfactual outcome. We use this to define a nearestneighbor approximation of the PEHE loss, PEHEnn (f ) = 1 n n i=1 (1 − 2t i )(y j(i) − y i ) − (f (x i , 1) − f (x i , 0)) 2 .</p><p>On IHDP, we use the objective value on the validation set for early stopping in CFR, and PEHEnn (f ) for hyperparameter selection. On the Jobs dataset, we use the policy risk on the validation set.</p><p>See <ref type="table" target="#tab_3">Table 2</ref> for a description of hyperparameters and search ranges. <ref type="figure">Figure 4</ref> show the representations learned by our CFR algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Learned representations</head><p>C.3. Absolute error for increasingly imbalanced data <ref type="figure">Figure 5</ref> shows the results of the same experiment as <ref type="figure" target="#fig_3">Figure  2</ref> of the main paper, but in absolute terms.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>David et al. (2007); Mansour et al. (2009); Ben-David et al. (2010);</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>strong ignorability). The treated and control distributions are the distribution of the features x conditioned on treatment: p t=1 (x) := p(x|t = 1), and p t=0 (x) := p(x|t = 0), respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Out-of-sample ITE error versus IPM regularization for CFR Wass, relative to the error at α = 0, on 500 realizations of IHDP, with high (q = 1), medium and low (artificial) imbalance between control and treated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Policy risk on Jobs as a function of treatment inclusion rate. Lower is better. Subjects are included in treatment in order of their estimated treatment effect given by the various methods. CFR Wass is similar to CFR and is omitted to avoid clutter. treatment effect on the treated by ATT = |T | −1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>For a one-to-one Φ, the distribution p Φ over R × {0, 1} can be obtained by the standard change of variables formula, using the determinant of the Jacobian of Ψ(r). See<ref type="bibr" target="#b12">(Ben- Israel, 1999)</ref> for the case of a mapping Φ between spaces of different dimensions. Lemma A2. For all r ∈ R, t ∈ {0, 1}:p Φ (t|r) = p(t|Ψ(r)) p(Y t |r) = p(Y t |Ψ(r)). Notation: p(x, t): distribution on X × {0, 1} u = p(t = 1): the marginal probability of treatment. p t=1 (x) = p(x|t = 1): treated distribution. p t=0 (x) = p(x|t = 0): control distribution. Φ: representation function mapping from X to R. Ψ: the inverse function of Φ, mapping from R to X . p Φ (r, t): the distribution induced by Φ on R × {0, 1}. p t=1 Φ (r), p t=0 Φ(r): treated and control distributions induced by Φ on R. L(·, ·): loss function, from Y × Y to R + . h,Φ (x, t): the expected loss of h(Φ(x), t) for the unit x and treatment t. F (h, Φ), CF (h, Φ): expected factual and counterfactual loss of h(Φ(x), t). τ (x) := E [Y 1 − Y 0 |x], the expected treatment effect for unit x. PEHE (f ): expected error in estimating the individual treatment effect of a function f (x, t). IPM G (p, q): the integral probability metric distance induced by function family G between distributions p and q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>t-SNE visualizations of the balanced representations of IHDP learned by our algorithms CFR, CFR MMD and CFR Wass. We note that the nearest-neighbor like quality of the Wasserstein distance results in a strip-like representation, whereas the linear MMD results in a ball-like shape in regions where overlap is small. Out-of-sample error in estimated ITE, as a function of IPM regularization parameter for CFR Wass, on 500 realizations of IHDP, with high (q = 1), medium and low (artificial) imbalance between control and treated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on IHDP (left) and Jobs (right). MMD is squared linear MMD. Lower is better. ± .3 .73 ± .04 .22 ± .0 .01 ± .00 OLS/LR-2 2.4 ± .1 .14 ± .01 .21 ± .0 .01 ± .01 BLR 5.8 ± .3 .72 ± .04 .22 ± .0 .01 ± .01 k-NN 2.1 ± .1 .14 ± .01 .02 ± .0 .21 ± .01 TMLE 5.0 ± .2 .30 ± .01 .22 ± .0 .02 ± .01 BART 2.1 ± .1 .23 ± .01 .23 ± .0 .02 ± .00 RAND.FOR. 4.2 ± .2 .73 ± .05 .23 ± .0 .03 ± .01 CAUS.FOR. 3.8 ± .2 .18 ± .01 .19 ± .0 .03 ± .01 BNN 2.2 ± .1 .37 ± .03 .20 ± .0 .04 ± .01 TARNET .88 ± .0 .26 ± .01 .17 ± .0 .05 ± .02 CFR MMD .73 ± .0 .30 ± .01 .18 ± .0 .04 ± .01 CFR WASS .71 ± .0 .25 ± .01 .17 ± .0 .04 ± .01 ± .2 .79 ± .05 .26 ± .0 .13 ± .05 BART 2.3 ± .1 .34 ± .02 .25 ± .0 .08 ± .03 RAND.FOR. 6.6 ± .3 .96 ± .06 .28 ± .0 .09 ± .04 CAUS.FOR. 3.8 ± .2 .40 ± .03 .20 ± .0 .07 ± .03 BNN 2.1 ± .1 .42 ± .03 .24 ± .0 .09 ± .04 TARNET .95 ± .0 .28 ± .01 .21 ± .0 .11 ± .04 CFR MMD .78 ± .0 .31 ± .01 .21 ± .0 .08 ± .03 CFR WASS .76 ± .0 .27 ± .01 .21 ± .0 .09 ± .03</figDesc><table><row><cell>Within-sample</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>√</cell><cell>IHDP PEHE</cell><cell>ATE</cell><cell>RPOL</cell><cell>JOBS</cell><cell>ATT</cell></row><row><cell cols="2">OLS/LR-1 5.8 Out-of-sample</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>√</cell><cell>IHDP PEHE</cell><cell>ATE</cell><cell>RPOL</cell><cell>JOBS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>03</cell></row><row><cell>k-NN</cell><cell>4.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>ATT OLS/LR-1 5.8 ± .3 .94 ± .06 .23 ± .0 .08 ± .04 OLS/LR-2 2.5 ± .1 .31 ± .02 .24 ± .0 .08 ± .03 BLR 5.8 ± .3 .93 ± .05 .25 ± .0 .08 ± .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Sun, Baochen, Feng, Jiashi, and Saenko, Kate. Return of frustratingly easy domain adaptation. In Thirtieth AAAI Conference on Artificial Intelligence, 2016. Swaminathan, Adith and Joachims, Thorsten. Batch learning from logged bandit feedback through counterfactual risk minimization. Journal of Machine Learning Research, 16:1731-1755, 2015.</figDesc><table><row><cell cols="3">Taddy, Matt, Gardner, Matt, Chen, Liyun, and Draper, David. A</cell></row><row><cell cols="3">nonparametric bayesian analysis of heterogenous treatment ef-</cell></row><row><cell cols="3">fects in digital experimentation. Journal of Business &amp; Eco-</cell></row><row><cell cols="2">nomic Statistics, 34(4):661-672, 2016.</cell></row><row><cell cols="3">Triantafillou, Sofia and Tsamardinos, Ioannis. Constraint-based</cell></row><row><cell cols="3">causal discovery from multiple interventions over overlapping</cell></row><row><cell cols="3">variable sets. Journal of Machine Learning Research, 16:</cell></row><row><cell cols="2">2147-2205, 2015.</cell></row><row><cell cols="3">Villani, Cédric. Optimal transport: old and new, volume 338.</cell></row><row><cell cols="3">Springer Science &amp; Business Media, 2008.</cell></row><row><cell cols="2">Wager, Stefan and Athey, Susan.</cell><cell>Estimation and in-</cell></row><row><cell cols="3">ference of heterogeneous treatment effects using random</cell></row><row><cell>forests.</cell><cell cols="2">arXiv preprint arXiv:1510.04342. https://</cell></row><row><cell cols="3">github.com/susanathey/causalTree, 2015.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Hyperparameters and ranges. Num. of representation layers {1, 2, 3} Num. of hypothesis layers {1, 2, 3} Dim. of representation layers {20, 50, 100, 200} Dim. of hypothesis layers {20, 50, 100, 200}</figDesc><table><row><cell>Parameter</cell><cell>Range</cell></row><row><cell>Imbalance parameter, α</cell><cell>{10 k/2 } 6 k=−10</cell></row><row><cell>Batch size</cell><cell>{100, 200, 500, 700}</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We wish to thank Aahlad Manas for his assistance with the experiments. We also thank Jennifer Hill, Marco Cuturi, Esteban Tabak and Sanjong Misra for fruitful conversations, and Stefan Wager for his help with the code for Causal Forests. DS and US were supported by NSF CA-REER award #1350965.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>we will make the following two assumptions:</p><p>∈ H x such that m t (x) = f Y t , k x (x, ·) Hx , i.e. the mean potential outcome functions m 0 , m 1 are in H x . Further assume that</p><p>Let Φ : X → Y be an invertible representation function, and let Ψ be its inverse. We assume there exists a bounded linear operator Γ Φ :</p><p>We further assume that the Hilbert-Schmidt norm (operator norm)</p><p>The two assumptions above amount to assuming that Φ can be represented as one-to-one linear map between the two Hilbert spaces H x and H r .</p><p>Under Assumptions A4 and A6 about m 0 , m 1 , and Φ, we <ref type="bibr" target="#b29">Grunewalder et al., 2013)</ref>.</p><p>Lemma A10. Let h : R × {0, 1} → R be an hypothesis, and assume that there exist f h t ∈ H r such that h(r, t) = f h t , k r (r, ·) Hr , and such that f h t Hr ≤ b. Under Assumption A4 about m 0 , m 1 , we have that h,</p><p>into a noise and mean fitting term, using r = Φ(x):</p><p>where equality <ref type="formula">(27)</ref> is by Definition A14 of η, and because</p><p>Moving to R, recall that r = Φ(x), x = Ψ(r). By linearity of the Hilbert space, we have that</p><p>k r (r, ·) Hr . By a well known result <ref type="bibr">(Steinwart &amp; Christmann, 2008, Theorem 7.25)</ref>, the product (Y t (Ψ(r))−h(r, t))·(Y t (Ψ(r))−h(r, t)) lies in the tensor product space H r ⊗ H r , and is equal to</p><p>Hr . This is the general Hilbert space version of the fact that for a vector w ∈ R d one has that ww F = w 2 2 , where · F is the matrix Frobenius norm, and · 2 2 is the square of the standard Euclidean norm. We therefore have a similar result for η 2 Yt , using Assumption A5: η 2</p><p>Hr . Overall this leads us to conclude, using Equation <ref type="formula">(28)</ref> that h,Φ (Ψ(r), t) ∈ H r ⊗ H r . Now we have, using <ref type="formula">(28)</ref>:</p><p>Inequality (29) is by the norms given above and the triangle inequality. Inequality (30) is because for any Hilbert space H, a − b 2 H ≤ 2 a 2 H + 2 b 2 H . Inequality (31) is by the definition of the operator norm. Equality (32) is because the norm of the adjoint operator is equal to the norm of the original operator, where we abused the notation · HS to mean both the norm of operators from H x to H r and vice-versa. Finally, inequality (33) is by Assumptions A4, A5 and A6, and by the Lemma's premise on the norm of f h T . Lemma A11. Let u = p(t = 1) be the marginal probability of treatment, and assume 0 &lt; u &lt; 1. Assume the distribution of Y t conditioned on x follows Assumptions A5 with constant M . Let Φ : X → R be a one-to-one representation function which obeys Assumption A6 with corresponding operator Γ Φ with operator norm K Φ . Let the functions Y 0 , Y 1 obey Assumption A4, with bounded Hilbert space norm K . Let h : R × {0, 1} → R be an hypothesis, and assume that there exist f h t ∈ H r such that h(r, t) = f h t , k r (r, ·) Hr , such that f h t Hr ≤ b. Assume that F and CF are defined with respect to L being the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="https://mathoverflow.net/questions/228964/functions-with-orthogonal-jacobian.Ac-cessed" />
		<title level="m">MathOverflow: functions with orthogonal Jacobian</title>
		<imprint>
			<date type="published" when="2016-05-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recursive partitioning for heterogeneous causal effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><surname>Imbens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="7353" to="7360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient inference of average treatment effects in high dimensions via approximate residual balancing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wager</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07125</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevay</forename><surname>Aude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08527</idno>
		<title level="m">Stochastic optimization for large-scale optimal transport</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An introduction to propensity score methods for reducing the effects of confounding in observational studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">C</forename><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate behavioral research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="399" to="424" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bounds on treatment effects from studies with imperfect compliance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Balke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">439</biblScope>
			<biblScope unit="page" from="1171" to="1176" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Controlling selection bias in causal inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Causal inference and the data-fusion problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="7345" to="7352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving quantitative studies of international conflict: A conjecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathaniel</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Langche</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Political Science Review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="21" to="35" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Christian. Inference on treatment effects after selection among highdimensional controls</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Belloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Review of Economic Studies</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="608" to="650" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fernando</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">137</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Vaughan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The change-of-variables formula using matrix volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adi</forename><surname>Ben-Israel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="300" to="312" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Random forests. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bounds on direct effects in the presence of confounded intermediate variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuroki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manabu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="695" to="701" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Double machine learning for treatment and causal parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chetverikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Demirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duflo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Esther</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Christian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00060</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">BayesTree: Bayesian Additive Regression Trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Mcculloch</surname></persName>
		</author>
		<ptr target="https://cran.r-project.org/web/packages/BayesTree" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><forename type="middle">A</forename><surname>Chipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">E</forename><surname>Mcculloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bart</surname></persName>
		</author>
		<title level="m">Bayesian additive regression trees. The Annals of Applied Statistics</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="266" to="298" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimating individual treatment effect: generalization bounds and algorithms Cuturi, Marco. Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">519</biblScope>
			<biblScope unit="page" from="2292" to="2300" />
		</imprint>
	</monogr>
	<note>Domain adaptation and sample bias correction theory and algorithm for regression</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast computation of Wasserstein barycenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud</forename><surname>Doucet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 31st International Conference on Machine Learning</title>
		<meeting>The 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="685" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename></persName>
		</author>
		<idno type="arXiv">arXiv:0907.1815</idno>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Propensity score-matching methods for nonexperimental causal studies. Review of Economics and statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><forename type="middle">H</forename><surname>Dehejia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadek</forename><surname>Wahba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">NPCI: Non-parametrics for Causal Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dorie</surname></persName>
		</author>
		<ptr target="https://github.com/vdorie/npci" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Doubly robust estimation of causal effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jonsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Westreich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Wiesen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stürmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Til</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Brookhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie</forename><surname>Davidian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of epidemiology</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="761" to="767" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Evgeniya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>François</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Victor</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v17/15-239.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Covariate shift by kernel mean matching. Dataset shift in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiayuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmittfull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A kernel twosample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno>1532-4435</idno>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">tmle: An r package for targeted maximum likelihood estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Der Laan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Smooth operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Grunewalder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretton</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML-13)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML-13)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1184" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Taddy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.09596</idno>
		<title level="m">Counterfactual prediction with deep instrumental variables networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric modeling for causal inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dominik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Recent developments in the econometrics of program evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guido</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Wooldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of economic literature</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="86" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning representations for counterfactual inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredrik</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>the 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Preconditioning of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Tabak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Evaluating the econometric evaluations of training programs with experimental data. The American economic review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">J</forename><surname>Lalonde</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="page" from="604" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Predicting causal effects in large-scale systems from observational data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marloes</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Diego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="247" to="248" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Distinguishing cause from effect using observational data: methods and benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joris</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dominik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Zscheischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">32</biblScope>
			<biblScope unit="page" from="1" to="102" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Integral probability metrics and their generating classes of functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Probability</title>
		<imprint>
			<biblScope unit="page" from="429" to="443" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinno</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jialin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>Neural Networks</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Causality</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Detecting latent heterogeneity. Sociological Methods &amp; Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">0049124115600597</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Combining observational and experimental data to find heterogeneous treatment effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Peysakhovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akos</forename><surname>Lada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02385</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
