<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention-Based Deep Neural Networks for Detection of Cancerous and Precancerous Esophagus Tissue on Histopathological Slides</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>MS</roleName><forename type="first">Naofumi</forename><surname>Tomita</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Data Science</orgName>
								<orgName type="department" key="dep2">Geisel School of Medicine at Dartmouth</orgName>
								<address>
									<postCode>03755</postCode>
									<settlement>Hanover</settlement>
									<region>NH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>PhD</roleName><forename type="first">Behnaz</forename><surname>Abdollahi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Data Science</orgName>
								<orgName type="department" key="dep2">Geisel School of Medicine at Dartmouth</orgName>
								<address>
									<postCode>03755</postCode>
									<settlement>Hanover</settlement>
									<region>NH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>BS</roleName><forename type="first">Jason</forename><surname>Wei</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Data Science</orgName>
								<orgName type="department" key="dep2">Geisel School of Medicine at Dartmouth</orgName>
								<address>
									<postCode>03755</postCode>
									<settlement>Hanover</settlement>
									<region>NH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Dartmouth College</orgName>
								<address>
									<postCode>03755</postCode>
									<settlement>Hanover</settlement>
									<region>NH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>MD</roleName><forename type="first">Bing</forename><surname>Ren</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Pathology and Laboratory Medicine</orgName>
								<orgName type="institution">Dartmouth-Hitchcock Medical Center</orgName>
								<address>
									<postCode>03756</postCode>
									<settlement>Lebanon</settlement>
									<region>NH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>MD</roleName><forename type="first">Arief</forename><surname>Suriawinata</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Pathology and Laboratory Medicine</orgName>
								<orgName type="institution">Dartmouth-Hitchcock Medical Center</orgName>
								<address>
									<postCode>03756</postCode>
									<settlement>Lebanon</settlement>
									<region>NH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>PhD</roleName><forename type="first">Saeed</forename><surname>Hassanpour</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biomedical Data Science</orgName>
								<orgName type="department" key="dep2">Geisel School of Medicine at Dartmouth</orgName>
								<address>
									<postCode>03755</postCode>
									<settlement>Hanover</settlement>
									<region>NH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Dartmouth College</orgName>
								<address>
									<postCode>03755</postCode>
									<settlement>Hanover</settlement>
									<region>NH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Epidemiology</orgName>
								<orgName type="department" key="dep2">Geisel School of Medicine at Dartmouth</orgName>
								<address>
									<postCode>03755</postCode>
									<settlement>Hanover</settlement>
									<region>NH</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention-Based Deep Neural Networks for Detection of Cancerous and Precancerous Esophagus Tissue on Histopathological Slides</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Importance:</head><p>Deep learning-based methods, such as the sliding window approach for cropped-image classification and heuristic aggregation for whole-slide inference, for analyzing histological patterns in high-resolution microscopy images have shown promising results. These approaches, however, require a laborious annotation process and are fragmented.</p><p>Objective: To evaluate a novel deep learning method that uses tissue-level annotations for high-resolution histological image analysis for Barrett esophagus (BE) and esophageal adenocarcinoma detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Design, Setting, and Participants</head><p>: This diagnostic study collected deidentified highresolution histological images (N = 379) for training a new model composed of a convolutional neural network and a grid-based attention network. Histological images of patients who underwent endoscopic esophagus and gastroesophageal junction mucosal biopsy between January 1, 2016, and December 31, 2018, at Dartmouth-Hitchcock Medical Center (Lebanon, New Hampshire) were collected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main Outcomes and Measures:</head><p>The model was evaluated on an independent testing set of 123 histological images with 4 classes: normal, BE-no-dysplasia, BE-with-dysplasia, and adenocarcinoma. Performance of this model was measured and compared with that of the current state-of-the-art sliding window approach using the following standard machine learning metrics: accuracy, recall, precision, and F1 score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>Of the independent testing set of 123 histological images, 30 (24.4%) were in the BE-no-dysplasia class, 14 (11.4%) in the BE-with-dysplasia class, 21 (17.1%) in the adenocarcinoma class, and 58 (47.2%) in the normal class. Classification accuracies of the proposed model were 0.85 (95% CI, 0.81-0.90) for the BE-no-dysplasia class, 0.89 (95% CI, 0.84-0.92) for the BE-with-dysplasia class, and 0.88 (95% CI, 0.84-0.92) for the adenocarcinoma class. The proposed model achieved a mean accuracy of 0.83 (95% CI, 0.80-0.86) and marginally outperformed the sliding window approach on the same testing set. The F1 scores of the attention-based model were at least 8% higher for each class compared with the sliding window approach: 0.68 (95% CI, 0.61-0.75) vs 0.61 (95% CI, 0.53-0.68) for the normal class, 0.72 (95% CI, 0.63-0.80) vs 0.58 (95% CI, 0.45-0.69) for the BE-no-dysplasia class, 0.30 (95% CI, 0.11-0.48) vs 0.22 (95% CI, 0.11-0.33) for the BE-with-dysplasia class, and 0.67 (95% CI, 0.54-0.77) vs 0.58 (95% CI, 0.44-0.70) for the adenocarcinoma class.</p><p>However, this outperformance was not statistically significant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions and Relevance</head><p>: Results of this study suggest that the proposed attention-based deep neural network framework for BE and esophageal adenocarcinoma detection is important because it is based solely on tissue-level annotations, unlike existing methods that are based on regions of interest. This new model is expected to open avenues for applying deep learning to digital pathology.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BACKGROUND Esophageal Cancer</head><p>Barrett's esophagus (BE) is a transformation of the normal squamous epithelium of the esophagus into metaplastic columnar epithelium. <ref type="bibr">1</ref> BE is important because it predisposes patients to the increased risk of adenocarcinoma of the esophagus and gastroesophageal junction. <ref type="bibr">2,</ref><ref type="bibr">3</ref> Compared to the general population, patients with BE have a 30 to 125 times higher risk of cancer. <ref type="bibr">4</ref> The average 5-year survival rate for esophageal adenocarcinoma (EAC) in the U.S. is less than 15%. <ref type="bibr">5</ref> Furthermore, the incidence of EAC increased dramatically in the U.S. over three decades. <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> Histologic diagnosis of BE requires the identification of metaplastic columnar epithelium with goblet cells (i.e., intestinal metaplasia). <ref type="bibr" target="#b10">11</ref> Evaluating the development of the premalignancy and malignancy in BE shows a moderate interobserver variability, with an average kappa coefficient of less than 0.50 even among subspecialized gastrointestinal pathologists. <ref type="bibr" target="#b11">12</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep Learning for Pathology Image Analysis</head><p>In the field of digital pathology, tissue slides are scanned as high-resolution images, which can have sizes up to 10,000×10,000 pixels. This high resolution is necessary because each slide contains thousands of cells, for which the cellular structures must be visible in order to identify regions of the tissue with diseases or lesions. However, the size of lesions is often relatively small, typically around 100×100 pixels, as most of the tissue areas in a given slide are normal. Therefore, the decisive regions of interest (ROIs) containing lesions usually comprise much less than one percent of the entire scanned tissue area. Even for highly trained pathologists, localizing these lesions for the classification of the whole slide is time consuming and often error-prone.</p><p>In recent years, deep learning has made considerable advances in classifying microscopy images. The most common approach in this domain involves a sliding window aproach for cropped-image classification, followed by statistical methods of aggregation for whole-slide inference. <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref> In this approach, pathologists annotate bounding boxes (i.e., <ref type="bibr">ROI)</ref> on whole slides in order to train a classifier on small cropped-images, typically of sizes in the range of 200×200 pixels to 500×500 pixels. For evaluating a whole slide, this cropped-image classifier is applied to extracted windows from the image, and then a heuristic, often developed in conjunction with a domain-expert pathologist, is used to determine how the distribution of cropped-image classification scores translates into a whole-slide diagnosis.</p><p>However, there are many limitations to this sliding window approach. The first is that since cropped-image classifiers are needed, all images in the training set must be annotated by pathologists with bounding boxes around each ROI. In addition, developing a heuristic for aggregating cropped-image classifications, which requires pathologists' insight, is dependent on the nature of the classification task and is not widely scalable. Finally, in the sliding window approach, cropped-images are classified independently of their neighbors and wholeslide classification does not consider correlations between neighboring windows. In order to overcome these limitations in this work, we propose to use an attention mechanism where the ROI is mined from high-resolution slides without explicit supervision.</p><p>Our work is inspired by attention models applied to regular image analysis tasks, especially image captioning. <ref type="bibr" target="#b23">24,</ref><ref type="bibr">25</ref> Attention mechanisms are described as a part of the prediction module that sequentially selects subsets of input to be processed. <ref type="bibr" target="#b23">24</ref> Although this definition is not applicable to non-sequential tasks, the essence of attention mechanisms can be restructred for neural networks to generate a dynamic representation of features through by weighting them to capture a holistic context of input. Unlike hard attention, where an ROI is selected by a stochastic sampling process, soft attention generates a non-discrete attention map that pays fractional attention to each region and produces better gradient flow, and thus is easier to optimize. Recent advancement of soft attention enabled end-to-end training on convolutional neural network (CNN) models. <ref type="bibr" target="#b24">[26]</ref><ref type="bibr" target="#b25">[27]</ref><ref type="bibr" target="#b26">[28]</ref><ref type="bibr" target="#b27">[29]</ref> For example, spatial transformer networks capture high-level information from inputs to derive affine transformation parameters, which are subsequently applied to spatial invariant input for a CNN. <ref type="bibr" target="#b27">29</ref> For semantic segmentation tasks, the attention mechanism is applied to learn multi-scale features. <ref type="bibr" target="#b24">26</ref> Residual attention networks use soft attention masks to extract features in different granularities. <ref type="bibr" target="#b26">28</ref> To analyze images in detail, a top-down recurrent attention CNN has been proposed. <ref type="bibr" target="#b25">27</ref> To put our work into perspective, our proposed model is based on the soft attention mechanism in feature space, but is designed for the classification of high-resolution images that are not typically encountered in the field of computer vision. There have been several applications of the attention mechanism in the medical domain, such as using soft attention to generate masks around lesion areas on CT images <ref type="bibr" target="#b28">30</ref> and employing recurrent attention models fused with reinforcement learning to locate lung nodules <ref type="bibr" target="#b29">31</ref> or enlarged hearts 32 in chest radiography images. In pathology, recorded navigation of pathologists has been used as attention maps to detect carcinoma. <ref type="bibr" target="#b31">33</ref> A soft attention approach has been deployed in two parallel networks for the classification of thorax disease. <ref type="bibr" target="#b28">30</ref> Although we draw inspiration from this work, our approach differs in that it provides a novel framework to directly reuse extracted features in a single attention network.</p><p>In this paper, we present a model that uses a convolutional attention-based mechanism to classify microscopy images. Our approach has three major advantages over the existing method. First, our model dynamically identifies ROIs in a high-resolution image and makes a whole-slide classification based on analyzing only these selected regions. This is analogous to how pathologists examine slides under the microscope. Second, the proposed model is Lastly, the model architecture is flexible with regard to input size for images. Inspired by fully convolutional network philosophy, <ref type="bibr" target="#b32">34</ref> our grid-based attention module uses a 3D convolution operation that does not require a fixed size input grid. The input size can be any rectangular shape that fits in the memory of graphic processing units (GPUs), which all modern deep learning frameworks utilize to accelerate computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MATERIALS AND METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>For this study, whole-slide images were collected from patients who underwent endoscopic esophagus and gastroesophageal junction mucosal biopsy since 2017 at Dartmouth-Hitchcock Medical Center (DHMC), a tertiary academic medical center in Lebanon, New Hampshire. The use of data collected in this project is approved by the Dartmouth Institutional Review Board (IRB) and the research conducted in this paper is in compliance with the World Medical Association Declaration of Helsinki on Ethical Principles for Medical Research Involving Human Subjects. A Leica Aperio scanner was used to digitize H&amp;E-stained whole-slide images at 20× magnification. We had a total of 180 whole-slide images, of which 116 (i.e., 64% of the dataset) were used as the development set and 64 (i.e., 36% of the dataset) were used as the test set. 20% of the development set wholeslide images were reserved for validation. Of note, these whole-slide images can cover multiple pieces of tissue. Therefore, the whole-slide images were separated into 256 highresolution images later in our preprocessing step, each image covering a single piece of tissue.</p><p>In order to determine labels for whole-slide images and to train the existing state-ofthe-art sliding method as our baseline, bounding boxes around lesions in these images were annotated by two expert pathologists from the Department of Pathology and Laboratory Medicine at DHMC. We considered these labels as the reference standard, as any disagreements in annotation were resolved through further discussion among annotators and consultation with a senior domain-expert pathologist. These bounding boxes were not needed in training our proposed attention-based model. This project used categories of esophageal cancer as defined by the Vienna classification system. <ref type="bibr" target="#b33">35</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>Our proposed approach has two steps, which is shown in <ref type="figure" target="#fig_3">Figure 1</ref>. The first step is grid-based feature extraction from the high-resolution image, where we analyze each grid cell in the whole slide to generate a feature map <ref type="figure" target="#fig_3">(Figure 1.a-b</ref>). In the second step, we apply our proposed attention mechanism on the extracted features for slide classification <ref type="figure" target="#fig_3">(Figure 1.c)</ref>.</p><p>Notably, the feature extractor is jointly optimized across all the grid cells along with the attention module in an end-to-end fashion. In the end-to-end training pipeline, the cross-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXPERIMENTS</head><p>To evaluate our attention-based classification model for high-resolution microscopy images, we applied our method to high-resolution scanned slides of tissues endoscopically removed from patients at risk of esophageal cancer. We compared the performance results of our proposed model to those generated by the state-of-the-art sliding window method. <ref type="bibr" target="#b21">22</ref> For preprocessing, we removed the white background from the slides and extracted only regions of the images that contained tissue. eFigure 1a shows a typical whole-slide image from our dataset. These whole-slide images can cover multiple pieces of tissue, so we separated them into large images with an average size of 5,131×5,875 pixels, with each only covering a single piece of tissue. Every image was given an overall label based on the labels of its lesions. If multiple lesions with different classes were present, we used the class with the highest risk as the corresponding label, as that lesion would have the highest impact clinically. If no abnormal lesions were found in an image, it was assigned to the normal class.</p><p>After this preprocessing step, each image was assigned to one of our four classes: normal, BE-no-dysplasia, BE-with-dysplasia, and adenocarcinoma (eFigure 1b). Our dataset included 379 images after preprocessing. One third of the dataset was reserved for testing. To avoid possible data leakage, tissues extracted from one whole-slide image were all placed into the same set of images when the development and test sets were split. <ref type="table">Table 1</ref> summarizes our test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sliding Window Baseline</head><p>In order to compare our model to previous methods for high-resolution image analysis, we implemented the current state-of-the-art sliding window method. <ref type="bibr" target="#b21">22</ref> In this method, we used our annotated bounding box labels to generate small cropped-images of size 224×224 pixels for training a cropped-image classifier. For preprocessing, we normalized the color channels and performed standard data augmentation, including color jittering, random flips, and rotations. For training, we initialized ResNet-18 with the MSRA initialization. <ref type="bibr" target="#b34">36</ref> We optimized the model with a cross-entropy loss function for 100 epochs, employing standard weight regularization techniques and learning rate decay. We trained our croppedimage classifier to predict the class of any given window on a high-resolution image. For whole-slide inference, we performed a grid search over our validation set to find optimal thresholds for filtering noise. Then, we consulted separately with two pathologists to develop heuristics for aggregating cropped-image predictions. We chose the thresholds and heuristic that performed the best on the validation set and applied that to the whole-slide images in the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Model</head><p>We implemented our attention model as described in the Methodology section. Given the size of features extracted from the ResNet-18 model, we used 512×3×3 3D convolutional filters in the attention module, with implicit zero-padding of (0, 1, 1) for depth, height, and width dimensions. We employed 64 of these filters to increase the robustness of the attention module, as patterns in the feature space are likely too complex to be recognized and attended by a single filter. To avoid overfitting and encourage each filter to capture different patterns, we regularized the attention module by applying dropout 37 with = 0.5 after concatenating all the feature vectors. We initialized the entire network with the MSRA initialization for convolutional filters, <ref type="bibr" target="#b34">36</ref> unit weight and zero-bias for batch normalizations, <ref type="bibr" target="#b36">38</ref> and the Glorot initialization for fully connected layers. <ref type="bibr" target="#b37">39</ref> Notably, only the cross-entropy loss against class labels was used in training. Other information such as the location of bounding boxes was not given to the network as guidance to optimal attention maps. Our model identified such ROIs automatically.</p><p>We first initialized our feature extraction network with weights pretrained on the ImageNet dataset. <ref type="bibr" target="#b38">40</ref> Input for the network were extracted grid cells of 492×492 pixels that were resized to 224×224 pixels. We normalized the input values by the mean and standard deviation of pixel values computed over all tissues in the traning set. In our training, the last fully connected layer of the network was removed, and all residual blocks except for the last one were frozen, serving as a regularization mechanism.</p><p>We trained the entire network on large, high-resolution images. For data augmentation, we applied random rotation and random scaling with a scaling factor between 0.8 and 1.2 during training. We used the Adam optimizer with an initial learning rate of 1e-3, decaying by 0.95 after each epoch, and reset the learning rate to 1e-4 every 50 epochs in a total of 200 epochs, similar to the cyclical learning rate. <ref type="bibr" target="#b39">41,</ref><ref type="bibr" target="#b40">42</ref> We set the mini batch size to 2 to maximize the utilization of memory on our Nvidia Titan Xp GPU. The model was implemented in PyTorch. <ref type="bibr" target="#b41">43</ref> At testing, the network took 0.34s on average for the analysis of a high-resolution image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EVALUATION AND RESULTS</head><p>We evaluated trained models on the test set and analyzed the classification performance from both quantitative and qualitative aspects. As a baseline, we referred to results from using the sliding-window method 22 for this classification task, which was trained on the same data split, but with annotated bounding boxes. For quantitative evaluation, we used four standard metrics for classification: accuracy, recall, precision, and F1 score using a one-vs-rest strategy. Our classification results on the test set are summarized in <ref type="table" target="#tab_1">Table 2</ref>. Compared to the baseline, our model achieved better accuracy and F1 score in all classes. Especially for F1 score, which is the harmonic mean of precision and recall, our model outperformed the baseline approach by at least 8% for each class; however, this outperformance was not significant at the 0.05 level of statistical significance. Quantitative analysis showed an exemplary performance of our model on the normal, BE-no-dysplasia, and adenocarcinoma classes. However, both our attention model and the baseline model did not perform as well in identifying images of the class BE-with-dysplasia. As shown in the confusion matrix in <ref type="figure" target="#fig_8">Figure 2</ref>, most samples of BE-with-dysplasia images that were misclassified by the attention model are predicted as normal tissue. This is likely because BE-with-dysplasia is the least frequent class in our dataset, comprising only 11% of images. For further comparison, the Receiver Operating Characteristic (ROC) curves of both models for each class are plotted in   The attention maps generated for all the testing images were visualized to verify the attention mechanism in our model. We present characteristic examples for the adenocarcinoma class in <ref type="figure" target="#fig_6">Figure 4</ref>. The distributions of the attention weights highlighted across different classes indicate that the attention module looks for specific features in the adenocarcinoma class <ref type="figure" target="#fig_6">(Figure 4.d)</ref>. For images without the target features, the attention weights are low over all regions <ref type="figure" target="#fig_6">(Figure 4</ref>.a-b). In <ref type="figure" target="#fig_6">Figure 4</ref>.c, we observe that the attention map is clinically on-target and is focused on specific regions in which BE-with-dysplasia progresses to adenocarcinoma as neoplastic epithelia begin to invade the muscularis mucosae. 44 Our proposed approach has some limitations. In terms of our dataset, one limitation is that all experiments were conducted on slides collected from a single medical center and scanned with the same equipment. Another is that our dataset is still relatively small in comparison to conventional datasets in deep learning; in particular, the number of slides of BE-with-dysplasia was small, resulting in lower performance for that class. In order to evaluate the robustness and generalizability of our novel approach, further verification with different classification tasks and larger datasets from various institutions is required and will be pursued in future work.</p><p>Furthermore, even with our method, which is built to analyze entire tissue regions, current GPUs do not have enough memory capacity to process very large images. For such slides, we can divide the tissue area into manageable sub-tissue images. Alternatively, the feature extractor, which is the largest source of memory consumption in our approach, can be optimized to address this issue. The ResNet-18 architecture used in our model achieved high performance with a relatively low number of parameters. However, there is still room for further reduction of parameters while maintaining high performance, which we will pursue in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CONCLUSION</head><p>We presented a new attention-based model for high-resolution microscopy image analysis.</p><p>Analogous to how pathologists examine slides under the microscope, our model utlizes weighted features from the entire slide for its classification. We showed that our model marginally outperforms the current sliding window method on a dataset of esophagus tissue with four classes of normal, BE-no-dysplasia, BE-with-dysplasia, and adenocarcinoma.</p><p>Previous methodology for analyzing microscopy images is limited by bounding box annotations and unscalable heuristics. Our model, on the other hand, is trained end-to-end with only labels at the tissue-level, removing the high cost of data annotation and creating new opportunities for the use of deep learning in digital pathology.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LEGENDS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>trainable end-to-end with only tissue-level labels. All components of our model are optimized through backpropagation. Unlike the current sliding window approach, our model does not need bounding box annotations for ROIs or a pathologist's insight for heuristic development.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The classification of normal includes normal squamous epithelium, normal squamous and columnar junctional epithelium, and normal columnar epithelium. BEno-dysplasia includes Barrett's esophagus negative for dysplasia. Barrett's esophagus is defined by columnar epithelium with goblet cells (intestinal metaplasia) and preservation of orderly glandular architecture of the columnar epithelium with surface maturation. BE-withdysplasia includes low-grade dysplasia (noninvasive low-grade neoplasia) and high-grade dysplasia (noninvasive high-grade neoplasia). Columnar epithelium with low-grade dysplasia is characterized by nuclear pseudostratification, mild to moderate nuclear hyperchromasia and irregularity, and the cytologic atypia extending to the surface epithelium. High-grade dysplasia demonstrates marked cytologic atypia including loss of polarity, severe nuclear enlargement and hyperchromasia, numerous mitotic figures, and architectural abnormalities such as lateral budding, branching, villous formation, as well as variation of the size and shape of crypts. Adenocarcinoma includes invasive carcinoma (intramucosal carcinoma and submucosal carcinoma and beyond) and high grade dysplasia suspicious for invasive carcinoma. Cases in the adenocrcinoma category may present the following features: single cell infiltration, sharply angulated glands, small glands in a back-to-back pattern, confluent glands, cribriform/solid growth, ulceration occurring within high-grade dysplasia, dilated dysplastic glands with necrotic debris, or dysplastic tubules incorporated into overlying squamous epithelium.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>entropy loss over all classes is computed on classification predictions. The loss is backpropagated to optimize all parameters in the network without any manual adjustment for attention modules. Of note, our model does not need bounding box annotations around ROIs, and all optimization is done with respect to only the labels at the tissue-level. Further details of the model architecture of the grid-based feature extraction and attention-based classification are provided in the Supplement Material section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1</head><label>1</label><figDesc>&lt;Please see the figure legend at the end of the manuscript.&gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>The attention model is trained without ROI annotations, yet achieved compelling area under the ROC curve (AUC) values for each class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 Figure 3</head><label>23</label><figDesc>&lt;Please see the figure legend at the end of the manuscript.&gt; &lt;Please see the figure legend at the end of the manuscript.&gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4</head><label>4</label><figDesc>&lt;Please see the figure legend at the end of the manuscript.&gt;DISCUSSIONOur results demonstrated the detection of BE/EAC using an attention-based deep learning architecture. The classification performance on our dataset is higher than that of the state-of-the-art sliding window model. This is significant because our proposed model only needs reference labels per tissue, while the existing sliding window models require bounding box annotations for each ROI in a tissue. While both models use a ResNet18 model for feature extraction, the attention mechanism of our model further directs the information flow and forces the network to identify local features that are useful for classification. Our proposed architecture is directly applicable to high-resolution images without resizing due to its flexible input design. Considering the time and resources required for annotating microscopy images, fewer requirements for these annotations would facilitate image analysis research and development on these images. Particularly, tissue-level annotations required for training our architecture can potentially be retrieved through searches in pathology reports associated with microscopy images. The proposed classification scheme is potentially applicable to histology images of other diseases for which training data is scarce or bounding box annotations are not available. Of note, our model is also the first to automate detection of BE and EAC on histopathology slides using a deep learning model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 .</head><label>1</label><figDesc>Overview of our attention-based model. (a) An input image is divided into r × c grid cells (dividing lines are shown only for visualization). (b) Features extracted from each grid cell build a grid-based feature map tensor U. (c) Learnable 3-dimensional convolutional filters of size k × d × d (where d denotes the height and width of the convolutional filters) are applied on U feature map to generate an attention map α, which operates as the weights for an affine combination of feature vectors in U. The α represents a 2-dimensional attention map whose size is r in height and c in width; CNN, convolutional neural network; r and c, the number of rows and columns of input tissue grid; U, a tensor of features extracted from each grid cell, and its size is r in height, c in width, and k in depth; and z, a vector of features representing a whole-input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 .</head><label>2</label><figDesc>The confusion matrix for different histologic classes related to esophageal cancer compares the classification agreement of our attention-based model with pathologists' consensus.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 .</head><label>3</label><figDesc>Receiver operating characteristic curves for (a) the sliding window approach and (b) our proposed attention-based method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 4 .</head><label>4</label><figDesc>Examples of attention maps generated by an attention module that is optimized for attending to the features of the adenocarcinoma class. Top row shows whole-slide images from the test set. The second row shows attention maps of the selected attention module for input images from four ground truth classes: (a) normal, (b) BE-no-dysplasia, (c) BE-withdysplasia, and (d) adenocarcinoma. Higher attention weight is denoted by white color and lower is denoted by black color. For visualization purposes, each map is normalized so its maximum value is 1. The accuracy of attended regions for the adenocarcinoma class images are verified qualitatively by two expert pathologists. In contrast, the attention module is inattentive to lower risk class images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>eFigure 2 .</head><label>2</label><figDesc>Additional Examples of Visualized Attention Maps Attending to Adenocarcinoma Class Features eTable. Class Distribution of Images in Our Dataset</figDesc><table><row><cell></cell><cell></cell><cell>Number (%)</cell><cell></cell></row><row><cell>Diagnosis</cell><cell>Training</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>Normal</cell><cell>115 (56.1%)</cell><cell>22 (43.1%)</cell><cell>58 (47.2%)</cell></row><row><cell>BE-no-dysplasia</cell><cell>37 (18.0%)</cell><cell>13 (25.5%)</cell><cell>30 (24.4%)</cell></row><row><cell>BE-with-dysplasia</cell><cell>23 (11.2%)</cell><cell>9 (17.6%)</cell><cell>14 (11.4%)</cell></row><row><cell>Adenocarcinoma</cell><cell>30 (14.6%)</cell><cell>7 (13.7%)</cell><cell>21 (17.1%)</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The authors would like to thank Lamar Moss and Maksim Bolonkin for their help with this manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FUNDING</head><p>This research was supported in part by grants R01LM012837 and P20GM104416 from the National Institutes of Health.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this study, two expert pathologists from the Department of Pathology and Laboratory Medicine at DHMC annotated each whole-slide image by drawing the smallest rectangular bounding boxes around characteristic lesions of each class in each image using Aperio ImageScope software and its Rectangle Tool. The marked ROIs are then extracted as cropped images in JPEG format.</p><p>The bounding box annotation is suitable in this study because this method is able to capture the histology patterns without well-defined boundaries, which is suitable for the diagnosis of Barrett's Esophagus based on continuous pathologic patterns. In addition, it reduces the annotation cost on our pathologists. In terms of the costs vs. benefits, a polygonbased annotation is suitable for dense predictions (e.g., a segmentation task), while a bounding box annotation is less demanding and is widely used for classification tasks due to its convenience and robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>eMethods 2. Details of Our Attention-Based Deep Learning Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grid-based Feature Extraction</head><p>To extract features on a high-resolution image through a CNN, we first divide the input image into smaller tiles with no overlap <ref type="figure">(Figure 1a</ref>), and then apply a CNN-based feature extraction on each tile (i.e., grid cell) of an r×c grid, with a k feature vector extracted from each cell, resulting in the formation of a structured grid-based feature map U of size k×r×c <ref type="figure">(Figure 1b</ref>). This feature map U is a high-level feature expression of a high-resolution image while preserving the geometric relationships of local features. While the grid-based approach is robust even if full view of a lesion is not in a grid cell due to training with tissue-level geometric augmentation (e.g., random rotation and translation), the granularity of analysis can be further controlled by using overlapping tiles at a higher computational cost. Whereas existing methodology makes a crop prediction solely based on a crop and later aggregates prediction results of crops to build a whole image prediction, our feature structure enables us to directly analyze the whole image through an attention mechanism, which we present in the next subsection below.</p><p>In the implementation of CNN architecture for feature extraction, we use the residual neural network (ResNet) architecture, 1 one of the state-of-the-art CNN models with high performance on the ImageNet Large Scale Visual Recognition Competition (ILSVRC) as well as many medical image classification tasks. <ref type="bibr">[2]</ref><ref type="bibr">[3]</ref><ref type="bibr">[4]</ref> Among several variants of ResNet models, we choose the pre-activation ResNet-18 model. <ref type="bibr">5</ref> This model achieves a good trade-off between performance and GPU memory usage, which is vital for processing high-resolution images. By removing the final fully-connected layer before the global pooling layer, the network produces a 512-feature vector (k=512) as output for a tile input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention-based Classification</head><p>After feature extraction, attention modules are applied to the feature map with their weights determining the importance or value of each tile in diagnostic relevancy <ref type="figure">(Figure 1c</ref>). The importance of each tile is estimated based on features extracted from the tile and its neighboring tiles because the adjoining areas of ROIs can also present informative characteristics. We compute a set of values, ∈ )×+ , for a grid. To implement this local valuation function in a deep learning framework while maintaining the robustness for an arbitrary size of grid input, we utilize 3D convolutional filters of size k×d×d, where corresponds to the size of features and denotes the height and width of the kernels. In this framework, applying a 3D convolution kernel to a feature map U generates a grid of value estimation . We normalize by applying a softmax function to build an attention map , where and are row and column indices: </p><p>This attention map shows the relative importance of each tile and thus we compute a whole-slide global feature vector using the attention map. Specifically, by treating the attention map as feature weights, the n-th components of the final feature vector are computed as follows:</p><p>The feature vector is subsequently used for whole-slide classification through fully connected layers and a non-linear activation function, allowing for classification of the entire whole-slide image by optimizing for a label.</p><p>Moreover, the use of multiple attention modules in our framework can potentially capture more local patterns for classification, increasing the capacity and robustness of the network, especially for medical images of high resolution. As such, we simultaneously apply 3D filters that generate attention maps and individually populate feature vectors. All feature vectors are concatenated to form a single vector, which is fed to the fully connected classifier.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Barrett&apos;s esophagus, dysplasia, and adenocarcinoma. Human pathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Haggitt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="982" to="993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Barrett&apos;s oesophagus and adenocarcinoma: burning questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Wild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Hardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reflux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Cancer</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">676</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Risk factors for Barrett&apos;s esophagus: a casecontrol study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Conio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Filiberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Blanchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of cancer</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="225" to="229" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Barrett&apos;s esophagus: pathogenesis, epidemiology, functional abnormalities, malignant degeneration, and surgical management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Siewert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dysphagia</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="276" to="288" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trends in survival for both histologic types of esophageal cancer in US surveillance, epidemiology and end results areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Polednak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of cancer</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="100" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Demographic variations in the rising incidence of esophageal adenocarcinoma in white males</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bollschweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wolfgarten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gutschow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Hölscher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer: Interdisciplinary International Journal of the American Cancer Society</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="549" to="555" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Esophageal cancer trends and risk factors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Blot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Seminars in oncology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="403" to="410" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">National Cancer Data Base report on esophageal carcinoma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Daly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Karnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Menck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer: Interdisciplinary International Journal of the American Cancer Society</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1820" to="1828" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Epidemiologic trends in esophageal and gastric cancer in the United States</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Devesa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Surgical oncology clinics of North America</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A global assessment of the oesophageal adenocarcinoma epidemic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Edgren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H-O</forename><surname>Adami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Weiderpass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nyrén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Gut</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1406" to="1414" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The histologic spectrum of Barrett&apos;s esophagus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Trier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Dalton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Camp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Loeb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Goyal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">295</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="476" to="480" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Interobserver variability in the diagnosis of crypt dysplasia in Barrett esophagus. The American journal of surgical pathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Coco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Hornick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="45" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Looking Under the Hood: Deep Neural Network Visualization to Interpret Whole-Slide Image Analysis Outcomes for Colorectal Polyps. Computer Vision and Pattern Recognition Workshops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Olofson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Miraflor</surname></persName>
		</author>
		<imprint>
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="821" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Patch-based convolutional neural network for whole slide tissue image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samaras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Kurc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Saltz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2424" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Automated gastric cancer diagnosis on h&amp;e-stained sections; ltraining a classifier on a large scale with multiple instance machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cosatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P-F</forename><surname>Laquerre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Malon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Pathology</title>
		<imprint>
			<biblScope unit="volume">8676</biblScope>
			<biblScope unit="page">867605</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Medical Imaging</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Efficient deep learning model for mitosis detection using breast histopathology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djcmi</forename><surname>Racoceanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Graphics</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="29" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Machine learning methods for histopathological image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Komura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sjc</forename><surname>Ishikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Journal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="34" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A deep convolutional neural network for segmenting and classifying epithelial and stromal regions in histopathological images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajn</forename><surname>Madabhushi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">191</biblScope>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Classification and mutation prediction from non-small cell lung cancer histopathology images using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Coudray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Ocampo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sakellaropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">1559</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Detecting cancer metastases on gigapixel pathology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gadepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<idno>arXiv:170302442. 2017</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Automated detection of celiac disease on duodenal biopsy slides: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suriawinata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hassanpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pathology Informatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="7" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pathologist-level classification of histologic patterns on resected lung adenocarcinoma slides with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Tafe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Linnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Vaickus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tomita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hassanpour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3358</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning for classification of colorectal polyps on whole-slide images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Olofson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Miraflor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of pathology informatics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural image caption generation with visual attention. International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4651" to="4659" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE conference on computer vision and pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Diagnose like a radiologist: Attention guided convolutional neural network for thorax disease classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yjapa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning to detect chest radiographs containing lung nodules using visual attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Pesce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P-P</forename><surname>Ypsilantis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Withey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bakewell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montana</forename><surname>Gjapa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning what to look in chest X-rays with a recurrent visual attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P-P</forename><surname>Ypsilantis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Montana</forename><surname>Gjapa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Training a cell-level classifier for detecting basal-cell carcinoma by combining human visual attention maps with low-level handcrafted features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corredor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Whitney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vla</forename><surname>Pedroza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Castro</forename><surname>Erjjomi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">21105</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The Vienna classification of gastrointestinal epithelial neoplasia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Riddell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kato</forename><surname>Yea</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="251" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salakhutdinov</forename><surname>Rjtjomlr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szegedy</forename><surname>Cjapa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cyclical learning rates for training neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="464" to="472" />
		</imprint>
		<respStmt>
			<orgName>Applications of Computer Vision (WACV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hutter</forename><surname>Fjapa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chanan</forename><forename type="middle">G</forename><surname>Pytorch</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Odze and Goldblum Surgical Pathology of the GI Tract, Liver, Biliary Tract and Pancreas E-Book</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Odze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Goldblum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Elsevier Health Sciences</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep convolutional neural networks enable discrimination of heterogeneous digital pathology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Imielinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Elemento</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ije</forename><surname>Hajirasouliha</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="317" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning Deep Representations of Medical Images using Siamese CNNs with Application to Content-Based Image Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W-</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hjapa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mdnet: A semantically and visually interpretable medical image diagnosis network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcgough</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6428" to="6436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<title level="m">Identity mappings in deep residual networks. European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Typical Examples of a Whole-Slide Image and Class-Associated Patches (a) A typical whole-slide image in our dataset. This particular slide contains three separate tissues and is of size 9,440 × 15,340 pixels. (b) Samples from each histology class in our dataset</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
