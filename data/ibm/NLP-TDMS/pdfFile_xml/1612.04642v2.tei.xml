<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Harmonic Networks: Deep Translation and Rotation Equivariance</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
							<email>d.worrall@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
							<email>s.garbin@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Turmukhambetov</surname></persName>
							<email>d.turmukhambetov@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
							<email>g.brostow@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="institution">University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Harmonic Networks: Deep Translation and Rotation Equivariance</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Translating or rotating an input image should not affect the results of many computer vision tasks. Convolutional neural networks (CNNs) are already translation equivariant: input image translations produce proportionate feature map translations. This is not the case for rotations. Global rotation equivariance is typically sought through data augmentation, but patch-wise equivariance is more difficult. We present Harmonic Networks or H-Nets, a CNN exhibiting equivariance to patch-wise translation and 360-rotation. We achieve this by replacing regular CNN filters with circular harmonics, returning a maximal response and orientation for every receptive field patch.</p><p>H-Nets use a rich, parameter-efficient and fixed computational complexity representation, and we show that deep feature maps within the network encode complicated rotational invariants. We demonstrate that our layers are general enough to be used in conjunction with the latest architectures and techniques, such as deep supervision and batch normalization. We also achieve state-of-the-art classification on rotated-MNIST, and competitive results on other benchmark challenges.</p><p>We include some proofs and derivations of the rotational equivariance properties of the circular harmonics, along with a demonstration of how we calculate the number of parameters for various network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Equivariance properties</head><p>In Section 3.2 we mentioned that cross-correlation with the circular harmonics is a 360 • -rotation equivariant feature transform. Here we provide the proof, and some of the properties mentioned in Arithmetic and Equivariance Condition.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We tackle the challenge of representing 360 • -rotations in convolutional neural networks (CNNs) <ref type="bibr" target="#b18">[19]</ref>. Currently, convolutional layers are constrained by design to map an image to a feature vector, and translated versions of the image map to proportionally-translated versions of the same feature vector <ref type="bibr" target="#b20">[21]</ref> (ignoring edge effects)-see <ref type="figure">Figure 1</ref>. However, until now, if one rotates the CNN input, then the feature vectors do not necessarily rotate in a meaningful or easy to predict manner. The sought-after property, directly relating input transformations to feature vector transformations, is called equivariance.</p><p>A special case of equivariance is invariance, where feature vectors remain constant under all transformations of the input. This can be a desirable property globally for a model, such as a classifier, but we should be careful not to restrict all intermediate levels of processing to be transformation invariant. For example, * http://visual.cs.ucl.ac.uk/pubs/harmonicNets/ <ref type="figure">Figure 1</ref>. Patch-wise translation equivariance in CNNs arises from translational weight tying, so that a translation π of the input image I, leads to a corresponding translation ψ of the feature maps f(I), where π =ψ in general, due to pooling effects. However, for rotations, CNNs do not yet have a feature space transformation ψ 'hard-baked' into their structure, and it is complicated to discover what ψ may be, if it exists at all. Harmonic Networks have a hard-baked representation, which allows for easier interpretation of feature maps-see <ref type="bibr">Figure 3.</ref> consider detecting a deformable object, such as a butterfly. The pose of the wings is limited in range, and so there are only certain poses our detector should normally see. A transformation invariant detector, good at detecting wings, would detect them whether they were bigger, further apart, rotated, etc., and it would encode all these cases with the same representation. It would fail to notice nonsense situations, however, such as a butterfly with wings rotated past the usual range, because it has thrown that extra pose information away. An equivariant detector, on the other hand, does not dispose of local pose information, and so it hands on a richer and more useful representation to downstream processes. Equivariance conveys more information about an input to downstream processes, it also constrains the space of possible learned models to those that are valid under the rules of natural image formation <ref type="bibr" target="#b29">[30]</ref>. This makes learning more reliable and helps with generalization. For instance, consider CNNs.</p><p>The key insight is that the statistics of natural images, embodied in the correlations between pixels, are a) invariant to translation, and b) highly localized. Thus features at every layer in a CNN are computed on local receptive fields, where weights are shared across translated receptive fields. This weight-tying serves both as a constraint on the translational structure of image statistics, and as an effective technique to reduce the number of learnable parameters-see <ref type="figure">Figure 1</ref>. In essence, translational equivariance has been 'baked' into the architecture of existing CNN models. We do the same for rotation and refer to it as hard-baking.</p><p>The current widely accepted practice to cope with rotation is to train with aggressive data augmentation <ref type="bibr" target="#b15">[16]</ref>. This certainly improves generalization, but is not exact, fails to capture local equivariances, and does not ensure equivariance at every layer within a network. How to maintain the richness of local rotation information, is what we present in this paper. Another disadvantage of data augmentation is that it leads to the so-called black-box problem, where there is a lack of feature map interpretability. Indeed, close inspection of first-layer weights in a CNN reveals that many of them are rotated, scaled, and translated copies of one another <ref type="bibr" target="#b33">[34]</ref>. Why waste computation learning all of these redundant weights?</p><p>In this paper, we present Harmonic Networks, or H-Nets. They design patch-wise 360 • -rotational equivariance into deep image representations, by constraining the filters to the family of circular harmonics. The circular harmonics are steerable filters <ref type="bibr" target="#b6">[7]</ref>, which means that we can represent all rotated versions of a filter, using just a finite, linear combination of steering bases. This overcomes the issue of learning multiple filter copies in CNNs, guarantees rotational equivariance, and produces feature maps that transform predictably under input rotation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multiple existing approaches seek to encode rotational equivariance into CNNs. Many of these follow a broad approach of introducing filter or feature map copies at different rotations. None has dominated as standard practice.</p><p>Steerable filters At the root of H-Nets lies the property of filter steerability <ref type="bibr" target="#b6">[7]</ref>. Filters exhibiting steerability can be constructed at any rotation as a finite, linear combination of base filters. This removes the need to learn multiple filters at different rotations, and has the bonus of constant memory requirements. As such, H-Nets could be thought of as using an infinite bank of rotated filter copies. A work, which combines steerable filters with learning is <ref type="bibr" target="#b22">[23]</ref>. They build shallow features from steerable filters, which are fed into a kernel SVM for object detection and rigid pose regression. H-Nets use the same filters with an added rotation offset term, so that filters in different layers can have orientation-selectivity relative to one another.</p><p>Hard-baked transformations in CNNs While H-Nets hard-bake patch-wise 360 • -rotation into the feature representation, numerous related works have encoded equivariance to discrete rotations. The following works can be grouped into those, which encode global equivariance versus patch-wise equivariance, and those which rotate filters versus feature maps. <ref type="bibr" target="#b2">[3]</ref> introduce equivariance to 90 • -rotations and dihedral flips in CNNs by copying the transformed filters at different rotation-flip combinations. More recently they generalized this theory to all group-structured transformations in <ref type="bibr" target="#b3">[4]</ref>, but they only demonstrated applications on finite groups-an extension to continuous transformations would require a treatment on anti-aliasing and bandlimiting. <ref type="bibr" target="#b23">[24]</ref> use a larger number of rotations for texture classification and <ref type="bibr" target="#b25">[26]</ref> also use many rotated handcrafted filter copies, opting not to learn the filters. To achieve equivariance to a greater number of rotations, these methods would need an infinite amount of computation. H-Nets achieve equivariance to all rotations, but with finite computation.</p><p>[6] feed in multiple rotated copies of the CNN input and fuse the output predictions. <ref type="bibr" target="#b16">[17]</ref> do the same for a broader class of global image transformations, and propose a novel per-pixel pooling technique for output fusion. As discussed, these techniques lead to global equivariances only and do not produce interpretable feature maps. <ref type="bibr" target="#b4">[5]</ref> go one step further and copy each feature map at four 90 • -rotations. They propose 4 different equivariance preserving feature map transformations. Their CNN is similar to <ref type="bibr" target="#b2">[3]</ref> in terms of what is being computed, but rotating feature maps instead of filters. A downside of this is that all inputs and feature maps have to be square; whereas, we can use any sized input.</p><p>Learning generalized transformations Others have tried to learn the transformations directly from the data. While this is an appealing idea, as we have said, for certain transformations it makes more sense to hard-bake these in for interpretability and reliability. <ref type="bibr" target="#b24">[25]</ref> construct a higher-order Boltzmann machine, which learns tuples of transformed linear filters in input-output pairs. Although powerful, they have only shown this to work on shallow architectures. <ref type="bibr" target="#b8">[9]</ref> introduced capsules, units of neurons designed to mimic the action of cortical columns. Capsules are designed to be invariant to complicated transformations of the input. Their outputs are merged at the deepest layer, and so are only invariant to global transformation. <ref type="bibr" target="#b21">[22]</ref> present a method to regress equivariant feature detectors using an objective, which penalizes representations, which lie far from the equivariant manifold. Again, this only encourages global equivariance; although, this work could be adapted to encourage equivariance at every layer of a deep pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem analysis</head><p>Many computer vision systems strive to be view independent, such as object recognition, which is invariant to affine transformations, or boundary detection, which is equivariant to non-rigid deformations. H-Nets hard-bake 360 • -rotation equivariance into their feature representation, by constraining the convolutional filters of a CNN to be from the family of circular harmonics. Below, we outline the formal definition of equivariance (Section 3.1), how the circular harmonics exhibit rotational equivariance (Section 3.2) and some properties of the circular harmonics, which we must heed for successful integration into the CNN framework (Section 3.2).</p><p>Continuous domain feature maps In deep learning we use <ref type="figure">Figure 2</ref>. Real and imaginary parts of the complex Gaussian filter Wm(r,φ ;e −r 2 ,0)=e −r 2 e imφ , for some rotation orders. As a simple example, we have set R(r)=e −r <ref type="bibr" target="#b1">2</ref> and β =0, but in general we learn these quantities. Cross-correlation, of a feature map of rotation order n with one of these filters of rotation order m, results in a feature map of rotation order m+n. Note the negative rotation order filters have flipped imaginary parts compared to the positive orders.</p><p>feature maps, which live in a discrete domain. We shall instead use continuous spaces, because the analysis is easier. Later on in Section 4.2 we shall demonstrate how to convert back to the discrete domain for practical implementation, but for now we work entirely in continuous Euclidean space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Equivariance</head><p>Equivariance is a useful property to have because transformations π of the input produce predictable transformations ψ of the features, which are interpretable and can make learning easier. Formally, we say that feature mapping f :X →Y is equivariant to a group of transformations if we can associate every transformation π ∈Π of the input x∈X with a transformation ψ ∈Ψ of the features; that is,</p><formula xml:id="formula_0">ψ[f(x)]=f(π[x]).<label>(1)</label></formula><p>This means that the order, in which we apply the feature mapping and the transformation is unimportant-they commute. An example is depicted in <ref type="figure">Figure 1</ref>, which shows that in CNNs the order of application of integer pixel-translations and the feature map are interchangeable. An important point of note is that π = ψ in general, so if we seek for Π to be rotations in the image domain, we do not require to find the set of f, such that Ψ "looks like" a rotation in feature space, rather we are searching for the set of f, such that there exists an equivalent class of transformations Ψ in feature space. A special case of equivariance is invariance, when Ψ={I}, the identity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Complex Circular Harmonics</head><p>With data augmentation CNNs may learn some rotation equivariance, but this is difficult to quantify <ref type="bibr" target="#b20">[21]</ref>. H-Nets take the simpler approach of hard-baking this structure in. If f is the feature mapping of a standard convolutional layer, then 360 • -rotational equivariance can be hard-baked in by restricting the filters to be of the from the circular harmonic family (proof in Supplementary Material) W m (r,φ;R,β)=R(r)e i(mφ+β) . Here r,φ are the spatial coordinates of image/feature maps, expressed in polar form, m ∈ Z is known as the rotation order, R : R + → R is a function, called the radial profile, which controls the overall shape of the filter, and β ∈ [0,2π) is a phase offset term, which gives the filter orientation-selectivity. During training, we learn the radial profile and phase offset terms. Examples of the real component of W m for a 'Gaussian envelope' and different rotation orders are shown in <ref type="figure">Figure 2</ref>. Since we are dealing with complex-valued filters, all filter responses are complex-valued, and we assume from now on that the reader understands that all feature maps are complex-valued, unless otherwise specified. Note that there are other works (e.g., <ref type="bibr" target="#b31">[32]</ref>), which use complex filters, but our treatment differs in that the complex phase of the response is explicitly tied to rotation angle. Rotational Equivariance of the Circular Harmonics Some deep learning libraries implement cross-correlation rather than convolution * , and since the understanding is slightly easier to follow, we consider correlation. Strictly, cross-correlation with complex functions requires that one of the arguments is conjugated, but we do not do this in our model/implementation, so</p><formula xml:id="formula_1">[W F](p ,q )= W(p−p ,q−q )F(p,q)dpdq (3) [W * F](p ,q )= W(p −p,q −q)F(p,q)dpdq.<label>(4)</label></formula><p>Consider correlating a circular harmonic of order m with a rotated image patch. We assume that the image patch is only able to rotate locally about the origin of the filter. This means that the cross-correlation response is a scalar function of input image patch rotation θ. Using the notation from Equation 1, and recalling that we are working in polar coordinates (r,φ), counter-clockwise rotation of an image F(r,φ) about the origin by an angle θ is F(r,π θ [φ]) = F(r,φ−θ). As a shorthand we denote F θ :=F(r,π θ [φ]). It is a well-known result <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7]</ref> (proof in Supplementary Material) that</p><formula xml:id="formula_2">[W m F θ ]=e imθ [W m F 0 ],<label>(5)</label></formula><p>where we have written W m in place of W m (r, φ; R, β) for brevity. We see that the response to a θ-rotated image F θ with a circular harmonic of order m is equivalent to the cross-correlation of the unrotated image F 0 with the harmonic, followed by multiplication by e imθ . While the rotation is done in input space, multiplication by e imθ is performed in feature space, and so, using the notation from Equation 1, ψ θ m [•] = e imθ · •. This process is shown in <ref type="figure" target="#fig_0">Figure 3</ref>. Note that we have included a subscript m on the feature space transformation. This is important, because the kind of feature space transformation we apply is dependent on the rotation order of the harmonic. Because the phase of the response rotates with the input at frequency m, we say that the response is an m-equivariant feature map. By thinking of an input image as a complex-valued feature map with zero imaginary part, we could think of it as 0-equivariant.</p><p>The rotation order of a filter defines its response properties to input rotation. In particular, rotation order m=0 defines invariance and m=1 defines linear equivariance. For m=0 this is be-</p><formula xml:id="formula_3">cause, denoting f m :=[W m F 0 ], then ψ θ 0 [f m ]=e i·0θ ·f m =f m , which is independent of θ. For m = 1, ψ θ 1 [f m ]</formula><p>= e i·1θ f m -as the input rotates, e iθ f m is a complex-valued number of constant magnitude f m , spinning round with a phase equal to θ. Naturally, we are not constrained to using rotation orders 0 or 1 only, and we make use of higher and negative orders in our work.</p><p>Arithmetic and the Equivariance Condition Further important properties of the circular harmonics, which are proven in the Supplementary Material, are: 1) Chained crosscorrelation of rotation orders m 1 and m 2 lead to a new response with rotation order m 1 + m 2 . 2) Point-wise nonlinearities h:C→C, acting solely on the magnitudes maintain rotational equivariance, so we can interleave cross-correlations with typical CNN nonlinearities adapted to the complex domain. 3) The summation of two responses of the same order m remains of order m. Thus to construct a CNN where the output is M-equivariant to the input rotation, we require that the sum of rotation orders along any path equals M, so</p><formula xml:id="formula_4">N i=1 m i =M.<label>(6)</label></formula><p>This is the fundamental condition underpinning the equivariance properties of H-Net, so we call it the equivariance condition. We note here that for our purposes, our filter W −m = W m (the complex conjugate), which saves on parameters, but this does not necessarily imply conjugacy of the responses unless F is real, which is only true at the input. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>We have considered the 360 • -rotational equivariance of feature maps arising from cross-correlation with the circular harmonics, and we determined that the rotation orders of chained cross-correlations sum. Next, we use these results to construct a deep architecture, which can leverage the equivariance properties of circular harmonics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Harmonic Networks</head><p>The rotation order of feature maps and filters sum upon crosscorrelation, so to achieve a given output rotation order, we must obey the equivariance condition. In fact, at every feature map, the equivariance condition must be met, otherwise, it should be possible to arrive at the same feature map along two different paths, with different summed rotation orders. The problem is that combining complex features, with phases, which rotate at different frequencies, leads to entanglement of the responses. The resultant feature map is no longer equivariant to a single rotation order, making it difficult to work with. We resolve this by enforcing the equivariance condition at every feature map.</p><p>Our solution is to create separate streams of constant rotation order responses running through the network-see <ref type="figure" target="#fig_1">Figure 4</ref>. These streams contain multiple layers of feature maps, separated by rotation order zero cross-correlations and nonlinearities. Moving between streams, we use cross-correlations of rotation order equal to the difference between those two streams. It is very easy to check that the equivariance condition holds in these networks.</p><p>When multiple responses converge at a feature map, we have multiple choices of how to combine them. We could stack them, we could pool across them, or we could sum them <ref type="bibr" target="#b4">[5]</ref>. To save on memory, we chose to sum responses of the same rotation order Y p is then fed into the next layer. Usually in our experiments, we use streams of orders 0 and 1, which we found to work well and is justified by the fact that CNN filters tend to contain very little high frequency information <ref type="bibr" target="#b11">[12]</ref>. Above, we see that the structure of the Harmonic Network is very simple. We replaced regular CNN filters with radially reweighted and phase shifted circular harmonics. This causes each filter response to be equivariant to input rotations with order m. To prevent responses of different rotation order from entangling upon summation, we separated filter responses into streams of equal rotation order.</p><formula xml:id="formula_5">Y p = m,n:m+n=p W m F n .<label>(7)</label></formula><p>Complex nonlinearities Between cross-correlations, we use complex nonlinearities, which act on the magnitudes of the complex feature maps only, to preserve rotational equivariance. An example is a complex version of the ReLU</p><formula xml:id="formula_6">C-ReLU b (Xe iφ )=ReLU(X +b)e iφ .<label>(8)</label></formula><p>We can provide similar analogues for other nonlinearities and for Batch Normalization <ref type="bibr" target="#b10">[11]</ref>, which we use in our experiments. We have thus far presented the Harmonic Network. Each layer is a collection of feature maps of different rotation orders, which transform predictably under rotation of the input to the network and the 360 • -rotation equivariance is achieved with finite computation. Next we show how to implement this in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation: Discrete cross-correlations</head><p>Until now, we have operated on a domain with continuous spatial dimensions Ω = R×R×{1,k }. However, the H-Net needs to operate on real-world images, which are sampled on a 2D-grid, thus we need to anti-alias the input to each discretized layer. We do this with a simple Gaussian blur. We can then use a regular CNN architecture without any problems. This works on the fact that the order of bandlimited sampling and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel filter</head><p>Polar filter Bandlimit and resample signal <ref type="figure">Figure 6</ref>. Images are sampled on a rectangular grid but our filters are defined in the polar domain, so we bandlimit and resample the data before cross-correlation via Gaussian resampling.</p><p>cross-correlation is interchangeable <ref type="bibr" target="#b6">[7]</ref>; so either we correlate in continuous space, then downsample, or downsample then correlate in the discrete space. Since point-wise nonlinearities and sampling also commute, the entire H-Net, seen as a deep feature-mapping, commutes with sampling. This could allow us to implement the H-Net on non-regular grids; although, we did not explore this.</p><p>Viewing cross-correlation on discrete domains sheds some insight into how the equivariance properties behave. In <ref type="figure" target="#fig_2">Figure  5</ref>, we see that the sampling strategy introduces multiple origins, one for each feature map patch. We call these, centers of equivariance, because a feature map will exhibit local rotation equivariance about each of these points. If we move to using more exotic sampling strategies, such as strided crosscorrelation or average pooling, then the centers of equivariance are ablated or shifted. If we were to use max-pooling, then the center of equivariance would be a complicated nonlinear function of the input image and harmonic weights. For this reason we have not used max-pooling in our experiments.</p><p>Complex cross-correlations On a practical note, it is worth mentioning, that complex cross-correlation can be implemented efficiently using 4 real cross-correlations</p><formula xml:id="formula_7">W Re m F Re −W Im m F Im real response +iW Re m F Im +W Im m F Re ) imaginary response .<label>(9)</label></formula><p>So circular harmonics can be implemented in current deep learning frameworks, with minor engineering. We implement a grid-resampled version of the filters W (x i )= j g i (r j )W (r j ),</p><p>with g i (x j ) ∝ e − ri−xj 2 2 /(2σ 2 ) (see <ref type="figure">Figure 6</ref>). The polar representation (r j ,φ j ) can be mapped from the components r j by r j = [r j cosφ j ,r j sinφ j ] . If we stack all the polar filter samples into a matrix we can write each point as the outer product of a radial tensor R j and trigonometric angular tensor [cosmΦ rj ,isinmΦ rj ] . The phase offset β can be separated out by noting that</p><formula xml:id="formula_8">W m (r j )= I i=1 R(r j ) Icosβ −Isinβ Isinβ Icosβ cosmΦ rj isinmΦ rj<label>(10)</label></formula><p>where the complex exponential and trigonometric terms are element-wise, and I is the identity matrix. This is just a reweighting of the ring elements. In full generality, we could also use a per-radius phase β ri , which would allow for spiral-like leftand right-handed features, but we did not investigate this.  <ref type="bibr" target="#b2">[3]</ref>. RIGHT deeply-supervised networks (DSN) <ref type="bibr" target="#b19">[20]</ref> for boundary segmentation, as per <ref type="bibr" target="#b32">[33]</ref>. Red boxes denote feature maps. Blue boxes are pooling (max for CNNs and average for H-Nets). Green boxes are side feature maps as per <ref type="bibr" target="#b32">[33]</ref>; these are connected to the DSN with dashed lines for ease of viewing. All main cross-correlations are 3×3, unless otherwise stated in the experiments section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Computational cost</head><p>We have increased the computational cost of crosscorrelation, in return for continuous rotational equivariance. Here we analyze the computational cost in terms of number of multiplications. In the standard cross-correlation, for an input of size h·w·i Z , (height, width, input channels) and filters of size k ·k ·o Z (height, width, output channels), the number of multiplications to form a feature map of the same size as the input is M(Z)=hwk 2 i Z o Z . In the H-Net, we have f rotation orders on the input and r rotation orders on the output, so perform fr complex cross-correlations. Each complex cross-correlation can be formed from 4 real cross-correlations, so the number of multiplications is 4M(H)fr, where i H and o H are the number of input and output channels, respectively. Thus for similar computational cost we equate the two to yield M(Z)=4M(H)fr. Rearranging; setting i H =o H , i Z =o Z and f =r; and taking the square root of both sides, we arrive at a simple rule of thumb for network design, i Z =2fi H . For example, if we want to build an H-Net with similar computational cost to a regular CNN with 64 channels per layer, then if we use 2 rotation orders m∈{0,1}, then the number of H-Net channels is 64/(2·2)=16.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We validate our rotation equivariant formulation below, performing some introspective investigations, and measuring against relevant baselines for classification on the rotated-MNIST dataset <ref type="bibr" target="#b17">[18]</ref> and boundary detection on the Berkeley Segmentation Dataset <ref type="bibr" target="#b0">[1]</ref>. We selected our baselines as representative examples of the current state-of-the-art and to demonstrate that H-Nets can be used on different architectures for different tasks. The networks we used are in <ref type="figure" target="#fig_0">Figure 13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test error (%) # params SVM <ref type="bibr" target="#b17">[18]</ref> 11.11 -Transformation RBM <ref type="bibr" target="#b30">[31]</ref> 4.2 -Conv-RBM <ref type="bibr" target="#b26">[27]</ref> 3.98 -CNN <ref type="bibr" target="#b2">[3]</ref> 5.03 22k CNN <ref type="bibr" target="#b2">[3]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Benchmarks</head><p>Here we compare H-Nets for classification and boundary detection. Classification is a typical rotation invariant task, and should suit H-Nets very well. In contrast, boundary detection is a rotation equivariant task. The key to the success of the H-Net is that it can achieve global equivariance, without sacrificing local equivariance of features.</p><p>MNIST Of course, this is a small dataset, with simple visual structures, but it is a good indication of how introducing the right equivariances into CNNs can aid inference. We investigate classification on the rotated MNIST dataset (new version) <ref type="bibr" target="#b17">[18]</ref> as a baseline. This has 10000 training images, 2000 validation images, and 50000 test images. The 360 • -rotations and small training set size make this a difficult task for classical CNNs. We compare against a collection of previous state-of-the-art papers and <ref type="bibr" target="#b2">[3]</ref>, who build a deep CNN with filter copies at 90 • -rotations. We try to mimic their network architecture for H-Nets as best as we can, using 2 rotation order streams with m ∈ {0,1} through to the deepest layer, and complex-valued versions of ReLU nonlinearities and Batch Normalization (see Method). We also replace max-pooling with mean-pooling layers, as shown in <ref type="figure" target="#fig_0">Figure 13</ref>. We perform stochastic gradient descent on a cross-entropy loss using Adam <ref type="bibr" target="#b12">[13]</ref> and an adaptive learning rate, which we divide by 10 if there has been no improvement in validation accuracy in the last 10 epochs. We train multiple models with randomly chosen hyperparameters, and report the test error of the model that performed best on the validation set, training on a combined training and validation set <ref type="table">Table 1</ref> lists our results. This model actually has 33k parameters, which is about 50% larger than the standard CNN and <ref type="bibr" target="#b2">[3]</ref>, which have 22k. This is because it uses 5×5 convolutions instead of 3×3. Interestingly, it does not overfit on such a small dataset and it still outperforms the standard CNN trained with rotation augmentations, which we do not use. We set the new state-ofthe-art, with a 26% improvement on the previous best model.</p><p>Deep Boundary Detection Boundary detection is equivariant to non-rigid transformations; although edge presence is locally invariant to orientation. The current state-of-the-art</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ODS</head><p>OIS # params HED, <ref type="bibr" target="#b32">[33]</ref>* 0.640 0.650 2346k HED, low # params <ref type="bibr" target="#b32">[33]</ref>* 0.697 0.709 115k Kivinen et al. <ref type="bibr" target="#b13">[14]</ref> 0.702 0.715 -H-Net (Ours) 0.726 0.742 116k CSCNN †, <ref type="bibr" target="#b9">[10]</ref> 0.741 0.759 DeepEdge †, <ref type="bibr" target="#b1">[2]</ref> 0.753 0.772 N 4 -Fields †, <ref type="bibr" target="#b7">[8]</ref> 0.753 0.769 DeepContour †, <ref type="bibr" target="#b27">[28]</ref> 0.756 0.773 HED †, <ref type="bibr" target="#b32">[33]</ref> 0.782 0.804 2346k DCNN + sPb †, <ref type="bibr" target="#b14">[15]</ref> 0.813 0.831 <ref type="table">Table 2</ref>. Our model beats the non-pretrained neural networks baselines on BSD500 <ref type="bibr" target="#b0">[1]</ref>. *Our implementation. †ImageNet pretrained depends on fine-tuning ImageNet-pretrained networks to regress boundary probabilities on a per-patch basis. To demonstrate that hard-baked rotation equivariance serves as a strong generalization tool, we compared against a previous state-of-the-art architecture <ref type="bibr" target="#b32">[33]</ref>, without pretraining. We tried to mimic <ref type="bibr" target="#b32">[33]</ref> as closely as possible, as shown in <ref type="figure" target="#fig_0">Figure  13</ref>. The main difference is that we divide the number of all feature maps by 2, for faster, more stable training. They use a VGG network <ref type="bibr" target="#b28">[29]</ref> extended with deeply supervised network (DSN) <ref type="bibr" target="#b19">[20]</ref> side-connections. These are 1 × 1-convolutions, which perform weighted averages across all relevant feature maps, resized to match the input. A binary cross-entropy loss is applied to each side connection, to stabilize learning. A final 'fusion' layer is created by taking a weighted linear combination of the side-connections, this is the final output. We adapt side-connections to H-Nets, by using the complex magnitude of feature maps before taking a weighted average. This means that the resultant boundary predictions are locally invariant to rotation. We added a small sparsity regularizer to our cost function, because we found it improved results slightly. We call the Harmonic variant of the DSN, an H-DSN. We also compare against <ref type="bibr" target="#b32">[33]</ref> with the number of parameters matched to H-DSN (the first layer has 7 features, instead of 16, and so on).</p><p>We also compared with <ref type="bibr" target="#b13">[14]</ref>, who use a mean-andcovariance-RBM. Their technique has five main contributions: 1) zero-mean, unit variance normalization of inputs, 2) sparsity regularization of hidden units, 3) averaged ground truth edge annotations, 4) average outputs to 16 input rotations, 5) non-maximum suppression of results by the Canny method. We perform the first 2 methods, but leave the last 3 alone. In particular, they do not pretrain on ImageNet, and attempt some kind of rotation averaging for global equivariance, so are a good baseline to measure against. We tested on the Berkeley Segmentation Dataset (BSD500) <ref type="bibr" target="#b0">[1]</ref>. As shown in <ref type="table">Table 2</ref> for non-pretrained models, H-Nets deliver superior performance over current state-of-the-art architectures, including <ref type="bibr" target="#b13">[14]</ref>, who also encode rotation equivariance. Most noticeable of all is that we only use 5% of the parameters of <ref type="bibr" target="#b32">[33]</ref>, showing how by restricting the search space of learnable models through hard-baking local rotation equivariance, we need not learn so many parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Model Insight</head><p>Here we investigate some of the properties of the H-Net implementation, making sure that the motivations behind H-Net design are achieved by the implementation.</p><p>Rotational stability As a sanity check, we measured the invariance of the magnitude response to rotation for m∈{0,1,2}. We show the result of rotating a random input to an H-Net layer in <ref type="figure" target="#fig_4">Figure 8</ref>. The response is very flat, with periodic small fluctuations due to the inexactness of anti-aliasing.</p><p>Filter Visualization The real parts of the filters, from the first layer of the boundary-detection-trained H-Net, are shown in <ref type="figure" target="#fig_5">Figure 9</ref>. They are aligned at zero phase (β = 0) for ease of viewing. Since the network is trained on zero-mean, unit variance, normalized color images, the weights do not have the natural colors we would see in real-world images. Nonetheless, there is useful information we can glean from inspecting these. Most 1 st layer filters detect color boundaries, there are no blank filters as one usually sees in CNNs, and there are few reoriented copies. We also see from the phase histograms that all phases are utilized by filters throughout the network, indicating full use . Randomly selected filters and phase histograms from the BSDS500 trained H-DSN. Filter are aligned at β =0; and the oriented circles represent phase. We see few filter copies and no blank filters, as usually seen in CNNs. We also see a balanced distribution over phases, indicating that boundaries, and their deep feature representations, are uniformly distributed in orientation.</p><p>of the phase information. This is interesting, because it means that the model's parameters are being used fully, with low redundancy, which we surmise comes from easier optimization on the equivariant manifold. Data ablation Here we investigate H-nets data-efficiency. CNNs are massively data-hungry. Krizhevsky's landmark paper <ref type="bibr" target="#b15">[16]</ref> used 60 million parameters, trained on 1.2 million 256× 256 RGB images quantized to 256 bits and split between 1000 classes, for a total of 10 bits of information per weight. Even this vast amount of data was insufficient for training, and data augmentation was needed to improve results. We ran an experiment on the rotated MNIST dataset to show that with hard-baked rotation equivariance, we require less data than competing methods, which is indeed the case (see <ref type="figure">Figure 10</ref>). Interestingly, and predictably, regular CNNs trained with data augmentation still perform worse than H-Nets, because they only learn global invariance to rotation, rather than local equivariances at each layer.</p><p>Feature maps We visualize feature maps in the lower layers of an MNIST trained H-Net (see <ref type="figure" target="#fig_6">Figure 11</ref>). For given input, we see the feature maps encode very complicated structures. Left to right, we see the H-Net learns to detect edges, corners, object presence, negative space, and outlines of objects. We perform this for the BSD500 trained H-DSN (see <ref type="figure" target="#fig_8">Figure 12</ref>). It shows equivariance is preserved through to the deepest feature maps. It also highlights the compact representation of feature presence and pose, which regular CNNs cannot do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We presented a convolutional neural network that is locally equivariant to patch-wise translation and, for the first time, to continuous 360 • -rotation. We achieved this by restricting the filters to circular harmonics, essentially hard-baking rotation into the architecture. This can be implanted onto other architectures too. The use of circular harmonics pays dividends in that we receive full rotational equivariance using few parameters. This leads to good generalization, even with less (or less augmented) training data. The only disadvantage we've seen so far is the higher per-filter computational cost, but our guidance for network design balances that cost against the more expressive  <ref type="figure">Figure 10</ref>. Data ablation study. On the rotated MNIST dataset, we experiment with test accuracy for varying sizes of the training set. We normalize the maximum test accuracy of each method to 1, for direct comparison of the falloff with training size. Clearly H-Nets are more data-efficient than regular CNNs, which need more data to discover rotational equivariance unaided. representation. The better interpretability of the feature maps is a bonus, because we know how they transform under input image rotations. We applied our network to the problem of classifying rotated-MNIST, setting a new state-of-the-art. We also applied our network to boundary detection, again achieving state-of-the-art results, for non-pretrained networks. We have shown that 360 • -rotational equivariance is both possible and useful. Our TensorFlow TM implementation is available on the project website.</p><p>Future work Extension of this work could involve hardbaking yet more transformations into the equivariance properties of the Harmonic Network, possibly extending to 3D. This will allow yet more expressibility in network representations, extending the benefits we have seen afforded by rotation equivariance to a larger class of models and applications.  The color wheel shows orientation coding. Note that between layers boundary orientations are colored differently because each feature has a different β. This visualization demonstrates the consistency of orientation within a feature map and across multiple layers. The images are taken from layers 2, 4, 6, 8, and 10 in a clockwise order from largest to smallest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Equivariance of the Circular Harmonics</head><p>We are interested in proving that there exists a filter W m , such that cross-correlation of F with W m yields a rotationally equivariant feature map. The proof requires us to introduce two different kinds of transformation: rotation R and translation T . To simplify the math, we use vector notation, so the spatial domain of the filter/image is R 2 . We write the filter as W m (x) and image as F(x) for x ∈ R 2 . We define the transformation operators R θ and T t , such that R θ F = F(R −θ x) and T t F = F(x−t), where R θ is a 2D rotation matrix for a θ counter-clockwise rotation. We introduce rotational cross-correlation . This is defined as</p><formula xml:id="formula_9">[W m F]= Φ R W m (rR φx )F(rR φx )drdφ,<label>(11)</label></formula><p>where we have used the decomposition x=rx, with r = x 2 ≥0 andx=x/r. The rotational cross-correlation is performed about the origin of the image. If we rotate the image, then we have</p><formula xml:id="formula_10">[W m R θ F]= Φ R W m (rR φx )F(rR φ R −θx )drdφ (12) = Φ R W m (rR φx )F(rR φ−θx )drdφ (13) = Φ R W m (rR φ +θx )F(rR φ x)drdφ .<label>(14)</label></formula><p>If we define W m (x)=W m (rx)=R(r)e i(mφ+β) , where φ=∠x, then</p><formula xml:id="formula_11">[W m R θ F]= Φ R W m (rR φ +θx )F(rR φ x)drdφ (15) = Φ R R(r)e i(m(φ +θ)+β) F(rR φ x)drdφ (16) =e imθ Φ R R(r)e i(mφ +β) F(rR φ x)drdφ (17) =e imθ [W m F].<label>(18)</label></formula><p>And so rotational cross-correlation is rotationally equivariant about the origin of rotation. In the next part, we build up to a result needed for proving the chained cross-correlation result.</p><p>Cross-correlation about t To perform the rotational cross-correlation about another point t, we first have to translate the image such that t is the new origin, so F t (x)=F(x−t), then perform the rotational cross-correlation, so</p><formula xml:id="formula_12">[W m T t F]=[W m F t ]<label>(19)</label></formula><formula xml:id="formula_13">= Φ R W m (rR φx )F t (rR φx )dr x dφ<label>(20)</label></formula><formula xml:id="formula_14">= Φ R W m (rR φx )F(rR φx −t)dr x dφ.<label>(21)</label></formula><p>Cross-correlation about t with rotated F about t In general, for every t this expression returns a different value. The response of a θ-rotated image about t is then</p><formula xml:id="formula_15">[W m R θ T t F]=[W m R θ F t ] (22) = Φ R W m (rR φx )F t (rR −θ R φx )dr x dφ (23) = Φ R W m (rR φx )F t (rR φ−θx )dr x dφ (24) = Φ R W m (rR φ +θx )F t (rR φ x)dr x dφ (25) =e imθ Φ Rz R(r z )e i(mφ +β) F t (rR φ x))drdφ (26) =e imθ [W m T t F].<label>(27)</label></formula><p>Cross-correlation about t with rotated F about origin Say we wish to perform the rotational cross-correlation about a point t, when the image has been rotated about the origin. Denoting F θ =R θ F, then the response is</p><formula xml:id="formula_16">[W m T t R θ F]=[W m T t F θ ]<label>(28)</label></formula><formula xml:id="formula_17">= Φ R W m (rR φx )F θ (r x R φx −t)dr x dφ (29) = Φ R W m (rR φx )F(r x R −θ R φx −R −θ t)dr x dφ (30) = Φ R W m (rR φx )F(r x R φ−θx −R −θ t)dr x dφ (31) = Φ R W m (rR φ +θx )F(r x R φ x−R −θ t)dr x dφ (32) =e imθ Φ R W m (rR φ x)F(r x R φ x−R −θ t)dr x dφ (33) =e imθ [W m T R −θ t F].<label>(34)</label></formula><p>Thus we see that cross-correlation of the rotated signal F θ with the circular harmonic filter W m =R(r)e i(mφ+β) is equal to the response at zero rotation [W F], multiplied by a complex phase shift e imθ . In the notation of the paper, we denote this multiplication by e imθ as ψ θ m [•]=e imθ ·•. Thus cross-correlation with W m yields a rotationally equivariant feature mapping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Properties</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Chained cross-correlation</head><p>We claimed in Arithmetic and Equivariance Condition, that the rotation order of a feature map resulting from chained cross-correlations is equal to the sum of the the rotation orders of the filters in the chain. We prove this for a chain of two filters, and the rest follows by induction. Consider taking a θ-rotated image F about the origin, then cross-correlating it with a filter W m as every point in the image plane t∈R 2 , followed by cross-correlation with W n as a point s∈R 2 . We already know that the response to the rotation is</p><formula xml:id="formula_18">[W m T t R θ F] = e imθ [W m T R −θ t F]</formula><p>, for all rotations θ of the input and all points t in the response plane, so we can write the chained convolution as</p><formula xml:id="formula_19">[W n T s [W m T t R θ F]]=[W n T s e imθ [W m T R −θ t F]]<label>(35)</label></formula><formula xml:id="formula_20">=e imθ W n T s [W m T R −θ t F]]<label>(36)</label></formula><p>We have used the property that the cross-correlation is linear and that we may pull the scalar factor e imθ outside. If we write</p><formula xml:id="formula_21">G(t)=[W m T t F] then [W m T R −θ t F]=G(R −θ t)=[R θ G](t), so [W n T s [W m T t R θ F]]=e imθ W n T s [W m T R −θ t F]] (37) =e imθ [W n T s R θ G] (38) =e imθ e inθ W n T R −θ s G .<label>(39)</label></formula><p>Thus we see that the chained cross-correlation results in a summation of the rotation orders of the individual filters W m and W n .</p><p>Setting s=0, such that we evaluate the cross-correlation at the center of rotation, we regain an equation similar to 18.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Magnitude nonlinearities</head><p>Point-wise nonlinearities acting on the magnitude of a feature map maintain rotational equivariance. Consider that we have a point on a feature map of rotation order m, which we can write as F e imθ , where F ≥ 0 is the magnitude of the feature map and e imθ is the phase component. The output of the nonlinearity g :R + →R is</p><formula xml:id="formula_22">g(F e imθ )=g(F )e imθ ,<label>(40)</label></formula><p>since g only acts on magnitudes. Since for fixed F the output is a function of m and θ only, the point-wise magnitude-acting nonlinearity preserves rotational equivariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Summation of feature maps</head><p>The summation of feature maps of the same rotation order is a new feature map of the same rotation order. Consider two feature maps F 1 and F 2 of rotation order m. Summation is a pointwise operation, so we only consider two corresponding points in the feature maps, which we denote F 1 e i(mθ+β1) and F 2 e i(mθ+β2) , where β 1 and β 2 are phase offsets. The sum is</p><formula xml:id="formula_23">F 1 e i(mθ+β1) +F 2 e i(mθ+β2) =e imθ F 1 e iβ1 +F 2 e iβ2 ,<label>(41)</label></formula><p>which for fixed F 1 ,F 2 ,β 1 ,β 2 is a function of m and θ only and so also rotationally equivariant with order m.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Number of parameters</head><p>Here we list a break down of how we computed the number of parameters for the various network architectures in the experiments section. The networks architectures used are in <ref type="figure" target="#fig_0">Figure 13</ref>. Red boxes are cross-correlations, blue boxes are pooling (average for H-Nets, max for regular CNNs), green boxes are 1×1-cross-correlations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Standard CNN</head><p>For a standard CNN layer with i input channels and o output channels, and k×k sized weights, the number of learnable parameters is iok 2 . Since there is one bias per output layer, this increases to iok 2 + o. If using batch normalization, then there is an extra per-channel scaling factor, which increases the number of learnable parameters to iok 2 +2o. The standard CNN for the rotated MNIST experiments has 6 layers of 3×3 cross-correlations, and 1 layer of 4×4-cross-correlations, with 20 feature maps per layer and 3 batch normalization layers so the number of learnable parameters is 21570. The calculations are shown in <ref type="table" target="#tab_3">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layer</head><p>Weights  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Harmonic networks</head><p>The learnable parameters of a Harmonic Network are the radial profile and the the per-filter phase offset. For a k×k filter, the number of radial profile elements is equal to the number of rings of equal distance from the center of the filter. For example, consider the <ref type="figure" target="#fig_1">Figure 14</ref>, which is an excerpt from the main paper. This is a 5×5 filter, with 6 rings of equal distance from the center of the filter (the smallest ring is just a single point). So this filter has 6 radial profile terms and 1 phase offset to learn. This contrasts with a regular filter, which would have 25 learnable parameters. Note, that for filters with rotation order m =0, the center pixel of the filter is in fact always zero, and so for a 5×5 rotation order m =0 filter, the number of radial profile terms is 6−1=5. So for the H-Net in the main paper with 5×5 filters and batch normalization in layers 2, 4, &amp; 6, the number of learnable parameters is 33347. The calculations are in <ref type="table" target="#tab_4">Table 4</ref>. Note that the final layer contains just one set of biases and no phase offsets. A similar set of calculations can be performed for the deeply supervised networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pixel filter</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Polar filter</head><p>Bandlimit and resample signal <ref type="figure" target="#fig_1">Figure 14</ref>. Each radius has a single learnable weight. Then there is a bias for the whole filter. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>DOWN: Cross-correlation of the input patch with Wm yields a scalar complex-valued response. ACROSS-THEN-DOWN: Crosscorrelation with the θ-rotated image yields another complex-valued response. BOTTOM: We transform from the unrotated response to the rotated response, through multiplication by e imθ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>An example of a 2 hidden layer H-Net with m = 0 output, input-output left-to-right. Each horizontal stream represents a series of feature maps (circles) of constant rotation order. The edges represent cross-correlations and are numbered with the rotation order of the corresponding filter. The sum of rotation orders along any path of consecutive edges through the network must equal M =0, to maintain disentanglement of rotation orders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>H-Nets operate in a continuous spatial domain, but we can implement them on pixel-domain data because sampling and crosscorrelation commute. The schematic shows an example of a layer of an H-Net (magnitudes only). The solid arrows follow the path of the implementation, while the dashed arrows follow the possible alternative, which is easier to analyze, but computationally infeasible. The introduction of the sampling defines centers of equivariance at pixel centers (yellow dots), about which a feature map is rotationally equivariant.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Networks used in our experiments. LEFT: MNIST networks, as per</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Stability of the response magnitude to input rotation angle. Black m=0, blue m=1, green m=2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9</head><label>9</label><figDesc>Figure 9. Randomly selected filters and phase histograms from the BSDS500 trained H-DSN. Filter are aligned at β =0; and the oriented circles represent phase. We see few filter copies and no blank filters, as usually seen in CNNs. We also see a balanced distribution over phases, indicating that boundaries, and their deep feature representations, are uniformly distributed in orientation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 11 .</head><label>11</label><figDesc>Feature maps from the MNIST network. The arrows display phase, and the colors display magnitude information (jet color scheme). There are diverse features encoding edges, corners, whole objects, negative spaces, and outlines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Acknowledgements</head><label></label><figDesc>Support is from Fight for Sight UK, a Microsoft Research PhD Scholarship, EPSRC Nature Smart Cities EP/K503745/1 and ENGAGE EP/K015664/1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>View best in color. Orientated feature maps for the H-DSN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 13 .</head><label>13</label><figDesc>Networks used</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Number of parameters for a regular CNN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Number of parameters for H-Net.</figDesc><table><row><cell>Layer</cell><cell>m=0</cell><cell>m=1</cell><cell cols="2">Batch Norm/Bias #Params</cell></row><row><cell>1</cell><cell>6·1·8+1·8</cell><cell>5·1·8+1·8</cell><cell>2·8</cell><cell>120</cell></row><row><cell>2</cell><cell>6·8·8+8·8</cell><cell>5·8·8+8·8</cell><cell>2·16</cell><cell>864</cell></row><row><cell>3</cell><cell>6·8·16+8·16</cell><cell>5·8·16+8·16</cell><cell>2·16</cell><cell>1696</cell></row><row><cell>4</cell><cell cols="2">6·16·16+16·16 5·16·16+16·16</cell><cell>2·32</cell><cell>3392</cell></row><row><cell>5</cell><cell cols="2">6·16·35+16·35 5·16·35+16·35</cell><cell>2·35</cell><cell>7350</cell></row><row><cell>6</cell><cell cols="2">6·35·35+35·35 5·35·35+35·35</cell><cell>2·70</cell><cell>16065</cell></row><row><cell>7</cell><cell>6·35·10</cell><cell>5·35·10</cell><cell>10</cell><cell>3860</cell></row><row><cell>Total</cell><cell></cell><cell></cell><cell></cell><cell>33347</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>Abstract</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deepedge: A multi-scale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="4380" to="4389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07576</idno>
		<title level="m">Group equivariant convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1612.08498</idno>
		<title level="m">Steerable cnns. CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">De</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02660</idno>
		<title level="m">Exploiting cyclic symmetry in convolutional neural networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rotation-invariant neoperceptron</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-08" />
			<biblScope unit="page" from="336" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The design and use of steerable filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">N 4 -fields: Neural network nearest neighbor fields for image transforms. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6558</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning -ICANN 2011 -21st International Conference on Artificial Neural Networks</title>
		<meeting><address><addrLine>Espoo, Finland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pixel-wise deep learning for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno>abs/1504.01989</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-11" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<idno>abs/1605.02971</idno>
		<title level="m">Structured receptive fields in cnns. CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Visual boundary prediction: A deep neural prediction network and quality dissection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Seventeenth International Conference on Artificial Intelligence and Statistics<address><addrLine>Reykjavik, Iceland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-04-22" />
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07386</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">TI-POOLING: transformation-invariant pooling for feature learning in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno>abs/1604.06318</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning, Proceedings of the Twenty-Fourth International Conference (ICML 2007), Corvallis</title>
		<meeting><address><addrLine>Oregon, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Handwritten digit recognition with a back-propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">E</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<meeting><address><addrLine>Denver, Colorado, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="396" to="404" />
		</imprint>
	</monogr>
	<note>NIPS Conference</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Eighteenth International Conference on Artificial Intelligence and Statistics<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Understanding image representations by measuring their equivariance and equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning covariant feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno>abs/1605.01224</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">2d/3d rotation-invariant detection using equivariant filters and kernel weighted mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Driever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Providence, RI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning rotation invariant convolutional filters for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.06720</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to represent spatial transformations with factored higher-order boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1473" to="1492" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep roto-translation scattering for object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oyallon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="2865" to="2873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning rotation-aware features: From invariant priors to equivariant descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2050" to="2057" />
			<date type="published" when="2012-06-16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deepcontour: A deep convolutional feature learned by positive-sharing loss for contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Boston, MA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-06-07" />
			<biblScope unit="page" from="3982" to="3991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Actionable information in vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<meeting><address><addrLine>Kyoto, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-09-27" />
			<biblScope unit="page" from="2138" to="2145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning invariant representations with local transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning, ICML 2012</title>
		<meeting>the 29th International Conference on Machine Learning, ICML 2012<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-07-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A mathematical motivation for complex-valued convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tygert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Piantino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="815" to="825" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2014 -13th European Conference</title>
		<meeting><address><addrLine>Zurich, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
