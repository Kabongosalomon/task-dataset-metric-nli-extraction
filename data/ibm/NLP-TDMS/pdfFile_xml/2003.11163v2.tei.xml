<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fusing Wearable IMUs with Multi-View Images for Human Pose Estimation: A Geometric Approach</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Southeast University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Southeast University</orgName>
								<address>
									<settlement>Nanjing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fusing Wearable IMUs with Multi-View Images for Human Pose Estimation: A Geometric Approach</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose to estimate 3D human pose from multi-view images and a few IMUs attached at person's limbs. It operates by firstly detecting 2D poses from the two signals, and then lifting them to the 3D space. We present a geometric approach to reinforce the visual features of each pair of joints based on the IMUs. This notably improves 2D pose estimation accuracy especially when one joint is occluded. We call this approach Orientation Regularized Network (ORN). Then we lift the multi-view 2D poses to the 3D space by an Orientation Regularized Pictorial Structure Model (ORPSM) which jointly minimizes the projection error between the 3D and 2D poses, along with the discrepancy between the 3D pose and IMU orientations. The simple two-step approach reduces the error of the state-of-the-art by a large margin on a public dataset. Our code will be released at https:// github.com/ CHUNYUWANG/ imu-human-pose-pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating 3D poses from images has been a longstanding goal in computer vision. With the development of deep learning models, the recent approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26]</ref> have achieved promising results on the public datasets. One limitation of the vision-based methods is that they cannot robustly solve the occlusion problem.</p><p>A number of works are devoted to estimating poses from wearable sensors such as IMUs <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. They suffer less from occlusion since IMUs can provide direct 3D measurements. For example, Roetenberg et al. <ref type="bibr" target="#b21">[22]</ref> place 17 IMUs with 3D accelerometers, gyroscopes and magnetometers at the rigid bones. If the measurements are accurate, the 3D pose is fully determined. In practice, however, the accuracy is limited by a number of factors such as calibration errors and the drifting problem.</p><p>Recently, fusing images and IMUs to achieve more robust pose estimation has attracted much attention <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28</ref>, * Work done when Zhe Zhang is an intern at Microsoft Research Asia. <ref type="figure">Figure 1</ref>. Our approach gets accurate 3D pose estimations even when severe self-occlusion occurs in the images. <ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref>. They mainly follow a similar framework of building a parametric 3D human model and optimizing its parameters to minimize its discrepancy with the images and IMUs. The accuracy of these approaches is limited mainly due to the hard optimization problem.</p><p>We present an approach to fuse IMUs with images for robust pose estimation. It gets accurate estimations even when occlusion occurs (see <ref type="figure">Figure 1</ref>). In addition, it outperforms the previous methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref> by a notable margin on the public dataset. We first introduce Orientation Regularized Network (ORN) to jointly estimate 2D poses for multi-view images as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. ORN differs from the previous multiview methods <ref type="bibr" target="#b18">[19]</ref> in that it uses IMU orientations as a structural prior to mutually fuse the image features of each pair of joints linked by IMUs. For example, it uses the features of the elbow to reinforce those of the wrist based on the IMU at the lower-arm.</p><p>The cross-joint-fusion allows to accurately localize the occluded joints based on their neighbors. The main challenge is to determine the relative positions between each pair of joints in the images, which we solve elegantly in the 3D space with the help of IMU orientations. The approach significantly improves the 2D pose estimation accuracy especially when occlusion occurs.</p><p>In the second step, we estimate 3D pose from multi-view 2D poses (heatmaps) by a Pictorial Structure Model (PSM) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2]</ref>. It jointly minimizes the projection error between the 3D and 2D poses, along with the discrepancy be-tween the 3D pose and the prior. The previous works such as <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref> often use the limb length prior to prevent from generating abnormal 3D poses. This prior is fixed for the same person and does not change over time. In contrast, we introduce an orientation prior that requires the limb orientations of the 3D pose to be consistent with the IMUs. The prior is complementary to the limb length and can reduce the negative impact caused by inaccurate 2D poses. We call this approach Orientation Regularized Pictorial Structure Model (ORPSM).</p><p>We evaluate our approach on two public datasets including Total Capture <ref type="bibr" target="#b26">[27]</ref> and H36M <ref type="bibr" target="#b8">[9]</ref>. On both datasets, ORN notably improves the 2D estimation accuracy especially for the frequently occluded joints such as ankle and wrist, which in turn decreases the 3D pose error. Take the Total Capture dataset as an example, on top of the 2D poses estimated by ORN, ORPSM obtains a 3D position error of 24.6mm which is much smaller than the previous state-ofthe-art <ref type="bibr" target="#b18">[19]</ref> (29mm) on this dataset. This result demonstrates the effectiveness of our visual-inertial fusion strategy. To validate the general applicability of our approach, we also experiment on the H36M dataset which has different poses from the Total Capture dataset. Since it does not provide IMUs, we synthesize virtual limb orientations and only show proof-of-concept results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Images-based We classify the existing image-based 3D pose estimation methods into three classes. The first class is model/optimization based <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> which defines a 3D parametric human body model and optimizes its parameters to minimize the discrepancy between model projections and extracted image features. These approaches mainly differ in terms of the used image features and optimization algorithms. These methods generally suffer from the difficult non-convex optimization which limits the 3D estimation accuracy to a large extent in practice.</p><p>With the development of deep learning, some approaches such as <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18]</ref> propose to learn a mapping from images to 3D pose in a supervised way. The lack of abundant ground truth 3D poses is their biggest challenge for achieving desired performance on wild images. Zhou et al. <ref type="bibr" target="#b32">[33]</ref> propose a multi-task solution to leverage the abundant 2D pose datasets for training. Yang et al. <ref type="bibr" target="#b31">[32]</ref> use adversarial training to improve the robustness of the learned model. Another limitation of this type of methods is that the predicted 3D poses by these methods are relative to their pelvis joints. So they are not aware of their absolute locations in the world coordinate system.</p><p>The third class of methods such as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19]</ref> adopt a two-step framework. It first estimates 2D poses in each camera view and then recovers the 3D pose in a world coordinate system with the help of camera pa-rameters. For example, Tome et al. <ref type="bibr" target="#b24">[25]</ref> build a 3D pictorial model and optimize the 3D locations of the joints such that their projections match the detected 2D pose heatmaps and meanwhile the spatial configuration of the 3D joints matches the prior pose structure. Qiu et al. <ref type="bibr" target="#b18">[19]</ref> propose to first estimate 2D poses for every camera view, and then estimate the 3D pose by triangulation or by pictorial structure model. This type of approaches has achieved the state-ofthe-art accuracy due to the significantly improved 2D pose estimation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IMUs-based</head><p>There are a small number of works which attempt to recover 3D poses using only IMUs. For example, Slyper et al. <ref type="bibr" target="#b22">[23]</ref> and Tautges et al. <ref type="bibr" target="#b23">[24]</ref> propose to reconstruct human pose from 5 accelerometers by retrieving prerecorded poses with similar accelerations from a database. They get good results when the test sequences are present in the training dataset. Roetenberg et al. <ref type="bibr" target="#b21">[22]</ref> use 17 IMUs equipped with 3D accelerometers, gyroscopes and magnetometers and all the measurements are fused using a Kalman Filter. By achieving stable orientation measurements, the 17 IMUs can fully define the pose of the subject. Marcard et al. <ref type="bibr" target="#b29">[30]</ref> propose to exploit a statistical body model and jointly optimize the poses over multiple frames to fit orientation and acceleration data. One disadvantage of the IMUs-only methods is that they suffer from drifting over time, and need a large amount of careful engineering work in order to make it work robustly in practice.</p><p>"Images+IMUs"-based Some works such as <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref> propose to combine images and IMUs for robust 3D human pose estimation. The methods can be categorized into two classes according to how image-inertial fusion is performed. The first class <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> estimate 3D human pose by minimizing an energy function which is related to both IMUs and image features. The second class <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b5">6]</ref> estimate 3D poses separately from the images and IMUs, and then combine them to get the final estimation. For example, Trumble et al. <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b5">6]</ref> propose a two stream network to concatenate the pose embeddings separately derived from images and IMUs for regressing the final pose.</p><p>Although the simple two-step framework has achieved the state-of-the-art performance in the image only setting, it is barely studied for "IMU+images"-based pose estimation because it is nontrivial to leverage IMUs in the two steps. Our main contribution lies in proposing two novel ways of exploiting IMUs in the framework. More importantly, we empirically show that this simple two-step approach can significantly outperform the previous state-of-the-arts.</p><p>Our work differs from the previous works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14]</ref> in two-fold. First, instead of estimating 3D poses or pose embeddings from images and IMUs separately and then fusing them in a late stage, we propose to fuse IMUs and image features in a very early stage with the aid of 3D geometry. This directly gives improved 2D poses rather than attempting to get accurate poses from two inaccurate ones as in late fusion. Second, in the 3D pose estimation step, we leverage IMUs in the pictorial structure model. Although pictorial model is not new, the effect of using IMUs has not been discussed. Finally, we hope this simple yet effective approach could promote more research in the twostep pose estimation direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">ORN for 2D Pose Estimation</head><p>We represent a 2D pose by a graph which consists of M joints J = {J 1 , J 2 , · · · , J M } and N edges E = {e 1 , e 2 , · · · , e N } as shown in <ref type="figure">Figure 3</ref> (c). Each J represents the state of a joint such as its 2D location in the image. Each edge e connects two joints, representing their conditional dependence. In this work, we attach IMUs to W limbs to obtain their 3D orientations</p><formula xml:id="formula_0">O = {o 1 , o 2 , · · · , o W }.</formula><p>This orientation information will be used to constrain relative positions between two joints. In the following, we will describe in detail how we estimate 2D poses with the help of orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Methodology</head><p>We start by describing how 3D limb orientations can be used to mutually enhance the features between pairs of joints linked by IMUs in the same camera view. Then we extend it to handle multi-view features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Same-View Fusion</head><p>We explain the main idea of our approach with a pair of joints J 1 and J 2 as an example. The two joints are connected by the limb e whose 3D orientation is o. In practice, we will apply the fusion operation to all pairs of joints linked by IMUs. <ref type="figure">Figure 3</ref> sketches the idea. Let the heatmaps of J 1 and J 2 be H 1 and H 2 , respectively. For a location Y P in H 1 , its heatmap value represents the confidence that J 1 is at Y P . We propose to enhance it by the confidence of the linked joint J 2 at K possible corresponding locations Y Q k , k = 1, · · · , K which are consistent with Y P according to limb orientation o.</p><p>The main challenge is to determine the locations of Y Q k . From <ref type="figure">Figure 3</ref> (a), it is clear that the corresponding 3D point P of Y P has to lie on the line defined by the camera center C 1 and Y P . Since the exact depth of P is unknown, we log-uniformly sample K locations P k , k = 1, · · · , K on the line as its candidates <ref type="bibr" target="#b0">1</ref> . In addition, we assume the limb length l between J 1 and J 2 is provided as a prior which is the average limb length computed on the training dataset. Together with the 3D orientation o between the two joints, we can compute the 3D locations of J 2 as follows:</p><formula xml:id="formula_1">Q k = P k + o * l ∀k = 1, · · · , K<label>(1)</label></formula><p>Finally we project Q k onto the image using the camera parameters and get the 2D locations as Y Q k . Intuitively, a high response at Y Q k in H 2 actually indicates J 1 has a high probability to be at Y P . This observation is the core of our fusion approach. However, there is ambiguity because we do not know which of the K candidates Y Q k is the corresponding point due to the lack of depth. Our solution is to find the maximum response among all locations Y Q k , k = 1, · · · , K:</p><formula xml:id="formula_2">H 1 (Y P ) ← λH 1 (Y P ) + (1 − λ) max k=1···K H 2 (Y Q k ) (2)</formula><p>Since fusion happens in the heatmap layer, ideally, Y Q k should have the largest response at the correct J 2 location and zeros at other locations. It means the noncorresponding locations will contribute no or little to the fusion. We set the balancing parameter λ to be 0.5 in our experiments. We sample 200 points whose depths range from zero to the maximum depth value, which is determined by the size of the room.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-View Fusion</head><p>One limitation of the Same-View Fusion is that the correct location Y Q k * which has the maximum response among the K candidates in H 2 , will contribute to multiple candidates like Y P in H 1 . These candidates also lie on a line. But most of such locations do not correspond to the joint type J 1 . In other words, some non-corresponding locations are mistakenly enhanced. For example, there are blurred lines in the "Enhanced Heatmap" in <ref type="figure" target="#fig_3">Figure 4</ref> with each from a different camera view.  <ref type="figure">Figure 3</ref>. Illustration of the cross-joint-fusion idea in ORN. (a) For a location YP in H1, we estimate its 3D points P k lying on the line defined by the camera center C1 and YP . Then based on the 3D limb orientation provided by IMU and the limb length, we get candidate 3D locations of J2 which are denoted as Q k . We project Q k to the image as YQ k and get the corresponding heatmap confidence. If the confidence is high, J1 has high confidence being located at YP . (b) We enhance the initial confidence of J1 at YP with the confidence of J2 at YQ k in all views. Similarly, we can fuse the heatmap of J2 using that of J1. (c) We show the skeleton model used in this work.</p><formula xml:id="formula_3">Y 1 1 Y 1 2 (c) 1 2 IMU Y 1 2 Y 2 2 1 Y 1 1 1 MaxPool … View-1 View-V Y 1 Y 1 MaxPool (b) …</formula><p>To resolve this problem, we propose to perform fusion across multiple views simultaneously:</p><formula xml:id="formula_4">H 1 (Y P ) ← λH 1 (Y P ) + (1 − λ) V V v=1 max k=1···K H v 2 (Y v Q k ), (3) where Y v Q k is the projection of Q k in the camera view v and H v 2 is the heatmap of J 2 in view v.</formula><p>The result is that the lines from multiple views will intersect at the correct location. Consequently, the correct location will be enhanced most which resolves the ambiguity. See the fused heatmap in <ref type="figure" target="#fig_3">Figure 4</ref> for illustration. Another desirable effect of cross-view fusion is that it helps solve the occlusion problem by fusing the features from multiple views because a joint occluded in one view may be visible in other views. This notably increases the joint detection rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation</head><p>We use the network proposed in <ref type="bibr" target="#b30">[31]</ref>, referred to as SimpleNet (SN) to estimate initial pose heatmaps. It uses ResNet50 <ref type="bibr" target="#b7">[8]</ref> as its backbone which was pre-trained on the ImageNet classification dataset. The image size is 256×256 and the heatmap size is 64 × 64. The orientation regularization module can either be trained end-to-end with SN, or added to a already trained SN as a plug-in since it has no learnable parameters. In this work, we train the whole ORN end-to-end. We generate ground-truth pose heatmaps as the regression targets and enforce l 2 loss on all views before and after feature fusion. In particular, we do not compute losses for background pixels of the fused heatmap since the background pixels may have been enhanced. The network is trained for 15 epochs. The parameter λ is 0.5 in all experiments. Other hyper-parameters such as learning rate and decay strategy are the same as in <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ORPSM for 3D Pose Estimation</head><p>A human is represented by a number of joints J = {J 1 , J 2 , · · · , J M }. Each J represents its 3D position in a world coordinate system. Following the previous works <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19]</ref>, we use the pictorial model to estimate 3D pose as it is more robust to inaccurate 2D poses. But different from the previous works, we also introduce and evaluate a novel limb orientation prior based on IMUs as will be described in detail later. Each J takes values from a discrete state space. An edge between two joints denotes their conditional dependence such as limb length. Given a 3D pose J and multi-view 2D pose heatmaps F, we compute the posterior as follows</p><formula xml:id="formula_5">p(J |F) = 1 Z(F) M i=1 φ conf i (J i , F) (m,n)∈E limb ψ limb (J m , J n ) (m,n)∈E IM U ψ IMU (J m , J n ),<label>(4)</label></formula><p>where Z(F) is the partition function, E limb and E IM U are sets of edges on which we enforce limb length and orientation constraints, respectively. The unary potential φ conf i (J i , F) is computed based on 2D pose heatmaps F. The pairwise potential ψ limb (J m , J n ) and ψ IMU (J m , J n ) encode the limb length and orientation constraints. We describe each term in detail as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discrete State Space</head><p>We first estimate the 3D location of the root joint by triangulation based on its 2D locations detected in all views. Note that this step is usually very accurate because the root joint can be detected in most times. Then the state space of the 3D pose is within a 3D bounding volume centered at the root joint. The edge length of the volume is set to be 2000mm which is large enough to cover every body joint. The volume is discretized by an N × N × N regular grid G. Each joint can take one of the bins of the grid as its 3D location. Note that all body joints share the same state space G which consists of N 3 discrete locations (bins). <ref type="table">Table 1</ref>. The 2D pose estimation accuracy (PCKh@t) on the Total Capture Dataset. "SN" means SimpleNet which is the baseline. ORN same and ORN, respectively, represent that the same-view and cross-view fusion are used. "Mean (six)" is the average result over the six joint types. "Others" is the average result over the rest of the joints. "Mean (All)" is the result over all joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>PCKh@ Unary Potential Every body joint hypothesis, i.e., a bin in the grid G, is defined by its 3D position. We project it to the pixel coordinate system of all camera views using the camera parameters, and get the corresponding joint confidence/response from F. We compute the average confidence/response over all camera views as the unary potential for the hypothesis.</p><p>Limb Length Potential For each pair of joints (J m ,J n ) in the edge set E limb , we compute the average distancel m,n on the training set as limb length prior. During inference, the limb length pairwise potential is defined as:</p><formula xml:id="formula_6">ψ limb (J m , J n ) = 1, if |l m,n −l m,n | ≤ , 0, otherwise ,<label>(5)</label></formula><p>where l m,n is the distance between J m and J n . The pairwise term favors 3D poses having reasonable limb lengths.</p><p>In our experiments, is set to be 150mm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limb Orientation Potential</head><p>We compute the dot product between the limb orientations of the estimated pose and the IMU orientations as the limb orientation potential</p><formula xml:id="formula_7">ψ IMU (J m , J n ) = J m − J n J m − J n 2 · o m,n ,<label>(6)</label></formula><p>where o m,n is the orientation (represented as a directional vector) of the limb measured by the IMU. This term favors poses whose limb orientations are consistent with the IMUs. We also experimented with the hard orientation constraint similar to what we did for limb length, but this soft limb orientation constraint gets better performance. A 3D pose estimator without/with orientation potential will be termed as PSM and ORPSM, respectively.</p><p>Inference We maximize the posterior probability, i.e. Eq. (4), over the discrete state space by the dynamic programming algorithm. In general, the complexity grows quadratically. In order to improve the speed, we adopt a recursive  variant of PSM <ref type="bibr" target="#b18">[19]</ref> which iteratively refines the 3D poses. In practice, it takes about 0.15 seconds to estimate one 3D pose on a single Titan Xp GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Datasets and Metrics</head><p>Total Capture <ref type="bibr" target="#b26">[27]</ref> To the best of our knowledge, this is the only benchmark providing images, IMUs and ground truth 3D poses. It places 8 cameras in the capture room to record the human motion. We use four of them (1, 3, 5 and 7) in our experiments for efficiency reasons. The performers wear 13 IMUs. We use eight of them as shown in <ref type="figure">Figure 3</ref> (c). There are five subjects performing four actions including Roaming(R), Walking(W), Acting(A) and Freestyle(FS) with each repeating 3 times. Following the previous work <ref type="bibr" target="#b26">[27]</ref>, we use Roaming 1,2,3, Walking 1,3, <ref type="figure">Figure 5</ref>. The grey line shows the 3D MPJPE error of the noFusion approach. The orange line shows the error difference between our method (ORN+ORPSM) and noFusion. If the orange line is below zero, it means our method has smaller errors. We split the testing samples into two groups according to the error scale of noFusion. The first group includes the samples whose errors are smaller than 80mm (shown in the left <ref type="figure">figure)</ref>. The second group includes the rest of the samples (shown in the right <ref type="figure">figure)</ref>. The samples are sorted by the orange line for the sake of readability. Freestyle 1,2 and Acting 1,2 of Subjects 1,2,3 for training our 2D pose estimator. We test on Walking 2, Freestyle 3 and Acting 3 of all subjects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H36M [9]</head><p>To validate the general applicability of our approach, we also conduct experiments on the H36M dataset. Since this dataset does not provide IMUs, we create virtual IMUs (limb orientations) using the ground truth 3D poses for both training and testing, and only show proofof-concept results. Following the dataset conventions, we use subjects 1, 5, 6, 7, 8 for training and subjects 9, 11 for testing. We train a single model for all actions.</p><p>Metrics The Percentage of Correct Keypoints (PCK) metric is used for 2D pose evaluation. Specifically, PCKh@t measures the percentage of the estimated joints whose distance from the ground-truth joints is smaller than t times of the head length. Previous works report results when t is 1 2 . In our experiments, we provide results when t is set to be 1 2 , 1 6 and 1 12 , respectively, in order to understand our approach more comprehensively. We use the Mean Per Joint Position Error (MPJPE) for 3D pose evaluation <ref type="bibr" target="#b8">[9]</ref>. It computes the distance between estimated poses and the ground truth poses. We report the average error over all joints and all instances. <ref type="table">Table 1</ref> shows the 2D pose estimation results of ORN and the baseline SimpleNet (SN). We keep SN the same as ORN except it does not perform fusion. We can see from the table that when the threshold t is set to be 1 2 as in most previous works, ORN outperforms SN by a large margin. The improvement for wrist, elbow and knee joints is most significant because they are frequently occluded by human body in the dataset. <ref type="figure" target="#fig_3">Figure 4</ref> shows some examples explaining how our approach improves localization over the baseline. For example, in the first example, initially, the right knee joint is not correctly detected because it is occluded by human body. Fusing the features from hip in multiple camera views helps localize it correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">2D Pose Estimation</head><p>We notice that the improvement (brought by visualinertial fusion) on the hip joints is small. There are two possible reasons for the phenomenon. First, the hip joints are visible for most images in the dataset. So IMUs provide barely no additional information. Second, the hip joint detection rate for the baseline method is already very high so it leaves little room for improvement. The estimation results for the joints without IMUs, which are represented as "Others" in the table, are similar for the baseline and our approach, which is expected.</p><p>When we use a more rigorous threshold, for example when t = <ref type="bibr">1 12</ref> , the detection rate for hip drops from 87.6% to 85.3% (SN vs. ORN). There are two reasons for this phenomenon: (1) the detection rate for hip is already very high for SN, leaving little space for improvement; (2) IMUs often have small noises which may affect fusion precision. This conclusion is supported by the subsequent experimental results on the H36M dataset: when we use GT IMUs, the detection rate also improves for hip. Actually, even on the Total Capture, the impact also becomes small when we use a larger threshold. For example, when the threshold is set to be <ref type="bibr">1 6</ref> , the accuracy of ORN is slightly better than SN (97.7% vs. 97.5%).</p><p>We also evaluate the impact of cross-view fusion in ORN. As can be seen in <ref type="table">Table 1</ref>, the multi-view fusion outperforms the same-view fusion consistently which validates its effectiveness. In addition, we find that the improvement is larger when we use a more rigorous threshold t. The results suggest that multi-view feature fusion helps localize the joints more precisely.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">3D Pose Estimation</head><p>We first evaluate our 3D pose estimator through a number of ablation studies. Then we compare our approach to the state-of-the-arts. Finally, we present results on the H36M dataset validating the generalization capability of the proposed approach.</p><p>Ablation Study We denote the baseline which uses SN and PSM to estimate 2D and 3D pose as noFusion baseline. The main results are shown in <ref type="table" target="#tab_1">Table 2</ref>. First, using ORN consistently decreases the 3D error no matter what 3D pose estimators we use. In particular, the improvement on the elbow and wrist joint is as large as 10mm when we use PSM as the 3D estimator. This significant error reduction is attributed to the improved 2D poses. <ref type="figure" target="#fig_4">Figure 6</ref> (a-c) visualize three typical examples where ORN gets better results: we project the estimated 3D poses to the images and draw the skeletons. It is guaranteed that if the 2D locations are correct for more than one view, then the 3D joint location is at the correct position. We also plot the 3D error of every testing sample in <ref type="figure">Figure 5</ref>. Our approach improves the accuracy for most cases because the orange line is mostly below zero. See the caption of the figure for the meanings of the lines. In addition, we can see that the improvement is larger when the noFusion baseline has large errors. There are a small number of cases where fusion does not improve joint detection results as shown in <ref type="figure" target="#fig_4">Figure 6</ref> (d-f). <ref type="table">Table 3</ref>. 3D pose estimation errors MPJPE (mm) of different methods on the Total Capture dataset. "Aligned" means whether we align the estimated 3D poses to the ground truth poses by Procrustes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>IMUs Temporal Aligned Subjects(S1,2,3) Subjects(S4,5) Mean W2 A3 FS3 W2 A3 FS3 PVH <ref type="bibr" target="#b26">[27]</ref> 48 Second, from the second and third rows of <ref type="table" target="#tab_1">Table 2</ref>, we can see that using ORPSM alone achieves a similar 3D error as ORN alone. This means 3D fusion is related to 2D fusion in some way-although 3D fusion does not directly improve the 2D heatmap quality, it uses 3D priors to select better joint locations having both large responses as well as small discrepancy with respect to the prior structures. But in some cases, for example, when the responses at the correct locations are too small, using the 3D prior is not sufficient. This is verified by the experimental results in the fourth row -if we enforce 2D and 3D fusion simultaneously, the error further decreases to 30.2mm. It suggests the two components are actually complementary.</p><p>State-of-the-arts Finally we compare our approach to the state-of-the-arts on the Total Capture dataset. The results are shown in <ref type="table">Table 3</ref>. First, we can see that IMUPVH <ref type="bibr" target="#b5">[6]</ref> which uses IMUs even gets worse results than LSTM-AE <ref type="bibr" target="#b25">[26]</ref> which does not use IMUs. The results suggest that getting better visual features is actually more effective than performing late fusion of the (possibly inaccurate) 3D poses obtained from images and IMUs, respectively. Our approach, which uses IMUs to improve the visual features, also outperforms <ref type="bibr" target="#b5">[6]</ref> by a large margin.</p><p>The error of the state-of-the-art is about 29mm <ref type="bibr" target="#b18">[19]</ref> which is larger than 24.6mm of ours. This validates the effectiveness of our IMU-assisted early visual feature fusion. Note that the error of VIP <ref type="bibr" target="#b27">[28]</ref> is obtained when the 3D pose estimations are aligned to ground truth which should be compared to 20.6mm of our approach.</p><p>We notice that the error of our approach is slightly larger than <ref type="bibr" target="#b25">[26]</ref> for the "W2 (walking)" action. We tend to think it is because LSTM can get significant benefits when it is applied to periodic actions such as "walking". This is also observed independently in another work <ref type="bibr" target="#b5">[6]</ref>. Besides, the error 14.3mm for "W2" of Subject 1,2,3 is not further reduced after fusion since noFusion method has already achieved extraordinarily high accuracy.</p><p>Generalization To validate the wide applicability of our approach, we conduct experiments on the H36M dataset <ref type="bibr" target="#b8">[9]</ref>. The results of different methods are shown in <ref type="table" target="#tab_2">Table 4</ref>. We can see that our approach (ORN+ORPSM) consistently outperforms the baseline noFusion which validates its general applicability. In particular, the improvement is significant for the Ankle joint which is often occluded. Since we use the ground truth IMU orientations in this experiment, the results are not directly comparable to other works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Summary and Future Work</head><p>We present an approach for fusing visual features through IMUs for 3D pose estimation. The main difference from the previous efforts is that we use IMUs in a very early stage. We evaluate the approach through a number of ablation studies, and observe consistent improvement resulted from the fusion. As the readings from the IMUs usually have noises, our future work will focus on learning a reliability indicator, for example based on temporal filtering, for each sensor to guide the fusion process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of ORN. It firstly takes multi-view images as input and estimates initial heatmaps (based on SimpleNet<ref type="bibr" target="#b30">[31]</ref>) independently for each camera view. Then with the aid of IMU orientations, it mutually fuses the heatmaps of the linked joints across all views. It enforces supervision on both initially estimated and fused heatmaps during the end-to-end training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Three sample heatmaps estimated by ORN. The initially estimated heatmaps without fusion are inaccurate. After fusing multi-view features, the "Enhanced Heatmap" localizes the correct joints. Note there are blurred lines in the "Enhanced Heatmap" with each corresponding to the confidence contributed from one camera view. The lines intersect at the correct location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Sample 3D poses estimated by our approach and noFusion. We project the estimated 3D poses to the images and draw the skeletons. Left and right limbs are drawn in green and orange colors, respectively. (a-c) show examples when our method improves over noFusion. (d-f) show three failure cases. These rare cases mainly happen when both joints of a limb have large errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>3D pose estimation errors (mm) of different variants of our approach on the Total Capture dataset. "Mean (six)" is the average error over the six joint types. "Others" is the average error over the rest of the joints. "Mean (All)" is the average error over all joints.</figDesc><table><row><cell>2D</cell><cell>3D</cell><cell cols="8">Hip Knee Ankle Shoulder Elbow Wrist Mean (Six) Others Mean (All)</cell></row><row><cell>SN</cell><cell>PSM</cell><cell>17.2 35.7</cell><cell>41.2</cell><cell>50.5</cell><cell>54.8</cell><cell>56.8</cell><cell>37.1</cell><cell>20.3</cell><cell>28.3</cell></row><row><cell>ORN</cell><cell>PSM</cell><cell>17.4 29.9</cell><cell>35.2</cell><cell>49.6</cell><cell>44.2</cell><cell>45.1</cell><cell>32.8</cell><cell>20.4</cell><cell>25.4</cell></row><row><cell>SN</cell><cell cols="2">ORPSM 18.3 25.8</cell><cell>34.0</cell><cell>44.8</cell><cell>44.2</cell><cell>49.8</cell><cell>32.1</cell><cell>19.9</cell><cell>25.5</cell></row><row><cell cols="3">ORN ORPSM 18.5 24.2</cell><cell>30.1</cell><cell>44.8</cell><cell>40.7</cell><cell>43.4</cell><cell>30.2</cell><cell>19.8</cell><cell>24.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>.3 94.3 122.3 84.3 154.5 168.5 107.3 Malleson et al. 3D pose estimation error (mm) on the H36M dataset.We use virtual IMUs in this experiment. We show results for the six joints which are affected by IMUs. "Mean (six)" is the average error over the six joint types. "Others" is the average error over the rest of the joints. "Mean (All)" is the average error over all joints.</figDesc><table><row><cell>[15]</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>65.3</cell><cell>-</cell><cell>64.0</cell><cell>67.0</cell><cell>-</cell></row><row><cell>VIP [28]</cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>26.0</cell></row><row><cell>LSTM-AE [26]</cell><cell></cell><cell></cell><cell cols="5">13.0 23.0 47.0 21.8 40.9</cell><cell>68.5</cell><cell>34.1</cell></row><row><cell>IMUPVH [6]</cell><cell></cell><cell></cell><cell cols="5">19.2 42.3 48.8 24.7 58.8</cell><cell>61.8</cell><cell>42.6</cell></row><row><cell>Qiu et al. [19]</cell><cell></cell><cell></cell><cell cols="5">19.0 21.0 28.0 32.0 33.0</cell><cell>54.0</cell><cell>29.0</cell></row><row><cell>SN + PSM</cell><cell></cell><cell></cell><cell cols="5">14.3 18.7 31.5 25.5 30.5</cell><cell>64.5</cell><cell>28.3</cell></row><row><cell>SN + PSM</cell><cell></cell><cell></cell><cell cols="5">12.7 16.5 28.9 21.7 26.0</cell><cell>59.5</cell><cell>25.3</cell></row><row><cell>ORN + ORPSM</cell><cell></cell><cell></cell><cell cols="5">14.3 17.5 25.9 23.9 27.8</cell><cell>49.3</cell><cell>24.6</cell></row><row><cell>ORN + ORPSM</cell><cell></cell><cell></cell><cell cols="5">12.4 14.6 22.0 19.6 22.4</cell><cell>41.6</cell><cell>20.6</cell></row><row><cell>Methods</cell><cell cols="9">Hip Knee Ankle Shoulder Elbow Wrist Mean (Six) Others Mean (All)</cell></row><row><cell>noFusion (SN + PSM)</cell><cell>23.2 28.7</cell><cell>49.4</cell><cell>29.1</cell><cell>28.4</cell><cell>32.3</cell><cell>31.9</cell><cell></cell><cell>18.3</cell><cell>27.9</cell></row><row><cell cols="2">ours (ORN + ORPSM) 20.6 18.6</cell><cell>28.2</cell><cell>25.1</cell><cell>21.8</cell><cell>24.2</cell><cell>23.1</cell><cell></cell><cell>18.3</cell><cell>21.7</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use log-uniform instead of uniform sampling to prevent from generating redundant collapsed 2D projections</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multi-view pictorial structures for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bernt Schiele, Nassir Navab, and Slobodan Ilic. 3d pictorial structures for multiple human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sikandar</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1669" to="1676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D pictorial structures for multiple view articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Burenius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josephine</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3618" to="3625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fast and robust multi-person 3d pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junting</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hujun</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04111</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Optimization and filtering for human motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">75</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fusing visual and inertial sensors with semantics for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="381" to="397" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Volumetric performance capture from minimal camera viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Volino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<title level="m">Human3. 6m: Large scale datasets and predictive methods for 3D human sensing in natural environments. T-PAMI</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Monet: Multiview semi-supervised keypoint via epipolar divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasamin</forename><surname>Jafarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00104</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Panoptic studio: A massively multiview system for social interaction capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanbyul</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xulong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Godisart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Nabbe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">T-PAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="190" to="204" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Depth sweep regression forests for estimating 3D human pose from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Markerless motion capture of interacting characters using multi-view image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yebin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Stoll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1249" to="1256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multiperson linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naureen</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Real-time full-body motion capture from video and imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Volino</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="449" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3D human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julieta</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Harvesting multiple views for marker-less 3D human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1253" to="1262" />
		</imprint>
	</monogr>
	<note>Derpanis, and Kostas Daniilidis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cross view fusion for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4342" to="4351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning monocular 3d human pose estimation from multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Spörri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Constantin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8437" to="8446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Xsens mvn: full 6dof human motion tracking using miniature inertial sensors. Xsens Motion Technologies BV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Roetenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henk</forename><surname>Luinge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Per</forename><surname>Slycke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Action capture with accelerometers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronit</forename><surname>Slyper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGGRAPH/Eurographics Symposium on Computer Animation</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="193" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Motion reconstruction using sparse accelerometer data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jochen</forename><surname>Tautges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arno</forename><surname>Zinke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Björn</forename><surname>Krüger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Baumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Helten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meinard</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Eberhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">18</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Lourdes Agapito, and Chris Russell. Rethinking pose in 3D: Multi-stage refinement and recovery for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Toso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="474" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep autoencoder for combined human pose estimation and body model upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Total capture: 3D human pose estimation fusing video and inertial sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Trumble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Hilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Collomosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recovering accurate 3d human pose in the wild using imus and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="601" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenhahn</surname></persName>
		</author>
		<title level="m">Human pose estimation from video and imus. T-PAMI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1533" to="1547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sparse inertial poser: Automatic 3d human pose estimation from sparse imus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Timo Von Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards 3D human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
