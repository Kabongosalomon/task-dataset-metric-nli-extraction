<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Concurrence-Aware Long Short-Term Sub-Memories for Person-Person Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">College of Computer Science and Technology</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Nanjing University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Nanjing University of Aeronautics and Astronautics</orgName>
								<orgName type="institution" key="instit3">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">College of Computer Science and Technology</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Nanjing University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Nanjing University of Aeronautics and Astronautics</orgName>
								<orgName type="institution" key="instit3">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
							<email>guojun.qi@ucf.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">College of Computer Science and Technology</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Nanjing University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Nanjing University of Aeronautics and Astronautics</orgName>
								<orgName type="institution" key="instit3">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">College of Computer Science and Technology</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Nanjing University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Nanjing University of Aeronautics and Astronautics</orgName>
								<orgName type="institution" key="instit3">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechao</forename><surname>Li</surname></persName>
							<email>zechao.li@njust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">College of Computer Science and Technology</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Nanjing University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Nanjing University of Aeronautics and Astronautics</orgName>
								<orgName type="institution" key="instit3">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">College of Computer Science and Technology</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Nanjing University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Nanjing University of Aeronautics and Astronautics</orgName>
								<orgName type="institution" key="instit3">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyan</forename><surname>Zhang</surname></persName>
							<email>zhangliyan@nuaa.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science and Engineering</orgName>
								<orgName type="department" key="dep2">College of Computer Science and Technology</orgName>
								<orgName type="department" key="dep3">Department of Computer Science</orgName>
								<orgName type="institution" key="instit1">Nanjing University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Nanjing University of Aeronautics and Astronautics</orgName>
								<orgName type="institution" key="instit3">University of Central Florida</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Concurrence-Aware Long Short-Term Sub-Memories for Person-Person Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, Long Short-Term Memory (LSTM) has become a popular choice to model individual dynamics for singleperson action recognition due to its ability of modeling the temporal information in various ranges of dynamic contexts. However, existing RNN models only focus on capturing the temporal dynamics of the person-person interactions by naively combining the activity dynamics of individuals or modeling them as a whole. This neglects the inter-related dynamics of how person-person interactions change over time. To this end, we propose a novel Concurrence-Aware Long Short-Term Sub-Memories (Co-LSTSM) to model the long-term inter-related dynamics between two interacting people on the bounding boxes covering people. Specifically, for each frame, two sub-memory units store individual motion information, while a concurrent LSTM unit selectively integrates and stores interrelated motion information between interacting people from these two sub-memory units via a new co-memory cell. Experimental results on the BIT and UT datasets show the superiority of Co-LSTSM compared with the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person-person interaction (e.g., handshake, hug, etc), as the basic unit in the human activity, is attracting much attention in the computer vision and pattern recognition communities <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref>. During a person-person interaction process, there are usually two individual motions from two interacting people respectively, some of which are concurrently inter-related with each other (e.g., two interact- <ref type="figure">Figure 1</ref>. Illustration of the proposed Co-LSTSM. For each frame, two sub-memory units are developed to store individual motion information, while a concurrent LSTM unit is developed to selectively integrate and store inter-related motion information between interacting people from two sub-memory units via a new co-memory cell πt (t = 1, 2, · · · ). Stacked concurrent LSTM units are recurrent to capture inter-related dynamics between interacting people over time.</p><p>ing people are stretching out hands in hug interaction). It has been proven that the concurrently inter-related motions between interacting people are discriminative for recognizing the person-person interactions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref>. In most cases of person-person interaction, the concurrently inter-related motions between two interacting people are either 1) quite symmetrically similar to each other (e.g., two interacting people are handshaking); or 2) not quite similar but are strongly interacting to each other (e.g., person A kicks person B, while person B retreats back).</p><p>There are mainly two types of solutions for personperson interaction recognition. One solution (e.g., <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40]</ref>) is to extract the individual motion descriptors (e.g., spatio-temporal interest points <ref type="bibr" target="#b6">[7]</ref>) from interacting people, and then predict the class label of an interaction by inferring the coherence between two individual motions. However, this solution regards the person-person interactions as two single-person actions, which ignores some inter-related motion information and brings in some irrelevant individual motion information. The other solution is to extract the motion descriptors on the interactive regions, and then train an interaction recognition model <ref type="bibr" target="#b13">[14]</ref>. However, it is hard to locate interactive region before close interacting. Usually, the difference between person-person interactions (e.g., boxing interaction and pat interaction) is subtle <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b25">26]</ref>, which brings in the challenge to recognize person-person interaction. Recently, due to the powerful ability of capturing the sequential motion information, Recurrent Neural Networks (RNN) <ref type="bibr" target="#b35">[36]</ref>, especially Long Short-Term Memory (LSTM) <ref type="bibr" target="#b10">[11]</ref>, has proven successful on human action recognition tasks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12]</ref>. To well address the problem of person-person interaction recognition, we aim to explore the long-term inter-related dynamics between two interacting people by leveraging state-of-theart LSTM model. However, existing LSTM models only modeling human individual dynamics independently do not consider the concurrently inter-related dynamics between interacting people. A naive way is to either 1) merge the individual actions at preprocessing stage <ref type="bibr" target="#b12">[13]</ref> (e.g., consider interacting people as a whole); or 2) utilize two LSTM networks to model the individual dynamics of each interacting person respectively, and then fuse the output sequences from two LSTM networks <ref type="bibr" target="#b11">[12]</ref>. However, this neglects the inter-related dynamics between interacting people of how person-person interactions can change over time.</p><p>To this end, we propose a novel Concurrence-Aware Long Short-Term Sub-Memories (Co-LSTSM) for personperson interaction recognition by modeling the long-term inter-related dynamics between two interacting people on the bounding boxes covering people. It has the ability to aggregate the inter-related memories from individual memories of interacting people over time, as shown in <ref type="figure">Figure 1</ref>. Specifically, we present a novel concurrent LSTM unit consisting of two sub-memory units that store the individual motion information on the bounding box covering people of each video frame. Following these two submemory units, a new co-memory cell selectively integrates and stores the memories from two sub-memory units to reveal the concurrently inter-related motion information between interacting people. Overall, two interacting people in each frame are jointly modeled by a concurrent LSTM unit on the bounding boxes covering people, which outputs the concurrently inter-related hidden representations between interacting people rather than the individual hidden representations from individual human. The stacked concurrent LSTM units are recurrent in a time sequence to capture the concurrently inter-related dynamics between two interact-ing people over time. Extensive experiments on the widelyused benchmarks well show the superior performance of the proposed Co-LSTSM compared with the state-of-theart methods and several baselines.</p><p>Our main contributions in this work are two-fold: (1) We propose a novel Concurrence-Aware Long Short-Term Memories (Co-LSTSM) to effectively address the problem of person-person interaction recognition. (2) To our best knowledge, our work is the first attempt in modeling concurrently long-term inter-related dynamics over time between multiple motion objects by the variants of LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Human Action Recognition</head><p>Human activity recognition aims to automatically understand the activities performed by people <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b24">25]</ref>, including group-person interaction recognition (e.g., walking, queueing, etc) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b32">33]</ref>, person-object interaction recognition (e.g., some people are eating, while the other people are riding a bike) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, and person-person interaction recognition <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>For group-person interaction recognition, one solution used in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b27">28]</ref> is to exploit the spatial distribution of human activities and present the spatio-temporal descriptors in capturing the spatial distribution of people. The other solution used in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33]</ref> is to track all body parts in a video, and then learn the holistic representations to estimate their collective activities. In particular, instead of treating the two problems (i.e., tracking multiple people and estimating their collective activities) separately, Choi et al. <ref type="bibr" target="#b4">[5]</ref> presented a unified framework to simultaneously track people and estimate their collective activities. Besides, Lan et al. <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> proposed to recognize the group-person activities by jointly capturing the group activity, the individual human actions, and the interactions among them.</p><p>For person-object interaction recognition, there are usually a number of concurrent individual activities (e.g., some people are riding a bike) and group activities (e.g., some people are walking together). To address this challenge, Amer et al. <ref type="bibr" target="#b0">[1]</ref> proposed a spatio-temporal AND-OR graph to jointly model the activity parts, person-person spatiotemporal relations, and person-object context, as well as enable multi-target tracking. Subsequently, Amer et al. <ref type="bibr" target="#b1">[2]</ref> used a three-layered AND-OR graph to jointly model group activities, individual actions, and participating objects. A key point is that these methods require a multitude of detectors at different levels.</p><p>For person-person interaction recognition, some representative works <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40]</ref> used several interactive phrases as the latent mid-level feature to infer the person-person interaction from the human individual actions. Interactive phrases incorporating rich human knowledge provide an ef-fective way to represent person-person interactions. However, the difference of some interactions (e.g., boxing and pat) is too stable to be discriminated only by the interactive phrases. Besides, some person-person interactions are complex, which cannot be described well by a certain amount of interactive phrases. Recently, Kong et al. <ref type="bibr" target="#b13">[14]</ref> developed a patch-aware latent SVM to recognize the interactions by inferring the closely interactive regions between interacting people. However, it is hard to capture the interactive regions before close interacting. Moreover, Chang et al. <ref type="bibr" target="#b3">[4]</ref> proposed to extract features of each interacting person and then learn an interaction matrix between interacting people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">RNN-based Action Recognition</head><p>As neural nets for handling sequential data with variable length, RNN, especially LSTM, has been successfully applied to action recognition <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b11">12]</ref>. Many RNN-based action recognition methods are embedded the LSTM layer into Convolutional Neural Networks (CNN) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b12">13]</ref>. For example, Wu et al. <ref type="bibr" target="#b36">[37]</ref> proposed to train three types of CNNs equipped with LSTM to model the spatial, short-term motion and audio clues corresponding to the inputs of video frames, stacked optical flows, and audio spectrogram, respectively. Besides, some skeletonbased action recognition methods utilized RNN to model the long-term contextual information of all skeletons. For example. Du et al. <ref type="bibr" target="#b8">[9]</ref> proposed a multilayer RNN framework to feed the five body parts from human skeletons into five subnets. As the number of layers increases, the representations outputs from several subnets are hierarchically fused to the inputs of the higher layers.</p><p>Some works aim to design the specific RNN architecture for the different action recognition tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b29">30]</ref>. For example, in order to capture the co-occurrences of discriminative joints, Zhu et al. <ref type="bibr" target="#b40">[41]</ref> added a mixed-norm regularization penalty to the deep LSTM networks. Moreover, the authors proposed an internal dropout technique to c operate on the gates, cells, and output responses of the LSTM nodes. To emphasize on the temporal change of motion information between two consecutive frames with the time, Veeriah et al. <ref type="bibr" target="#b33">[34]</ref> proposed a Differential RNN architecture equipped with the Derivative of States between the LSTM gates. Recently, Shahroudy et al. <ref type="bibr" target="#b29">[30]</ref> proposed a Partaware LSTM that separates the memory cell into the several sub-cells corresponding to the different body parts and explicitly models the dependencies over spatial and temporal domains concurrently. Likewise, Liu et al. <ref type="bibr" target="#b22">[23]</ref> also proposed the similar LSTM architecture by pushing the traditional LSTM-based learning into temporal domains and spatial domains simultaneously.</p><p>Unlike existing RNN-based action recognition works, we consider the more challenging action recognition scenario within person-person interactions. To capture the in-teractive motion information rather than the individual motion information, the proposed Co-LSTSM explicitly models the concurrently inter-related dynamics between interacting people. The most related works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref> either combine the individual dynamics of each person or treat the two interacting people as a whole. To our best knowledge, our work is the first time to model the concurrently long-term inter-related dynamics over time between interacting people by the LSTM-based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminary: RNN for Individual Action</head><p>Given an input video clip {x t ∈ R n |t = 1, · · · , T } with the length T , RNN <ref type="bibr" target="#b35">[36]</ref> models its dynamics through a sequence of hidden states {h t ∈ R m |t = 1, · · · , T } with M hidden units, which can be mapped to an output sequence {z t ∈ R k |t = 1, · · · , T } (k is the number of the classes of actions), i.e.,</p><formula xml:id="formula_0">h t = ϕ(W hx · x t + W hh · h t−1 + b h );</formula><p>(1)</p><formula xml:id="formula_1">z t = ϕ(W zh · h t + b z ),<label>(2)</label></formula><p>where ϕ(·) denotes tanh(·), W h * and W z * are the weight matrices, and b * is the bias vector. Finally, the output z t at time step t can be solved by a softmax function, i.e.,</p><formula xml:id="formula_2">y t,l = exp(z t,l )/ j=1 exp(z t,j ), where the j-th element z t,j</formula><p>denotes the encoding of the confidence score on the j-th class action. Due to the exponential decay in retaining the context information of video frames, Long Short-Term Memory <ref type="bibr" target="#b10">[11]</ref>, a variant of RNN, provides a solution by allowing the network to learn when to forget previous hidden states and when to update hidden states given new information <ref type="bibr" target="#b7">[8]</ref>.</p><p>Usually, each LSTM unit contains a memory cell (denoted by c t ) storing the memory of the input sequence up to the time step t. In order to store the memory w.r.t the motion information in the long time, three types of gates (i.e., input gate i t , forget gate f t and output gate o t ) are incorporated into the LSTM unit to control what information would enter and leave the memory cell over time <ref type="bibr" target="#b10">[11]</ref>, as follows,</p><formula xml:id="formula_3">i t = σ(W ix · x t + W ih · h t−1 + b i );</formula><p>(3)</p><formula xml:id="formula_4">f t = σ(W f x · x t + W f h · h t−1 + b f );<label>(4)</label></formula><formula xml:id="formula_5">o t = σ(W ox · x t + W oh · h t−1 + b o ),<label>(5)</label></formula><p>where σ(·) is a sigmoid function; W * x and W * h are the weight matrices; b * is the bias vector. In addition to three gates, the memory cell c t can be expressed as</p><formula xml:id="formula_6">c t = f s t c t−1 + i t g t ,<label>(6)</label></formula><p>where g t = ϕ(W gx ·x t +W gh ·h t−1 +b g ), and denotes the element-wise product. Finally, a hidden state h t at time step t can be expressed as</p><formula xml:id="formula_7">h t = o t ϕ(c t ).<label>(7)</label></formula><p>4. The Proposed Co-LSTSM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The Architecture</head><p>For person-person interaction recognition, each video frame contain two concurrent individual actions from interacting people, some of which are inter-related with each other. Existing LSTM models targeting to singe-person actions cannot handle the person-person interactions well. As mentioned before, we can roughly treat two interacting people as a whole before training the LSTM network. However, this solution will bring in some individual-special motion information. Besides, we can also model the individual dynamics of each person by two LSTM networks respectively, and then naively combine (e.g., concatenate or pool) the output sequences from two LSTM networks into the final representation. However, it is intuitive that this strategy loses some concurrently inter-related motion information between interacting people.</p><p>To this end, we propose a Concurrence-Aware Long Short-Term Memories (Co-LSTSM) to capture the concurrently inter-related dynamics between interacting people rather than the individual dynamics of each person. Our key idea is to develop two sub-memory units to store the individual motion information of each person respectively, and a concurrent LSTM unit to selectively integrate and store the concurrently inter-related motion information between interacting people from the individual motion information. <ref type="figure">Figure 2</ref> illustrates the architecture of a concurrent LSTM unit of the proposed Co-LSTSM. Overall, the concurrent LSTM unit at each time step consists of two specific sub-memory units, two cell gates, a common output gate and a new co-memory cell. Specifically, these two sub-memory units include their respective input gates, forget gates, memory cells. And the co-memory cell between two sub-memory units selectively integrates the individual motion information from two memory units and memorizes the inter-related motion information.</p><p>Formally, {x a t ∈ R n |t = 1, · · · , T } and {x b t ∈ R n |t = 1, · · · , T } denote two sequences of two concurrent people, respectively; i a t , f a t and c a t denote the input gate, forget gate and sub-memory cell in sub-memory unit 1 at time step t, respectively; i b t , f b t and c b t denote the input gate, forget gate and sub-memory cell in sub-memory unit 2 at time step t, respectively. All of them can be expressed in the following equations</p><formula xml:id="formula_8">i s t = σ(W s ix · x s t + W s ih · h t−1 + b s i ), s ∈ {a, b}; (8) f s t = σ(W s f x · x s t + W s f h · h t−1 + b s f ), s ∈ {a, b}; (9) g s t = ϕ(W s gx · x s t + W s gh · h t−1 + b s g ), s ∈ {a, b}; (10) c s t = f s t c s t−1 + i s t g s t , s ∈ {a, b},<label>(11)</label></formula><p>where W s * x and W s * h are the weight matrices, and b * is the bias vector.  <ref type="figure">Figure 2</ref>. Illustration of a concurrent LSTM unit in the proposed Co-LSTSM. For the concurrent inputs x a t and x b t at time step t, a concurrent LSTM unit consists of two specific sub-memory units, a common output gate ot, two new cell gates (i.e., π a t and π b t ) and a new co-memory cell ct. These two sub-memory units includes the respective input gates (i.e., i a t and i b t ), forget gates (i.e., f a t and f b t ), sub-memory cells (i.e., c a t and c b t ). In particular, two submemory cells (i.e., c a t and c b t ) are jointly fed into the co-memory cell ct, followed by the hidden representation ht.</p><p>Two cell gates π a t and π b t following the sub-memory unit 1 and the sub-memory unit 1 respectively aim to control what memories from two sub-memory units enter and leave at each time step. Unlike the traditional gates, the cell gate π s t (s ∈ {a, b}) is activated by a nonlinear function of two inputs x a t and x b t and the past hidden state h t−1 , i.e.,</p><formula xml:id="formula_9">π s t = σ(W s πx · x s t + W πh · h t−1 + b π ), s ∈ {a, b},<label>(12)</label></formula><p>where s ∈ {a, b}, W π * are the weight matrices, and b π is the bias vector. Based on the consistent interactions between two interacting people, these two cell gates π s t (s ∈ {a, b}) allow more concurrently inter-related motion information between interacting people to enter the co-memory cell c t and contribute to one common hidden state. In this work, the co-memory cell c t can be expressed as</p><formula xml:id="formula_10">c t = π a t c a t + π b t c b t .<label>(13)</label></formula><p>In the concurrent LSTM unit, two sub-memory units share a common output gate o t . The activation of the cell gate o t is similar to the activation of the cell gate, i.e.,</p><formula xml:id="formula_11">o t = σ(W ox · x a t x b t + W oh · h t−1 + b o ).<label>(14)</label></formula><p>Finally, a hidden state h t at time step t can be expressed as</p><formula xml:id="formula_12">h t = o t ϕ(c t ).<label>(15)</label></formula><p>Briefly, at time step t, the proposed Co-LSTSM model proceeds in the following order.</p><p>• Compute input gates i s t and forget gates f s t by Eq (8) and Eq (9), respectively;</p><p>• Update sub-memory cells c s t by Eq (11); • Compute cell gates π s t by Eq (12); • Compute co-memory gate c t by Eq (13);</p><p>• Compute output gate o t by Eq <ref type="formula" target="#formula_4">(14)</ref>;</p><p>• Output h t by Eq (15).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Learning Algorithm</head><p>We employ a loss function to learn the model parameters of Co-LSTSM by measuring the deviation between the target class l t and y t at time step t, i.e., (y t , l t )= − log y t,lt . Both types of loss functions can be minimized by Back Propagation Through Time (BPTT) algorithm <ref type="bibr" target="#b5">[6]</ref>, which unfolds the Co-LSTSM model over several time steps and then runs the back propagation algorithm to train the model. specifically, LSTM usually uses the truncated BPTT to prevent the back-propagation errors. The idea is that once the back-propagated error leaves the LSTM unit or gates, it will not be allowed to enter the LSTM unit again. Here, we also do not allow the errors to re-enter the concurrent LSTM unit once they leave the co-memory cell.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Dataset</head><p>We conduct experiments to evaluate the performance of the proposed Co-LSTSM by comparing with the state-ofthe-art methods and some baselines on the following two widely-used benchmarks.</p><p>BIT dataset <ref type="bibr" target="#b15">[16]</ref>. It consists of eight classes of human interactions, i.e., bow, boxing, handshake, high-five, hug, kick, pat, and push. In each class, there are 50 videos, which are captured in real scenarios within the cluttered backgrounds. For some videos, there are partially occluded bodies, moving objects, as well as devise appearances, scales, poses, illuminations and viewpoints. Following the setting in <ref type="bibr" target="#b16">[17]</ref>, 34 videos per class are randomly chosen as training data and the remaining ones for testing.</p><p>UT dataset <ref type="bibr" target="#b28">[29]</ref>. It consists of ten videos, each of which contains six classes of human interactions, i.e., handshake, hug, kick, point, punch and push. These videos are captured with different scales and illuminations. The authors provide the interaction labels for each frame. After extracting the frames, we obtain 60 video clips in total, namely 10 video clips per class. The leave-one-out cross validation training strategy is adopted for the experiments, i.e., nine video clips per class are used for training while the remaining one for cross validation. Finally, averaged accuracy on 10 times is reported as the final performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>In the preprocessing step, the bounding box corresponding to each interacting person is detected and tracked over all frames by an object detector <ref type="bibr" target="#b9">[10]</ref> and object tracker <ref type="bibr" target="#b38">[39]</ref>. Since some works validated that placing the LSTM network on fc6 of CNN performs better than fc7 of CNN <ref type="bibr" target="#b7">[8]</ref>, we employ the pre-trained AlexNet model <ref type="bibr" target="#b18">[19]</ref> to extract the two types of fc6 features on two bounding boxes around two concurrent people, respectively.</p><p>For BIT dataset and UT dataset, the length T of time steps is set to 30 and 40, respectively. The sub-memory cell nodes are set 2048 on both BIT and UT. The time steps of each video clip in BIT dataset and UT dataset are set 30 and 40 respectively. We use Torch toolbox and Caffe as the deep learning platform and a NVIDIA Tesla K20 GPU to run the experiments. The learning rate, momentum and decay rate are set 1 × 10 −5 , 0.9 and 0.95, respectively. We plot the learning curve for training Co-LSTSM model on BIT dataset and UT dataset in <ref type="figure" target="#fig_1">Figure 3</ref>. We can see that the training of Co-LSTSM begins to converge after about 600 and 1300 epochs on the BIT dataset and the UT dataset, respectively.</p><p>In experiments, three baselines are conducted to illustrate the novelty of the proposed Co-LSTSM.</p><p>• Person-box CNN. The pre-trained AlexNet model is deployed on two bounding boxes around the two concurrent people at each time step respectively, where two fc6 features corresponding to two interacting people are concatenated into a long vector. Then the concatenated features over all time steps are pooled into a single feature. All features from each video clip are trained and tested on the softmax classifier. This baseline can illustrate the importance of deep features.</p><p>• One CNN+LSTM. This baseline treats two individual actions as a whole. First, two bounding boxes corresponding two interacting people at each time step are merged into a bigger bounding box. Second, fc6 features are extracted by AlexNet on this "bigger" bounding box at each time step. Third, we use the fc6 features at each time step as inputs to train a LSTM model. The model of this baseline is similar to Longterm Recurrent Convolutional Networks (LRCN) <ref type="bibr" target="#b7">[8]</ref>.</p><p>• Two CNN+LSTM. This baseline models the individual dynamics of two people by two LSTM networks, respectively. First, AlexNet is deployed on the two bounding boxes around two interacting people at each time step to extract fc6 features. Second, fc6 features from two individuals are feed to one LSTM networks to capture the individual dynamics, respectively. Third, the softmax scores output from these two LSTM  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results on the BIT dataset</head><p>Comparison with baselines. <ref type="table">Table 1</ref> shows the recognition accuracy of the proposed Co-LSTSM compared with the baselines. As shown in this table, Co-LSTSM significantly outperforms the baseline methods. We can see that adding the temporal information by employing LSTM (i.e., "One CNN+LSTM", and "Two CNN+LSTM") improves the performance of "Person-box CNN" without temporal information. In particular, "Two CNN+LSTM" achieves the higher accuracy than "One CNN+LSTM". It is illustrated that an single LSTM model can capture a single motioning object better than multiple motioning objects.</p><p>Comparison with state-of-the-art methods. We also compare Co-LSTSM with the state-of-the-art methods for person-person interaction recognition, i.e., hand-crafted spatio-temporal interest points <ref type="bibr" target="#b6">[7]</ref> based methods of Lan et al. <ref type="bibr" target="#b20">[21]</ref>, Liu et al. <ref type="bibr" target="#b21">[22]</ref>, and Kong et al. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b13">14]</ref>, ws well as LSTM-based methods of Donahue et al. <ref type="bibr" target="#b7">[8]</ref> and Ke et al. <ref type="bibr" target="#b12">[13]</ref>. <ref type="table">Table 1</ref> lists the experimental results, in which some results are reported in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14]</ref>. We can see Co-LSTSM performs better than the comparative methods, especially all LSTM-based methods, i.e., Donahue et al. <ref type="bibr" target="#b7">[8]</ref> and Ke et al. <ref type="bibr" target="#b12">[13]</ref>. In particular, compared with the state-of-the-art LSTM-based methods (i.e., Ke et al. <ref type="bibr" target="#b12">[13]</ref> with 85.20%), Co-LSTSM has gained about 8% improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results on the UT dataset</head><p>Comparison with baselines. <ref type="table">Table 2</ref> shows the recognition accuracy of the proposed Co-LSTSM compared with the baselines. It is observed that Co-LSTSM performs consistently better than all baselines. "One CNN+LSTM" and "Two CNN+LSTM" considering the temporal information performs better than "Person-box CNN" without temporal information. In particular, "Two CNN+LSTM" achieves the better performance than "One CNN+LSTM".</p><p>Comparison with state-of-the-art methods.</p><p>Co-LSTSM is also compared with the state-of-the-art methods, including some traditional methods (i.e., Ryoo et al. <ref type="bibr" target="#b28">[29]</ref>, Yu et al. <ref type="bibr" target="#b37">[38]</ref>, Kong et al. <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b13">14]</ref>, Raptis &amp; Sigal <ref type="bibr" target="#b25">[26]</ref>, Shariat &amp; Pavlovic <ref type="bibr" target="#b30">[31]</ref>, and Zhang et al. <ref type="bibr" target="#b39">[40]</ref>), deep learning method (i.e., Wang et al. <ref type="bibr" target="#b34">[35]</ref>), as well as LSTM-based methods (i.e., Ke et al. <ref type="bibr" target="#b12">[13]</ref> and Donahue et al. <ref type="bibr" target="#b7">[8]</ref>). The comparison results are shown in <ref type="table">Table 2</ref>. We can see that Co-LSTSM also achieves the state-of-the-arts result, i.e., 95.00% by Zhang et al. <ref type="bibr" target="#b39">[40]</ref> and Wang et al. <ref type="bibr" target="#b34">[35]</ref>. It is noted that Wang et al. <ref type="bibr" target="#b34">[35]</ref> adopted deep context features on the event neighborhood, where the size of event neighborhood need be manually defined in the preprocessing step; Zhang et al. proposed a spatio-temporal phrase to capture a certain number of local movements between interacting people, where the number of local movements increases when the interaction becomes complex. As new exploration by leveraging LSTM model, the proposed Co-LSTSM performs better than other LSTM-based methods, i.e., Donahue et al. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Evaluation on Human Interaction Prediction</head><p>In this work, we also evaluate the proposed Co-LSTSM on human interaction prediction. Unlike person-person interaction recognition, human interaction prediction is defined to recognize an ongoing interaction activity before the interaction is completely executed <ref type="bibr" target="#b12">[13]</ref>. Due to the large variations in appearance and the evolution of scenes, interaction prediction at an early stage is a challenging task.</p><p>Following experimental setting in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b14">15]</ref>, a testing video clip is divided into 10 incomplete action executions by using 10 observation ratios (i.e., from 0 to 1 with a step size of 0.1), which represent the increasing amount of sequential data with time. For example, given a testing video clip with the length T , a prediction accuracy under an observation ratio of 0.3 denotes that the accuracy is tested with the first length 0.3 × T frames. When the observation ratio is 1, namely the entire video clip is used, Co-LSTSM acts as the person-person interaction recognition model.</p><p>The comparative methods includes Dynamic Bagof-Words (DBoW) <ref type="bibr" target="#b26">[27]</ref>, Sparse Coding (SC) <ref type="bibr" target="#b2">[3]</ref>, Sparse Coding with Mixture of training video Segments (MSSC) <ref type="bibr" target="#b2">[3]</ref>, Multiple Temporal Scales based on SVM (MTSSVM) <ref type="bibr" target="#b17">[18]</ref>, Max-Margin Action Prediction Machine (MMAPM) <ref type="bibr" target="#b14">[15]</ref>, Long-term Recurrent Convolutional Networks (LRCN) <ref type="bibr" target="#b7">[8]</ref>, and Spatial-Structural-Temporal Feature Learning (SSTFL) <ref type="bibr" target="#b12">[13]</ref>. The comparison results on the BIT dataset with different observation ratios are listed in <ref type="figure" target="#fig_4">Figure 4</ref>. Overall, Co-LSTSM outperforms all comparative methods for all observation ratios. Specifically, we can see that 1) the improvement of Co-LSTSM is more significant when the observation ratio is 0.6; 2) the accuracy of Co-LSTSM increases rapidly when the observation ratio is 0.5, which illustrates the close interaction is happening; and 3) the accuracy of Co-LSTSM becomes stable when the observation ratio is 0.7, which illustrates the close interaction is ending.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and Future Work</head><p>In this work, for person-person interaction recognition, we propose a novel Concurrence-Aware Long Short-Term Sub-Memories (Co-LSTSM) to aggregate the interactive motions between interacting people over time. Specifically, interacting people at each time step are jointly modeled by a novel concurrent LSTM unit, which captures the concurrently inter-related motion information from two submemory units. Experimental results on person-person interaction recognition and prediction have demonstrated the superior performance of the proposed Co-LSTSM compared with the state-of-the-art methods. In future, we will extend Co-LSTSM for addressing the problem of complex group collective activity analysis. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgments</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Objective loss curve over training epochs.networks are fused. This idea of this baseline is the same as Two-Stream Convolutional Networks<ref type="bibr" target="#b31">[32]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>[ 8 ]</head><label>8</label><figDesc>and Ke et al.<ref type="bibr" target="#b12">[13]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Performance of human interaction prediction on BIT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>This work was supported by the National Key Research and Development Program of China (Grant No. 2016YFB1001001), the National Natural Science Foundation of China (Grant No. 61522203 and 61672285), the Natural Science Foundation of Jiangsu Province (Grant No. BK20140058), the Fundamental Research Funds for the Central Universities (Grant No. 30917015105), and the National Ten Thousand Talent Program of China (Young Top-Notch Talent).</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monte carlo tree search for scheduling activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cost-sensitive top-down/bottom-up inference for multiscale activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recognize human activities from partially observed videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning personperson interaction in collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1905" to="1918" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified framework for multitarget tracking and collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An application of non-linear programming to train recurrent neural networks in time series prediction problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Cuéllar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Del Carmen Pegalajar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiménez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICEIS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VS-PETS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06040</idno>
		<title level="m">A hierarchical deep temporal model for group activity recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Spatial, structural and temporal feature learning for human interaction prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bossaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.05267</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Close human interaction recognition using patch-aware models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="167" to="178" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Max-margin action prediction machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1844" to="1858" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning human interaction by interactive phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Interactive phrases: Semantic descriptionsfor human interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1775" to="1788" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A discriminative model with multiple temporal scales for action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond actions: Discriminative models for contextual group activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative latent models for recognizing contextual group activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Robinovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1549" to="1562" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Structured learning of human interactions in tv shows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patron-Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2441" to="2453" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A survey on vision-based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="976" to="990" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Poselet key-framing: A model for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Human activity prediction: Early recognition of ongoing activities from streaming videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Recognition of composite human activities through context-free grammar based representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatio-temporal relationship match: Video structure comparison for recognition of complex human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A new adaptive segmental matching measure for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A discriminative key pose sequence model for recognizing human interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranjbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hierarchical context modeling for video event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multistream multi-class fusion of deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Real-time action recognition by spatiotemporal semantic and structural forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Gmcp-tracker: Global multi-object tracking using generalized minimum clique graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Spatiotemporal phrases for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Co-occurrence feature learning for skeleton based action recognition using regularized deep lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.07772</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
