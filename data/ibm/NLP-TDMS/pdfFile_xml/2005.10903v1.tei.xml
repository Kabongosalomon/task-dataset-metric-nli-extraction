<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SPOTFAST NETWORKS WITH MEMORY AUGMENTED LATERAL TRANSFORMERS FOR LIPREADING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peratham</forename><surname>Wiriyathammabhum</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<postCode>20742</postCode>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SPOTFAST NETWORKS WITH MEMORY AUGMENTED LATERAL TRANSFORMERS FOR LIPREADING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-lipreading</term>
					<term>deep learning</term>
					<term>memory aug- mented neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a novel deep learning architecture for word-level lipreading. Previous works suggest a potential for incorporating a pretrained deep 3D Convolutional Neural Networks as a front-end feature extractor. We introduce a SpotFast networks, a variant of the state-of-the-art Slow-Fast networks for action recognition, which utilizes a temporal window as a spot pathway and all frames as a fast pathway. We further incorporate memory augmented lateral transformers to learn sequential features for classification. We evaluate the proposed model on the LRW dataset. The experiments show that our proposed model outperforms various state-ofthe-art models and incorporating the memory augmented lateral transformers makes a 3.7% improvement to the SpotFast networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Lipreading or visual speech recognition is an interesting ability to recognize words from lip movements representing phonemes. Those lip movements are also named as visual speeches/sounds or visemes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Visemes for different letters such as 'p' and 'b' can be very similar due to McGurk effect <ref type="bibr" target="#b2">[3]</ref>. These letters are called homophones which are ambiguous from visual cues but can be disambiguated using additional language cues such as neighboring characters. Lipreading has many beneficial real world applications such as surveillance or assistive systems. This paper focuses on word-level automatic lipreading where the system tries to recognize a word being said given only a video sequence of moving lips without audio. The standard lipreading system pipeline includes mouth region cropping, mouth region compression and sequence modelling. Recently, there has been significant improvements in automatic lipreading based on deep learning systems. The current state-of-the-art <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> consists of a 3D Convolutional Neural Networks (3DCNNs) as a front-end feature extractor and a 2-layered bidirectional Long-short Term Memory (biLSTM) as a back-end classifier. The system classifies the moving lips into 1-of-n words in a multi-class classifica- <ref type="figure">Fig. 1</ref>. SpotFast networks with lateral transformers utilize two pathways, consisting of a temporal window pathway and an all frame pathway. The temporal window pathway models fast evolving actions. The all frame pathway models the whole video which can incorporate more temporal contexts into the system. The adaptive average pooling layer reshapes the pre-fused features. The dual temporal convolution fuse both streams temporally and output final word-classes prediction probabilities. The lateral transformers further model sequential information. tion setting. Another approach <ref type="bibr" target="#b5">[6]</ref> uses a transformer as a back-end instead of a biLSTM.</p><p>We combine the strength from both approaches. First, we use a varient of the state-of-the-art 3DCNNs, SpotFast networks, which we replace the slow pathway of the SlowFast networks <ref type="bibr" target="#b6">[7]</ref> with the spot pathway (temporal window) to model fast lip gestures along with a fixed spotted estimation of their temporal viseme boundaries. We also utilize a pretrained SlowFast model convolutional parameters from the Kinetics-400 dataset <ref type="bibr" target="#b7">[8]</ref> by introducing an additional adaptive average pooling layer to reshape the convolutional feature from the all frame pathway into a fixed temporal window size of the spot pathway. We additionally put a dual temporal convolution layer on top of the networks to temporally convolve each pathway with different kernel sizes then fuse them into word-classes prediction. By using this dual temporal convolution back-end, we observe a more stabilize training empirically.</p><p>Second, we introduce memory augmented lateral transformers to further model temporal sequential information from the feature set extracted from convolutional layers of the SpotFast networks. Lateral transformers are two vanilla transformers with a lateral connection after each hidden layer which fuse features from the all frame pathway to the temporal window pathway. We also augment the product-key memory <ref type="bibr" target="#b8">[9]</ref> to the layer before the last of each pathway to increase the model capacity and stability. Utilizing the three stage training procedure <ref type="bibr" target="#b9">[10]</ref>, we achieve the new state-ofthe-art for word-level lipreading using only RGB/grayscale input.</p><p>The contributions of this paper are (i) We propose a novel state-of-the-art deep learning architecture for end-toend word-level lipreading. (ii) We evaluate the effect of the temporal window sizes on our proposed SpotFast networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head><p>Pretrained video models. The state-of-the-art action recognition models are primary choices when we want to perform transfer learning and finetune video models for other tasks. Lipreading in the deep learning era incorporates recent advancements in action recognition such as C3D (3D convolution) <ref type="bibr" target="#b10">[11]</ref> or I3D (deep 3DCNNs) <ref type="bibr" target="#b7">[8]</ref> as a better front-end and achieves state-of-the-art <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4]</ref>. We further incorporate another recent advancement in action recognition, SlowFast networks <ref type="bibr" target="#b6">[7]</ref>, as our front-end feature extractor. Transformers. As a recent promising alternative to biLSTMs and biGRUs in sequence modelling, a transformer model <ref type="bibr" target="#b12">[13]</ref> is widely deployed in many state-of-the-art systems in various NLP tasks. A transformer consists of a stack of multihead self attention and feed forward modules in an autoencoder setting. There is no recurrence in the model so training a transformer can be easily paralleled. Lipreading also incorporates the transformer model and makes substantial improvements on many datasets <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b5">6]</ref>. We follow this direction and propose a lateral transformer for sequence modelling in our SlowFast-based architecture. Memory. Incorporating memory is an approach to increase the capacity of a neural networks without increasing too much computation. Those memory augmented neural networks can be an efficient and effective way to represent variable length inputs for question answering <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> or learning with limited data for few-shot learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. A recently proposed product-key memory <ref type="bibr" target="#b8">[9]</ref> is a promising neural network layer which can be incorporated into transformer-based models and greatly increase the capacity with only half computation. The memory holds a table of key-value entries with a multi-head mechanism which can be trained end-to-end. Each memory head has its own query networks and sub-keys but shares the same values with other heads. For each head, an input query will be compared to all keys in a nearest neighbor setting and the sparse weighted-sum of the corresponding memory values of the k nearest keys will be the output. The output of the memory layer will be the sum of all outputs from all heads. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">SpotFast Networks</head><p>SlowFast networks <ref type="bibr" target="#b6">[7]</ref> consist of two pathways, slow and fast. The slow pathway is not suitable for lipreading since lip movements are fast evolving actions. We use the knowledge that the target word to be lipreaded for word-level lipreading is always keyword spotted. We then propose an alternative spot pathway to the slow pathway such that the fast evolving action will be captured by the networks. The spot pathway is a temporal window centered at the keyword-spotted frame. We keep the fast pathway as all frames which is the same as the original SlowFast networks. To fuse fast pathway to spot pathway via lateral connections, we use convolution fusion as in the original SlowFast networks with additional adaptive average pooling to temporally reshape the features from all frames into a fixed length temporal window. We use the SpotFast networks as a front-end to extract spatio-temporal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dual 1D Temporal Convolution Networks (TC)</head><p>For the back-end, we deploy a simple two-layered 1D temporal convolution to aggregate temporal information on each pathway. The output from each pathway of the SpotFast networks is average-pooled to 3 modes, [batch size, feature size, time step]. The first layer doubles the in channel using a filter with a kernel size of 3 and a stride of 2 (temporal window) or a kernel size of 5 and a stride of 2 (all frames). Then, we apply a sequence of batchnorm, ReLU activation and max pooling with a kernel size of 2 and a stride of 2. The second layer has the same parameters as the first layer while doubles the out channel from the first layer to quadruple the initial in channel. Next, the features from both pathways are averaged-pooled and fused using concatenation. Lastly, a linear feed forward layer maps the fused feature into word-class probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Memory Augmented Lateral Transformers</head><p>To increase the capacity of the back-end, we put a transformer encoder on top of each pathway of the SpotFast networks to further learn features for classification. For each pathway, we use a 6-layered transformer encoder (base model) consisting of a multi-head attention and a feed forward module which maps the input features in an autoencoder setting. We augment each transformer with the product-key memory at the layer before the last as in <ref type="bibr" target="#b8">[9]</ref> (layer 5 for the base transformer model) to increase the capacity and stabilize training. The output feature after the feed forward layer is fed into the memory and is added to the output of the memory module via a skip connection. However, we empirically observe that independently putting the transformer on top of each pathway makes learning difficult. The loss does not going down in a reasonable rate or even diverged. To solve this problem, we add lateral connections to all layers (except the last layer) of both transformer encoders from the all frame pathway to the temporal window pathway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation details</head><p>We use the Kinetics-400 pretrained model from the 8x8 ResNet-50 SlowFast networks with non-local blocks. The temporal window pathway has the feature size of 2048. The all frame pathway has the feature size of 256. The outputs of both pathway are average-pooled with a kernel size of <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b3">4]</ref> and a stride of 1.</p><p>The transformer encoder has 8 heads. The positional encoders for the same position in both pathway encode with the same values by using offset index added to the temporal window. The positional encoders has a dropout of 0.1. The lateral connections are consisted of 1x1x1 convolutions with padding of 1 and an adaptive average pooling in the temporal dimension for the all frame pathway. Then, the features from the temporal window pathway and the lateral connection are fused by concatenation followed by a sequence of linear feed forward, batchnorm and ReLU activation.</p><p>The product-key memory has 4 heads. The vectors are of 128 dimensions. The memory uses 32 nearest neighbors. We use query batchnorm and a value dropout of 0.1. The temporal window pathway has 168 2 values and the all frame pathway has 50 2 values. We apply a layernorm to the output of the memory.</p><p>Our implementation is based on the PyTorch library <ref type="bibr" target="#b18">[19]</ref>. All models are trained using 4 NVIDIA P6000 GPUs and 16 CPUs. The networks are trained using the Adam optimiser <ref type="bibr" target="#b19">[20]</ref> and label smoothed cross entropy loss with the smoothing parameter of 0.1. In the first phrase, we train the SpotFast networks end-to-end using an initial learning rate of 2.5e − 4, a weight decay of 1e − 4 and a batch size of 84 for 10 epochs. In the second phase, we fixed the parameters of the Spot-Fast networks and train only the lateral transformers and the dual temporal convolutions using an initial learning rate of 2.25e − 4, a weight decay of 3e − 4 and a batch size of 84 for 5 epochs. In the third phrase, we finetune the whole network end-to-end using an initial learning rate of 1.566e − 4, a weight decay of 1e − 4 and a batch size of 64. We use the cosine annealing scheduler with restart <ref type="bibr" target="#b20">[21]</ref> where the parameters are T 0 = 5, T mul = 1 and eta min = 0 for all phrases. We use a linear warmup with 2000 steps for the first phrase and 1000 steps for the second and the third phrases. For the third phrase, we chain the ReduceLR scheduler in addition to the cosine scheduler and reduce the learning rate by a factor of 2 when the validation loss does not decrease. The third phrase takes 30 epochs in total.</p><p>The codebase along with the pretrained models will be released to foster research in the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset, preprocessing and augmentations</head><p>We conduct the experiments using the Lip Reading in the Wild (LRW) dataset <ref type="bibr" target="#b21">[22]</ref>. The LRW dataset consists of 488,766 training, 25,000 validation and 25,000 testing short video clips. The video clips are talking face videos extracted from BBC TV broadcasts. There are 500 target words in this dataset. For each video clip, there are 29 frames. We <ref type="table">Table 1</ref>. Top-1 test accuracies of the state-of-the-art methods on the LRW dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy LRW <ref type="bibr" target="#b21">[22]</ref> 61.1 WAS <ref type="bibr" target="#b22">[23]</ref> 76.2 ResNet+biLSTMs <ref type="bibr" target="#b11">[12]</ref> 83.0 ResNet+biGRUs <ref type="bibr" target="#b4">[5]</ref> 83.4 ResNet+focal block+transformer <ref type="bibr" target="#b5">[6]</ref> 83.7 I3D+biLSTMs <ref type="bibr" target="#b3">[4]</ref> 84.1 SpotFast with lateral transformers 84.4 (ours) use RGB frames in our experiments because the pretrained Kinetics models are in RGB. The dataset contains words with similar visemes such as 'SPEND' and 'SPENT' (tenses) or 'BENEFIT' and 'BENEFITS' (plural forms). The words are not totally isolated and the word boundaries are not given. Those words are surrounded by irrelevant parts of the utterances which may provide contexts or are just noises.</p><p>The preprocessing consists of cropping mouth regions using fixed coordinates since the LRW dataset is already centered spatially. We then normalize all frames into [0, 1], subtract them with 0.45 and divide them with 0.225 which are preprocessing parameters from the Kinetics pretrained models.</p><p>We perform data augmentations like other previous works. The augmentations are done during training consisting of random upsampling <ref type="bibr">([122, 146]</ref>), random crop (crop to 112 × 112 pixels) and random horizontal flip (with a probability of 0.5). We augment the same way for every frame in both pathways. During validation and testing phase, we upsampling to 122 × 122 pixels and uniform crop to 112 × 112 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with the state-of-the-arts</head><p>We summarize the state-of-the-art methods in <ref type="table">Table.</ref>1. Our final SpotFast with memory-augmented lateral transformers outperform previous state-of-the-art methods, including the one using optical flow information. Our proposed method makes an improvement over the prior state-of-the-art <ref type="bibr" target="#b3">[4]</ref> by 0.3% and the prior RGB/grayscale state-of-the-art <ref type="bibr" target="#b5">[6]</ref> by 0.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The effects of the window sizes in SpotFast Networks</head><p>We determine the optimal temporal window size for the temporal window pathway using a grid search over a set of 3 values, {15, 19, 23}. The validation and test accuracies are summarized in <ref type="table">Table.</ref>2. The validation and test accuracies are comparable (SpotFast front-end increases the accuracy from 74.6% of ResNet+temporalConv <ref type="bibr" target="#b11">[12]</ref>.) with the window size of 23 being the best (6.1% increase in accuracy.). We then proceed the training of the SpotFast networks to the next phase with the temporal window size of 23. The 3 values are heuristics from the word-boundary statistics estimated from the training partition of the LRW dataset. The word boundary distribution has the mean of 10.59 (We round it to 11.) and the standard deviation of 3.2 (We round it to 4.). For an approximately normal distribution, we use the 68-95-99.7 rule (three-sigma rule of thumb which follows the cumulative distribution function of the normal distribution. It will become 0-75-89 by Chebyshev's inequality for any distributions.) to create 3 values where the bands will cover 68-95-99.7 percents of the data population using {µ + σ, µ + 2σ, µ + 3σ} which becomes {15, 19, 23}. We can also observe from Table.2 that incorporating the lateral transformers into SpotFast networks and training the whole system end-to-end can further increase the test accuracy upto 3.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SUMMARY</head><p>We propose a SpotFast networks with lateral transformers for word-level lipreading. We utilize a two-stream deep 3DC-NNs, a temporal window pathway and an all frame pathway. We evaluate a heuristics based on the 68-95-99.7 rule to select the optimal temporal window size. We show that incorporating lateral transformer can improve the accuracy for 3.7%. We also show that our proposed model can outperform various state-of-the-art models. Some possible future directions include incorporating an optical flow input to the model and increasing the augmented-memory capacity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>SpotFast networks Fig. 3. Dual Temporal Convolutions 3. PROPOSED METHOD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Memory Augmented Lateral Transformers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>The top-1 accuracies of the SpotFast networks (without lateral transformers) varying the temporal window sizes.</figDesc><table><row><cell cols="3">Temporal window size Validation Test</cell></row><row><cell>SpotFast-15</cell><cell>81.3</cell><cell>80.6</cell></row><row><cell>SpotFast-19</cell><cell>81.2</cell><cell>80.4</cell></row><row><cell>SpotFast-23</cell><cell>81.5</cell><cell>80.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic units of visual speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sarah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshe</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry-John</forename><surname>Mahler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Theobald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM SIGGRAPH/Eurographics Symposium on Computer Animation. Eurographics Association</title>
		<meeting>the ACM SIGGRAPH/Eurographics Symposium on Computer Animation. Eurographics Association</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="275" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Phoneme-to-viseme mappings: the good, the bad, and the ugly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Helen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harvey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Speech Communication</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="40" to="67" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hearing lips and seeing voices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Mcgurk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Macdonald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="issue">5588</biblScope>
			<biblScope unit="page">746</biblScope>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning Spatio-Temporal Features with Two-Stream Deep 3D CNNs for Lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshuo</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kris</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingehuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feipeng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6548" to="6552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatio-temporal fusion based convolutional sequence learning for lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large memory layers with product keys</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludovic</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hervé</forename><surname>Jégou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8546" to="8557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-toend visual speech recognition with lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stavros</forename><surname>Petridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2592" to="2596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Combining residual networks with lstms for lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Themos</forename><surname>Stafylakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3652" to="3656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Triantafyllos</forename><surname>Afouras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon</forename><forename type="middle">Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Compound memory networks for few-shot video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="751" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lip reading in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Son</forename><surname>Joon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="87" to="103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lip reading sentences in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon Son</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3444" to="3453" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
