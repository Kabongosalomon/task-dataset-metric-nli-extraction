<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Fusion Generative Adversarial Networks for Text-to-Image Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songsong</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Guangdong University of Petrochemical Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyuan</forename><surname>Jing</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Guangdong University of Petrochemical Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Wuhan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingkun</forename><surname>Bao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Posts and Telecommunications</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Fusion Generative Adversarial Networks for Text-to-Image Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Synthesizing high-quality realistic images from text descriptions is a challenging task. Almost all existing textto-image Generative Adversarial Networks employ stacked architecture as the backbone. They utilize cross-modal attention mechanisms to fuse text and image features, and introduce extra networks to ensure text-image semantic consistency. In this work, we propose a much simpler, but more effective text-to-image model than previous works. Corresponding to the above three limitations, we propose: 1) a novel one-stage text-to-image backbone which is able to synthesize high-quality images directly by one pair of generator and discriminator, 2) a novel fusion module called deep text-image fusion block which deepens the text-image fusion process in generator, 3) a novel target-aware discriminator composed of matching-aware gradient penalty and one-way output which promotes the generator to synthesize more realistic and text-image semantic consistent images without introducing extra networks. Compared with existing text-to-image models, our proposed method (i.e., DF-GAN) is simpler but more efficient to synthesize realistic and text-matching images and achieves better performance. Extensive experiments on both Caltech-UCSD Birds 200 and COCO datasets demonstrate the superiority of the proposed model in comparison to state-of-the-art models. The source code and trained models are available at https://github.com/tobran/DF-GAN .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The last few years have witnessed the great success of Generative Adversarial Networks (GANs) <ref type="bibr" target="#b4">[5]</ref> for a variety of applications. Among them, text-to-image synthesis is one of the important applications of GANs. It aims to generate realistic and text-consistent images from given natural language descriptions. Due to its practical applications, text-to-image synthesis has become an active research area recently <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Most recently proposed text-to-image synthesis models are based on Stacked GANs <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> to generate high- resolution images, which employ concatenating and crossmodal attention to fuse text and image features <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref> and then introduce DAMSM loss <ref type="bibr" target="#b32">[33]</ref>, cycle consistency <ref type="bibr" target="#b22">[23]</ref>, and Siamese network <ref type="bibr" target="#b33">[34]</ref> to ensure the textimage semantic consistency by extra networks.</p><p>Although impressive results have been presented by previous work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b5">6]</ref>, there still remain three problems. First, the entanglements between generators in stacked architecture makes the final refined images look like a simple combination of fuzzy shape and some details (see <ref type="figure" target="#fig_0">Figure 1</ref>). Second, introducing extra network to ensure textimage semantic consistency increase the training complexity and bring a large additional computational cost. Third, simply concatenating text and image features can not make full use of text information, and cross-modal attention can only applied two times on 64×64 and 128×128 image features due to large computational cost. It makes the generator cannot fuse text and image features effectively.</p><p>To address these three issues, we propose a novel text-toimage generation method named as Deep Fusion Generative Adversarial Network (DF-GAN). For the first issue, we re-place the stacked backbone with a one-stage backbone composed of hinge loss <ref type="bibr" target="#b35">[36]</ref> and residual networks <ref type="bibr" target="#b7">[8]</ref>. Based on this one-stage text-to-image backbone, our model can synthesize high-resolution images directly with better object shapes and realistic fine-grained details.</p><p>For the second issue, we propose a novel targetaware discriminator composed of Matching-Aware Gradient Penalty (MA-GP) and one-way output to ensure textimage semantic consistency. MA-GP is a regularization strategy on the discriminator. By pushing the real and matching input pairs to the place where the gradients of the discriminator loss function are zero, the generator will be promoted to synthesize more realistic and text-image semantic consistent images. Simultaneously, MA-GP can stabilize the training process and ensure the text-image semantic consistency. Moreover, we point out that the previous two-ways output is harmful to text-image semantic consistency and slows the convergence process of the generator under MA-GP. To solve this problem, we replace it with a one-way output. One-way output can further promote the effectiveness of MA-GP and accelerate the convergence process of the generator. Compared with previous methods, our model can synthesize images with higher quality and better text-image semantic consistence without introducing extra networks.</p><p>For the third issue, we propose a Deep text-image Fusion Block (DFBlock) to effectively fuse the text information into image features. The DFBlock consists of several Affine Transformations. The Affine Transformation is a lightweight module which only manipulates the visual feature maps through channel-wise scaling and shifting operation. Stacking multiple DFBlocks at all image scales deepens the text-image fusion process which promotes the generator to fuse text and image features effectively. In addition, we concatenate the noise vector on the sentence vector and apply truncation operation on the noise, it converts a single, discrete sentence vector into a beam of sentence vectors to provide the generator with a stable and continuous text latent space. We call this process as skip-z with truncation.</p><p>Overall, our contributions are summarized as follows: • We propose a novel one-stage text-to-image backbone which can synthesize high-resolution images with higher image quality. • We propose a novel target-aware discriminator composed of Matching-Aware Gradient Penalty (MA-GP) and oneway output, it significantly improves the image quality and text-image semantic consistency, and accelerate the convergence of generator without introducing extra networks. • We propose a novel Deep text-image Fusion Block (DF-Block) which fuses text and image features more effectively and deeply.</p><p>• We propose a novel method called the skip-z with truncation to provide the generator with a stable and better text latent space for synthesizing higher-quality images. • Extensive qualitative and quantitative experiments on two challenging datasets demonstrate that the proposed DF-GAN outperforms exiting state-of-the-art text-to-image models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b4">[5]</ref> are an attractive framework that can be used to mimic complex real-world distributions by solving a min-max optimization problem between generator and discriminator <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b27">28]</ref>. For instance, Reed et al. first apply the conditional GAN to generate plausible images from text descriptions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26]</ref>. StackGAN <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> generates high-resolution images by stacking multiple generators and discriminators and provides the text information to the generator by naively concatenating text vectors to the input noises and intermediate features. Next, AttnGAN <ref type="bibr" target="#b32">[33]</ref> introduces the crossmodal attention mechanism to help the generator synthesize images with more details. MirrorGAN <ref type="bibr" target="#b22">[23]</ref> regenerates text descriptions from generated images for text-image semantic consistency <ref type="bibr" target="#b38">[39]</ref>. SD-GAN <ref type="bibr" target="#b33">[34]</ref> employs the Siamese structure <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> to distill the semantic commons from texts for image generation consistency. Obj-GAN <ref type="bibr" target="#b15">[16]</ref> generates complex scenes from pre-generated semantic layouts and text descriptions and employs a Fast R-CNN <ref type="bibr" target="#b3">[4]</ref> to compute an object-wise loss. Since Obj-GAN introduces bounding box and class label information besides text description, we did not compare it within our experiment. Finally, DM-GAN <ref type="bibr" target="#b39">[40]</ref> introduces the Memory Network <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32]</ref> to cope with the first aforementioned problem of stacked architecture. But memory network only alleviates the dependence on initial images, it does not solve the problem completely. And adding Memory Network on stacked architecture makes the network require a considerable computational cost. Recently, there are some new text-to-image methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b17">18]</ref> based on large transformer model that autoregressively models the text and image tokens as a single stream of data.</p><p>Our proposed method is much different from previous methods. First, our DF-GAN generates high-resolution images directly by a one-stage text-to-image backbone. Second, our model adopts a target-aware discriminator to ensure image quality and text-image semantic consistency without introducing extra network. Third, our model fuses text and image features more deeply and efficiently through a sequence of DFBlocks. Finally, the skip-z with truncation method is employed to provide the generator with a stable and continuous text latent space. Compared with previous models, our DF-GAN is simpler and more effective to synthesize realistic and text-matching images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Deep Fusion GANs</head><p>In this paper, we propose a novel cross-modal Generative Adversarial Network named as Deep Fusion GAN (DF-GAN) for text-to-image generation (see <ref type="figure" target="#fig_1">Figure 2</ref>). We start to present DF-GAN in this section. First, we introduce the overall structure of DF-GAN and also describe the components of the generator and discriminator. Then, we illustrate the proposed Matching-Aware Gradient Penalty (MA-GP), one-way output, Deep text-image Fusion Block (DFBlock), and the skip-z with truncation in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Overview</head><p>The goal of our proposed DF-GAN is to generate realistic and text-image semantic consistent images from given text descriptions. The entire network is composed of a generator, a discriminator, and pre-trained text encoder <ref type="bibr" target="#b32">[33]</ref>. In the following, we describe the structure of the generator and discriminator, respectively.</p><p>The generator has two inputs, a sentence vector that is encoded by text encoder and a noise vector sampled from the Gaussian distribution to ensure the diversity of generated images. The noise vector is first fed into a fully connected layer and the output is reshaped to (−1, 4, 4). We then apply a series of UPBlocks to upsample the image features. The UPBlock is composed of an upsample layer, a residual block, and DFBlocks (see <ref type="figure" target="#fig_4">Figure 5</ref>(d)) to fuse the text and image features during the image generation pro-cess. Finally, a convolution layer converts image features into images.</p><p>The discriminator is composed of several DownBlocks and convolution layers. First, the discriminator converts images into feature maps and the output is downsampled by a series of DownBlocks. Then the sentence vector will be replicated and concatenated on the image feature. An adversarial loss will be predicted to evaluate the visual realism and semantic consistency of inputs. By distinguishing generated images from real samples, the discriminator promotes the generator to synthesize images with higher quality and text-image semantic consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">One-Stage Text-to-Image Backbone</head><p>Previous text-to-image GANs are based on stacked architecture <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38]</ref> to generate high-resolution images. However, the entanglements between different generators makes the generated image look like a simple combination of coarse shape synthesized by G 0 and G 1 , and some details synthesized by G 2 (see <ref type="figure" target="#fig_0">Figure 1</ref>). It makes the final generated images unrealistic.</p><p>Inspired by recent models proposed for unconditional image generation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b35">36]</ref>, we propose a one-stage text-toimage backbone which can synthesize high-resolution images directly by one pair of generator and discriminator. We employ the hinge loss <ref type="bibr" target="#b16">[17]</ref> to stabilize the training process.</p><p>Since there is only one pair of generator and discriminator in our backbone, the model only attends to the quality of the final generated images. Without the entanglements between generated images of different scales in stacked architecture, our one-stage text-to-image backbone can control the object shape and fine-grained visual details in final generated images directly through a deeper generator network with a sequence of residual networks. The formulation of our onestage method is as follows:</p><formula xml:id="formula_0">L D = − E x∼Pr [min(0, −1 + D(x, e))] − (1/2)E G(z)∼Pg [min(0, −1 − D(G(z), e))] − (1/2)E x∼Pmis [min(0, −1 − D(x, e))] L G = − E G(z)∼Pg [D(G(z), e)]<label>(1)</label></formula><p>where z is the noise vector sampled from Gaussian distribution; e is the sentence vector; P g , P r , P mis denote the synthetic data distribution, real data distribution, and mismatching data distribution, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Target-Aware Discriminator</head><p>In this work, we also propose a novel Target-Aware Discriminator which is composed of Matching-Aware Gradient Penalty (MA-GP) and one-way output. The Target-Aware Discriminator promotes the generator to synthesize more realistic and text-image semantic-consistent images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Matching-Aware Gradient Penalty</head><p>Taking text-to-image synthesis as an example in <ref type="figure" target="#fig_2">Figure 3</ref>, there are four kinds of data pairs in two-dimensional data space: synthetic images with matching text, synthetic images with mismatched text, real images with matching text, real images with mismatched text. To generate textmatching and realistic images from given text descriptions, the discriminator should put real and matching data points to the minimum point of the discriminator loss function surface and put other inputs at high points. Moreover, the discriminator should ensure a smooth vicinity of real and matching data points to help the generator converge to the minimum point more easily. Therefore, we propose the Matching-Aware Gradient Penalty (MA-GP) which applies the gradient penalty on the target data point: real images with the matching sentences. The whole formulation of our model with MA-GP is as follows: where k and p are two hyper-parameters to balance the effectiveness of gradient penalty.</p><formula xml:id="formula_1">L D = − E x∼Pr [min(0, −1 + D(x, e))] − (1/2)E G(z)∼Pg [min(0, −1 − D(G(z), e))] − (1/2)E x∼Pmis [min(0, −1 − D(x, e))] + kE x∼Pr [( ∇ x D(x, e) + ∇ e D(x, e) ) p ] L G = − E G(z)∼Pg [D(G(z), e)]<label>(2)</label></formula><p>Compared with previous methods for ensuring the image quality and semantic consistency, our proposed MA-GP does not introduce extra networks to compute the textimage semantic similarity. We consider that the discriminator itself is a sufficiently strong network. The addition of extra networks is not necessary. But it is very important to construct a proper discriminator loss function surface to meet our expectations. In text-to-image generation task, we want the generator to synthesize realistic and text-image semantic consistent images. So we propose the MA-GP which can ensure that the real and text-matching data points are at the minimum points of the discriminator loss function surface, and the vicinity of real and text-matching data points is smooth. Armed with MA-GP, our model is able to synthesize more realistic images with better text-image semantic consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">One-Way Output</head><p>In previous text-to-image GANs <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33]</ref>, the discriminator first extracts the image feature through a series of downsampling operations. Then the image feature will be used in two ways. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, one way determines whether the image is real or fake, another way concatenates the image feature and sentence vector to evaluate text-image semantic consistency. So there are two kinds of loss computed, the unconditional loss and the conditional loss.</p><p>We found that the two-ways output weakens the effec- tiveness of MA-GP and slows the convergence of the generator. As depicted in <ref type="figure" target="#fig_2">Figure 3</ref>, the discriminator should put wrong data at high points and right data at minimum points. The proposed MA-GP ensures a smooth loss surface to help the generator converge to the right data (minimum points) through gradient descending. But the two-ways output decomposes the adversarial loss into conditional loss and unconditional loss. The conditional loss gives a gradient γ pointing to the real and matching inputs after backpropagation. But the unconditional loss gives a gradient β only pointing to the real images. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the final gradient γ + β does not point to the real and matching data points directly. This will make the convergence of the generator deviate from the target data point (real and match). Therefore, we propose the one-way output for text-toimage synthesis. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>(b), our discriminator concatenates the image feature and sentence vector, then outputs one adversarial loss through two convolution layers. The one-way output only gives one gradient γ pointing to the target data points (real and match) directly, it gives a more clear convergence goal to the generator. Armed with MA-GP and one-way output, the discriminator will guide the generator to synthesize more realistic images with better text-image semantic consistency more directly. Our experiments and ablation studies also demonstrate that the oneway output can further promote the effectiveness of MA-GP and accelerate the convergence process of the generator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Efficient Text-Image Fusion</head><p>To fuse text and image features efficiently, we propose a novel Deep text-image Fusion Block (DFBlock) which fuses text and visual information more effectively and deeply during the generation process. Furthermore, we propose the method called the skip-z with truncation to pro-vides the generator with a stable and better text latent space to synthesize higher-quality images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Deep Text-Image Fusion Block</head><p>In this work, we propose a novel Deep text-image Fusion Block (DFBlock) (see <ref type="figure" target="#fig_4">Figure 5)</ref> which fuses text and visual information more effectively and deeply. <ref type="figure" target="#fig_4">Figure 5</ref>(a) shows a typical upsample Residual Block in generator <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b0">1]</ref>, there are two Fusion Blocks stacked in Residual Block. There are many implementations of Fusion Block, the Fusion Block with Conditional Batch Normalization (CBN) is implemented as <ref type="figure" target="#fig_4">Figure 5</ref>(b) <ref type="bibr" target="#b33">[34]</ref>. Batch Normalization (BN) normalizes the feature maps, and the Affine Transformation manipulates the output by scaling and shifting parameters predicted from conditions. We consider that BN in Fusion Block2 will reduce the effectiveness of Affine Transformation in Fusion Block1. Since BN transforms the feature maps into a normal distribution, it can be regarded as a unconditional reverse operation of Affine Transformation and reduces the distance between each feature map in a batch, it is not beneficial for the conditional generation process. So we extract the Affine Transformation from CBN, and employ Affine Transformations to manipulate visual feature maps conditioned on natural language descriptions. We adopt two one-hidden-layer MLPs to predict the language-conditioned channel-wise scaling parameters γ and shifting parameters β from sentence vector e, respectively:</p><formula xml:id="formula_2">γ = M LP 1 (e), β = M LP 2 (e).<label>(3)</label></formula><p>If the input is feature map X∈R B×C×H×W , the output of MLP will be a vector of size C. We first conduct the channel-wise scaling operation on X with the scaling parameter γ, then we apply the channel-wise shifting operation on X with the shifting parameter β. This process can be formally expressed as follows:</p><formula xml:id="formula_3">AF F (x i |e) = γ i · x i + β i ,<label>(4)</label></formula><p>where AF F is Affine Transformation; x i is the i th channel of visual feature maps; e is the sentence vector; γ i and β i are scaling parameter and shifting parameter for the i th channel of visual feature maps. Through channel-wise scaling and shifting, the generator can capture the semantic information in text description and synthesize realistic images matching with given text descriptions. We denote this module which fuses text and image features through one Affine Transformation as AFFBlock (see <ref type="figure" target="#fig_4">Figure 5(c)</ref>). Furthermore, after decomposing the effectiveness of Affine Transformation from CBN, we can fuse image features and conditions in a more free way. So we propose the Deep text-image Fusion Block (DFBlock) which stacks multiple Affine Transformations and ReLU layers in Fusion Block. As shown in <ref type="figure" target="#fig_4">Figure 5(d)</ref>, there are two Affine Transformations and ReLU layers stacked sequentially in one Fusion Block. The DFBlock deepens the depth of the text-image fusion process. For neural networks, a deeper network always means a stronger ability. We consider that deepening the fusion process can bring two main benefits for text-to-image generation: First, it gives the generator more chances to fuse text and image features, so that the text information can be fully exploited. Second, deepening the fusion process makes the fusion network have more nonlinearities, which is beneficial to generate semantic consistent images from different text descriptions.</p><p>We conduct extensive experiments to compare the effectiveness between CBNBlock ( <ref type="figure" target="#fig_4">Figure 5(b)</ref>) <ref type="bibr" target="#b20">[21]</ref>, AFFBlock ( <ref type="figure" target="#fig_4">Figure 5(c)</ref>), and DFBlock ( <ref type="figure" target="#fig_4">Figure 5(d)</ref>) in the ablation studies. Experimental results show that our DFBlock is more effective to fuse the text and image features, which proves its superiority and efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Skip-Z with Truncation</head><p>In text-to-image synthesis works, the input text description will be encoded into a sentence vector by a fixed pretrained text encoder. Thus, each description in the training set only corresponds to a fixed sentence vector, it makes the text latent space discontinuous and leads to overfitting on training set. To deal with this problem, we concatenate the noise vector on the sentence vector, it converts a single, discrete sentence vector into a beam of sentence vectors (see <ref type="figure" target="#fig_2">Figure 3)</ref>. We call this noise concatenation as skip-z. Skip-z brings variety to sentence vector, but the variety is not entirely beneficial. Since the noise is sampled from the Gaussian distribution, when the model is used to generate im-ages from text descriptions and noises, uncommon noises will cause negative effects on the synthesized image quality. So we introduce Truncation Trick <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b10">11]</ref> to truncate the noise vector through resampling the values with magnitude above a chosen threshold. The skip-z with Truncation provides the generator with a stable and better text latent space which ensures the synthesized image quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first introduce the datasets, training details, and evaluation metrics used in our experiments, then evaluate DF-GAN and its variants quantitatively and qualitatively. Datasets. We follow previous work <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b22">23]</ref> and evaluate the proposed model on two challenging datasets, i.e., CUB bird <ref type="bibr" target="#b30">[31]</ref> and COCO <ref type="bibr" target="#b18">[19]</ref>. The CUB dataset contains 11788 images belonging to 200 bird species. Each bird image has ten language descriptions. The COCO dataset contains 80k images for training and 40k images for testing. Each image in this dataset has five language descriptions. Training Details. We optimize our network using Adam <ref type="bibr" target="#b12">[13]</ref> with β 1 =0.0 and β 2 =0.9. The learning rate is set to 0.0001 for the generator and 0.0004 for the discriminator according to Two Timescale Update Rule (TTUR) <ref type="bibr" target="#b8">[9]</ref>. Evaluation Details. Following previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40]</ref>, we choose the Inception Score (IS) <ref type="bibr" target="#b26">[27]</ref> and Fréchet Inception Distance (FID) <ref type="bibr" target="#b8">[9]</ref> to evaluate the performance of our network. Specifically, IS computes the Kullback-Leibler (KL) divergence between conditional distribution and marginal distribution. Higher IS means higher quality of the generated images and each image clearly belongs to a specific class. FID <ref type="bibr" target="#b8">[9]</ref> is another assessment which computes the Fréchet distance between the distribution of the synthetic images and real-world images in the feature space of a pre- <ref type="figure">Figure 6</ref>: Examples of images synthesized by AttnGAN <ref type="bibr" target="#b32">[33]</ref>, DM-GAN <ref type="bibr" target="#b39">[40]</ref>, and our proposed DF-GAN conditioned on text descriptions from the test set of COCO and CUB datasets.</p><p>trained Inception v3 network. Contrary to IS, more realistic images have a lower FID. To compute both IS and FID, each model generates 30,000 images (256×256 resolution) from text descriptions randomly selected from the test dataset.</p><p>It should be pointed out that, as observed in Obj-GAN paper <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24]</ref>, we also found that IS on the COCO dataset completely fails in evaluating the synthesized image quality of text-to-image models. Thus, we do not compare IS on the COCO dataset. While FID is more robust and aligns human qualitative evaluation on the COCO dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Evaluation</head><p>We compare the proposed method with several state-ofthe-art methods including AttnGAN <ref type="bibr" target="#b32">[33]</ref>, MirrorGAN <ref type="bibr" target="#b22">[23]</ref>, SD-GAN <ref type="bibr" target="#b33">[34]</ref>, and DM-GAN <ref type="bibr" target="#b39">[40]</ref>, which have achieved the remarkable success of text-to-image synthesis by using stacked structures. As shown in <ref type="table" target="#tab_0">Table 1</ref>, our proposed DF-GAN achieves the highest Inception Scores (IS) and lowest Fréchet Inception Distance (FID) compared with other leading models.</p><p>Specifically, compared with AttnGAN <ref type="bibr" target="#b32">[33]</ref> which employs cross-modal attention to fuse text and image features, our DF-GAN improves the IS metric from 4.36 to 5.10 and decreases the FID metric from 23.98 to 14.81 on the CUB dataset, and decreases FID from 35.49 to 21.42 on the COCO dataset. Compared with MirrorGAN <ref type="bibr" target="#b22">[23]</ref> and SD-GAN <ref type="bibr" target="#b33">[34]</ref> which employ cycle consistency and Siamese network to ensure text-image semantic consistency, our DF-GAN improves IS from 4.56 and 4.67 to 5.10 respectively on the CUB dataset. Compared with DM-GAN <ref type="bibr" target="#b39">[40]</ref> which introduces Memory Network to refine fuzzy image contents, our model also improves IS from 4.75 to 5.10 and decrease FID from 16.09 to 14.81 on CUB, and decrease </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Evaluation</head><p>We also compare the visualization results synthesized by AttnGAN <ref type="bibr" target="#b32">[33]</ref>, DM-GAN <ref type="bibr" target="#b39">[40]</ref>, and the proposed DF-GAN. We first compare the quality of the synthesized images and text-image semantic consistency on the CUB dataset, then compare the results on a more challenging COCO dataset.</p><p>It can be seen that images synthesized by AttnGAN <ref type="bibr" target="#b32">[33]</ref> and DM-GAN <ref type="bibr" target="#b39">[40]</ref> in <ref type="figure">Figure 6</ref> look like a simple combination of fuzzy shape and some visual details (1 st , 3 rd , 5 th , 7 th , and 8 th columns). The reason is the employment of their stacked architecture and cross-modal spatial attention. Although DM-GAN <ref type="bibr" target="#b39">[40]</ref> introduces Memory Network to alleviate this problem, the problem is not completely solved. As shown in the 5 th , 7 th , and 8 th columns, the birds synthesized by AttnGAN <ref type="bibr" target="#b32">[33]</ref> and DM-GAN <ref type="bibr" target="#b39">[40]</ref> contain wrong shapes. Since DF-GAN employs a novel one-stage text-toimage backbone and a sequence of Deep text-image Fusion Blocks (DFBlock), the images synthesized by our DF-GAN have better object shapes and realistic fine-grained details (e.g., 1 st , 3 rd , 7 th and 8 th columns). Besides, the posture of the bird in our DF-GAN result is also more natural (e.g., 7 th and 8 th columns).</p><p>Comparing the text-image semantic consistency with other models, we find that our DF-GAN is also able to capture more fine-grained details in text descriptions. For example, as the result shown in 1 st , 2 th , 6 th columns in <ref type="figure">Figure 6</ref>, other models cannot synthesize the "holding ski poles", "train track", and "a black stripe by its eyes" described in the text well, but the proposed DF-GAN can synthesize them more correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In this section, we conduct ablation studies on the testing set of the CUB dataset to verify the effectiveness of each component in the proposed DF-GAN. The components include One-Stage text-to-image Backbone (OS-B), Matching-Aware Gradient Penalty (MA-GP), One-Way Output (OW-O), Deep text-image Fusion Block (DFBlock), and Skip-z with Truncation (SZ-T). Our baseline is a stacked text-to-image GAN which employs two-ways output. In baseline, the sentence vector is naively concatenated to the input noise and intermediate feature maps. We first evaluate the effectiveness of OS-B, MA-GP, and OW-O. We conduct user study to evaluate the text-image semantic consistency (SC), and we asked 10 users to score the 100 synthesized images with text descriptions. The scores range from 1 (worst) to 5 (best). The results on the CUB dataset are shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>Compared with baseline, our proposed One-Stage textto-image Backbone (OS-B) improves IS from 3.96 to 4.11, and decreases FID from 43.45 to 32.52. The results prove that our one-stage backbone is more effective than stacked architecture. Armed with MA-GP, the model further improves IS to 4.46, SC to 3.55, and decreases FID to 32.52 significantly. It demonstrates that the proposed MA-GP can promote the generator to synthesize more realistic and textimage semantic consistent images. The proposed OW-O also improves IS from 4.46 to 4.57, SC from 3.55 to 4.61, and decreases FID from 32.52 to <ref type="bibr">23.16</ref>. It also demonstrates that the one-way output is more effective than a two-ways output in the text-to-image generation task.</p><p>To prove the superiority of DFBlock, we compare the DFBlock with CBNBlock, AFFBlock which employs conditional Batch Normalization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b0">1]</ref>, and one Affine Transformation layer to fuse text and image features, respectively. Their differences are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. There results are listed in <ref type="table" target="#tab_2">Table 3</ref>. MA-GP GAN is the model that employs One-Stage text-to-image Backbone, Matching-Aware Gradient Penalty, and One-Way Output.</p><p>From the results in <ref type="table" target="#tab_2">Table 3</ref>, we find that compared with other fusion methods, concatenating cannot efficiently fuse text and image features. The comparison between CBN-Block and AFFBlock proves that Batch Normalization is not essential in Fusion Block, and removing normalization even slightly improves the results. The comparison between DFBlock and AFFBlock demonstrates the effectiveness of deepening the text-image fusion process. The comparison result proves the effectiveness of our proposed DFBlock. Moreover, we compare the model with Skip-z (SZ), Truncation (T), Skip-z with Truncation (SZ-T) respectively. The MA-GP GAN with DFBLK and SZ-T (DF-GAN) achieves the best results compared with other models, it also demonstrates that the proposed SZ-T can promote the generator to synthesize higher-quality images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a novel Deep Fusion Generative Adversarial Networks (DF-GAN) for text-to-image generation tasks which is able to directly synthesize more realistic and text-image semantic consistent images without stacking architecture and extra networks. Moreover, we propose a novel target-aware discriminator composed of Matching-Aware Gradient Penalty (MA-GP) and one-way output, it significantly improves the image quality and textimage semantic consistency, and accelerate the convergence of generator. Besides, we propose a novel Deep text-image Fusion Block (DFBlock) which fuses text and image features more effectively and deeply. Furthermore, we present a novel method called the skip-z with truncation to provide the generator with a stable and better text latent space for synthesizing higher-quality images. Extensive experiment results show that our proposed DF-GAN significantly outperforms state-of-the-art models on the CUB dataset and more challenging COCO dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Existing text-to-image models stack multiple generators and discriminators to generate high-resolution images. (b) Our proposed DF-GAN generates high-quality images directly and fuses the text and image features deeply by our deep text-image fusion blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The architecture of the proposed DF-GAN for text-to-image synthesis. DF-GAN generates high-resolution images directly by one pair of generator and discriminator and fuses the text information and visual feature maps through multiple Deep text-image Fusion Blocks (DFBlock) in UPBlocks. Armed with Matching-Aware Gradient Penalty (MA-GP) and one-way output, our model can synthesize more realistic and text-matching images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>A diagram for Matching-Aware Gradient Penalty (MA-GP). The data point (real, match) marked by a circle should be applied MA-GP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Comparison between two-ways output and our proposed one-way output. The two-ways output predicts conditional loss and unconditional loss and sums them up as the final adversarial loss. Our one-way output predicts the whole adversarial loss directly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>We redesign the architecture of the Fusion Block and compare DFBlock with AFFBlock and CBNBlock. (a) A typical UPBlock in the generator network. The UPBlock upsamples the image features and fuses text and image features by two Fusion Blocks. (b) The CBNBlock is a Fusion Block which employs the Conditional Batch Normalization to fuse text and image features. (c) AFFBlock is a simplified version of CBNBlock which removes the Batch Normalization layer. (d) The DFBlock is an enhanced version of AFFBlock, it deepens the text-image fusion process by stacking multiple Affine Transformations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The results of IS and FID compared with the stateof-the-art methods on the test set of CUB and COCO.</figDesc><table><row><cell cols="4">Method CUB-IS↑ CUB-FID↓ COCO-FID↓</cell></row><row><cell>AttnGAN [33]</cell><cell>4.36</cell><cell>23.98</cell><cell>35.49</cell></row><row><cell>MirrorGAN [23]</cell><cell>4.56</cell><cell>18.34</cell><cell>34.71</cell></row><row><cell>SD-GAN [34]</cell><cell>4.67</cell><cell>-</cell><cell>-</cell></row><row><cell>DM-GAN [40]</cell><cell>4.75</cell><cell>16.09</cell><cell>32.64</cell></row><row><cell>DF-GAN (Ours)</cell><cell>5.10</cell><cell>14.81</cell><cell>21.42</cell></row><row><cell cols="4">FID from 32.64 to 21.42 on COCO. The quantitative com-</cell></row><row><cell cols="4">parisons of IS show that our proposed DF-GAN is able to</cell></row><row><cell cols="4">synthesize higher-quality images from text descriptions.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The performance of different components of our model on the test set of CUB.</figDesc><table><row><cell>Architecture</cell><cell cols="2">IS ↑ FID ↓ SC ↑</cell></row><row><cell>Baseline</cell><cell>3.96 51.34</cell><cell>-</cell></row><row><cell>OS-B</cell><cell>4.11 43.45</cell><cell>1.46</cell></row><row><cell>OS-B w/ MA-GP</cell><cell>4.46 32.52</cell><cell>3.55</cell></row><row><cell cols="2">OS-B w/ MA-GP w/ OW-O 4.57 23.16</cell><cell>4.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The performance of MA-GP GAN with different modules on the test set of CUB. GP GAN w/ DFBLK w/ SZ-T (DF-GAN) 5.10 14.81    </figDesc><table><row><cell>Architecture</cell><cell>IS↑</cell><cell>FID ↓</cell></row><row><cell>MA-GP GAN w/ Concat</cell><cell cols="2">4.57 23.16</cell></row><row><cell>MA-GP GAN w/ CBNBLK</cell><cell cols="2">4.71 21.77</cell></row><row><cell>MA-GP GAN w/ AFFBLK</cell><cell cols="2">4.75 20.71</cell></row><row><cell>MA-GP GAN w/ DFBLK</cell><cell cols="2">4.88 18.84</cell></row><row><cell>MA-GP GAN w/ DFBLK w/ SZ</cell><cell cols="2">4.86 19.21</cell></row><row><cell>MA-GP GAN w/ DFBLK w/ T</cell><cell cols="2">4.81 19.97</cell></row><row><cell>MA-</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rifegan: Rich feature generation for text-toimage synthesis from prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanling</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10911" to="10920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérémie</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Mary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6594" to="6604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Segattngan: Text to image generation with segmentation attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchuan</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiancheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12444</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynamic neural turing machine with continuous and discrete addressing schemes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarath</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="857" to="884" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Inferring semantic layout for hierarchical textto-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7986" to="7994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Controllable text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Lukasiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2065" to="2075" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Exploring global and local linguistic representation for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangxiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Object-driven text-to-image synthesis via adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="12174" to="12182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Hyun</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Geometric gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Men</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianyan</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00823</idno>
		<title level="m">A chinese multimodal pretrainer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3481" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05637</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">cgans with projection discriminator. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learn, imagine and create: Text-to-image generation from prior knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="887" to="897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mirrorgan: Learning text-to-image generation by redescription</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingting</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duanqing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1505" to="1514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Zero-shot text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12092</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1060" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Scott E Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Xinggan for person image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gated siamese convolutional neural network architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mrinal</forename><surname>Rahul Rama Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Haloi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="791" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A siamese long short-term memory architecture for human re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Rahul Rama Varior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="135" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Attngan: Finegrained text to image generation with attentional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1316" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantics disentangling for text-toimage generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guojun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ckd: Cross-task knowledge distillation for text-to-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkuan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Peng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dimitris Metaxas, and Augustus Odena. Self-attention generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Stack-gan++: Realistic image synthesis with stacked generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoting</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1947" to="1962" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dm-gan: Dynamic memory generative adversarial networks for textto-image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minfeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pingbo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5802" to="5810" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
