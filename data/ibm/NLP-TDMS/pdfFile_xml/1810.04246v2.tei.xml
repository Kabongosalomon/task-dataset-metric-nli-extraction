<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep clustering: On the link between discriminative models and K-means</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2019-12-15">15 Dec 2019</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Jabi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pedersoli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Mitiche</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><forename type="middle">Ben</forename><surname>Ayed</surname></persName>
						</author>
						<author>
							<affiliation>
								<orgName>1</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep clustering: On the link between discriminative models and K-means</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2019-12-15">15 Dec 2019</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep Clustering</term>
					<term>Convolutional Neural Networks</term>
					<term>Alternating Direction Methods</term>
					<term>K-means</term>
					<term>Mutual Information</term>
					<term>Kullback-Leibler (KL) divergence</term>
					<term>Regularization</term>
					<term>Multilogit Regression</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the context of recent deep clustering studies, discriminative models dominate the literature and report the most competitive performances. These models learn a deep discriminative neural network classifier in which the labels are latent. Typically, they use multinomial logistic regression posteriors and parameter regularization, as is very common in supervised learning. It is generally acknowledged that discriminative objective functions (e.g., those based on the mutual information or the KL divergence) are more flexible than generative approaches (e.g., K-means) in the sense that they make fewer assumptions about the data distributions and, typically, yield much better unsupervised deep learning results. On the surface, several recent discriminative models may seem unrelated to K-means. This study shows that these models are, in fact, equivalent to K-means under mild conditions and common posterior models and parameter regularization. We prove that, for the commonly used logistic regression posteriors, maximizing the L 2 regularized mutual information via an approximate alternating direction method (ADM) is equivalent to minimizing a soft and regularized K-means loss. Our theoretical analysis not only connects directly several recent state-of-the-art discriminative models to K-means, but also leads to a new soft and regularized deep K-means algorithm, which yields competitive performance on several image clustering benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>O NE of the most fundamental unsupervised learning problem, clustering aims at grouping data into categories. Obtaining meaningful categorical representations of data without supervision is fundamental in a breadth of applications of data analysis and visualization. With the excessive amounts of high-dimensional data (e.g., images) routinely collected everyday, the problem is currently attracting substantial research interest, in both the learning and computer vision communities.</p><p>Clustering performance heavily depends on the structure of the input data. Therefore, representation learning methods, which encode the original data in feature spaces where the grouping tasks become much easier, are widely used in conjunction with clustering algorithms. Typically, feature learning and clustering are performed sequentially <ref type="bibr" target="#b0">[1]</ref>. However, with the success of deep neural networks (DNNs), a large number of recent studies, e.g., <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, investigated joint learning of feature embedding (via DNNs) and estimation of latent cluster assignments (or labels). Commonly, these recent models are stated as the optimization of objective functions that integrate two types of losses: (1) a clustering loss, which depends on both latent cluster assignments and deep network parameters and, (2) a reconstruction loss as a datadependent regularization, e.g., via an auto-encoder <ref type="bibr" target="#b3">[4]</ref>, to prevent the embedding from over-fitting.</p><p>Clustering objectives fall into two main categories, generative, e.g., K-means and Gaussian Mixture Models <ref type="bibr" target="#b9">[10]</ref>, and discriminative, e.g., graph clustering clustering <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref> and information-theoretic models <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. Generative objectives explicitly model the density of data points within the clusters via likelihood functions, whereas discriminative objectives learn the decision boundaries in-between clusters via conditional probabilities (posteriors) over the labels given the inputs. In the context of recent deep clustering models, discriminative objectives dominate the literature and report the most competitive performances <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>. For instance, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref> learned deep discriminative neural network classifiers that maximize the mutual information (MI) between data inputs and latent labels (cluster assignments), following much earlier MI-based clustering works <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. In another very recent line of deep discriminative clustering investigations, e.g., <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, the problem is addressed by introducing auxiliary target distributions, which can be viewed as latent probabilistic point-to-cluster assignments. Then, it is stated as the minimization of a mixed-variable objective containing the Kullback-Leibler (KL) divergence between these auxiliary targets and the posteriors of a discriminative deep network classifier, typically expressed as standard multilogit regression functions <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>. The minimization is carried out by alternating two substeps, until convergence. The first sub-step fixes the network parameters and optimizes the objective w.r.t the targets. The second fixes target assignments, and optimizes the objective w.r.t network parameters. Conveniently, this sub-step takes the form of standard supervised classifiers, in which the ground-truth labels are given by the latent auxiliary targets. The KL divergence is used in conjunction with other terms, to favor balanced partitions and to regularize model parameters.</p><p>Generative models were also investigated in the context of deep clustering <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>. For instance, in <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, a DNN is trained with a loss function that includes the standard K-means clustering objective. However, in the literature, it is commonly acknowledged that discriminative models are more flexible in the sense that they make fewer assumptions about data distributions and, typically, yield better unsupervised learning results, e.g., "Generally it has been argued that the discriminative models often have better results compared to their generative counterparts" <ref type="bibr" target="#b3">[4]</ref> " . . . discriminative clustering techniques represent the boundaries or distinctions between categories. Fewer assumptions about the nature of categories are made, making these methods powerful and flexible in real world applications" <ref type="bibr" target="#b13">[14]</ref> Furthermore, the results reported in the literature suggest that the performances of discriminative models are significantly better. For instance, the DEPICT model in <ref type="bibr" target="#b3">[4]</ref>, which is based on the KL-divergence between multilogit regression posteriors and targets, reports a state-of-the-art performance on MNIST nearly approaching supervised learning performance. This discriminative model outperforms significantly the K-means loss investigated recently in the deep clustering model in <ref type="bibr" target="#b2">[3]</ref>, with 14% difference in accuracy; see <ref type="table" target="#tab_1">Table  2</ref>. On the surface, the several recent discriminative models based on either the MI or KL objectives, e.g., <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, may seem completely unrelated to K-means. Our study shows that they are, in fact, equivalent to K-means under mild conditions and commonly used posterior models and parameter regularization. The following lists the main contributions of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>For the commonly used logistic regression posteriors, we prove that maximizing the L 2 regularized mutual information via an approximate alternating direction method (ADM) is equivalent to a soft and regularized K-means loss (Proposition 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We establish the link between state-of-the-art KLbased models, e.g., DEPICT <ref type="bibr" target="#b3">[4]</ref>, and the standard mutual information objective <ref type="bibr" target="#b13">[14]</ref>, which is used in a number recent deep clustering works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>. In particular, we show that optimizing the KL objective, in conjunction with a balancing term, can be viewed as an approximate ADM solution for optimizing the mutual information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We give theoretical results that connect directly several recent discriminative formulations to K-means. Furthermore, this leads to a new soft and regularized version of deep K-means, which has approximately the same competitive performances as state-of-theart discriminative algorithms on several benchmarks ( <ref type="table" target="#tab_1">Table 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DEEP DISCRIMINATIVE CLUSTERING MODELS</head><p>Let X = x 1 , . . . , x N be an unlabeled data set composed of N samples, each of dimension d x , i.e., x i ∈ R dx . The purpose is to cluster the N samples into K categories (clusters). The data samples are embedded into a feature space Z = z 1 , . . . , z N using a mapping φ W : X → Z, where W are learnable parameters and z i ∈ R dz , with d z &lt;&lt; d x , i.e., the dimensionality of Z is much smaller than X . In recent deep clustering models, as in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, for instance, the embedding function φ W is learned jointly with latent cluster assignments (or labels) using a Deep Neural Network (DNN), in which case W denotes the set of network parameters. These models are stated as the optimization of an objective that integrates two types of loss terms: (1) a clustering loss, which depends on both latent cluster assignments and network parameters, and, (2) a reconstruction loss R(Z) as a data-dependent regularization, e.g., via an auto-encoder <ref type="bibr" target="#b3">[4]</ref>, to prevent the embedding from over-fitting. While all the recent deep clustering objectives discussed in the following used a reconstruction loss, we will focus only on the clustering losses in this section for the sake of clarity; we will discuss a reconstruction loss in more detail in section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Mutual information</head><p>Following the works in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, maximizing the mutual information between data inputs and latent cluster assignments is commonly used in discriminative clustering. Also, very recently, the concept is revisited in several deep clustering studies, e.g., <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, which learned discriminative neural network classifiers that maximize the mutual information and obtained competitive performances. In general, the problem amounts to maximizing the following clustering loss:</p><formula xml:id="formula_0">I(X, K) = H(K) − H(K|X),<label>(1)</label></formula><p>where H(·) and H(·|·) are the entropy and conditional entropy, respectively. K ∈ {1, . . . , K} and X ∈ X denote random variables for cluster assignments (latent labels) and data samples, respectively. The objective is to learn a conditional probability (posterior) over the labels given the input data, which we denote p ik . The marginal distribution of labels can be estimated as follows <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_1">p k = 1 N N i=1 p ik<label>(2)</label></formula><p>Thus, the entropy terms appearing in the mutual information can be expressed with the posteriors as follows <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_2">H(K) = − K k=1p k log p k<label>(3)</label></formula><formula xml:id="formula_3">H(K|X) = − 1 N N i=1 K k=1 p ik log p ik<label>(4)</label></formula><p>Minimizing the conditional entropy of the posteriors, H(K|X), inhibits the uncertainty associated to the assignment of labels to each data point. Each point-wise conditional entropy in the sum reaches its minimum when a single label k has the maximum posterior for point i, i.e., p ik = 1, whereas each of the other labels verifies p ij = 0, j = k. In the semi-supervised setting, it is well known that this conditional entropy models effectively the cluster assumption <ref type="bibr" target="#b15">[16]</ref>: The decision boundaries of the discriminative model should not occur at dense regions of the inputs. However, using this term alone in the unsupervised setting yields degenerate solutions, in which decision boundaries are removed <ref type="bibr" target="#b13">[14]</ref>. Maximizing the entropy of the marginal distribution of labels, H(K), avoids degenerate solutions as it biases the results towards balanced partitions 1 .</p><p>Finally, one has to choose a parametric model for posteriors p ik , e.g., the widely used multilogit regression function <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_4">p ik ∝ exp(θ T k z i + b k ),<label>(5)</label></formula><p>where O = {θ 1 , . . . , θ K , b 1 , . . . , b K } is the set of weight vectors θ k and bias values b k for each cluster k. We note here that p ik is also related to the DNN parameters W,</p><formula xml:id="formula_5">i.e., p ik ≡ p ik (θ k , b k , W), since z i = φ W (x i ).</formula><p>In the reminder of the paper, we will use probability-simplex vectors p i ∈ [0, 1] K to denote (p i1 , . . . , p iK ) t and matrix P = (p i1 , . . . , p iK ) ∈ [0, 1] K×N to denote the posteriors of all data points. To simplify the notation, we will omit the explicit dependence of posteriors p ik on model parameters {O, W}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The KL divergence and auxiliary targets</head><p>In another very recent line of deep discriminative clustering investigations, e.g., <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, the problem is stated by introducing auxiliary target distributions q i = (q i1 , . . . , q iK ) t ∈ [0, 1] K , which are latent probabilistic point-to-cluster assignments within the simplex. Then, the problem is formulated as the minimization the KL divergences between these auxiliary targets and the posteriors of a discrimintaive deep network classifier, which we denote p i , as earlier in the case of the mutual information. Conveniently, in this case, the sub-problem of optimizing w.r.t the network parameters takes the form of a standard supervised classifier, in which the ground truth labels are given by the auxiliary targets. For instance, the recent state-of-the-art model in <ref type="bibr" target="#b3">[4]</ref>, referred to as DEPICT, follows from minimizing the KL divergence and a term that encourages balanced cluster assignments, subject to simplex constraints:</p><formula xml:id="formula_6">min Φ,Q KL(Q P ) + γ K k=1q k log q k s.t. q t i 1 = 1; q i ≥ 0 ∀i (6) where matrix Q = (q i1 , . . . , q iK ) ∈ [0, 1] K×N contains the targets for all points, Φ = {O, W} andq k = 1 N N i=1</formula><p>q ik is the empirical distribution of the target assignments. The KL divergence is defined as:</p><formula xml:id="formula_7">KL(Q P ) = 1 N N i=1 K k=1 q ik log q ik p ik .<label>(7)</label></formula><p>The model in (6) depends on two different types of variables: auxiliary targets Q and classifier parameters Φ. Therefore, it is solved by alternating two sub-steps, until convergence:</p><p>1. Notice that H(K) is equal up to an additive constant to the Kullback-Leiber (KL) divergence between the label distribution and the uniform distribution: KL((p k , k = 1, . . . , K) (û k , k = 1, . . . , K)), withû k = 1/K ∀k. Also, note that it is possible to encourage label distributionp k to match any prior distributiond k , not necessarily uniform, simply by using KL((p k , k = 1, . . . , K) (d k , k = 1, . . . , K) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>.</p><p>• Parameter-learning step: This step fixes target assignments Q and optimizes (6) w.r.t network parameters Φ. Notice that, ignoring constant terms, this sub-step becomes equivalent to a cross-entropy loss, exactly as in standard supervised classifiers, with ground-truth labels given by fixed targets Q:</p><formula xml:id="formula_8">min Φ − 1 N N i=1 K k=1 q ik log p ik (8) •</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Target-estimation step:</head><p>This sub-step finds the target variable Q that minimizes <ref type="formula">(6)</ref>, with the network parameters fixed. Setting the approximated gradient equal to zero, it is easy to show that the optimal solution is given by <ref type="bibr" target="#b3">[4]</ref>:</p><formula xml:id="formula_9">q ik ∝ p ik N i ′ =1 p i ′ k 1/2<label>(9)</label></formula><p>The above algorithm alternates two steps until convergence: (1) updates of network parameters Φ via backpropagation and stochastic gradient descent (SGD) corresponding to the standard cross-entropy loss; and (2) updates of target assignments q ik , according to closed-form solution <ref type="bibr" target="#b8">(9)</ref>. While a direct maximization of the MI is based on SGD solely, this alternating scheme has an additional computational load of O(N K), which comes from target-variable updates in <ref type="bibr" target="#b8">(9)</ref>. The computational load associated with updates (9) is marginal in comparison to the load associated with SGD network training. In practice, the training time associated with this alternating ADM scheme is approximately of the same order as a direct SGD maximization of the MI.</p><p>The following proposition establishes the link between the state-of-the-art DEPICT model in <ref type="bibr" target="#b5">(6)</ref>, which was introduced recently in <ref type="bibr" target="#b3">[4]</ref>, and the standard mutual information objective in <ref type="bibr" target="#b0">(1)</ref>, which was used in a number of other recent deep clustering works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Proposition 1. Alternating steps <ref type="bibr" target="#b7">(8)</ref> and <ref type="formula" target="#formula_9">(9)</ref> for optimizing mixed-variable objective (6) can be viewed as an approximate Alternating Direction Method (ADM) 2 <ref type="bibr" target="#b16">[17]</ref> for maximizing the mutual information I(X, K) in (1) via the following constrained decomposition of the problem:</p><formula xml:id="formula_10">max Φ,Q 1 N N i=1 K k=1 q ik log(p ik ) − K k=1q k log q k s.t. Q = P ; q t i 1 = 1; q i ≥ 0 ∀i<label>(10)</label></formula><p>Proof. It is easy to see that equality-constrained problem (10) is an ADM decomposition of the mutual information maximization in <ref type="bibr" target="#b0">(1)</ref>. Notice that, when constraint Q = P is satisfied, one can replace each auxiliary target q ik in the objective of (10) by posterior p ik , which yields exactly the mutual information in <ref type="formula" target="#formula_0">(1)</ref>: Rather than optimizing directly mutual information <ref type="bibr" target="#b10">(11)</ref> with respect to the parameters of posteriors P , ADM splits the problem into two sub-problems by introducing auxiliary variable Q and enforcing Q = P . Now, notice that one can solve constrained problem (10) with a penalty approach. This replaces constraint Q = P by adding a term to the objective, which penalizes some divergence between Q and P , e.g., KL <ref type="bibr" target="#b2">3</ref> :</p><formula xml:id="formula_11">I(X, K) = 1 N N i=1 K k=1 p ik log(p ik ) − K k=1p k log p k<label>(11)</label></formula><formula xml:id="formula_12">max Φ,Q 1 N N i=1 K k=1 q ik log(p ik ) − K k=1q k log q k − KL(Q P ) s.t. q t i 1 = 1; q i ≥ 0 ∀i<label>(12)</label></formula><p>This is closely related to the principle of ADMM (Alternating Direction Method of Multipliers) <ref type="bibr" target="#b16">[17]</ref>, except that KL is not a typical choice for a penalty to replace the equality constraints. Typically, ADMM methods use multiplier-based quadratic penalties for enforcing the equality constraint (also referred to as augmented Lagrangian). We will discuss more details on maximizing the MI directly or via an alternating direction method in Sections 4 and 5. Furthermore, we will discuss the standard approach based on multiplierbased quadratic penalties and its link to the KL penalties for simplex variables.</p><p>Using expression <ref type="formula" target="#formula_7">(7)</ref> of KL in the objective of (12), and after some manipulations, we can show that the problem in (12) is equivalent to:</p><formula xml:id="formula_13">min Φ,Q KL(Q P ) + 1 2 K k=1q k log q k + 1 2 H(Q) s.t. q t i 1 = 1; q i ≥ 0 ∀i<label>(13)</label></formula><p>where</p><formula xml:id="formula_14">H(Q) = − 1 N N i=1 K k=1 q ik log(q ik )</formula><p>is the entropy of auxiliary variable Q. Notice that the first two terms in <ref type="bibr" target="#b12">(13)</ref> correspond to the DEPICT model in (6) for γ = 1/2. The last term H(Q) encourages peaked auxiliary distributions q i = (q i1 , . . . , q iK ). Each point-wise entropy in the sum reaches its minimum at one vertex of the simplex: a single label k has the maximum target variable for point i, i.e., q ik = 1, whereas each of the other variables verifies q ij = 0, j = k. Term H(Q) is close to zero near the vertices of the simplex (peaked distributions q i ). Therefore, the mutual information objective we obtained in model <ref type="bibr" target="#b12">(13)</ref>, which we refer to as MI-ADM, can be viewed as an approximation of the DEPICT model in <ref type="bibr" target="#b5">(6)</ref>. In fact, as we will see in our experiments, the additional entropy term in <ref type="bibr" target="#b12">(13)</ref>, H(Q), has almost no effect on the results: DEPICT and MI-ADM have approximately the same performances; see <ref type="table" target="#tab_1">Table 2</ref>.</p><p>With the model parameters fixed, setting the approximated gradient of (13) w.r.t the target variables equal to zero, we obtain the following updates:</p><formula xml:id="formula_15">q ik ∝ p 2 ik N i ′ =1 p 2 i ′ k 1/2<label>(14)</label></formula><p>Notice that these updates are slightly different from the DEPICT updates in <ref type="formula" target="#formula_9">(9)</ref>, due to additional entropy term <ref type="bibr" target="#b2">3</ref>. KL is non-negative and is equal to zero if and only if the two distributions are equal.</p><p>H(Q). It is worth noting that the recent deep discrimintaive clustering algorithm in <ref type="bibr" target="#b8">[9]</ref> updated q ik as follows:</p><formula xml:id="formula_16">q ik ∝ p 2 ik N i ′ =1 p i ′ k (15)</formula><p>This expression was found experimentally, and was not based on a formal statement of the problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">DEEP K-MEANS</head><p>The standard generative K-means objective, integrated with a reconstruction loss, was recently investigated in the context of deep clustering <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b17">[18]</ref>. In this case, a DNN is trained with a loss function that includes the classical Kmeans clustering objective, which takes the following form:</p><formula xml:id="formula_17">N i=1 K k=1 s ik z i − µ k 2 s.t. K k=1 s ik = 1; s i,k ∈ {0, 1} ∀i, k<label>(16)</label></formula><p>where µ k is the cluster prototype (mean of features z i ) and s ik is a binary integer variable for assigning data point i to cluster k: s ik = 1 when point i is assigned to cluster k, and s ik = 0 otherwise. Similarly to earlier, z i denotes features that are learned jointly with clustering via an additional reconstruction loss R(Z). On the surface, the discriminative mutual information objective in Eq. (11) and its ADM approximation in the DEPICT model in Eq. (6) may seem completely different from the K-means loss in <ref type="bibr" target="#b15">(16)</ref>. The following proposition shows that they are, in fact, equivalent under mild conditions. Proposition 2. For balanced partitions and multiclass logistic regression posteriors of the form in <ref type="bibr" target="#b4">(5)</ref>, ADM maximization of a regularized mutual information defined by</p><formula xml:id="formula_18">I(X, K) − λ K k=1 θ T k θ k ,<label>(17)</label></formula><p>is equivalent to the minimization of the following regularized soft K-means loss function 4 :</p><formula xml:id="formula_19">N i=1 K k=1 q ik z i − θ ′ k 2 + λK N i=1 K k=1 q ik log(q ik ) − N i=1 z T i z i ,<label>(18)</label></formula><p>where λ ∈ R is the regularization parameter and θ ′ k is a soft cluster prototype (mean) defined by:</p><formula xml:id="formula_20">θ ′ k = N i=1 q ik z i N i=1 q ik .<label>(19)</label></formula><p>The function including the first two terms in (18) can be viewed as a soft K-means objective. The first term corresponds exactly to <ref type="bibr" target="#b15">(16)</ref>, except that the integer constraints on assignment variables are relaxed: s ik ∈ {0, 1} are hard assignments (vertices of the simplex) whereas q ik ∈ [0, 1] are soft assignments (within the simplex). The second term in (18) is a negative entropy, which favors assignment softness. <ref type="bibr" target="#b3">4</ref>. We omitted simplex constraints q t i 1 = 1 and q i ≥ 0 ∀i in both Eqs. <ref type="formula" target="#formula_0">(17)</ref> and <ref type="formula" target="#formula_0">(18)</ref>. This simplifies the presentation without causing any ambiguity.</p><p>It reaches its maximum and vanishes (i.e., becomes equal to zero) for hard binary assignments q i,k ∈ {0, 1}: at the vertices of the simplex, the function including the first two terms in (18) becomes exactly the hard K-means objective in <ref type="bibr" target="#b15">(16)</ref>. It is also worth noting that optimizing this soft Kmeans objective, with features z i fixed, yields softmin Kmeans updates that are known in the literature; see <ref type="bibr">[19, p. 289]</ref>.</p><p>Proof. Consider the ADM approximation of the mutual information in Eq. (12), augmented with regularization term λ K k=1 θ T k θ k . Using this approximation, along with the expression of KL in <ref type="bibr" target="#b6">(7)</ref>, it is easy to see that maximizing the regularized mutual information in <ref type="bibr" target="#b16">(17)</ref> can be stated as minimizing the following expression:</p><formula xml:id="formula_21">K k=1q k log q k − 1 N N i=1 K k=1 q ik log p ik + 1 N N i=1 K k=1 q ik log q ik p ik + λ K k=1 θ T k θ k (20) = K k=1q k log q k + 1 N N i=1 K k=1 q ik log q ik − 2 N N i=1 K k=1 q ik log exp(θ T k z i + b k ) + λ K k=1 θ T k θ k (21) = K k=1q k log q k + 1 N N i=1 K k=1 q ik log q ik − 2 N N i=1 K k=1 q ik b k + 1 N N i=1 K k=1 −2q ik θ T k z i + N λ K k=1 θ T k θ k ,<label>(22)</label></formula><p>where we replaced p ik by its expression in Eq. <ref type="bibr" target="#b4">(5)</ref>. We recall that, in Eq. (5), b k is the bias for cluster k.</p><p>Notice that the first term in the expression above is the negative entropy of the marginal distribution of labels. Minimization of this term prefers balanced partitions and, in fact, its global minimum is attained for a clustering verifyingq k = 1 K ∀k. Now, assuming that the empirical label distribution is approximately uniform, i.e.,q k ≈ 1 K , we show in the Appendix that:</p><formula xml:id="formula_22">N i=1 K k=1 −2q ik θ T k z i + N λ K k=1 θ T k θ k ≈ N i=1 K k=1 q ik 1 √ λK z i − √ λKθ k 2 − 1 λK N i=1 z T i z i .<label>(23)</label></formula><p>Using <ref type="formula" target="#formula_1">(23)</ref>, we obtain the following approximation of the regularized mutual information in <ref type="formula" target="#formula_1">(22)</ref>:</p><formula xml:id="formula_23">1 N N i=1 K k=1 q ik logq k − 2 N N i=1 K k=1 q ik b k + 1 N N i=1 K k=1 q ik log q ik − 1 N λK N i=1 z T i z i + 1 N N i=1 K k=1 q ik 1 √ λK z i − √ λKθ k 2 (24) = KL q k exp(2b k ) + 1 N N i=1 K k=1 q ik log(q ik ) − 1 N λK N i=1 z T i z i + 1 N λK N i=1 K k=1 q ik z i − θ ′ k 2 ,<label>(25)</label></formula><p>where θ ′ k = λKθ k . Notice that we re-wrote the first two terms in <ref type="bibr" target="#b23">(24)</ref> in the form of a KL divergence. The convenience of this will soon become clear. The optimization problem we obtained in <ref type="bibr" target="#b24">(25)</ref> can be solved by alternating optimization w.r.t assignments q i,k , parameters θ ′ k and biases b k . Since KL q k exp(2b k ) ≥ 0 and is equal to 0 if and only the distributions are equal, the optimal b k can be expressed in closed-form as:</p><formula xml:id="formula_24">exp(2b k ) =q k ⇐⇒ b k = 1 2 log q k .<label>(26)</label></formula><p>Substituting these optimal biases back into (25), the KL term vanishes and (25) becomes equivalent to the regularized soft K-means in <ref type="bibr" target="#b17">(18)</ref>.</p><p>We refer to the soft and regularized K-means objective in (18) as SR-K-means. Using this objective jointly with a reconstruction loss, the problem amounts to alternating optimization w.r.t Q, {θ ′ 1 , . . . , θ ′ K } and network parameters W. Setting the partial derivatives of (18) with respect to θ ′ k and q ik equals to zero, we obtain the corresponding optima in closed form as:</p><formula xml:id="formula_25">θ ′ k = N i=1 q ik z i N i=1 q ik ,<label>(27)</label></formula><p>and</p><formula xml:id="formula_26">q ik ∝ exp − 1 λK z i − θ ′ k 2<label>(28)</label></formula><p>These updates clearly correspond to the well-known generative K-means algorithm. Eq. (28) uses a softmin function: it is a soft version of the standard hard (binary) assignments rule of K-means: q ik = 1 if k = argmin l z i − θ ′ l . Such soft K-means updates are known in the literature; see <ref type="bibr">[19, p. 289</ref>]. Also, Eq. (27) is clearly a soft version of the mean updates in the standard K-means. Notice that, here, the θ-updates are in closed-form, unlike earlier for discriminative models DEPICT and MI-ADM, in which θ-updates are performed within network training via stochastic gradient descent. It is also worth noting that balancing term K k=1q k log q k has disappeared from our formulation in (18) due to <ref type="bibr" target="#b25">(26)</ref>. This makes sense because it is well known that K-means has an implicit bias towards balanced partitions <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPERTIES OF ADM OPTIMIZATION FOR THE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MUTUAL INFORMATION</head><p>In this section, we prove that I(X, K) does not decrease at each two-step iteration of the ADM optimization in <ref type="bibr" target="#b11">(12)</ref>, which can be viewed as a bound maximization 5 of the mutual information. We will also discuss the standard constrainedoptimization approach based on multiplier-based quadratic penalties and its link to the KL penalties for simplex variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Bound-optimization interpretation</head><p>Let I (t) (X, K) denotes the MI at iteration t, i.e.,</p><formula xml:id="formula_27">I (t) (X, K) 1 N N i=1 K k=1 p (t) ik log(p (t) ik ) − K k=1p (t) k log p (t) k .<label>(29)</label></formula><p>Proposition 3. I (t) (X, K) is a non-deceasing function of iteration counter t for the two-step mixed-variable ADM optimization in <ref type="bibr" target="#b11">(12)</ref>.</p><p>Proof. Given the network parameter at time t or, equivalently, the resulting posteriors P (t) , the target estimation step at time t explicitly implies:</p><formula xml:id="formula_28">1 N N i=1 K k=1 q (t) ik log(p (t) ik ) − K k=1q (t) k log q (t) k − KL(Q (t) P (t) ) ≥ 1 N N i=1 K k=1 q ik log(p (t) ik ) − K k=1q k log q k − KL(Q P (t) ),<label>(30)</label></formula><p>for all Q. Applying (30) to Q = P (t) , we have the following upper bound on the mutual information at iteration t:</p><formula xml:id="formula_29">1 N N i=1 K k=1 q (t) ik log(p (t) ik ) − K k=1q (t) k log q (t) k − KL(Q (t) P (t) ) ≥ I (t) (X, K).<label>(31)</label></formula><p>Using the estimated Q (t) , the parameter-learning step at time t + 1 implies:</p><formula xml:id="formula_30">1 N N i=1 K k=1 q (t) ik log(p (t+1) ik ) − K k=1q (t) k log q (t) k − KL(Q (t) P (t+1) ) ≥ 1 N N i=1 K k=1 q (t) ik log(p ik ) − K k=1q (t) k log q (t) k − KL(Q (t) P ),<label>(32)</label></formula><p>5. Bound optimization, also known as MM (Minorize-Maximization) framework <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, is a general principle, which updates the current solution to the next as the optimum of an auxiliary function (a tight lower bound on the original objective). This guarantees that the original objective function we want to maximize does not decrease at each iteration. The principle is widely used in machine learning as one trades a difficult optimization problem with a sequence of easier sub-problems <ref type="bibr" target="#b21">[22]</ref>. Examples of well-known bound optimizers include expectation maximization (EM) algorithms, the concave-convex procedure (CCCP) <ref type="bibr" target="#b22">[23]</ref> and submodular-supermodular procedures (SSP) <ref type="bibr" target="#b23">[24]</ref>, among others. for all P . Applying inequality <ref type="bibr" target="#b31">(32)</ref> to P = P (t) , and combining the result with inequality (31), we obtain the following upper bound on the mutual information at iteration t:</p><formula xml:id="formula_31">1 N N i=1 K k=1 q (t) ik log(p (t+1) ik ) − K k=1q (t) k log q (t) k − KL(Q (t) P (t+1) ) ≥ I (t) (X, K).<label>(33)</label></formula><p>Finally, using the fact that P (t+1) = Q (t) , (33) can be rewritten as:</p><formula xml:id="formula_32">I (t+1) (X, K) ≥ I (t) (X, K),<label>(34)</label></formula><p>which terminates the proof.</p><p>It is worth noting that the generative SR-Kmeans procedure discussed earlier can also be viewed as a bound optimizer for the mutual information in Eq. <ref type="formula" target="#formula_0">(17)</ref>, but it uses a bound (auxiliary function) different from the discriminative ADM procedure. In the discriminative ADM procedure, we optimizes directly Eq. <ref type="bibr" target="#b19">(20)</ref>, with the assignment-variable updates derived from setting the approximate gradient of Eq. (20) with respect to assignment variables q ik equal to zero. In the generative SR-Kmeans procedure, we still optimize <ref type="bibr" target="#b19">(20)</ref> with respect to q ik , but in an indirect way using the equivalent K-means objective in Eq. (18) and a two-step process: one step updating cluster prototypes (means) with Eq. <ref type="bibr" target="#b18">(19)</ref> and the other updating assignment variables. In fact, for the standard K-means procedure, one can show that this two-step process with prototype updates is a bound optimization 6 ; See, for instance, Theorem 1 in <ref type="bibr" target="#b11">[12]</ref>. Therefore, as optimizing the ADM version in Eq. (20) is a bound optimizer for the mutual information (Proposition 3), the SR-Kmeans procedure can also be viewed as a bound optimizer for the mutual information, but with a different auxiliary function, as it uses a prototype-based bound on Eq. (20).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Other optimization alternatives and the link between KL and quadratic penalties for simplex variables</head><p>In this section, we discuss other alternatives for solving constrained optimization problem <ref type="bibr" target="#b9">(10)</ref>. In fact, the standard alternating direction method of multipliers (ADMM) method <ref type="bibr" target="#b24">[25]</ref> solves (10) iteratively via the following updates:</p><formula xml:id="formula_33">Φ (t+1) = argmax Φ L Φ, Q (t) , {λ (t) i,k } 1≤k≤K 1≤i≤N , ρ ,<label>(35)</label></formula><formula xml:id="formula_34">Q (t+1) = argmax Q L Φ (t+1) , Q, {λ (t) i,k } 1≤k≤K 1≤i≤N , ρ ,<label>(36)</label></formula><formula xml:id="formula_35">λ (t+1) i,k = λ (t) i,k + ρ · (q ik − p ik ), 1 ≤ i ≤ N , 1 ≤ k ≤ K,<label>(37)</label></formula><p>6. The prototype updates correspond to building a bound (auxiliary function) on high-order K-means objective expressed solely as a function of the assignment variables (i.e., the prototypes in the K-means objective are expressed with assignment variables).</p><p>where ρ &gt; is called the penalty parameter, {λ i,k } 1≤k≤K 1≤i≤N are the Lagrange multipliers and L is the augmented Lagrangian function defined as:</p><formula xml:id="formula_36">L Φ, Q, {λ i,k } 1≤k≤K 1≤i≤N , ρ = 1 N N i=1 K k=1 q ik log(p ik ) − K k=1q k log q k − N i=1 K k=1 λ i,k q ik − p ik − ρ 2 Q − P 2 2<label>(38)</label></formula><p>In <ref type="bibr" target="#b37">(38)</ref>, equality constraint Q = P is handled via a multiplier-based quadratic penalty, i.e., the last two terms in <ref type="bibr" target="#b37">(38)</ref>. The use of the KL penalty has important computational advantages over this standard augmented Lagrangian formulation. First, it is not straightforward to solve <ref type="bibr" target="#b35">(36)</ref> analytically by setting to zero the partial derivative of L, which is given by:</p><formula xml:id="formula_37">∂L Φ (t+1) , Q, {λ (t) i,k } 1≤k≤K 1≤i≤N , ρ ∂q ik = 1 N log p (t+1) ik − log 1 N N i=1 q ik − 1 N − λ (t) i,k − ρ · (q ik − p (t+1) ik )<label>(39)</label></formula><p>Here, a numerical method, with additional inner iterations, might be needed. We can use a faster, penalty-based version of (38) by removing the Lagrange-multiplier term (third term). This removes point-wise multiplier updates (37), but the quadratic penalty would still require inner iterations for solving <ref type="bibr" target="#b35">(36)</ref>. Second, while quadratic penalties are more standard in the general context of constrained optimization, the KL penalty we have in (12) has important computational advantages in the case of simplex constraints. In fact, the KL penalty in (12) has a negative-entropy barrier term, which completely removes extra Lagrangian-dual iterations/projections to handle simplex constraints q t i 1 = 1 and q i ≥ 0 ∀i. Such a barrier forces each assignment variable to be non-negative, which removes the need for extra dual variables for constraints q i ≥ 0, and conveniently yields closed-form updates for the dual variables of constraints q t i 1 = 1. These computational advantages over quadratic penalties are important, more so when dealing with large data sets. It is worth noting that, for dealing with simplex constraints, KL-based penalties are common in the context of Bregman-proximal optimization <ref type="bibr" target="#b25">[26]</ref>, with established computational and memory advantages over quadratic penalties <ref type="bibr" target="#b25">[26]</ref>. However, to our knowledge, they are less common in the clustering literature.</p><p>Moreover, for simplex variables, there is an interesting link between KL and quadratic penalties, which comes directly from the Pinsker's inequality <ref type="bibr" target="#b26">[27]</ref>. In fact, For any P and Q containing probability simplex vectors, Pinsker's inequality states that the quadratic penalty is upper-bounded by KL (up to a multiplicative constant):</p><formula xml:id="formula_38">Q − P 2 2 ≤ 2 KL(Q P )<label>(40)</label></formula><p>Therefore, for simplex variables, minimizing KL corresponds also to minimizing an upper bound on the quadratic penalty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Reconstruction loss and implementation details</head><p>We adopted the reconstruction loss and DNN architecture proposed recently in <ref type="bibr" target="#b3">[4]</ref> for our experiments. The architecture consists of a multi-layer convolutional denoising autoencoder with stridded convolutional layers in the decoder part. It is composed of three components:</p><p>• A corrupted encoder, which maps the noisy input into the embedding space. The output of each noisy encoder layer is given by:</p><formula xml:id="formula_39">z l = Dropout g W l eẑ l−1 ,<label>(41)</label></formula><p>where Dropout(·) is a stochastic mapping that randomly sets a portion of its inputs to zero <ref type="bibr" target="#b27">[28]</ref>, g is the activation function and W l e denotes the weights of the l-th encoder. L denotes the depth of the autoencoder.</p><p>• A clean decoder, which follows the corrupted encoder. The reconstruction of each layer is defined as:</p><formula xml:id="formula_40">z l−1 = g W l dz l−1 (42)</formula><p>where W l d are the weights of the l-th decoder layer.</p><p>• A clean encoder, which has the same weights as the corrupted one, i.e., the output of the l-th layer is expressed as:</p><formula xml:id="formula_41">z l = W l e (z l−1 )<label>(43)</label></formula><p>We used the rectified linear units (ReLUs) <ref type="bibr" target="#b28">[29]</ref> as activation functions. For further details on the architecture, refer to <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">Sec. 3.2]</ref>. We note that the adopted architecture is similar to the Ladder network <ref type="bibr" target="#b29">[30]</ref>, where the clean pathway is used for prediction while the corrupted one guaranties that the network is noise-invariant. As in <ref type="bibr" target="#b3">[4]</ref>, and in order to avoid over-fitting, we add a reconstruction loss function to our objectives MI-ADM in Eq. (13) and in Eq. <ref type="formula" target="#formula_0">(18)</ref>:</p><formula xml:id="formula_42">R(Z) = 1 N N i=1 L−1 l=0 1 |z l i | z l i −z l i 2 ,<label>(44)</label></formula><p>where |z l i | is the output size of the l-th layer. In the experiments described below, MI-ADM refers to the process that alternates the target updates of Eq. (14) with learning network parameters that optimize the following loss:</p><formula xml:id="formula_43">min Φ − 1 N N i=1 K k=1 q ik log p ik + R(Z)<label>(45)</label></formula><p>SR-K-means refer to the process that alternates the soft Kmeans updates in Eqs. <ref type="bibr" target="#b26">(27)</ref> and <ref type="bibr" target="#b27">(28)</ref> and learning network parameters that optimize the following loss:</p><formula xml:id="formula_44">min W 1 N λK N i=1 K k=1 q ik z i − θ ′ k 2 − 1 N λK N i=1 z T i z i + R(Z)<label>(46)</label></formula><p>Note that the deep network parameters W are firstly initialized considering the auto-encoder only <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b30">[31]</ref>: min Φ R(Z). Then the initial features are clustered via soft K-means to obtain the initial targets Q. Regarding the optimization method and hyper-parameter selection, we use the ones adopted in <ref type="bibr" target="#b3">[4]</ref>, except for new regularization parameter λ, which we introduced in <ref type="bibr" target="#b16">(17)</ref>. Namely, as stochastic optimizer, we adopt Adam <ref type="bibr" target="#b31">[32]</ref> with default parameters β 1 = 0.9, β 2 = 0.999 and ǫ = 1e −8 . We initialize weights using Xavier approach <ref type="bibr" target="#b30">[31]</ref>. The mini-batch size, learning rate and dropout parameter are set to 100, 1e −3 and 0.1, respectively. Regarding λ, as models <ref type="formula" target="#formula_0">(1)</ref> and <ref type="formula" target="#formula_0">(17)</ref> become equivalent when λ → 0, we tested the values 10 i , i = −1, . . . , −5. We found that λ = 10 −4 yields the best performance for model <ref type="bibr" target="#b16">(17)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Data sets</head><p>In order to confirm the theoretical link in Proposition 2 between discriminative model MI-ADM in <ref type="bibr" target="#b12">(13)</ref> and generative model SR-K-means in <ref type="bibr" target="#b17">(18)</ref>, we evaluated them on two handwriting datasets (USPS and MNIST) and three face datasets (Youtube-Face, CMU-PIE and FRGG <ref type="bibr" target="#b7">[8]</ref>). <ref type="table" target="#tab_0">Table 1</ref> presents a summary of the statistics of the data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Performance metrics</head><p>We adopt two standard unsupervised evaluation metrics: the accuracy (ACC) and the normalized mutual information (NMI). ACC captures the best matching between the unsupervised clustering results and the ground truth <ref type="bibr" target="#b32">[33]</ref>. NMI translates the similarity between pairs of clusters, and is invariant w.r.t permutations <ref type="bibr" target="#b33">[34]</ref>. <ref type="table" target="#tab_0">Table 1</ref> reports the results 7 of discriminative model MI-ADM in <ref type="bibr" target="#b12">(13)</ref> and generative model SR-K-means in <ref type="bibr" target="#b17">(18)</ref>. We also include the results of several related models: (1) the DEPICT model <ref type="bibr" target="#b3">[4]</ref> based on KL and logistic regression posteriors, which achieves a state-of-the-art performance on MNIST;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Evaluation of clustering algorithms</head><p>(2) DEC <ref type="bibr" target="#b8">[9]</ref>, also a KL-based approach assuming t-distribution between embedded points and cluster prototypes; and DCN <ref type="bibr" target="#b2">[3]</ref>, which optimizes a loss containing a hard K-means term and a reconstruction term.</p><p>The numerical results show that MI-ADM and SR-Kmeans algorithms may yield comparable results even for unbalanced data sets, e.g., YTF. We recall here that our analysis was done assuming the clusters are balanced. Also, notice that MI-ADM and DEPICT have approximately the same performance, confirming our earlier discussion: MI-ADM in <ref type="bibr" target="#b12">(13)</ref> can be viewed as an approximation of DEPICT in <ref type="bibr" target="#b5">(6)</ref>. The additional entropy term in <ref type="bibr" target="#b12">(13)</ref>, H(Q), has almost no effect on the results. Finally, notice the substantial difference in performance (11%) between our regularized and soft Kmeans and DCN <ref type="bibr" target="#b2">[3]</ref>, which is based on a hard K-means loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Direct maximization of the mutual information</head><p>In this section, we report the clustering results when mutual information I(X, K) is maximized directly, as proposed in <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, which is different from the two-step ADM approach in (9) &amp; <ref type="bibr" target="#b7">(8)</ref>. This amounts to solving directly the 7. Our code is publicly available at: https://github.com/MOhammedJAbi/SoftKMeans following optimization problem via SGD: max Φ I(X, K).</p><p>We refer to such direct maximization as MI-D, and report the results <ref type="bibr" target="#b7">8</ref> in <ref type="table">Table (</ref>3). As we can see, while the results of the two algorithms are practically the same for the majority of considered datasets, the MI-ADM optimizer outperforms MI-D on FRGC and CMU-PIE in terms of performance measures NMI and ACC. Regarding optimization performance, MI-ADM converges to a local optimum that is "better" than the one obtained with MI-D on the FRGC data set, but this is not the case for MNIST, for instance, where both methods converge approximately to the same solution. Our results may not be enough to claim that MI-ADM is a better optimizer than MI-D, in general, as none of the two optimization methods (MI-ADM and MI-D) provides optimality guarantee or bounds for highly non-convex problems involving deep networks. However, our results are consistent with recent optimization works, e.g., <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, which showed that variants of ADMM could be effective alternatives to SGD for supervised deep learning problems. Those differences in optimization performances do not provide a full explanation of the fact that MI-ADM outperforms MI-D on FRGC and CMU-PIE. In fact, to the best of our knowledge, there is no rigorous quantification of how maximizing the mutual information, i.e., I(X, K), which is computed based on the predictions, is maximizing the commonly used performance metrics, i.e., the accuracy (ACC) and normalized mutual information (NMI), which are both computed based on the true ground-truth labels, for unsupervised clustering problems.</p><p>Finally, notice that <ref type="figure" target="#fig_2">Fig. 1</ref> confirms Proposition 3 experimentally, i.e. the MI does not increase during the iterations of KL-based ADM, similarly to a direct SGD maximization. We used the FRGC data set for this plot as a typical example, but the MI evolution during the iterations of MI-ADM follows the same form for the remaining data sets. <ref type="bibr" target="#b7">8</ref>. Again, here, and as done for MI-ADM and SR-K-means, we added the reconstruction term when learning the network parameters, i.e., we solve min Φ −I(X, K) + R(Z) in MI-D.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>We showed that several prevalent state-of-the-art models for deep clustering are equivalent to K-means under mild conditions and commonly used posterior models and parameter regularization. We proved that, for the standard logistic regression posteriors, maximizing the L 2 regularized mutual information via the alternating direction method (ADM) is equivalent to a soft and regularized K-means loss. Our theoretical analysis not only connected directly several recent discriminative models to K-means, but also led to a new soft and regularized deep K-means algorithm, which gave competitive results on several image clustering benchmarks. Furthermore, our result suggests several interesting extensions for future works. For instance, it is well known that simple parametric prototypes such as the means, as in K-means, may not be good representatives of manifold-structured and high-dimensional inputs such as images. Investigating other prototype-based objectives such as K-modes <ref type="bibr" target="#b36">[37]</ref> may provide better representatives of the data. Also, for manifold-structured inputs, investigating pairwise clustering objectives such as normalized cut <ref type="bibr" target="#b37">[38]</ref>, in conjunction with reconstruction losses, might be more appropriate for deep image clustering. Namely, it is interesting to see how using the loss function of the aforementioned algorithms, e.g., [37, Eq. (1)] and <ref type="bibr" target="#b37">[38,</ref><ref type="bibr">Eq. (3)</ref>], instead of the K-means components in (46), will affect the results. Also, it is worthy to investigate a possible link between MI and the loss functions of those algorithms. As a final comment, we add here the KL divergence to enforce constraint Q = P when going from (10) to <ref type="bibr" target="#b11">(12)</ref>. As a a future work, it will be interesting to analyze the optimality and convergence of MI-ADM if different distance measures, e.g., Bhattacharyya measures family <ref type="bibr" target="#b38">[39]</ref>, and penalty methods for constrained optimization, e.g., generalized quadratic penalty <ref type="bibr" target="#b39">[40]</ref>, are adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A</head><p>This appendix derives the approximation in <ref type="bibr" target="#b22">(23)</ref>. Assuming that the empirical label distribution is approximately uniform, i.e.,q k = 1</p><formula xml:id="formula_45">N N i=1 q ik ≈ 1 K ⇐⇒ K N N i=1 q ik ≈ 1 ∀k, we have N i=1 K k=1 −2q ik θ T k z i + N λ K k=1 θ T k θ k (47) ≈ N i=1 K k=1 −2q ik θ T k z i + N λ K k=1 K N N i=1 q ik θ T k θ k (48) = N i=1 K k=1 −2q ik θ T k z i + Kλ K k=1 N i=1 q ik θ T k θ k (49) = N i=1 K k=1 −2q ik θ T k z i + Kλ N i=1 K k=1 q ik θ T k θ k + 1 λK N i=1 z T i z i − 1 λK N i=1 z T i z i (50) = N i=1 K k=1 −2q ik θ T k z i + Kλ N i=1 K k=1 q ik θ T k θ k + 1 λK N i=1 K k=1 q ik z T i z i − 1 λK N i=1 z T i z i (51) = N i=1 K k=1 q ik − 2θ T k z i + √ Kλ 2 θ T k θ k + 1 √ λK 2 z T i z i − 1 λK N i=1 z T i z i (52) = N i=1 K k=1 q ik 1 √ λK z i − √ λKθ k 2 − 1 λK N i=1 z T i z i .<label>(53)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>•</head><label></label><figDesc>M. Jabi is with Ultra Electronics-TCS, Montreal, Canada. • M. Pedersoli and I. Ben Ayed are with ETS Montreal, Canada. • A. Mitiche is with INRS, Montreal, Canada.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>2 .</head><label>2</label><figDesc>The most basic form of the ADM approach transforms a singlevariable problem of the form minx u(x) + v(x) into a constrained twovariable problem of the form maxx,y u(x) + v(y) s.t. x = y. This splits the original problem into two easier sub-problems, alternating optimization over variables x and y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1 :</head><label>1</label><figDesc>Evolution of the MI, I(X, K), during the iterations of MI-ADM and MI-D algorithms for the FRGC data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 :</head><label>1</label><figDesc>Description of the datasets</figDesc><table><row><cell>Dataset</cell><cell># Samples</cell><cell># Classes</cell><cell>#Dimensions</cell><cell cols="2">% of smallest class % of largest class</cell></row><row><cell>USPS</cell><cell>11000</cell><cell>10</cell><cell>1 × 16 × 16</cell><cell>10 %</cell><cell>10 %</cell></row><row><cell>MNIST-test</cell><cell>10000</cell><cell>10</cell><cell>1 × 28 × 28</cell><cell>8.92 %</cell><cell>11.35 %</cell></row><row><cell>MNIST-full</cell><cell>79000</cell><cell>10</cell><cell>1 × 28 × 28</cell><cell>9.01 %</cell><cell>11.25 %</cell></row><row><cell cols="2">Youtube-Face (YTF) 10000</cell><cell>41</cell><cell>3 × 55 × 55</cell><cell>0.31 %</cell><cell>6.94 %</cell></row><row><cell>CMU-PIE</cell><cell>2856</cell><cell>68</cell><cell>1 × 32 × 32</cell><cell>1.47 %</cell><cell>1.47 %</cell></row><row><cell>FRGC</cell><cell>2462</cell><cell>20</cell><cell>3 × 32 × 32</cell><cell>0.24%</cell><cell>10.51 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 :</head><label>2</label><figDesc>Comparison of clustering algorithms on four date sets based on accuracy and normalized mutual information. The results of SR-K-means algorithm are obtained using λ = 10 −4 . (-), (*), ( † ) stands for "not reported" and "reported" in<ref type="bibr" target="#b2">[3]</ref> and<ref type="bibr" target="#b3">[4]</ref>, respectively.</figDesc><table><row><cell>Dataset</cell><cell cols="2">USPS</cell><cell cols="2">MNIST-test</cell><cell cols="2">MNIST-full</cell><cell cols="2">YTF</cell><cell cols="2">CMU-PIE</cell><cell cols="2">FRGC</cell></row><row><cell></cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell></row><row><cell>MI-ADM</cell><cell>0.948</cell><cell>0.979</cell><cell>0.885</cell><cell>0.871</cell><cell>0.922</cell><cell>0.969</cell><cell>0.801</cell><cell>0.606</cell><cell>0.965</cell><cell>0.858</cell><cell>0.580</cell><cell>0.431</cell></row><row><cell>SR-K-means</cell><cell>0.936</cell><cell>0.974</cell><cell>0.873</cell><cell>0.863</cell><cell>0.866</cell><cell>0.939</cell><cell>0.806</cell><cell>0.605</cell><cell>0.945</cell><cell>0.902</cell><cell>0.487</cell><cell>0.413</cell></row><row><cell cols="2">DEPICT [4] DCN (K-means based) [3] -0.945</cell><cell>0.978 -</cell><cell>0.886 -</cell><cell>0.872 -</cell><cell>0.925 0.81  *</cell><cell>0.971 0.83  *</cell><cell>0.802 -</cell><cell>0.611 -</cell><cell>0.964 -</cell><cell>0.850 -</cell><cell>0.583 -</cell><cell>0.432 -</cell></row><row><cell>DEC (KL based) [9]</cell><cell>0.586  †</cell><cell>0.619  †</cell><cell>0.827  †</cell><cell>0.859  †</cell><cell>0.816  †</cell><cell>0.844  †</cell><cell>0.446  †</cell><cell>0.371  †</cell><cell>0.924  †</cell><cell>0.801  †</cell><cell>0.505  †</cell><cell>0.378  †</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3 :</head><label>3</label><figDesc>Comparison of MI-ADM and MI-D on four datasets (accuracy and normalized mutual information).</figDesc><table><row><cell>Dataset</cell><cell cols="2">USPS</cell><cell cols="2">MNIST-test</cell><cell cols="2">MNIST-full</cell><cell cols="2">YTF</cell><cell cols="2">CMU-PIE</cell><cell cols="2">FRGC</cell></row><row><cell></cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell><cell>NMI</cell><cell>ACC</cell></row><row><cell cols="2">MI-ADM 0.948</cell><cell cols="2">0.979 0.885</cell><cell>0.871</cell><cell cols="2">0.922 0.969</cell><cell cols="2">0.801 0.606</cell><cell>0.965</cell><cell cols="2">0.858 0.580</cell><cell>0.431</cell></row><row><cell>MI-D</cell><cell>0.948</cell><cell cols="2">0.979 0.880</cell><cell>0.867</cell><cell cols="2">0.921 0.967</cell><cell cols="2">0.800 0.616</cell><cell>0.867</cell><cell cols="2">0.705 0.444</cell><cell>0.322</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A deep semi-nmf model for learning hidden representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1692" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthijs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference On Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1692" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards kmeans-friendly spaces: Simultaneous deep learning and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Sidiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3861" to="3870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering via joint convolutional autoencoder embedding and relative entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dizaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5747" to="5756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning discrete representations via information maximizing self-augmented training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tokui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matsumoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1558" to="1567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Variational deep embedding: A generative approach to clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1965" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1965" to="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Joint unsupervised learning of deep representations and image clusters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5147" to="5156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Assessing a mixture model for clustering with the integrated completed likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Biernacki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Celeux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Govaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="719" to="725" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kernel cuts: Kernel and spectral clustering meet regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Kernel clustering: density biases and solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative clustering by regularized information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Gomes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="775" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised classifiers, mutual information and &quot;phantom targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Bridle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J R</forename><surname>Heading</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="1096" to="1101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Aljalbout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07648</idno>
		<title level="m">Clustering with deep learning: Taxonomy and new methods</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J C</forename><surname>Mackay</surname></persName>
		</author>
		<title level="m">Information Theory, Inference and Learning Algorithms</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Volumetric bias in segmentation and reconstruction: Secrets and solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Isack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1769" to="1777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimization transfer using surrogate objective functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Hunter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and graphical statistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Surrogate maximization/minimization algorithms and extensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="1" to="33" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The concave-convex procedure (CCCP)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1033" to="1040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A submodular-supermodular procedure with applications to discriminative structure learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3020336.3020387" />
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="404" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">EE364b lecture notes in alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bregman-proximal augmented lagrangian approach to multiphase image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scale Space and Variational Methods in Computer Vision (SSVM</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="524" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Csiszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Körner</surname></persName>
		</author>
		<title level="m">Information Theory: Coding Theorems for Discrete Memoryless Systems</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note>2nd ed</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">From neural PCA to deep unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Valpola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7783</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Res. Logist. Quart</title>
		<imprint>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Document clustering based on non-negative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="267" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Training neural networks without gradients: A scalable ADMM approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Burmeister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2722" to="2731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ADMM for efficient deep learning with global convergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scalable laplacian kmodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ben Ayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">SpectralNet: Spectral clustering using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kluger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The divergence and Bhattacharyya distance measures in signal selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kailath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communication Technology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="60" />
			<date type="published" when="1967-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">On penalty and multiplier methods for constrained minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM J. control and optimization</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
