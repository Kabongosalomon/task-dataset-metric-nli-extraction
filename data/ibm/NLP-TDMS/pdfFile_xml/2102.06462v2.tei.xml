<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milad</forename><surname>Hashemi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danai</forename><surname>Koutra</surname></persName>
						</author>
						<title level="a" type="main">Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most graph neural networks (GNN) perform poorly in graphs where neighbors typically have different features/classes (heterophily) and when stacking multiple layers (oversmoothing). These two seemingly unrelated problems have been studied independently, but there is recent empirical evidence that solving one problem may benefit the other. In this work, going beyond empirical observations, we theoretically characterize the connections between heterophily and oversmoothing, both of which lead to indistinguishable node representations. By modeling the change in node representations during message propagation, we theoretically analyze the factors (e.g., degree, heterophily level) that make the representations of nodes from different classes indistinguishable. Our analysis highlights that (1) nodes with high heterophily and nodes with low heterophily and low degrees relative to their neighbors (degree discrepancy) trigger the oversmoothing problem, and (2) allowing "negative" messages between neighbors can decouple the heterophily and oversmoothing problems. Based on our insights, we design a model that addresses the discrepancy in features and degrees between neighbors by incorporating signed messages and learned degree corrections. Our experiments on 9 real networks show that our model achieves state-of-the-art performance under heterophily, and performs comparably to existing GNNs under low heterophily (homophily). It also effectively addresses oversmoothing and even benefits from multiple layers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, graph neural networks or GNNs <ref type="bibr" target="#b4">(Defferrard et al., 2016;</ref><ref type="bibr" target="#b7">Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b24">Veličković et al., 2017)</ref> have been used effectively in applications ranging from so- cial science <ref type="bibr" target="#b8">(Li &amp; Goldwasser, 2019)</ref> and biology <ref type="bibr" target="#b29">(Yan et al., 2019)</ref> to program understanding <ref type="bibr" target="#b0">(Allamanis et al., 2018;</ref><ref type="bibr" target="#b22">Shi et al., 2019)</ref>. A typical GNN architecture <ref type="bibr" target="#b27">(Xu et al., 2018a)</ref> for the node classification task can be decomposed into two coarse steps: propagation/aggregation, and combination. Messages are first exchanged between neighboring nodes, then aggregated. Afterwards, the messages are combined with the self-representations (a.k.a., the current node representations) to update the node representations. Though GNNs are effective, they have some key limitations.</p><p>The first limitation is known as the "oversmoothing" problem <ref type="bibr" target="#b10">(Li et al., 2018)</ref>: the performance of GNNs degrade when stacking many layers. Recent work has found that oversmoothing could be caused by GNNs exponentially losing expressive power in the node classification task <ref type="bibr" target="#b13">(Oono &amp; Suzuki, 2019)</ref> and that the node representations converge to a stationary state which is decided by the degree of the nodes and the initial features <ref type="bibr" target="#b2">(Chen et al., 2020;</ref><ref type="bibr" target="#b26">Wang et al., 2019;</ref><ref type="bibr" target="#b16">Rong et al., 2019;</ref><ref type="bibr" target="#b17">Rossi et al., 2020)</ref>. These works focus on the analysis of the steady state in the limit of infinite layers, but they do not explore the dynamics on how oversmoothing is triggered, or which nodes tend to cause it. To date, works like <ref type="bibr" target="#b28">(Xu et al., 2018b;</ref><ref type="bibr" target="#b26">Wang et al., 2019;</ref><ref type="bibr" target="#b2">Chen et al., 2020)</ref> have provided heuristic-based model designs to alleviate the oversmoothing problem. To fill in this gap, we seek to theoretically analyze the dynamics around oversmoothing.</p><p>The second limitation of GNNs is their poor performance on heterophily (or low-homophily) graphs <ref type="bibr" target="#b15">(Pei et al., 2019;</ref><ref type="bibr" target="#b33">Zhu et al., 2020)</ref>, which-unlike homophily graphs-have many neighboring nodes that belong to different classes <ref type="bibr" target="#b12">(Newman, 2002)</ref>. For instance, in protein networks, amino acids of different types tend to form links <ref type="bibr" target="#b33">(Zhu et al., 2020)</ref>, and in transaction networks, fraudsters are more likely to connect to accomplices than to other fraudsters <ref type="bibr" target="#b14">(Pandit et al., 2007)</ref>. Most GNNs <ref type="bibr" target="#b7">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b24">Veličković et al., 2017)</ref> fail to effectively capture heterophily. Though emerging works have proposed some effective designs <ref type="bibr" target="#b15">(Pei et al., 2019;</ref><ref type="bibr" target="#b33">Zhu et al., 2020)</ref>, it remains an under-explored area.</p><p>These two limitations have mostly been studied independently, but recent work on oversmoothing <ref type="bibr" target="#b2">(Chen et al., 2020)</ref> was shown empirically to address heterophily. Motivated by this empirical observation, we set out to understand the theo-arXiv:2102.06462v2 <ref type="bibr">[cs.</ref>LG] 24 Feb 2021 retical connection between the oversmoothing problem and GNNs' poor performance on heterophily graphs by studying the change in node representations during message passing. Specifically, we make the following contributions:</p><p>• Theory: We investigate the dynamics of the node representations in the first and deeper layers. We model node features as random vectors and theoretically analyze the factors (e.g., degree, heterophily) that make the representations of nodes from different classes indistinguishable (oversmoothing). • Insights: We find that (i) nodes with high heterophily and nodes with low heterophily &amp; low degrees compared to their neighbors (degree discrepancy) trigger the oversmoothing problem; and (ii) allowing signed messages between neighbors can decouple the heterophily and oversmoothing problems. • Powerful model: Based on these insights, we design a robust, generalized model, GGCN, that allows negative interactions between nodes and compensates for the effect of low-degree nodes through a learned rescaling scheme. Our empirical results show that our model is highly robust, achieving state-of-the-art performance on datasets with high levels of heterophily, and competitive performance on homophily datasets. Moreover, GGCN does not suffer from oversmoothing but rather improves or remains stable when stacking more layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>We first provide the notations and definitions that we use throughout the paper, and a brief background on GNNs.</p><p>Notation. We denote an unweighted and self-loop-free graph as G (V, E) and its adjacency matrix as A. We represent the degree of node v i ∈ V by d i , and the degree matrix-which is a diagonal matrix whose elements are node degrees-by D. Let N i be the set of nodes directly connected to v i , i.e., its neighbors. I is the identity matrix.</p><p>We denote the node representations/features/output at l-th layer as F l and the weight matrix learned at the l-th layer as W l . Node v i has a feature vector f i (the i-th row of F) and l-th layer representation vector f l i . Homophily and Heterophily. Given a set of node labels / classes, the notion of homophily captures the tendency of a node to have the same class as its neighbors. Specifically, the homophily of node v i is defined as</p><formula xml:id="formula_0">h i = E( |N s i | |Ni| ), where N s</formula><p>i is the set of neighboring nodes with the same label as v i , | · | is the cardinality operator, and the expectation is taken over the randomness of the node labels. High homophily corresponds to low heterophily, and vice versa, so we use these terms interchangeably throughout the paper.</p><p>Supervised Node Classification Task. We focus on node classification: Given a subset of labeled nodes (from a label set L), the goal is to learn a mapping F :</p><formula xml:id="formula_1">f i → χ(v i )</formula><p>between each node v i and its ground truth label χ(v i ) ∈ L.</p><p>Graph Neural Networks. In general, for the node classification task, GNNs can be decomposed into two steps <ref type="bibr" target="#b27">(Xu et al., 2018a)</ref>: (1) neighborhood propagation and aggregation:</p><formula xml:id="formula_2">f l i = AGGREGATE(f l j , v j ∈ N i ), and (2) com- bination: f l+1 i = COMBINE(f l i , f l i )</formula><p>. The loss is given by CrossEntropy(softmax(f l i ), χ(v i )). A popular form of GNN is the Graph Convolutional Network (GCN). Among the various GCN variants ( § 6), the vanilla GCN model suggests a renormalization trick on the adjacency A to prevent gradient explosion <ref type="bibr" target="#b7">(Kipf &amp; Welling, 2016)</ref>. The (l + 1)-th output is given by: F l+1 = σ(ÃF l W l ), whereÃ =D −1/2 (I + A)D −1/2 ,D is the degree matrix of I + A, and σ is ReLU. An attention mechanism can be used to improve performance. For instance, the graph attention network (GAT) <ref type="bibr" target="#b24">(Veličković et al., 2017)</ref> computes nonnegative attention weights on neighborhood nodes and replacesÃ with the learned attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theoretical Analysis</head><p>In this section, we analyze the theoretical connections between the oversmoothing and heterophily problems. Unlike previous works <ref type="bibr" target="#b2">(Chen et al., 2020;</ref><ref type="bibr" target="#b26">Wang et al., 2019)</ref> that study the final node representations in the limit of infinite layers, we focus on the dynamic analysis of the node representations that are gradually passed to deeper layers.</p><p>Setup. We consider a binary node classification task on an unweighted, self-loop-free graph G with adjacency matrix A. We denote the set of nodes in the first and second class as V 1 and V 2 , respectively. When v i ∈ V 1 , f 0 i follows a distribution with mean µ and variance Σ, and when v i ∈ V 2 , f 0 i follows a distribution with −µ and Σ. Our analysis uses GCN layers ( § 2) without nonlinearities and focuses on binary classification, but we demonstrate empirically that our results still apply to general settings ( § 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Heterophily &amp; Oversmoothing</head><p>At a high level, our analysis highlights that both the oversmoothing problem and the heterophily problem result from less linear separability of learned representations for nodes in different classes after propagation and aggregation across layers. Specifically, our theory demonstrates thatin expectation-the layer-wise transformation of a node's representation depends on two factors: (1) the level of homophily (proportion of neighbors with the same class label), and (2) the relative degree of the node (how many neighbors it has relative to its neighbors). These factors determine if the node representation will tend to move towards the initial mean feature vector of the opposing class (become less linearly separable) or away (become more linearly separable). As shown in the left panel of <ref type="figure" target="#fig_0">Fig. 1</ref>, in more detail:</p><p>• In heterophily (low homophily) graphs, the decrease in the linear separability is severe, causing performance drop even in shallow layers (case 1). • In homophily graphs, high-degree nodes may initially benefit (case 3), leading to performance gain in shallow layers. However, the low-degree nodes eventually reduce the linear separability (case 2).</p><p>In more detail, the layer-wise message propagation and aggregation in GCNs is done via theÃF l operation. Thus, for simplicity, our dynamic analysis investigates the impact of this operation on the node class and its relation to the level of homophily. However, the following conclusions still hold after scalingÃF l by the learnable weight matrix W l . Next, we consider two stages: the initial stage at shallow layers of propagation and aggregation ( § 3.1), and the developing stage at deeper layers ( § 3.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. INITIAL STAGE: SHALLOW LAYERS</head><p>Theorem 1. [Initial Stage] During message passing in shallow layers, the nodes with low homophily h i ≤ 0.5 and the nodes with high homophily h i &gt; 0.5 but small degree d i (w.r.t. their neighbors) are prone to be misclassified. Defining r ij ≡ di+1 dj +1 and r i ≡ 1 di j∈Ni r ij , the expectation of feature f 1 i after operationÃF 0 at layer 1 is:</p><formula xml:id="formula_3">E(f 1 i ) = (2h i − 1)d i r i + 1 d i + 1 E(f 0 i ) ≡ γ 1 i E(f 0 i ), (1) where γ 1 i ∈      (−∞, 1 2 ], if h i ≤ 0.5 ( 1 2 , 1], if h i &gt; 0.5 &amp; r i ≤ 1 2hi−1 (1, ∞), otherwise.</formula><p>(2)</p><p>For independent features f 0 i , the variance of each node is reduced to 1</p><formula xml:id="formula_4">(di+1) 1 (di+1) + j∈Ni 1 (dj +1) Σ ≤ 1 2 Σ.</formula><p>Proof. We provide the proof in App. A.1.</p><p>Intuitively, under heterophily (case 1), the expected feature vector will move heavily towards the mean feature vector of the opposing class. For high homophily but low degrees (case 2), it will contract towards the origin. Otherwise, it will move away from the opposing class mean (case 3).</p><p>We note that our analysis shows that the variance of each node is reduced in the initial stage regardless of the homophily level. However, this does not mean that the variance of the nodes in the same class is reduced. The class variance is decided not only by the node variance but also how dispersed their mean is.</p><p>. DEVELOPING STAGE: DEEP LAYERS Based on Eqs.</p><p>(1)-(2), after propagation in several layers, the features of low-homophily nodes, and those of highhomophily but low-degree nodes may flip their sign. Assuming initial features f 0 i , propagation through l layers, and accumulated discount factor at the l-th layer γ l i &gt; 0, we denote the sign-flipping features as f l i = −γ l i f 0 i , and the remaining features as f l i = γ l i f 0 i . We further define the effective homophily h l i of node v i as the ratio of the neighbors that have the same sign as v i at the l-th layer. Similar to the analysis in the initial stage, we have:</p><p>Theorem 2. [Developing Stage] During message passing in deeper layers, when the effective homophily h l i is sufficiently low ( h l i ≤ 0.5), the higher-degree nodes (w.r.t. its neighbors) are prone to misclassification. Specifically, the expectation of the feature f l+1 i after the operationÃF l at the (l + 1)-th layer is given by:</p><formula xml:id="formula_5">E(f l+1 i ) = (2 h l i − 1)d i r i + 1 d i + 1 γ l i E(f 0 i ).<label>(3)</label></formula><p>Proof. The derivation is similar to that of Theorem 1 except that we replace the ground truth heterophily h i with the effective heterophily h l i and E(f l i ) with γ l i E(f 0 i ). Since graph G is connected, the node degrees d i ≥ 1; thus, di di+1 and r i are increasing functions of d i , while 1 di+1 is a decreasing function of d i . When h l i ≤ 0.5, we have (2 h l i − 1) ≤ 0, and (2 h l i − 1) · di di+1 · r i + 1 di+1 is a decreasing function of d i in Eq. (3). This suggests that the higher-degree nodes move towards the opposing class more.</p><p>We note that Eq. (3) also holds for layers l ≥ 2 up to the layer when the effective homophilyĥ l i becomes smaller or equal to 0.5 for all the nodes. We regard these layers part of the initial stage, as they have three cases similar to Eq. (2).</p><p>. SUMMARY OF THEORETICAL RESULTS Three conjectures follow from Theorems 1 and 2: (1) After message propagation, the mean vectors E(f 1 i ) will be dispersed. For a high-homophily graph, in the initial stage, the features of low-degree nodes (where r i 1 2hi−1 ) will first move towards the opposing class mean and have relatively lower accuracy (case 2). Later, when propagating to more layers, if the accuracy of low-degree nodes drops to a level such that the effective homophily of higher degree nodes satisfies h l i ≤ 0.5, then their features begin to move towards the opposing class mean vector (case 1) because (2 h l i − 1) ≤ 0 in Eq. (3).</p><p>(2) The message-passing based GNN model performs differently on graphs with different degree distributions. For homophily graphs with power-law distribution (with many low-degree nodes), the performance of low-degree nodes will degenerate fast in the initial stage and the model will quickly enter the developing stage. Then, due to few highdegree nodes, the performance will degenerate more slowly.</p><p>(3) The normalization scheme of the adjacency matrix of a graph makes a difference. The reason that node degrees appear in Eqs.</p><p>(2)-(3) is the renormalization trick on A in GCN ( § 2). Another normalization scheme such as rownormalization makes the scenario equivalent to a d-regular graph where nodes have the same degree (App. A.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Signed Messages for Heterophily &amp; Oversmoothing</head><p>Attention-based GNN models <ref type="bibr" target="#b24">(Veličković et al., 2017)</ref> typically use non-negative attention weights to discount the contribution from dissimilar neighbors. However, our theory highlights that signed messages are crucial for enhancing performance in heterophily graphs and alleviating the oversmoothing problem. Due to space limitations, we only show the effect of signed messages in the initial stage; similar results can be derived in the developing stage.</p><p>Setup. Consider the scenario where the "messages" sent by neighbors of a different class must be negated, and those by neighbors of the same class must be sent as-is. For node v i , we define m l i as the ratio of neighbors that send incorrect "messages" at the l-th layer (i.e., different-class neighbors that send non-negated messages and same-class neighbors that send negated messages). We also define the l-th layer error rate as e l i = E(m l i ), where the expectation is over the randomness of the neighbors that send incorrect messages. We make two assumptions: (1) Conditioned on m l i , whether the neighbor v j will wrongly send the information is independent of its class χ(v j ); (2) The variable m l i is independent of the classes that nodes v j ∈ N i belong to.</p><p>Theorem 3. With the independence assumptions and by allowing the messages to be optionally multiplied by a negative sign, in the initial stage, the mean distance between the nodes in different classes will not be affected by the initial homophily level h i , but will be reduced by a bigger error rate e l i . Specifically, the multiplicative factor γ 1 i after the operationÃF 0 at first layer is given by:</p><formula xml:id="formula_6">E(f 1 i ) = (1 − 2e 0 i )d i r i + 1 d i + 1 E(f 0 i ) ≡ γ 1 i E(f 0 i ), (4) where γ 1 i ∈      (−∞, 1 2 ], if e 0 i ≥ 0.5 ( 1 2 , 1], if e 0 i &lt; 0.5 &amp; r i ≤ 1 1−2e 0 i (1, ∞), otherwise.<label>(5)</label></formula><p>Proof. A detailed proof is provided in App. A.2.</p><p>From Eq. (4), we conclude that: (1) Allowing neighbors of a different class to negate their messages prevents low homophily from degenerating performance. For a model that achieves low error rate at shallow layers (i.e, e 0 i 0.5), the low-degree nodes (r i</p><formula xml:id="formula_7">1 1−2e 0 i ) trigger oversmooth- ing.</formula><p>(2) If a model does not work well under heterophily (i.e., high error rate e l i in identifying neighbors from other classes), it is prone to oversmoothing even in shallow layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model Design</head><p>Based on our theoretical analysis, we propose two new, simple mechanisms to address both the heterophily and oversmoothing problems: signed messages and degree corrections. We integrate these mechanisms, along with a decaying combination of the current and previous node representations <ref type="bibr" target="#b2">(Chen et al., 2020)</ref>, into a robust, generalized GCN model, GGCN. In § 5, we empirically show its effectiveness in addressing the two closely-related problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Signed Messages</head><p>Theorem 3 points out the important role of signed messages in tackling the heterophily and oversmoothing problems. Our first proposed mechanism incorporates signed messages through cosine similarity between the nodes.</p><p>For expressiveness, as in GCN <ref type="bibr" target="#b7">(Kipf &amp; Welling, 2016)</ref>, we first perform a learnable linear transformation of each node's representation at the l-th layer:F l = F l W l + b l , where b l is the bias vector at the l-th layer. Then, we define a sign function to be multiplied with the messages exchanged between neighbors. To allow for backpropagation of the gradient information, we approximate the sign function with a proxy, the correlation matrix of feature vectors without the self-correlation (i.e., I):</p><formula xml:id="formula_8">S l =F lFl T max diag(F lFl T ) · diag(F lFl T ) T , δ − I,</formula><p>where diag(·) is a column vector with the diagonal elements of the enclosed matrix, and δ is a very small constant (e.g., 10 −9 ) to prevent numerical instability. In addition to providing the necessary sign information, the correlation weights themselves provide more modeling flexibility.</p><p>In order to separate the contribution of similar neighbors (likely in the same class) from that of dissimilar neighbors (unlikely to be in the same class), we split S l into a positive matrix S l pos and a negative matrix S l neg . Thus, our proposed GGCN model learns a weighted combination of the selffeatures, the positive messages, and the negative messages:</p><formula xml:id="formula_9">F l+1 = σ αl (β l 0F l +β l 1 (S l pos Ã )F l +β l 2 (S l neg Ã )F l ) ,<label>(6)</label></formula><p>whereβ l 0 ,β l 1 andβ l 2 are scalars obtained by applying soft-max to the learned scalars β l 0 , β l 1 and β l 2 ; the non-negative scaling factorα l = softplus(α l ) is derived from the learned scalar α l ; is element-wise multiplication; and σ is the nonlinear function Elu. We note that we learn different α and β parameters per layer for flexibility. We also require the combined weights,α lβl x , to be non-negative so that they do not negate the intended effect of the signed information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Degree Corrections</head><p>Our analysis in § 3.2 highlights that, when allowing signed messages, oversmoothing is initially triggered by lowdegree nodes. Thus, our second proposed mechanism aims to compensate for low degrees via degree corrections.</p><p>Based on Eq. (5), and given that most well-trained graph models have a relatively low error rate at the shallow layers (i.e, e l i 0.5 for small l), we require that the node degrees satisfy r i ≥ 1 1−2e 0 i to prevent oversmoothing. As we mentioned in § 3.1, the degrees appear in our theorems due to the renormalization trick on the adjacency matrix ( § 2). Since the node degrees cannot be modified, our strategy is to rescale them. Specifically, starting with the original formulation after a propagation at layer l + 1:</p><formula xml:id="formula_10">(ÃF l )[i, :] =F l [i, :] di + 1 + v j ∈N iF l [j, :] √ di + 1 dj + 1 ,<label>(7)</label></formula><p>we correct the degrees by multiplying with scalars τ l ij :</p><formula xml:id="formula_11">(ÃF l )[i, :] =F l [i, :] di + 1 + v j ∈N i τ l i,jF l [j, :] √ di + 1 dj + 1 .<label>(8)</label></formula><p>This multiplication is equivalent to changing the ratio r ij in</p><formula xml:id="formula_12">Thm. 1 to (τ l ij ) 2 (di+1) dj +1</formula><p>. That is, a larger τ l ij increases the effective r i at layer l.</p><p>Training independent τ l i,j is not practical because it would require O(|V| 2 ) additional parameters per layer, which can lead to overfitting. Moreover, low-rank parameterizations suffer from unstable training dynamics. Intuitively, when r ij is small, we would like to compensate for it via a larger τ l i,j . Thus, we set τ l i,j to be a function of r ij as follows:</p><formula xml:id="formula_13">τ l ij = softplus λ l 0 1 r ij − 1 + λ l 1 ,<label>(9)</label></formula><p>where λ l 0 and λ l 1 are learnable parameters. We subtract 1 so that when r ij = 1 (i.e., d i = d j ), then τ l ij = softplus(λ l 1 ) is a constant bias.</p><p>In our proposed GGCN model, we combine the signed messages in Eq. (6) and our degree correction mechanism to obtain the features at layer l + 1:</p><formula xml:id="formula_14">F l+1 = σ αl βl 0F l +β l 1 ( S l pos Ã T T T l )F l (10) +β l 2 ( S l neg Ã T T T l )F l ,<label>(11)</label></formula><p>where T T T l is a matrix with elements τ l ij .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Decaying Aggregation</head><p>In addition to our two proposed mechanisms that are theoretically grounded in our analysis ( § 3), we also incorporate into GGCN an existing design-decaying aggregation of messages-that empirically increases performance. However, we note that, even without this design, our GNN architecture still performs well under heterophily and is robust to oversmoothing ( §5.4).</p><p>Decaying aggregation was introduced in <ref type="bibr" target="#b2">(Chen et al., 2020)</ref> as a way to slow down the convergence rate of node features in a K-layer GNN model with residual connections <ref type="bibr" target="#b13">(Oono &amp; Suzuki, 2019)</ref>. Inspired by this work, we modify the decaying function,η, and integrate it to our GGCN model:</p><formula xml:id="formula_15">F l+1 = F l +η σ αl (β l 0F l +β l 1 (S l pos Ã T T T l )F l (12) +β l 2 (S l neg Ã T T T l )F l ) .</formula><p>In practice, we found that the following decaying function works well:η ≡ ln( η l k + 1), iff l ≥ l 0 ;η = 1, otherwise. The hyperparameters k, l 0 , η are tuned on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Additional Considerations: Batch vs. Layer norm</head><p>Other mechanisms, such as batch or layer norm, may be seen as solutions to the heterophily and oversmoothing problems. However, batch norm cannot compensate for the dispersion of the mean vectors ( § 3) due to different degrees and homophily levels of the nodes. Although, to some extent, it reduces the speed by which the feature vectors of the susceptible nodes (case 1 &amp; 2) move towards the other class (good for oversmoothing), it also prevents the feature vectors of the nodes that could benefit from the propagation (case 3) from increasing the distances (drop in accuracy). In our experiments (App. B.1), we find that, in general, adding batch norm significantly decreases the accuracy in heterophily datasets, but helps alleviate oversmoothing. Layer norm is a better option <ref type="bibr" target="#b32">(Zhou et al., 2020)</ref> for oversmoothing and leads to a smaller decrease in accuracy than batch norm in most datasets. However, layer norm leads to a significant accuracy drop in some datsets when a subset of features are more important than the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We aim to answer four questions: (Q1) Compared to the baselines, how does GGCN perform on homophily and heterophily graphs? (Q2) How robust is it against oversmoothing under homophily and heterophily? (Q3) How do different design choices affect its performance? (Q4) How does the performance for nodes with different degrees change with the number of layers in real datasets? ( § 3.1) <ref type="table" target="#tab_7">Table 1</ref>. Real data: mean accuracy ± stdev over different data splits. Per GNN model, we report the best performance across different layers. Best model per benchmark highlighted in gray. The " † " results (GraphSAGE) are obtained from <ref type="bibr" target="#b33">(Zhu et al., 2020)</ref>.  Datasets. We evaluate the performance of our GGCN model and existing GNNs in node classification on various real-world datasets <ref type="bibr" target="#b23">(Tang et al., 2009;</ref><ref type="bibr" target="#b18">Rozemberczki et al., 2019;</ref><ref type="bibr" target="#b20">Sen et al., 2008;</ref><ref type="bibr" target="#b11">Namata et al., 2012;</ref><ref type="bibr" target="#b21">Shchur et al., 2018)</ref>. We provide their summary statistics in <ref type="table" target="#tab_7">Table 1</ref>, where we compute the homophily level h of a graph as the average of h i of all nodes v i ∈ V. For all benchmarks, we use the feature vectors, class labels, and 10 random splits (48%/32%/20% of nodes per class for train/validation/test 1 ) from <ref type="bibr" target="#b15">(Pei et al., 2019)</ref>.</p><p>Baselines. For baselines we use (1) classic GNN models for node classification: vanilla GCN <ref type="bibr" target="#b7">(Kipf &amp; Welling, 2016)</ref>, GAT <ref type="bibr" target="#b24">(Veličković et al., 2017)</ref> and GraphSage <ref type="bibr" target="#b6">(Hamilton et al., 2017)</ref>;</p><p>(2) recent models tackling heterophily: Geom-GCN <ref type="bibr" target="#b15">(Pei et al., 2019)</ref> and H2GCN <ref type="bibr" target="#b33">(Zhu et al., 2020)</ref>; (3) the state-of-the-art model for oversmoothing, GCNII <ref type="bibr" target="#b2">(Chen et al., 2020)</ref>; and (4) 2-layer MLP (with dropout and Elu non-linearity). For GCN, Geom-GCN, GCNII and H2GCN, we use the original codes provided by the authors. For GAT, we use the code from a well-accepted Github repository 2 . For GraphSage, we report the results from <ref type="bibr" target="#b33">(Zhu et al., 2020)</ref>, which uses the same data and splits. For the baselines that have multiple variants (Geom-GCN, GCNII, H2GCN), we 1 <ref type="bibr" target="#b15">(Pei et al., 2019)</ref> claims that the ratios are 60%/20%/20%, which is different from the actual data splits shared on GitHub.</p><p>2 https://github.com/Diego999/pyGAT choose the best variant for each dataset and denote them as [model]*. We set the same hyperparameters that are provided by the paper or the authors' github repository, and we match their reported results if they used the same dataset and split as us. The hyperparameter settings for GGCN can be found in App. B.3.</p><p>Machine. We ran our experiments on Nvidia V100 GPU. <ref type="table" target="#tab_7">Table 1</ref> provides the accuracy of different GNNs on the supervised node classification task over datasets with varying homophily levels (arranged from low homophily to high homophily). We report the best performance of each model across different layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Q1. Performance Under Homophily &amp; Heterophily</head><p>We observe that GGCN performs the best in terms of average rank (1.78) across all datasets, which suggests its strong adaptability to graphs of various homophily levels. In particular, GGCN achieves the highest accuracy in 5 out of 6 heterophily graphs (h ≤ 0.5). For datasets like Chameleon and Cornell, GGCN enhances accuracy by around 6% and 3% compared to the second-best model. On homophily datasets (Citeseer, Pubmed, Cora), the accuracy of GGCN is within a 1% difference of the best model.</p><p>Our experiments highlight that MLP is a good baseline in heterophily datasets. In heterophily graphs, the models that are not specifically designed for heterophily usually perform worse than an MLP. Though H2GCN* is the second best model in heterophily datasets, we can still see that in the Actor dataset, MLP performs better. Geom-GCN*, which is specifically designed for heterophily, achieves better performance than classic GNNs (GCN and GAT), but it is outperformed by MLP on heterophily datasets (except Chameleon and Squirrel). Our GGCN model is the only model that performs better than MLP across all the datasets.</p><p>In general, GNN models perform well in homophily datasets. GCNII* performs the best, and GGCN, H2GCN*, and Geom-GCN* also achieve high performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Q2. Oversmoothing</head><p>We also test how robust the models are to oversmoothing.</p><p>To this end, we measure the supervised node classification accuracy for 2 to 64 layers. <ref type="table" target="#tab_2">Table 2</ref> presents the results for two homophily datasets (top) and two heterophily datasets (bottom). Per model, we also report the layer at which the best performance is achieved (column 'Best').</p><p>According to <ref type="table" target="#tab_2">Table 2</ref>, GGCN and GCNII* achieve increase in accuracy when stacking more layers. Models that are not designed for oversmoothing have various issues. The performance of GCN and Geom-GCN* drops rapidly as the number of layers grows; H2GCN* requires concatenating all the intermediate outputs and quickly reaches memory capacity; GAT's attention mechanism also has high memory requirements. We also find that GAT needs careful initialization when stacking many layers as it may suffer from numerical instability in sparse tensor operations.</p><p>In general, models like GGCN, GCNII*, and H2GCN* that perform well under heterophily usually exhibit higher resilience against oversmoothing. One exception is Geom-GCN*, which suffers more than GCN. This model incorporates structurally similar nodes into each node's neighborhood; this design may benefit Geom-GCN* in the shallow layers as the node degrees increase. However, as we point out in Thm. 2, when the effective homophily is low, higher degrees are harmful. If the structurally similar nodes introduce lower homophily levels, their performance will rapidly degrade once the effective homophily is lower than 0.5. On the other hand, GGCN virtually changes the degrees thanks to the degree correction mechanism ( § 4.2), and, in prac-tice, this design has positive impact on its robustness to oversmoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Q3. Ablation Study for the Proposed Mechanisms</head><p>We now study the impact of our two proposed mechanisms (signed messages and degree correction, presented in § 4).</p><p>To better show their effects, we add each design choice to a base model and track the changes in the node classification performance. As the base model, we choose a GCN variant that uses the message passing mechanism <ref type="bibr" target="#b7">(Kipf &amp; Welling, 2016)</ref> with weight bias, a residual connection (but not decaying aggregation) for robustness to oversmoothing, and Elu non-linearity σ. We denote the model that replaces the message passing with our signed messages mechanism as +sign, and the model that incorporates the degree correction as +deg. The model that uses both designs is denoted as +sign,deg. <ref type="table" target="#tab_3">Table 3</ref> gives the accuracy of the models in the supervised node classification task for different layers.</p><p>We observe that both mechanisms alleviate the oversmoothing problem. Specifically, the base model has a sharp performance decrease after 32 layers, while the other models have significantly higher performance. In general, the +deg model is better than +sign in alleviating oversmoothing, and has a consistent performance gain across different data. For Chameleon, we observe increase in accuracy as we stack more layers; the large performance gain of GGCN results from the degree correction. In Cora and Cornell, +sign,deg consistently performs better across different layers than the model with only one design, demonstrating the constructive effect from our two proposed mechanisms.</p><p>The signed message design has an advantage in heterophily datasets. In the Cornell dataset, using signed information rather than plain message passing results in over 20% gain, which explains the strong performance of GGCN. However, in homophily datasets, the benefit from signed messages is limited, as these datasets have few different-class neighbors.</p><p>As we pointed out in Thm. 3, when signs are included, the error rate affects the performance. For the models that do not include signed messages, Thms. 1 and 2 indicate that the effective homophily level affects the performance. In order to achieve gain in performance, we need to make sure that <ref type="figure">Figure 2</ref>. Accuracy per (logarithmic) degree bin on Citeseer. Initial stage: when averageĥi ≥ 0.5, the accuracy increases as the degree increases. Developing stage: when averageĥi &lt; 0.5, the accuracy of high-degree nodes decreases more sharply.</p><p>the error rate is lower than 1 −ĥ. In the homophily datasets, since the effective homophilyĥ is high, the signed messages are not very beneficial. However, in the heterophily datasets, h is low, and our proposed mechanism helps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Q4. Case Study: Initial &amp; Developing Stages</head><p>Using the vanilla GCN model <ref type="bibr" target="#b7">(Kipf &amp; Welling, 2016)</ref>, we validate our theorems by measuring the accuracy and effective homophily for different node degrees (binned logarithmically) on real datasets. We estimate the effective homophily as the portion of the same-class neighbors that are correctly classified before the last propagation. <ref type="figure">Figure 2</ref> shows the results for Citeseer; we see that initially the accuracy increases with the degree, but the trend changes when the average effectiveĥ i drops below 0.5, with highdegree nodes being impacted the most, as predicted by our theorems. We give details for this analysis in App. B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>Graph Neural Networks. Early on, <ref type="bibr" target="#b4">Defferrard et al. (2016)</ref> proposed a GNN model that combines spectral filtering of graph signals and non-linearity for supervised node classification. The scalability and numerical stability of GNNs was later improved with a localized first-order approximation of spectral graph convolutions proposed in <ref type="bibr" target="#b7">(Kipf &amp; Welling, 2016)</ref>. Graph attention was introduced to improve the neighborhood aggregation step <ref type="bibr" target="#b24">(Veličković et al., 2017)</ref>. <ref type="bibr" target="#b24">Veličković et al. (2017)</ref> proposes the first graph attention network to improve neighborhood aggregation. Many more GNN variants have been proposed for different applications such as: computer vision <ref type="bibr" target="#b19">(Satorras &amp; Estrach, 2018)</ref>, social science <ref type="bibr" target="#b8">(Li &amp; Goldwasser, 2019)</ref>, biology <ref type="bibr" target="#b29">(Yan et al., 2019)</ref>, algorithmic tasks <ref type="bibr">(Veličković et al., 2020;</ref><ref type="bibr" target="#b30">Yan et al., 2020)</ref>, and inductive classification <ref type="bibr" target="#b6">Hamilton et al. (2017)</ref>.</p><p>Oversmoothing. The oversmoothing problem was first discussed in <ref type="bibr" target="#b10">(Li et al., 2018)</ref>, which proved that by repeatedly applying Laplacian smoothing, the features of nodes within each connected component of the graph converge to the same value. Since then, various empirical solutions have been proposed: residual connections and dilated con-volutions ; skip links <ref type="bibr" target="#b28">(Xu et al., 2018b)</ref>; new normalization strategies <ref type="bibr" target="#b31">(Zhao &amp; Akoglu, 2019)</ref>; edge dropout <ref type="bibr" target="#b16">(Rong et al., 2019)</ref>; and a new model that even increases performance as more layers are stacked <ref type="bibr" target="#b2">(Chen et al., 2020)</ref>. Some recent works provide theoretical analyses: <ref type="bibr" target="#b13">Oono &amp; Suzuki (2019)</ref> showed that a k-layer renormalized graph convolution with a residual link simulates a lazy random walk and <ref type="bibr" target="#b2">Chen et al. (2020)</ref> proved that the convergence rate is related to the spectral gap of the graph.</p><p>Heterophily &amp; GNNs. Heterophily has recently been recognized as an important issue for GNNs. It is first outlined in the context of GNNs in <ref type="bibr" target="#b15">(Pei et al., 2019)</ref>. <ref type="bibr" target="#b33">Zhu et al. (2020)</ref> identified a set of effective designs that allow GNNs to generalize to challenging heterophily settings, and <ref type="bibr" target="#b34">Zhu et al. (2021)</ref> introduced a new GNN model that leverages ideas from belief propagation <ref type="bibr" target="#b5">(Gatterbauer et al., 2015)</ref>. Though recent work <ref type="bibr" target="#b2">(Chen et al., 2020)</ref> focused on solving the oversmoothing problem, it also empirically showed improvement on heterophily datasets; these empirical observations formed the basis of our work. Finally, <ref type="bibr" target="#b3">Chien et al. (2021)</ref> recently proposed a pagerank-based model that performs well under heterophily and alleviates the oversmoothing problem. However, they view the two problems independently, while our work provides theoretical and empirical connections.</p><p>Our Work. Our work lies at the intersection of the aforementioned areas. We theoretically and empirically unveil connections between the oversmoothing and heterophily problems. We also provide a new perspective to analyze these problems, and point out key factors that impact their interplay within the GCN message propagation dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>Our work provides the first theoretical and empirical analysis that unveils the connections between the oversmoothing and heterophily problems. By modeling the node features as random vectors, we study the dynamics of message propagation in GCNs and analyze whether the vectors tend to move closer to/away from the other classes in expectation. Through this new perspective, we obtain two important insights: (1) nodes with high heterophily and nodes with low heterophily and low degrees compared to their neighbors (degree discrepancy) trigger the oversmoothing problem, and (2) allowing 'negative' messages between neighbors can decouple the heterophily and oversmoothing problems. Based on these insights, we design a robust model, GGCN, which simultaneously addresses the heterophily and oversmoothing problems. Though other designs may also address the two problems, our work points out two effective directions which are theoretically grounded ( § 3). How to effectively incorporate signed information, and jointly optimize signed information and degree corrections are still open problems worth further exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detailed Proofs of Theorems in § 3</head><p>A.1. Proof of Theorem 1</p><p>Proof. Before the operation ofÃF 0 , the distance between the mean vectors of the two classes is: 2||µ|| and the variance within each class is Σ. The smaller the distance between the two classes is, the less distinguishable the node features of the two classes are.</p><p>After this operation, the node representation of any node v i is given by:</p><formula xml:id="formula_16">f 1 i = f 0 i d i + 1 + j∈Ni f 0 j √ d i + 1 · d j + 1 .<label>(13)</label></formula><p>Without loss of generality, we assume node v i is in the first class V 1 and denote |N s i | |Ni| as k i . Then, we can express the expectation of f 1 i as:</p><formula xml:id="formula_17">E(f 1 i ) = E(E(f 1 i |k i )) (14) E(f 1 i |k i ) =E f 0 i d i + 1 |k i + j∈Ni E f 0 j √ d i + 1 · d j + 1 |k i = µ d i + 1 + j∈Ni E f 0 j √ d i + 1 · d j + 1 |k i , v j ∈ V 1 · P(v j ∈ V 1 |k i ) +E f 0 j √ d i + 1 · d j + 1 |k i , v j ∈ V 2 · P(v j ∈ V 2 |k i ) = µ d i + 1 + j∈Ni k i √ d i + 1 · d j + 1 µ − (1 − k i ) √ d i + 1 · d j + 1 µ = µ d i + 1 + (2k i − 1)µ √ d i + 1 j∈Ni 1 d j + 1 .<label>(15)</label></formula><p>Given E(k i ) = h, Equation <ref type="formula">(14)</ref> and Equation (equation 15), we have:</p><formula xml:id="formula_18">E(f 1 i ) =E   µ d i + 1 + (2k i − 1)µ √ d i + 1 j∈Ni 1 d j + 1   = µ d i + 1 + (2h i − 1)µ √ d i + 1 j∈Ni 1 d j + 1 =   1 d i + 1 + j∈Ni 1 √ dj +1 √ d i + 1 (2h i − 1)   µ =    1 d i + 1 + j∈Ni √ di+1 √ dj +1 d i + 1 (2h i − 1)    µ = 1 + (2h i − 1)d i r i d i + 1 µ ≡ γ 1 i µ.<label>(16)</label></formula><p>Where</p><formula xml:id="formula_19">r i ≡ 1 di j∈Ni √ di+1 √ dj +1 .</formula><p>Next, we consider three cases:</p><formula xml:id="formula_20">(1) h i ≤ 0.5, (2) h i &gt; 0.5 &amp; r i ≤ 1 , and (3) h i &gt; 0.5 &amp; r i &gt; 1 .</formula><p>Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks</p><formula xml:id="formula_21">• CASE 1: h i ≤ 0.5 h i ≤ 0.5 h i ≤ 0.5 • Upper Bound</formula><p>We have:</p><formula xml:id="formula_22">γ 1 i ≤ 1 d i + 1 ≤ 1 2 .<label>(17)</label></formula><p>• Lower Bound</p><p>To see if there exists a lower bound, we first show that when h i ≤ 0.5, 1+(2hi−1)diri di+1 is a decreasing function of d i given that d i ≥ 1.</p><p>When h i ≤ 0.5, we have:</p><formula xml:id="formula_23">1. (2h i − 1)d i ≤ 0 2. di di+1 is an increasing function of d i 3. r i is an increasing function of d i 4. 1 di+1 is a decreasing function of d i Thus 1+(2hi−1)diri di+1</formula><p>is a decreasing function of d i .</p><p>When</p><formula xml:id="formula_24">h i = 0.5, 1+(2hi−1)diri di+1 = 1 di+1 and 0 &lt; 1 di+1 ≤ 1 2 . When h i &lt; 0.5, 1 + (2h i − 1)d i r i d i + 1 ≤ (2h i − 1)d i r i d i + 1 + 1 2 ≤ (2h i − 1)r i 2 + 1 2 .<label>(18)</label></formula><p>And we know that when h i &lt; 0.5:</p><formula xml:id="formula_25">lim di→∞ (2h i − 1)r i 2 + 1 2 = −∞.<label>(19)</label></formula><p>Thus,</p><formula xml:id="formula_26">lim di→∞ 1 + (2h i − 1)d i r i d i + 1 = −∞.<label>(20)</label></formula><p>• CASE 2:</p><formula xml:id="formula_27">h i &gt; 0.5 &amp; r i ≤ 1 h i &gt; 0.5 &amp; r i ≤ 1 h i &gt; 0.5 &amp; r i ≤ 1</formula><p>For h i &gt; 0.5, let = 2h i − 1 and 0 &lt; ≤ 1. Then, we have:</p><formula xml:id="formula_28">E(f 1 i ) = 1 + d i r i d i + 1 µ.<label>(21)</label></formula><p>If r i ≤ 1 , then 0 &lt; r i ≤ 1, so 1 2 &lt; γ 1 i ≤ 1.</p><formula xml:id="formula_29">• CASE 3: h i &gt; 0.5 &amp; r i &gt; 1 h i &gt; 0.5 &amp; r i &gt; 1 h i &gt; 0.5 &amp; r i &gt; 1</formula><p>In this case, Equation (21) still holds, since h i &gt; 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Lower Bound</head><p>If r i &gt; 1 , then r i &gt; 1 and therefore γ 1 i &gt; 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• Upper Bound</head><p>When &gt; 0, </p><formula xml:id="formula_30">1 + d i r i d i + 1 &gt; d i r i d i + 1 ≥ r i 2 .<label>(22)</label></formula><formula xml:id="formula_31">r i 2 = ∞,<label>(23)</label></formula><p>we have:</p><formula xml:id="formula_32">lim di→∞ 1 + d i r i d i + 1 = ∞.<label>(24)</label></formula><p>To sum it up,</p><formula xml:id="formula_33">γ 1 i ∈      (−∞, 1 2 ], if h i ≤ 0.5 ( 1 2 , 1], if h i &gt; 0.5 &amp; r i ≤ 1 2hi−1 (1, ∞), otherwise.<label>(25)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remarks: Special cases</head><p>For different types of graphs and different nodes in the graph, d i r i might be different. For a d-regular graph whose nodes have constant degree or a graph whose adjacency matrix is row-normalized, we will have:</p><formula xml:id="formula_34">γ 1 i = 1 + (2h i − 1)d i r i d i + 1 ≤( 1 (d i + 1) + d i (d i + 1) ) = 1<label>(26)</label></formula><p>Equality is achieved if and only if h i = 1. To note that, h i = 1 is not achievable for every node as long as there are more than one class. Boundary nodes will suffer most.</p><p>Then, we will take a look at the variance within each class. If we assume that the feature vectors are independent of each other, then we will have:</p><formula xml:id="formula_35">V(f 1 t ) =V f 0 i d i + 1 + j∈Ni V f 0 j √ d i + 1 · d j + 1 =   1 (d i + 1) 2 + j∈Ni 1 (d i + 1)(d j + 1)   Σ = 1 d i + 1   1 d i + 1 + j∈Ni 1 d j + 1   Σ (27) 1 d i + 1   1 d i + 1 + j∈Ni 1 d j + 1   ≤ 1 2<label>(28)</label></formula><p>Note that we can derive similar results for any node v t in the second class.</p><p>From Equation 27, we can have the following conclusion:</p><p>AF 0 will reduce the variance of the node feature distribution and the reduction is irrelevant to the homophily h i . The larger degree of the node, the greater the reduction will be in the variance. Note that the reduction of the variance of each node does not mean the reduction of the variance of the class as a whole. When the degree and the homophily of the nodes are diverse, the variance of the class can increase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Proof of Theorem 3</head><p>Proof. Similar to Equation 14, we can express the expectation of f 1 i as:</p><formula xml:id="formula_36">E(f 1 i ) =E(E(f 1 i |m 0 i )) =E   E f 0 i d i + 1 |m 0 i + j∈Ni E f 0 j √ d i + 1 · d j + 1 |m 0 i   =E µ d i + 1 + j∈Ni E f 0 j √ d i + 1 · d j + 1 |m 0 i , v j ∈ V 1 P(v j ∈ V 1 |m 0 i ) +E f 0 j √ d i + 1 · d j + 1 |m 0 i , v j ∈ V 2 P(v j ∈ V 2 |m 0 i ) .<label>(29)</label></formula><p>Next, we will show how to compute the conditional expectation and conditional probability in the summand.</p><formula xml:id="formula_37">E f 0 j √ d i + 1 · d j + 1 |m 0 i , v j ∈ V 1 =E f 0 j √ d i + 1 · d j + 1 |m 0 i , v j ∈ V 1 , v j wrongly send information · P(v j wrongly send information|m 0 i , v j ∈ V 1 ) +E f 0 j √ d i + 1 · d j + 1 |m 0 i , v j ∈ V 1 , v j correctly send information · P(v j correctly send information|m 0 i , v j ∈ V 1 ).<label>(30)</label></formula><p>Combined with the conditional independence assumption, we have:</p><formula xml:id="formula_38">P(v j wrongly send information|m 0 i , v j ∈ V 1 ) = P(v j wrongly send information, v j ∈ V 1 |m 0 i ) P(v j ∈ V 1 |m 0 i ) = P(v j wrongly send information|m 0 i ) · P(v j ∈ V 1 |m 0 i ) P(v j ∈ V 1 )|m 0 i ) =P(v j wrongly send information|m 0 i ) = m 0 i .<label>(31)</label></formula><p>Similarly, we obtain:</p><formula xml:id="formula_39">P(v j correctly send information|m 0 i , v j ∈ V 1 ) = 1 − m 0 i .<label>(32)</label></formula><p>Then Equation <ref type="formula" target="#formula_5">(30)</ref> can be rewritten as:</p><formula xml:id="formula_40">E f 0 j √ d i + 1 · d j + 1 |m 0 i , v j ∈ V 1 =E f 0 j √ d i + 1 · d j + 1 |m 0 i , v j ∈ V 1 , v j wrongly send information · m 0 i +E f 0 j √ d i + 1 · d j + 1 |m 0 i , v j ∈ V 1 , v j correctly send information · (1 − m 0 i ) = − m 0 i µ √ d i + 1 · d j + 1 + (1 − m 0 i )µ √ d i + 1 · d j + 1 = (1 − 2m 0 i )µ √ d i + 1 · d j + 1 .<label>(33)</label></formula><p>Similarly, we have: Consider the independence between m 0 i and node class, and insert Equation <ref type="formula" target="#formula_5">(33)</ref> and Equation <ref type="formula" target="#formula_5">(34)</ref> into Equation <ref type="formula" target="#formula_13">(29)</ref>, we will have:</p><formula xml:id="formula_41">E f 0 j √ d i + 1 · d j + 1 |m 0 i , v j ∈ V 2 = (1 − 2m 0 i )µ √ d i + 1. · d j + 1 .<label>(34)</label></formula><formula xml:id="formula_42">E(f 1 i ) =E   µ d i + 1 + j∈Ni ( (1 − 2m 0 i )k i µ √ d i + 1 · d j + 1 + (1 − 2m 0 i )(1 − k i )µ √ d i + 1 · d j + 1 )   =E   µ d i + 1 + j∈Ni (1 − 2m 0 i )µ √ d i + 1 · d j + 1   = µ d i + 1 + j∈Ni (1 − 2e 0 i )µ √ d i + 1 · d j + 1 = 1 + (1 − 2e 0 i )d i r i d i + 1 µ ≡ γ 1 i E(f 0 i ).<label>(35)</label></formula><p>The ranges of γ 1 i under different conditions of e 0 i and r i can be derived similarly as shown by the Proof A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Batch norm &amp; Layer norm</head><p>As mentioned in §4.4, our theorems predict that batch norm and layer norm will help alleviate oversmoothing, but they will also decrease the accuracy in the supervised node classification task. This phenomenon is more prominent in the heterophily datasets. In this section, we verify this conjecture by comparing the prediction accuracy of the base model with models that use either batch norm or layer norm. We use the following base model (the same model used in §5.4): a GCN <ref type="bibr" target="#b7">(Kipf &amp; Welling, 2016)</ref> with weight bias, Elu non-linearity and residual connection. We do not include any of our designs so as to exclude any other factors that can affect the performance. The models we compare against are +BN and +LN, which represent the models that add batch norm and layer norm right before the non-linear activation, respectively.  <ref type="table" target="#tab_7">Table B</ref>.1 shows that both batch norm and layer norm can help with oversmoothing. Moreover, adding layer norm is in general better than adding batch norm. This is expected because the scaling effect caused by the propagation can be alleviated by normalizing across the node features. Thus, the dispersion of the expected feature vectors can be mitigated. On the other hand, batch norm normalizes across all the nodes, so it requires sacrificing the nodes that benefit (case 3) to compensate for the nodes that are prone to moving towards the other classes (case 1 &amp; case 2). As a result, batch norm is less effective in mitigating oversmoothing and leads to a bigger decrease in accuracy.</p><p>Another finding is that both layer norm and batch norm lead to a significant accuracy decrease (2%-3%) in the heterophily datasets. +BN has a clear accuracy drop even in the homophily datasets. As Thm. 1 points out: higher heterophily level may result in sign flip. If the features flip the sign, using batch norm or layer norm will not revert the sign, but they may instead encourage the features to move towards the other class more.</p><p>Given the limitations shown above, we do not use either batch norm or layer norm in our proposed model, GGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. More on the Initial &amp; Developing Stages</head><p>Section 5.5 shows how the node classification accuracy changes for nodes of different degrees with the number of layers on Citeseer. Here, we provide more details of this experiment and give the results on another dataset, Cora.</p><p>Datasets. According to Thm. 1 and 2, in order to see both the initial and the developing stage, we need to use homophily datasets. Recall that in heterophily data,ĥ i &lt; 0.5 for all the nodes, so the initial stage does not exist and only case 2 applies.</p><p>Measurement of effective homophilyĥ î h î h i . In a multi-class setting, effective homophilyĥ i is defined as the portion of neighbors whose features are inside the decision boundary of class χ(v i ). To align this with our theorems-which apply to binary classification-, the decision boundary is defined as the curve that maximizes the margins between the initial features in different classes. In binary classification, the decision boundary is the origin.</p><p>To estimate the effective homophilyĥ i , we measure the portion of neighbors that have the same ground truth label as v i and are correctly classified. We use this ratio to approximate the neighbors whose features are still inside the decision boundary of class χ(v i ). Following our theory, we estimateĥ i before the last propagation takes place and analyze its impact on the accuracy of the final layer. In more detail, we obtain the node features before the last propagation from a trained vanilla GCN <ref type="bibr" target="#b7">(Kipf &amp; Welling, 2016)</ref> and then perform a linear transformation using the weight matrix and bias vector from the last layer. Then, we use the transformed features to classify the neighbors of node v i and computeĥ i . We note that we leverage the intermediate features only for the estimation ofĥ i ; the accuracy of the final layer is still measured using the outputs from the final layer. Degree intervals. To investigate the change in GNN accuracy with different layers for nodes with different degrees, we categorize the nodes in n degree intervals. For the degree intervals, we use logarithmic binning (base 2). In detail, we denote the highest and lowest degree by d max and d min , respectively, and let Ω ≡ log 2 dmax−log 2 dmin n . Then, we divide the nodes into n intervals, where the j-th interval is defined as: [d min · 2 (j−1)Ω , d min · 2 jΩ ).</p><p>Dataset: Citeseer. <ref type="figure">Figure 2 and</ref>  <ref type="table" target="#tab_7">Table B</ref>.2 show how the accuracy changes with the number of layers for different node degree intervals. We observe that in the initial stage (whenh i &gt; 0.5), the accuracy increases as the degree and h i increase. However, in the developing stage (h i ≤ 0.5), the accuracy of high-degree nodes drops more sharply than that of low-degree nodes.</p><p>Dataset: Cora. The results for Cora are shown in <ref type="figure">Figure B</ref>.1 and <ref type="table" target="#tab_7">Table B</ref>.3. In the initial stage, the nodes with lower  degrees usually have lower accuracy. One exception is the nodes with degrees in the range <ref type="bibr">[3,</ref><ref type="bibr">7]</ref>. These nodes have higher accuracy because the average effective homophilyh i of that degree group is the second highest. In the developing stage, the accuracy of the high-degree nodes drops more than the accuracy of the remaining node groups.</p><p>GCN's behavior on both Citesser and Cora datasets verifies our conjecture based on our theorems in § 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Hyperparameter Settings</head><p>Experiments for <ref type="table" target="#tab_2">Table 1 &amp; Table 2</ref> For the baselines, we set the same hyperparameters that are provided by the original papers or the authors' github repositories, and we match the results they reported in their respective papers. In our experiments, we find that the original hyperparameters set by the authors are already well-tuned.</p><p>All the models use Adam as the optimizer. GAT sets the initial learning rate as 0.005 and Geom-GCN uses a custom learning scheduler. All the other models (include GGCN) use the initial learning rate 0.01.</p><p>For GGCN, we use the following hyperparameters:</p><p>• k in the decaying aggregation: 3</p><p>• Initialization of λ l 0 and λ l 1 : 0.5 and 0, respectively • Initialization of α l , β l 0 , β l 1 and β l 2 : 2, 0, 0, 0, respectively.</p><p>We tune the parameters in the following ranges:</p><p>•  <ref type="table" target="#tab_3">Table 3</ref> and <ref type="table" target="#tab_7">Table B</ref>.1</p><p>The hyperparameters that are used in all the models (Base, +deg, +sign, +deg,sign, +BN, +LN) are set to be the same and they are tuned for every dataset. Those common hyperparameters are:</p><p>• For models that use signed messages, we tune the following hyperparameter:</p><p>• Initialization of α l : [0.2, 1.0] (same in every layer)</p><p>For models that use degree correction, we tune the following hyperparameter:</p><p>• Initialization of λ l 0 : [0.2, 1.5] (same in every layer)</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An illustration of node representation dynamics during neighborhood aggregation. The expectation of the representation from class 1 &amp; 2 are denoted by µ and −µ, respectively. The bars show the expected 1D node representations (scalars) of node vi before and after the neighborhood aggregation. LS stands for linearly separability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure B. 1 .</head><label>1</label><figDesc>Cora: Accuracy per (logarithmic) degree bin.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>University of Michigan 2 Google Research 3 University of California, Berkeley. Correspondence to: Yujun Yan &lt;yujun-yan@umich.edu&gt;.</figDesc><table><row><cell>Under review.</cell></row></table><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>84.86±4.55 86.86±3.29 37.54±1.56 55.17±1.58 71.14±1.84 85.68±6.63 77.14±1.45 89.15±0.37 87.95±1.05 1.78 H2GCN* 84.86±7.23 87.65±4.98 35.70±1.00 36.48±1.86 60.11±2.15 82.70±5.28 77.11±1.57 89.49±0.38 87.87±1.20 3.44 GCNII* 77.57±3.83 80.39±3.4 37.44±1.30 38.47±1.58 63.86±3.04 77.86±3.79 77.33±1.48 90.15±0.43 88.37±1.25 3.11 Geom-GCN* 66.76±2.72 64.51±3.66 31.59±1.15 38.15±0.92 60.00±2.81 60.54±3.67 78.02±1.15 89.95±0.47 85.35±1.57 5.22 GraphSAGE † 82.43±6.14 81.18±5.56 34.23±0.99 41.61±0.74 58.73±1.68 75.95±5.01 76.04±1.30 88.45±0.50 86.90±1.04 5.00 GCN 55.14±5.16 51.76±3.06 27.32±1.10 53.43±2.01 64.82±2.24 60.54±5.3 76.50±1.36 88.42±0.5 86.98±1.27 5.67 GAT 52.16±6.63 49.41±4.09 27.44±0.89 40.72±1.55 60.26±2.5 61.89±5.05 76.55±1.23 86.33±0.48 87.30±1.10 6.00 MLP 80.81±4.75 85.29±3.31 36.53±0.70 28.77±1.56 46.21±2.99 81.89±6.40 74.02±1.90 87.16±0.37 75.69±2.00 5.78</figDesc><table><row><cell></cell><cell>Texas</cell><cell>Wisconsin</cell><cell>Actor</cell><cell cols="3">Squirrel Chameleon Cornell</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Cora</cell><cell></cell></row><row><cell>Hom. level h #Nodes #Edges #Classes</cell><cell>0.11 183 295 5</cell><cell>0.21 251 466 5</cell><cell>0.22 7,600 26,752 5</cell><cell>0.22 5,201 198,493 5</cell><cell>0.23 2,277 31,421 5</cell><cell>0.3 183 280 5</cell><cell>0.74 3,327 4,676 7</cell><cell>0.8 19,717 44,327 3</cell><cell>0.81 2,708 5,278 6</cell><cell>Avg Rank</cell></row><row><cell>GGCN (ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Model performance for different layers: mean accuracy ± stdev over different data splits. Per dataset and GNN model, we also report the layer at which the best performance (given inTable 1) is achieved. 'OOM': out of memory; 'INS': numerical instability. 00±1.15 87.48±1.32 87.63±1.33 87.51±1.19 87.95±1.05 87.28±1.41 32 76.83±1.82 76.77±1.48 76.91±1.56 76.88±1.56 76.97±1.52 76.65±1.38 10 GCNII* 85.35±1.56 85.35±1.48 86.38±0.98 87.12±1.11 87.95±1.23 88.37±1.25 64 75.42±1.78 75.29±1.90 76.00±1.66 76.96±1.38 77.33±1.48 77.18±1.98±1.27 83.24±1.56 31.03±3.08 31.05±2.36 30.76±3.43 31.89±2.08 2 76.50±1.36 64.33±8.27 24.18±1.71 23.07±2.95 25.3±1.77 24.73±1.66 2 78±6.73 83.78±6.16 84.86±5.69 83.78±6.73 83.78±6.51 84.32±5.90 6 70.77±1.42 69.58±2.68 70.33±1.70 70.44±1.82 70.29±1.62 70.20±1.95 5 GCNII* 67.57±11.34 64.59±9.63 73.24±5.91 77.84±3.97 75.41±5.47 73.78±4.37 16 61.07±4.10 63.86±3.04 62.89±1.18 60.20±2.10 56.97±1.81 55.99±2.27 4 H2GCN* 81.89±5.98 82.70±6.27 80.27±6.63 54±3.67 23.78±11.64 12.97±2.91 12.97±2.91 12.97±2.91 12.97±2.91 2 60.00±2.81 19.17±1.66 19.58±1.73 19.58±1.73 19.58±1.73 19.58±1.73 2 GAT 61.89±5.05 58.38±4.05 58.38±3.86 54±5.30 59.19±3.30 58.92±3.15 58.92±3.15 58.92±3.15 58.92±3.15 2 64.82±2.24 53.11±4.44 35.15±3.14 35.39±3.23 35.20±3.25 35.50±3.08 2</figDesc><table><row><cell>Layers</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>Best</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>Best</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Cora (h=0.81)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Citeseer (h=0.74)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GGCN (ours)</cell><cell cols="14">87.47 32</cell></row><row><cell>H2GCN*</cell><cell cols="3">87.87±1.20 86.10±1.51 86.18±2.10</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>2</cell><cell cols="3">76.90±1.80 76.09±1.54 74.10±1.83</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>1</cell></row><row><cell>Geom-GCN*</cell><cell cols="7">85.35±1.57 21.01±2.61 13.98±1.48 13.98±1.48 13.98±1.48 13.98±1.48 2</cell><cell cols="6">78.02±1.15 23.01±1.95 7.23±0.87 7.23±0.87 7.23±0.87 7.23±0.87</cell><cell>2</cell></row><row><cell>GAT</cell><cell cols="3">87.30±1.10 86.50±1.20 84.97±1.24</cell><cell>INS</cell><cell>INS</cell><cell>INS</cell><cell>2</cell><cell cols="3">76.55±1.23 75.33±1.39 66.57±5.08</cell><cell>INS</cell><cell>INS</cell><cell>INS</cell><cell>2</cell></row><row><cell>GCN</cell><cell cols="4">86.Cornell (h=0.3)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Chameleon (h=0.23)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>GGCN (ours)</cell><cell cols="4">83.OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>1</cell><cell cols="2">59.06±1.85 60.11±2.15</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>4</cell></row><row><cell>Geom-GCN*</cell><cell cols="4">60.INS</cell><cell>INS</cell><cell>INS</cell><cell>2</cell><cell cols="3">60.26±2.50 48.71±2.96 35.09±3.55</cell><cell>INS</cell><cell>INS</cell><cell>INS</cell><cell>2</cell></row><row><cell>GCN</cell><cell>60.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>5.1. Experimental Setup</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Ablation study: degree correction has consistent benefits (robust to oversmoothing &amp; ability to handle heterophily) in different datasets while signed information has more benefits in heterophily datasets. Best performance of each model is highlighted in gray. 56±1.21 86.04±0.72 85.51±1.51 85.33±0.72 85.37±1.58 72.17±8.89 76.51±1.63 75.03±1.67 73.96±1.52 73.59±1.51 71.91±1.94 32.08±15.74 +deg 86.72±1.29 86.02±0.97 85.49±1.32 85.27±1.59 85.27±1.51 84.21±1.22 76.63±1.38 74.64±1.97 74.15±1.61 73.73±1.31 73.61±1.84 70.56±2.27 +sign 84.81±1.63 86.06±1.7 85.67±1.26 85.39±0.97 84.85±0.98 78.57±6.73 77.13±1.69 74.56±2.02 73.64±1.65 72.31±2.32 71.98±3.44 68.68±6.72 +deg,sign 86.96±1.38 86.20±0.89 85.63±0.78 85.47±1.18 85.55±1.66 77.81±7.95 76.81±1.71 74.68±1.97 74.69±2.35 73.28±1.45 71.81±2.28 69.91±3.97 89±3.72 60.00±5.24 58.92±5.24 56.49±5.73 58.92±3.15 49.19±16.70 64.98±1.84 62.65±3.09 62.43±3.28 54.69±2.58 47.68±2.63 29.74±5.21 +deg 63.78±5.57 62.70±5.90 59.46±4.52 56.49±5.73 57.57±4.20 58.92±3.15 66.54±2.19 68.31±2.70 68.99±2.38 67.68±3.70 56.86±8.80 41.95±9.56 +sign 85.41±7.27 76.76±7.07 70.00±5.19 67.57±9.44 63.24±6.07 63.24±6.53 65.31±3.20 53.55±6.35 53.05±2.28 51.93±4.00 57.17±3.39 51.93±8.95 +deg,sign 84.32±6.37 78.92±8.09 73.51±5.90 70.81±5.64 68.11±5.14 62.43±6.67 65.75±1.81 61.49±7.38 53.73±7.79 52.43±5.37 55.92±5.14 56.95±3.93</figDesc><table><row><cell>Layers</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Cora (h=0.81)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Citeseer (h=0.74)</cell><cell></cell><cell></cell></row><row><cell>Base</cell><cell cols="4">86.Cornell (h=0.3)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Chameleon (h=0.23)</cell><cell></cell><cell></cell></row><row><cell>Base</cell><cell>61.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks Because lim</figDesc><table><row><cell>di→∞</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Table B.1. Effects of using batch norm &amp; layer norm: decrease in accuracy but improvement in oversmoothing. Best performance of each model across different layers is highlighted in gray.</figDesc><table><row><cell>Layers</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>64</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table B .</head><label>B</label><figDesc>2. Citeseer: Accuracy (Acc) and average effective homophily (hi) for nodes with different degrees across various layers. Last layer of initial stage (hi &gt; 0.5 ∀ degree) marked in gray.</figDesc><table><row><cell></cell><cell>Degrees</cell><cell></cell></row><row><cell>Layers</cell><cell cols="2">[1, 2] [3, 6] [7, 15] [16, 39] [40, 99]</cell></row><row><cell>2</cell><cell cols="2">Acc 74.44 78.51 84.04 96.00 100.00 h i 0.65 0.67 0.72 0.83 0.91</cell></row><row><cell>3</cell><cell cols="2">Acc 70.92 77.59 83.03 92.75 100.00 h i 0.63 0.66 0.71 0.84 0.92</cell></row><row><cell>4</cell><cell cols="2">Acc 60.54 68.56 77.63 94.00 100.00 h i 0.54 0.58 0.66 0.79 0.84</cell></row><row><cell>5</cell><cell>Acc 41.66 48.70 56.97 61.33 h i 0.36 0.38 0.45 0.48</cell><cell>11.11 0.11</cell></row><row><cell>6</cell><cell>Acc 23.61 28.87 41.17 31.83 h i 0.18 0.19 0.27 0.22</cell><cell>0.00 0.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table B .</head><label>B</label><figDesc>3. Cora: Accuracy and average effective homophily (hi) for nodes with different degrees across different layers. Last layer of initial stage (hi &gt; 0.5 ∀ degree) marked in gray.</figDesc><table><row><cell></cell><cell>Degrees</cell><cell></cell></row><row><cell>Layers</cell><cell cols="2">[1, 2] [3, 7] [8, 21] [22, 60] [61, 168]</cell></row><row><cell>2</cell><cell>Acc 85.14 88.52 83.78 90.00 h i 0.77 0.77 0.72 0.63</cell><cell>100.00 0.81</cell></row><row><cell>3</cell><cell>Acc 82.28 87.26 84.42 85.00 h i 0.77 0.78 0.73 0.64</cell><cell>100.00 0.84</cell></row><row><cell>4</cell><cell>Acc 79.57 85.52 83.26 85.00 h i 0.75 0.76 0.71 0.63</cell><cell>100.00 0.80</cell></row><row><cell>5</cell><cell>Acc 77.38 83.75 82.83 85.00 h i 0.72 0.73 0.68 0.56</cell><cell>96.19 0.83</cell></row><row><cell>6</cell><cell>Acc 63.35 68.20 66.01 70.00 h i 0.57 0.60 0.54 0.52</cell><cell>78.57 0.67</cell></row><row><cell>7</cell><cell>Acc 30.91 30.62 29.12 17.50 h i 0.25 0.26 0.21 0.26</cell><cell>21.43 0.23</cell></row><row><cell>8</cell><cell>Acc 31.48 31.02 28.44 32.50 h i 0.24 0.26 0.20 0.28</cell><cell>25.24 0.24</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Learning to represent programs with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khademi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simple and deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adaptive universal generalized pagerank graph neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Milenkovic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Linearized and single-pass belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gatterbauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. VLDB Endow</title>
		<meeting>VLDB Endow</meeting>
		<imprint>
			<date type="published" when="2015-01" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="581" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Encoding social information with graph convolutional networks forpolitical perspective detection in news media</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldwasser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deepgcns: Can gcns go as deep as cnns?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9267" to="9276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Query-driven active surveying for collective classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Workshop on Mining and Learning with Graphs</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Assortative mixing in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page">208701</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph neural networks exponentially lose expressive power for node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Oono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Netprobe: a fast and scalable system for fraud detection in online auction networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pandit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Chau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="201" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Geom-gcn: Geometric graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. C.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards deep graph convolutional networks on node classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dropedge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On proximity and structural role-based embeddings in networks: Misconceptions, techniques, and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Knowl. Discov. Data</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2020-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Multiscale attributed node embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.13021</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Satorras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Estrach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Relational Representation Learning Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning execution through neural code fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Social influence analysis in large-scale networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 15th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointer graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Overlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Two Sides of the Same Coin: Heterophily and Oversmoothing in Graph Convolutional Neural Networks Veličković</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improving graph attention networks with large margin-based constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11945</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5453" to="5462" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Grouping-based interpretable neural network for classification of limited, noisy brain data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Solarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sripada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Groupinn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="772" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural execution engines: Learning to execute subroutines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ranganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hashemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Tackling oversmoothing in gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pairnorm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Understanding and resolving performance degradation in graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Beyond homophily in graph neural networks: Current limitations and effective designs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heimann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Akoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Graph neural networks with heterophily</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Lipka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koutra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cora</forename></persName>
		</author>
		<idno>h=0.81) Citeseer (h=0.74) Base 86.56±1.21 86.04±0.72 85.51±1.51 85.33±0.72 85.37±1.58 72.17±8.89 76.51±1.63 75.03±1.67 73.96±1.52 73.59±1.51 71.91±1.94 32.08±15.74 +BN 84.73±1.10 83.76±1.61 83.94±1.51 84.57±1.22 84.63±1.58 85.17±1.18 71.62±1.48 71.58±1.00 72.18±1.39 72.45±1.42 72.76±1.31 72.61±1.41 +LN 84.73±1.63 86.60±1.01 86.72±1.36 86.08±1.16 85.67±1.23 85.13±1.20 76.11±1.80 74.02±2.77 75.00±1.95 74.50±0.96 74.49±2.10 73.94±2.03</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornell</forename></persName>
		</author>
		<idno>h=0.3) Chameleon (h=0.23) Base 61.89±3.72 60.00±5.24 58.92±5.24 56.49±5.73 58.92±3.15 49.19±16.70 64.98±1.84 62.65±3.09 62.43±3.28 54.69±2.58 47.68±2.63 29.74±5.21 +BN 58.38±6.42 59.19±4.59 55.41±6.65 57.30±3.15 57.57±6.29 57.02±6.19 60.88±2.24 61.38±2.17 61.84±4.08 61.97±3.01 59.04±3.79 57.84±3.67 +LN 58.11±6.19 55.68±6.19 58.92±7.63 59.19±3.07 58.92±3.15 58.00±3.03 61.86±1.73 62.17±2.48 62.41±2.99 60.37±2.36 58.25±3.03 58.92±3.15</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
