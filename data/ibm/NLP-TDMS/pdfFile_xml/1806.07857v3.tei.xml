<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">RUDDER: Return Decomposition for Delayed Rewards</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">A</forename><surname>Arjona-Medina</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LIT AI Lab Institute for Machine Learning</orgName>
								<orgName type="department" key="dep2">Institute of Advanced Research in Artificial Intelligence (IARAI)</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gillhofer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LIT AI Lab Institute for Machine Learning</orgName>
								<orgName type="department" key="dep2">Institute of Advanced Research in Artificial Intelligence (IARAI)</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Widrich</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LIT AI Lab Institute for Machine Learning</orgName>
								<orgName type="department" key="dep2">Institute of Advanced Research in Artificial Intelligence (IARAI)</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LIT AI Lab Institute for Machine Learning</orgName>
								<orgName type="department" key="dep2">Institute of Advanced Research in Artificial Intelligence (IARAI)</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Brandstetter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LIT AI Lab Institute for Machine Learning</orgName>
								<orgName type="department" key="dep2">Institute of Advanced Research in Artificial Intelligence (IARAI)</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">LIT AI Lab Institute for Machine Learning</orgName>
								<orgName type="department" key="dep2">Institute of Advanced Research in Artificial Intelligence (IARAI)</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">RUDDER: Return Decomposition for Delayed Rewards</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose RUDDER, a novel reinforcement learning approach for delayed rewards in finite Markov decision processes (MDPs). In MDPs the Q-values are equal to the expected immediate reward plus the expected future rewards. The latter are related to bias problems in temporal difference (TD) learning and to high variance problems in Monte Carlo (MC) learning. Both problems are even more severe when rewards are delayed. RUDDER aims at making the expected future rewards zero, which simplifies Q-value estimation to computing the mean of the immediate reward. We propose the following two new concepts to push the expected future rewards toward zero. (i) Reward redistribution that leads to return-equivalent decision processes with the same optimal policies and, when optimal, zero expected future rewards. (ii) Return decomposition via contribution analysis which transforms the reinforcement learning task into a regression task at which deep learning excels. On artificial tasks with delayed rewards, RUD-DER is significantly faster than MC and exponentially faster than Monte Carlo Tree Search (MCTS), TD(λ), and reward shaping approaches. At Atari games, RUDDER on top of a Proximal Policy Optimization (PPO) baseline improves the scores, which is most prominent at games with delayed rewards. Source code is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Assigning credit for a received reward to past actions is central to reinforcement learning <ref type="bibr" target="#b127">[128]</ref>. A great challenge is to learn long-term credit assignment for delayed rewards <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b105">106]</ref>. Delayed rewards are often episodic or sparse and common in real-world problems <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b75">76]</ref>. For Markov decision processes (MDPs), the Q-value is equal to the expected immediate reward plus the expected future reward. For Q-value estimation, the expected future reward leads to biases in temporal difference (TD) and high variance in Monte Carlo (MC) learning. For delayed rewards, TD requires exponentially many updates to correct the bias, where the number of updates is exponential in the number of delay steps. For MC learning the number of states affected by a delayed reward can grow exponentially with the number of delay steps. (Both statements are proved after theorems A8 and A10 in the appendix.) An MC estimate of the expected future reward has to average over all possible future trajectories, if rewards, state transitions, or policies are probabilistic. Delayed rewards make an MC estimate much harder.</p><p>The main goal of our approach is to construct an MDP that has expected future rewards equal to zero. If this goal is achieved, Q-value estimation simplifies to computing the mean of the immediate rewards. To push the expected future rewards to zero, we require two new concepts. The first new concept is reward redistribution to create return-equivalent MDPs, which are characterized by having the same optimal policies. An optimal reward redistribution should transform a delayed reward MDP into a return-equivalent MDP with zero expected future rewards. However, expected future rewards equal to zero are in general not possible for MDPs. Therefore, we introduce sequence-Markov decision processes (SDPs), for which reward distributions need not to be Markov. We construct a reward redistribution that leads to a return-equivalent SDP with a second-order Markov reward distribution and expected future rewards that are equal to zero. For these return-equivalent SDPs, Qvalue estimation simplifies to computing the mean. Nevertheless, the Q-values or advantage functions can be used for learning optimal policies. The second new concept is return decomposition and its realization via contribution analysis. This concept serves to efficiently construct a proper reward redistribution, as described in the next section. Return decomposition transforms a reinforcement learning task into a regression task, where the sequence-wide return must be predicted from the whole state-action sequence. The regression task identifies which state-action pairs contribute to the return prediction and, therefore, receive a redistributed reward. Learning the regression model uses only completed episodes as training set, therefore avoids problems with unknown future state-action trajectories. Even for sub-optimal reward redistributions, we obtain an enormous speed-up of Q-value learning if relevant reward-causing state-action pairs are identified. We propose RUDDER (RetUrn Decomposition for DElayed Rewards) for learning with reward redistributions that are obtained via return decompositions.</p><p>To get an intuition for our approach, assume you repair pocket watches and then sell them. For a particular brand of watch you have to decide whether repairing pays off. The sales price is known, but you have unknown costs, i.e. negative rewards, caused by repair and delivery. The advantage function is the sales price minus the expected immediate repair costs minus the expected future delivery costs. Therefore, you want to know whether the advantage function is positive. -Why is zeroing the expected future costs beneficial? -If the average delivery costs are known, then they can be added to the repair costs resulting in zero future costs. Using your repairing experiences, you just have to average over the repair costs to know whether repairing pays off. -Why is return decomposition so efficient? -Because of pattern recognition. For zero future costs, you have to estimate the expected brand-related delivery costs, which are e.g. packing costs. These brand-related costs are superimposed by brand-independent general delivery costs for shipment (e.g. time spent for delivery). Assume that general delivery costs are indicated by patterns, e.g. weather conditions, which delay delivery. Using a training set of completed deliveries, supervised learning can identify these patterns and attribute costs to them. This is return decomposition. In this way, only brand-related delivery costs remain and, therefore, can be estimated more efficiently than by MC.</p><p>Related Work. Our new learning algorithm is gradually changing the reward redistribution during learning, which is known as shaping <ref type="bibr" target="#b119">[120,</ref><ref type="bibr" target="#b127">128]</ref>. In contrast to RUDDER, potential-based shaping like reward shaping <ref type="bibr" target="#b86">[87]</ref>, look-ahead advice, and look-back advice <ref type="bibr" target="#b143">[144]</ref> use a fixed reward redistribution. Moreover, since these methods keep the original reward, the resulting reward redistribution is not optimal, as described in the next section, and learning can still be exponentially slow. A monotonic positive reward transformation <ref type="bibr" target="#b90">[91]</ref> also changes the reward distribution but is neither assured to keep optimal policies nor to have expected future rewards of zero. Disentangled rewards keep optimal policies but are neither environment nor policy specific, therefore can in general not achieve expected future rewards being zero <ref type="bibr" target="#b27">[28]</ref>. Successor features decouple environment and policy from rewards, but changing the reward changes the optimal policies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref>. Temporal Value Transport (TVT) uses an attentional memory mechanism to learn a value function that serves as fictitious reward <ref type="bibr" target="#b58">[59]</ref>. However, expected future rewards are not close to zero and optimal policies are not guaranteed to be kept. Reinforcement learning tasks have been changed into supervised tasks <ref type="bibr" target="#b107">[108,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b111">112]</ref>. For example, a model that predicts the return can supply update signals for a policy by sensitivity analysis. This is known as "backpropagation through a model" <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b141">142,</ref><ref type="bibr" target="#b110">111,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5]</ref>. In contrast to these approaches, (i) we use contribution analysis instead of sensitivity analysis, and (ii) we use the whole state-action sequence to predict its associated return.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Reward Redistribution and Novel Learning Algorithms</head><p>Reward redistribution is the main new concept to achieve expected future rewards equal to zero. We start by introducing MDPs, return-equivalent sequence-Markov decision processes (SDPs), and reward redistributions. Furthermore, optimal reward redistribution is defined and novel learning algorithms based on reward redistributions are introduced.</p><p>MDP Definitions and Return-Equivalent Sequence-Markov Decision Processes (SDPs). A finite Markov decision process (MDP) P is 5-tuple P = (S, A, R, p, γ) of finite sets S of states s (random variable S t at time t), A of actions a (random variable A t ), and R of rewards r (random variable R t+1 ). Furthermore, P has transition-reward distributions p(S t+1 = s , R t+1 = r | S t = s, A t = a) conditioned on state-actions, and a discount factor γ ∈ [0, 1]. The marginals are p(r | s, a) = s p(s , r | s, a) and p(s | s, a) = r p(s , r | s, a). The expected reward is r(s, a) = r rp(r | s, a). The return G t is G t = ∞ k=0 γ k R t+k+1 , while for finite horizon MDPs with sequence length T and γ = 1 it is G t = T −t k=0 R t+k+1 . A Markov policy is given as action distribution π(A t = a | S t = s) conditioned on states. We often equip an MDP P with a policy π without explicitly mentioning it. The action-value function q π (s, a) for policy π is q π (s, a) = E π [G t | S t = s, A t = a]. The goal of learning is to maximize the expected return at time t = 0, that is v π 0 = E π [G 0 ]. The optimal policy π * is π * = argmax π [v π 0 ]. A sequence-Markov decision process (SDP) is defined as a decision process which is equipped with a Markov policy and has Markov transition probabilities but a reward that is not required to be Markov. Two SDPsP and P are return-equivalent if (i) they differ only in their reward distribution and (ii) they have the same expected return at t = 0 for each policy π:ṽ π 0 = v π 0 . They are strictly return-equivalent if they have the same expected return for every episode and for each policy π. Strictly return-equivalent SDPs are return-equivalent. Return-equivalent SDPs have the same optimal policies. For more details see Section A2.2 in the appendix.</p><p>Reward Redistribution. Strictly return-equivalent SDPsP and P can be constructed by reward redistributions. A reward redistribution given an SDPP is a procedure that redistributes for each sequence s 0 , a 0 , . . . , s T , a T the realization of the sequence-associated return variablẽ G 0 = T t=0R t+1 or its expectation along the sequence. Later we will introduce a reward redistribution that depends on the SDPP. The reward redistribution creates a new SDP P with the redistributed reward R t+1 at time (t + 1) and the return variable G 0 = T t=0 R t+1 . A reward redistribution is second order Markov if the redistributed reward R t+1 depends only on (s t−1 , a t−1 , s t , a t ). If the SDP P is obtained from the SDPP by reward redistribution, thenP and P are strictly returnequivalent. The next theorem states that the optimal policies are still the same forP and P (proof after Section Theorem S2). Theorem 1. Both the SDPP with delayed rewardR t+1 and the SDP P with redistributed reward R t+1 have the same optimal policies.</p><p>Optimal Reward Redistribution with Expected Future Rewards Equal to Zero. We move on to the main goal of this paper: to derive an SDP via reward redistribution that has expected future rewards equal to zero and, therefore, no delayed rewards. At time (t − 1) the immediate reward is R t with expectation r(s t−1 , a t−1 ). We define the expected future rewards κ(m, t − 1) at time (t − 1) as the expected sum of future rewards from R t+1 to R t+1+m . Definition 1. For 1 t T and 0 m T − t, the expected sum of delayed rewards at time (t − 1) in the interval [t + 1, t + m + 1] is defined as κ(m, t − 1) = E π [ m τ =0 R t+1+τ | s t−1 , a t−1 ]. For every time point t, the expected future rewards κ(T − t − 1, t) given (s t , a t ) is the expected sum of future rewards until sequence end, that is, in the interval [t + 2, T + 1]. For MDPs, the Bellman equation for Q-values becomes q π (s t , a t ) = r(s t , a t ) + κ(T − t − 1, t). We aim to derive an MDP with κ(T − t − 1, t) = 0, which gives q π (s t , a t ) = r(s t , a t ). In this case, learning the Q-values simplifies to estimating the expected immediate reward r(s t , a t ) = E [R t+1 | s t , a t ]. Hence, the reinforcement learning task reduces to computing the mean, e.g. the arithmetic mean, for each state-action pair (s t , a t ). A reward redistribution is defined to be optimal, if κ(T − t − 1, t) = 0 for 0 t T − 1. In general, an optimal reward redistribution violates the Markov assumptions and the Bellman equation does not hold (proof after Theorem A3 in the appendix). Therefore, we will consider SDPs in the following. The next theorem states that a delayed reward MDPP with a particular policy π can be transformed into a return-equivalent SDP P with an optimal reward redistribution. Theorem 2. We assume a delayed reward MDPP, where the accumulated reward is given at sequence end. A new SDP P is obtained by a second order Markov reward redistribution, which ensures that P is return-equivalent toP. For a specific π, the following two statements are equivalent: (I) κ(T − t − 1, t) = 0, i.e. the reward redistribution is optimal, (II) E [R t+1 | s t−1 , a t−1 , s t , a t ] =q π (s t , a t ) −q π (s t−1 , a t−1 ) .</p><p>(1)</p><p>An optimal reward redistribution fulfills for 1 t T and 0 m T − t: κ(m, t − 1) = 0.</p><p>The proof can be found after Theorem A4 in the appendix. Equation κ(T − t − 1, t) = 0 implies that the new SDP P has no delayed rewards, that is, E π [R t+1+τ | s t−1 , a t−1 ] = 0, for 0 τ T − t − 1 (Corollary A1 in the appendix). The SDP P has no delayed rewards since no state-action pair can increase or decrease the expectation of a future reward. Equation <ref type="bibr" target="#b0">(1)</ref> shows that for an optimal reward redistribution the expected reward has to be the difference of consecutive Q-values of the original delayed reward. The optimal reward redistribution is second order Markov since the expectation of R t+1 at time (t + 1) depends on (s t−1 , a t−1 , s t , a t ).</p><p>The next theorem states the major advantage of an optimal reward redistribution:q π (s t , a t ) can be estimated with an offset that depends only on s t by estimating the expected immediate redistributed reward. Thus, Q-value estimation becomes trivial and the the advantage function of the MDPP can be readily computed. Theorem 3. If the reward redistribution is optimal, then the Q-values of the SDP P are given by q π (s t , a t ) = r(s t , a t ) =q π (s t , a t ) − E st−1,at−1 [q π (s t−1 , a t−1 ) | s t ] =q π (s t , a t ) − ψ π (s t ) .</p><p>(2) The SDP P and the original MDPP have the same advantage function. Using a behavior policyπ the expected immediate reward is Eπ [R t+1 | s t , a t ] =q π (s t , a t ) − ψ π,π (s t ) .</p><p>(</p><p>The proof can be found after Theorem A5 in the appendix. If the reward redistribution is not optimal, then κ(T − t − 1, t) measures the deviation of the Q-value from r(s t , a t ). This theorem justifies several learning methods based on reward redistribution presented in the next paragraph.</p><p>Novel Learning Algorithms Based on Reward Redistributions. We assume γ = 1 and a finite horizon or an absorbing state original MDPP with delayed rewards. For this setting we introduce new reinforcement learning algorithms. They are gradually changing the reward redistribution during learning and are based on the estimations in Theorem 3. These algorithms are also valid for nonoptimal reward redistributions, since the optimal policies are kept (Theorem 1). Convergence of RUDDER learning can under standard assumptions be proven by the stochastic approximation for two time-scale update rules <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b63">64]</ref>. Learning consists of an LSTM and a Q-value update. Convergence proofs to an optimal policy are difficult, since locally stable attractors may not correspond to optimal policies.</p><p>According to Theorem 1, reward redistributions keep the optimal policies. Therefore, even nonoptimal reward redistributions ensure correct learning. However, an optimal reward redistribution speeds up learning considerably. Reward redistributions can be combined with methods that use Q-value ranks or advantage functions. We consider (A) Q-value estimation, (B) policy gradients, and (C) Q-learning. Type (A) methods estimate Q-values and are divided into variants (i), (ii), and (iii). Variant (i) assumes an optimal reward redistribution and estimatesq π (s t , a t ) with an offset depending only on s t . The estimates are based on Theorem 3 either by on-policy direct Q-value estimation according to Eq. (2) or by off-policy immediate reward estimation according to Eq. (3). Variant (ii) methods assume a non-optimal reward redistribution and correct Eq. (2) by estimating κ. Variant (iii) methods use eligibility traces for the redistributed reward. RUDDER learning can be based on policies like "greedy in the limit with infinite exploration" (GLIE) or "restricted rank-based randomized" (RRR) <ref type="bibr" target="#b117">[118]</ref>. GLIE policies change toward greediness with respect to the Q-values during learning. For more details on these learning approaches see Section A2.7.1 in the apendix.</p><p>Type (B) methods replace in the expected updates E π [∇ θ log π(a | s; θ)q π (s, a)] of policy gradients the value q π (s, a) by an estimate of r(s, a) or by a sample of the redistributed reward. The offset ψ π (s) in Eq. <ref type="bibr" target="#b1">(2)</ref> or ψ π,π (s) in Eq. (3) reduces the variance as baseline normalization does. These methods can be extended to Trust Region Policy Optimization (TRPO) <ref type="bibr" target="#b112">[113]</ref> as used in Proximal Policy Optimization (PPO) <ref type="bibr" target="#b114">[115]</ref>. The type (C) method is Q-learning with the redistributed reward.</p><p>Here, Q-learning is justified if immediate and future reward are drawn together, as typically done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Constructing Reward Redistributions by Return Decomposition</head><p>We now propose methods to construct reward redistributions. Learning with non-optimal reward redistributions does work since the optimal policies do not change according to Theorem 1. However, reward redistributions that are optimal considerably speed up learning, since future expected rewards introduce biases in TD methods and high variances in MC methods. The expected optimal redistributed reward is the difference of Q-values according to Eq. <ref type="bibr" target="#b0">(1)</ref>. The more a reward redistribution deviates from these differences, the larger are the absolute κ-values and, in turn, the less optimal the reward redistribution gets. Consequently, to construct a reward redistribution which is close to optimal we aim at identifying the largest Q-value differences.</p><p>Reinforcement Learning as Pattern Recognition. We want to transform the reinforcement learning problem into a pattern recognition task to employ deep learning approaches. The sum of the Q-value differences gives the difference between expected return at sequence begin and the expected return at sequence end (telescope sum). Thus, Q-value differences allow to predict the expected return of the whole state-action sequence. Identifying the largest Q-value differences reduces the prediction error most. Q-value differences are assumed to be associated with patterns in state-action transitions. The largest Q-value differences are expected to be found more frequently in sequences with very large or very low return. The resulting task is to predict the expected return from the whole sequence and identify which state-action transitions have contributed the most to the prediction. This pattern recognition task serves to construct a reward redistribution, where the redistributed reward corresponds to the different contributions. The next paragraph shows how the return is decomposed and redistributed along the state-action sequence.</p><p>Return Decomposition. The return decomposition idea is that a function g predicts the expectation of the return for a given state-action sequence (return for the whole sequence). The function g is neither a value nor an action-value function since it predicts the expected return when the whole sequence is given. With the help of g either the predicted value or the realization of the return is redistributed over the sequence. A state-action pair receives as redistributed reward its contribution to the prediction, which is determined by contribution analysis. We use contribution analysis since sensitivity analysis has serious drawbacks: local minima, instabilities, exploding or vanishing gradients, and proper exploration <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b109">110]</ref>. The major drawback is that the relevance of actions is missed since sensitivity analysis does not consider the contribution of actions to the output, but only their effect on the output when slightly perturbing them. Contribution analysis determines how much a state-action pair contributes to the final prediction. We can use any contribution analysis method, but we specifically consider three methods: (A) differences of return predictions, (B) integrated gradients (IG) <ref type="bibr" target="#b124">[125]</ref>, and (C) layer-wise relevance propagation (LRP) <ref type="bibr" target="#b2">[3]</ref>. For (A), g must try to predict the sequence-wide return at every time step. The redistributed reward is given by the difference of consecutive predictions. The function g can be decomposed into past, immediate, and future contributions to the return. Consecutive predictions share the same past and the same future contributions except for two immediate state-action pairs. Thus, in the difference of consecutive predictions contributions cancel except for the two immediate state-action pairs. Even for imprecise predictions of future contributions to the return, contribution analysis is more precise, since prediction errors cancel out. Methods (B) and (C) rely on information later in the sequence for determining the contribution and thereby may introduce a non-Markov reward. The reward can be viewed to be probabilistic but is prone to have high variance. Therefore, we prefer method (A).</p><p>Explaining Away Problem. We still have to tackle the problem that reward causing actions do not receive redistributed rewards since they are explained away by later states. To describe the problem, assume an MDPP with the only reward at sequence end. To ensure the Markov property, states inP have to store the reward contributions of previous state-actions; e.g. s T has to store all previous contributions such that the expectationr(s T , a T ) is Markov. The explaining away problem is that later states are used for return prediction, while reward causing earlier actions are missed.</p><p>To avoid explaining away, we define a difference function ∆(s t−1 , a t−1 , s t , a t ) between a stateaction pair (s t , a t ) and its predecessor (s t−1 , a t−1 ). That ∆ is a function of (s t , a t , s t−1 , a t−1 ) is justified by Eq. <ref type="bibr" target="#b0">(1)</ref>, which ensures that such ∆s allow an optimal reward redistribution. The sequence of differences is ∆ 0:T := ∆(s −1 , a −1 , s 0 , a 0 ), . . . , ∆(s T −1 , a T −1 , s T , a T ) . The components ∆ are assumed to be statistically independent from each other, therefore ∆ cannot store reward contributions of previous ∆. The function g should predict the return by g(∆ 0:T ) =r(s T , a T ) and can be decomposed into g(∆ 0:T ) = T t=0 h t . The contributions are h t = h(∆(s t−1 , a t−1 , s t , a t )) for 0 t T . For the redistributed rewards R t+1 , we ensure E [R t+1 | s t−1 , a t−1 , s t , a t ] = h t . The rewardR T +1 ofP is probabilistic and the function g might not be perfect, therefore neither g(∆ 0:T ) =r T +1 for the return realizationr T +1 nor g(∆ 0:T ) =r(s T , a T ) for the expected return holds. Therefore, we need to introduce the compensationr T +1 − T τ =0 h(∆(s τ −1 , a τ −1 , s τ , a τ )) as an extra reward R T +2 at time T + 2 to ensure strictly return-equivalent SDPs. If g was perfect, then it would predict the expected return which could be redistributed. The new redistributed rewards R t+1 are based on the return decomposition, since they must have the contributions h t as mean:</p><formula xml:id="formula_1">E [R 1 | s 0 , a 0 ] = h 0 , E [R t+1 | s t−1 , a t−1 , s t , a t ] = h t , 0 &lt; t T , R T +2 =R T +1 − T t=0 h t ,</formula><p>where the realizationr T +1 is replaced by its random variableR T +1 . If the prediction of g is perfect, then we can redistribute the expected return via the prediction. Theorem 2 holds also for the correction R T +2 (see Theorem A6 in the appendix). A g with zero prediction errors results in an optimal reward redistribution. Small prediction errors lead to reward redistributions close to an optimal one. RUDDER: Return Decomposition using LSTM. RUDDER uses a Long Short-Term Memory (LSTM) network for return decomposition and the resulting reward redistribution. RUDDER consists of three phases. (I) Safe exploration. Exploration sequences should generate LSTM training samples with delayed rewards by avoiding low Q-values during a particular time interval. Low Q-values hint at states where the agent gets stuck. Parameters comprise starting time, length, and Q-value threshold.</p><p>(II) Lessons replay buffer for training the LSTM. If RUDDER's safe exploration discovers an episode with unseen delayed rewards, it is secured in a lessons replay buffer <ref type="bibr" target="#b73">[74]</ref>. Unexpected rewards are indicated by a large prediction error of the LSTM. For LSTM training, episodes with larger errors are sampled more often from the buffer, similar to prioritized experience replay <ref type="bibr" target="#b108">[109]</ref>. (III) LSTM and return decomposition. An LSTM learns to predict sequence-wide return at every time step and, thereafter, return decomposition uses differences of return predictions (contribution analysis method (A)) to construct a reward redistribution. For more details see Section A8.4 in the appendix.</p><p>Feedforward Neural Networks (FFNs) vs. LSTMs. In contrast to LSTMs, FNNs are not suited for processing sequences. Nevertheless, FNNs can learn a action-value function, which enables contribution analysis by differences of predictions. However, this leads to serious problems by spurious contributions that hinder learning. For example, any contributions would be incorrect if the true expectation of the return did not change. Therefore, prediction errors might falsely cause contributions leading to spurious rewards. FNNs are prone to such prediction errors since they have to predict the expected return again and again from each different state-action pair and cannot use stored information. In contrast, the LSTM is less prone to produce spurious rewards: (i) The LSTM will only learn to store information if a state-action pair has a strong evidence for a change in the expected return. If information is stored, then internal states and, therefore, also the predictions change, otherwise the predictions stay unchanged. Hence, storing events receives a contribution and a corresponding reward, while by default nothing is stored and no contribution is given. (ii) The LSTM tends to have smaller prediction errors since it can reuse past information for predicting the expected return. For example, key events can be stored. (iii) Prediction errors of LSTMs are much more likely to cancel via prediction differences than those of FNNs. Since consecutive predictions of LSTMs rely on the same internal states, they usually have highly correlated errors.</p><p>Human Expert Episodes. They are an alternative to exploration and can serve to fill the lessons replay buffer. Learning can be sped up considerably when LSTM identifies human key actions. Return decomposition will reward human key actions even for episodes with low return since other actions that thwart high returns receive negative reward. Using human demonstrations in reinforcement learning led to a huge improvement on some Atari games like Montezuma's Revenge <ref type="bibr" target="#b92">[93,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Limitations. In all of the experiments reported in this manuscript, we show that RUDDER significantly outperforms other methods for delayed reward problems. However, RUDDER might not be effective when the reward is not delayed since LSTM learning takes extra time and has problems with very long sequences. Furthermore, reward redistribution may introduce disturbing spurious reward signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>RUDDER is evaluated on three artificial tasks with delayed rewards. These tasks are designed to show problems of TD, MC, and potential-based reward shaping. RUDDER overcomes these problems. Next, we demonstrate that RUDDER also works for more complex tasks with delayed rewards. Therefore, we compare RUDDER with a Proximal Policy Optimization (PPO) baseline on 52 Atari games. All experiments use finite time horizon or absorbing states MDPs with γ = 1 and reward at episode end. For more information see Section A4.1 in the appendix.</p><p>Artificial Tasks (I)-(III). Task (I) shows that TD methods have problems with vanishing information for delayed rewards. Goal is to learn that a delayed reward is larger than a distracting immediate reward. Therefore, the correct expected future reward must be assigned to many state-action pairs. Task (II) is a variation of the introductory pocket watch example with delayed rewards. It shows that MC methods have problems with the high variance of future unrelated rewards. The expected future reward that is caused by the first action has to be estimated. Large future rewards that are not associated with the first action impede MC estimations. Task (III) shows that potential-based reward shaping methods have problems with delayed rewards. For this task, only the first two actions are relevant, to which the delayed reward has to be propagated back.</p><p>The tasks have different delays, are tabular (Q-table), and use an -greedy policy with = 0.2. We compare RUDDER, MC, and TD(λ) on all tasks, and Monte Carlo Tree Search (MCTS) on task (I). Additionally, on task (III), SARSA(λ) and reward shaping are compared. We use λ = 0.9 as suggested <ref type="bibr" target="#b127">[128]</ref>. Reward shaping methods are the original method, look-forward advice, and look-back advice with three different potential functions. RUDDER uses an LSTM without output and forget gates, no lessons buffer, and no safe exploration. For all tasks contribution analysis is performed with difference of return predictions. A Q-table is learned by an exponential moving average of the redistributed reward (RUDDER's Q-value estimation) or by Q-learning. Performance is measured by the learning time to achieve 90% of the maximal expected return. A Wilcoxon signed-rank test determines the significance of performance differences between RUDDER and other methods.</p><p>(I) Grid World shows problems of TD methods with delayed rewards. The task illustrates a time bomb that explodes at episode end. The agent has to defuse the bomb and then run away as far as possible since defusing fails with a certain probability. Alternatively, the agent can immediately run away, which, however, leads to less reward on average. The Grid World is a 31 × 31 grid with bomb at coordinate <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b14">15]</ref> and start at <ref type="bibr">[30 − d, 15]</ref>, where d is the delay of the task. The agent can move up, down, left, and right as long as it stays on the grid. At the end of the episode, after 1.5d steps, the agent receives a reward of 1000 with probability of 0.5, if it has visited bomb. At each time step, the agent receives an immediate reward of c · t · h, where c depends on the chosen action, t is the current time step, and h is the Hamming distance to bomb. Each move toward the bomb, is immediately penalized with c = −0.09. Each move away from the bomb, is immediately rewarded with c = 0.1. The agent must learn the Q-values precisely to recognize that directly running away is not optimal. <ref type="figure" target="#fig_7">Figure 1</ref>(I) shows the learning times to solve the task vs. the delay of the reward averaged over 100 trials. For all delays, RUDDER is significantly faster than all other methods with p-values &lt; 10 −12 . Speed-ups vs. MC and MCTS, suggest to be exponential with delay time. RUDDER is exponentially faster with increasing delay than Q(λ), supporting Theorem A8 in the appendix. RUDDER significantly outperforms all other methods.</p><p>(II) The Choice shows problems of MC methods with delayed rewards. This task has probabilistic state transitions, which can be represented as a tree with states as nodes. The agent traverses the tree from the root (initial state) to the leafs (final states). At the root, the agent has to choose between the left and the right subtree, where one subtree has a higher expected reward. Thereafter, it traverses the tree randomly according to the transition probabilities. Each visited node adds its fixed share to the final reward. The delayed reward is given as accumulated shares at a leaf. The task is solved when the agent always chooses the subtree with higher expected reward. <ref type="figure" target="#fig_7">Figure 1</ref>(II) shows the learning times to solve the task vs. the delay of the reward averaged over 100 trials. For all delays, RUDDER is significantly faster than all other methods with p-values &lt; 10 −8 . The speed-up vs. MC, suggests to be exponential with delay time. RUDDER is exponentially faster with increasing delay than Q(λ), supporting Theorem A8 in the appendix. RUDDER significantly outperforms all other methods.</p><p>(III) Trace-Back shows problems of potential-based reward shaping methods with delayed rewards. We investigate how fast information about delayed rewards is propagated back by RUDDER, Q(λ), SARSA(λ), and potential-based reward shaping. MC is skipped since it does not transfer back information. The agent can move in a 15×15 grid to the 4 adjacent positions as long as it remains on the grid. Starting at <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b6">7)</ref>, the number of moves per episode is T = 20. The optimal policy moves the agent up in t = 1 and right in t = 2, which gives immediate reward of −50 at t = 2, and a delayed reward of 150 at the end t = 20 = T . Therefore, the optimal return is 100. For any other policy, the agent receives only an immediate reward of 50 at t = 2. For t 2, state transitions are deterministic, while for t &gt; 2 they are uniformly distributed and independent of the actions. Thus, the return does not depend on actions at t &gt; 2. We compare RUDDER, original reward shaping, look-ahead advice, and look-back advice. As suggested by the authors, we use SARSA instead of Q-learning for look-back advice. We use three different potential functions for reward shaping, which are all based on the reward redistribution (see appendix). At t = 2, there is a distraction since the immediate reward is −50 for the optimal and 50 for other actions. RUDDER is significantly faster than all other methods with p-values &lt; 10 −17 . <ref type="figure" target="#fig_7">Figure 1</ref>(III) shows the learning times averaged over 100 trials. RUDDER is exponentially faster than all other methods and significantly outperforms them.</p><p>Atari Games. RUDDER is evaluated with respect to its learning time and achieves scores on Atari games of the Arcade Learning Environment (ALE) <ref type="bibr" target="#b10">[11]</ref> and OpenAI Gym <ref type="bibr" target="#b17">[18]</ref>. RUDDER is used on top of the TRPO-based <ref type="bibr" target="#b112">[113]</ref> policy gradient method PPO that uses GAE <ref type="bibr" target="#b113">[114]</ref>. Our PPO baseline differs from the original PPO baseline <ref type="bibr" target="#b114">[115]</ref> in two aspects. (i) Instead of using the sign function of the rewards, rewards are scaled by their current maximum. In this way, the ratio between different rewards remains unchanged and the advantage of large delayed rewards can be recognized. (ii) The safe exploration strategy of RUDDER is used. The entropy coefficient is replaced by Proportional Control <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12]</ref>. A coarse hyperparameter optimization is performed for the PPO baseline. For all 52 Atari games, RUDDER uses the same architectures, losses, and hyperparameters, which were optimized for the baseline. The only difference to the PPO baseline is that the policy network predicts the value function of the redistributed reward to integrate reward redistribution into the PPO framework. Contribution analysis uses an LSTM with differences of return predictions. Here ∆ is the pixel-wise difference of two consecutive frames augmented with the current frame. LSTM training and reward redistribution are restricted to sequence chunks of 500 frames. Source code is provided upon publication. <ref type="table">Bowling  192  56  200  strike pins  Solaris  1,827  616  122  navigate map  Venture  1,350  820  150  find treasure  Seaquest  4,770  1,616  272  collect divers   Table 1</ref>: Average scores over 3 random seeds with 10 trials each for delayed reward Atari games. "delay": frames between reward and first related action. RUDDER considerably improves the PPO baseline on delayed reward games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RUDDER baseline delay delay-event</head><p>Policies are trained with no-op starting condition for 200M game frames using every 4th frame. Training episodes end with losing a life or at maximal 108K frames. All scores are averaged over 3 different random seeds for network and ALE initialization. We asses the performance by the learning time and the achieved scores. First, we compare RUDDER to the baseline by average scores per game throughout training, to assess learning speed <ref type="bibr" target="#b114">[115]</ref>. For 32 <ref type="bibr" target="#b19">(20)</ref> games RUDDER (baseline) learns on average faster. Next, we compare the average scores of the last 10 training games. For 29 <ref type="bibr" target="#b22">(23)</ref> games RUDDER (baseline) has higher average scores. In the majority of games RUDDER, improves the scores of the PPO baseline. To compare RUDDER and the baseline on Atari games that are characterize by delayed rewards, we selected the games Bowling, Solaris, Venture, and Seaquest. In these games, high scores are achieved by learning the delayed reward, while learning the immediate reward and extensive exploration (like for Montezuma's revenge) is less important. The results are presented in <ref type="table">Table 1</ref>. For more details and further results see Section A4.2 in the appendix.  : RUDDER redistributes rewards to key events in the Atari game Bowling. Originally, rewards are delayed and only given at episode end. The first 120 out of 200 frames of the episode are shown. RUDDER identifies key actions that steer the ball to hit all pins.</p><p>Conclusion. We have introduced RUDDER, a novel reinforcement learning algorithm based on the new concepts of reward redistribution and return decomposition. On artificial tasks, RUDDER significantly outperforms TD(λ), MC, MCTS and reward shaping methods, while on Atari games it improves a PPO baseline on average but most prominently on long delayed rewards games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1 Definition of Finite Markov Decision Processes</head><p>We consider a finite Markov decision process (MDP) P, which is a 5-tuple P = (S, A, R, p, γ):</p><p>• S is a finite set of states; S t is the random variable for states at time t with value s ∈ S. S t has a discrete probability distribution. • A is a finite set of actions (sometimes state-dependent A(s)); A t is the random variable for actions at time t with value a ∈ A. A t has a discrete probability distribution. • R is a finite set of rewards; R t+1 is the random variable for rewards at time (t + 1) with value r ∈ R. R t has a discrete probability distribution. • p(S t+1 = s , R t+1 = r | S t = s, A t = a) are the transition and reward distributions over states and rewards, respectively, conditioned on state-actions, • γ ∈ [0, 1] is a discount factor for the reward. The Markov policy π is a distribution over actions given the state: π(A t = a | S t = s). We often equip an MDP P with a policy π without explicitly mentioning it. At time t, the random variables give the states, actions, and rewards of the MDP, while low-case letters give possible values. At each time t, the environment is in some state s t ∈ S. The policy π takes an action a t ∈ A, which causes a transition of the environment to state s t+1 and a reward r t+1 for the policy. Therefore, the MDP creates a sequence</p><formula xml:id="formula_2">(S 0 , A 0 , R 1 , S 1 , A 1 , R 2 , S 2 , A 2 , R 3 , . . .) .<label>(A1)</label></formula><p>The marginal probabilities for</p><formula xml:id="formula_3">p(s , r | s, a) = Pr [S t+1 = s , R t+1 = r | S t = s, A t = a]<label>(A2)</label></formula><p>are:</p><formula xml:id="formula_4">p(r | s, a) = Pr [R t+1 = r | S t = s, A t = a] = s p(s , r | s, a) ,<label>(A3)</label></formula><formula xml:id="formula_5">p(s | s, a) = Pr [S t+1 = s | S t = s, A t = a] = r p(s , r | s, a) .<label>(A4)</label></formula><p>We use a sum convention: a,b goes over all possible values of a and b, that is, all combinations which fulfill the constraints on a and b. If b is a function of a (fully determined by a), then a,b = a . We denote expectations:</p><p>• E π is the expectation where the random variable is an MDP sequence of states, actions, and rewards generated with policy π. • E s is the expectation where the random variable is S t with values s ∈ S.</p><p>• E a is the expectation where the random variable is A t with values a ∈ A. • E r is the expectation where the random variable is R t+1 with values r ∈ R.</p><p>• E s,a,r,s ,a is the expectation where the random variables are S t+1 with values s ∈ S, S t with values s ∈ S, A t with values a ∈ A, A t+1 with values a ∈ A, and R t+1 with values r ∈ R. If more or fewer random variables are used, the notation is consistently adapted.</p><p>The return G t is the accumulated reward starting from t + 1:</p><formula xml:id="formula_6">G t = ∞ k=0 γ k R t+k+1 .<label>(A5)</label></formula><p>The discount factor γ determines how much immediate rewards are favored over more delayed rewards. For γ = 0 the return (the objective) is determined as the largest expected immediate reward, while for γ = 1 the return is determined by the expected sum of future rewards if the sum exists.</p><p>State-Value and Action-Value Function. The state-value function v π (s) for policy π and state s is defined as</p><formula xml:id="formula_7">v π (s) = E π [G t | S t = s] = E π ∞ k=0 γ k R t+k+1 | S t = s .<label>(A6)</label></formula><p>Starting at t = 0:</p><formula xml:id="formula_8">v π 0 = E π ∞ t=0 γ t R t+1 = E π [G 0 ] ,<label>(A7)</label></formula><p>the optimal state-value function v * and policy π * are v * (s) = max</p><formula xml:id="formula_9">π v π (s) ,<label>(A8)</label></formula><formula xml:id="formula_10">π * = arg max π v π (s) for all s .<label>(A9)</label></formula><p>The action-value function q π (s, a) for policy π is the expected return when starting from S t = s, taking action A t = a, and following policy π:</p><formula xml:id="formula_11">q π (s, a) = E π [G t | S t = s, A t = a] = E π ∞ k=0 γ k R t+k+1 | S t = s, A t = a .<label>(A10)</label></formula><p>The optimal action-value function q * and policy π * are</p><formula xml:id="formula_12">q * (s, a) = max π q π (s, a) ,<label>(A11)</label></formula><p>π * = arg max π q π (s, a) for all (s, a) .</p><p>The optimal action-value function q * can be expressed via the optimal value function v * :</p><formula xml:id="formula_14">q * (s, a) = E [R t+1 + γ v * (S t+1 ) | S t = s, A t = a] .<label>(A13)</label></formula><p>The optimal state-value function v * can be expressed via the optimal action-value function q * using the optimal policy π * :</p><formula xml:id="formula_15">v * (s) = max a q π * (s, a) = max a E π * [G t | S t = s, A t = a] = (A14) max a E π * [R t+1 + γ G t+1 | S t = s, A t = a] = max a E [R t+1 + γ v * (S t+1 ) | S t = s, A t = a] .</formula><p>Finite time horizon and no discount. We consider a finite time horizon, that is, we consider only episodes of length T , but may receive reward R T +1 at episode end at time T + 1. The finite time horizon MDP creates a sequence</p><formula xml:id="formula_16">(S 0 , A 0 , R 1 , S 1 , A 1 , R 2 , S 2 , A 2 , R 3 , . . . , S T −1 , A T −1 , R T , S T , A T , R T +1 ) .<label>(A15)</label></formula><p>Furthermore, we do not discount future rewards, that is, we set γ = 1. The return G t from time t to T is the sum of rewards:</p><formula xml:id="formula_17">G t = T −t k=0 R t+k+1 .<label>(A16)</label></formula><p>The state-value function v for policy π is</p><formula xml:id="formula_18">v π (s) = E π [G t | S t = s] = E π T −t k=0 R t+k+1 | S t = s<label>(A17)</label></formula><p>and the action-value function q for policy π is The expected return at time t = 0 for policy π is</p><formula xml:id="formula_19">q π (s, a) = E π [G t | S t = s, A t = a] = E π T −t k=0 R t+k+1 | S t = s, A t = a (A18) = E π [R t+1 + G t+1 | S t = s, A t = a] =</formula><formula xml:id="formula_20">v π 0 = E π [G 0 ] = E π T t=0 R t+1 ,<label>(A21)</label></formula><formula xml:id="formula_21">π * = argmax π v π 0 .</formula><p>The agent may start in a particular starting state S 0 which is a random variable. Often S 0 has only one value s 0 .</p><p>Learning. The goal of learning is to find the policy π * that maximizes the expected future discounted reward (the return) if starting at t = 0. Thus, the optimal policy π * is</p><formula xml:id="formula_22">π * = argmax π v π 0 .<label>(A22)</label></formula><p>We consider two learning approaches for Q-values: Monte Carlo and temporal difference.</p><p>Monte Carlo (MC). To estimate q π (s, a), MC computes the arithmetic mean of all observed returns (G t | S t = s, A t = a) in the data. When using Monte Carlo for learning a policy we use an exponentially weighted arithmetic mean since the policy steadily changes. For the ith update Monte Carlo tries to minimize <ref type="bibr">1 2</ref> </p><formula xml:id="formula_23">M (s t , a t ) 2 with the residual M (s t , a t ) M (s t , a t ) = (q π ) i (s t , a t ) − T −t−1 τ =0 γ τ r t+1+τ ,<label>(A23)</label></formula><p>such that the update of the action-value q at state-action (s t , a t ) is</p><formula xml:id="formula_24">(q π ) i+1 (s t , a t ) = (q π ) i (s t , a t ) − α M (s t , a t ) .<label>(A24)</label></formula><p>This update is called constant-α MC <ref type="bibr" target="#b127">[128]</ref>.</p><p>Temporal difference (TD) methods. TD updates are based on the Bellman equation. If r(s, a) and E s ,a [q π (s , a ) | s, a] have been estimated, the Q-values can be updated according to the Bellman equation:</p><formula xml:id="formula_25">(q π ) new (s, a) = r(s, a) + γ E s ,a [q π (s , a ) | s, a] .<label>(A25)</label></formula><p>The update is applying the Bellman operator with estimates E s ,a [q π (s , a ) | s, a] and r(s, a) toq π to obtain (q π ) new . The new estimate (q π ) new is closer to the fixed point q π of the Bellman operator, since the Bellman operator is a contraction (see Section A7.1.3 and Section A7.1.2). Since the estimates E s ,a [q π (s , a ) | s, a] and r(s, a) are not known, TD methods try to minimize It is possible to estimate r(s, a) separately via an unbiased minimal variance estimator like the arithmetic mean and then perform TD updates with the Bellman error using the estimated r(s, a) <ref type="bibr" target="#b102">[103]</ref>. Q-learning <ref type="bibr" target="#b139">[140]</ref> is an off-policy TD algorithm which is proved to converge <ref type="bibr" target="#b140">[141,</ref><ref type="bibr" target="#b19">20]</ref>. The proofs were later generalized <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b132">133]</ref>. Q-learning uses E s ,a [q π (s , a )] ≈ max aq (S t+1 , a) .</p><p>(A28)</p><p>The action-value function q, which is learned by Q-learning, approximates q * independently of the policy that is followed. More precisely, with Q-learning q converges with probability 1 to the optimal q * . However, the policy still determines which state-action pairs are encountered during learning. The convergence only requires that all action-state pairs are visited and updated infinitely often.</p><p>A2 Reward Redistribution, Return-Equivalent SDPs, Novel Learning Algorithms, and Return Decomposition</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.1 State Enriched MDPs</head><p>For MDPs with a delayed reward the states have to code the reward. However, for an immediate reward the states can be made more compact by removing the reward information. For example, states with memory of a delayed reward can be mapped to states without memory. Therefore, in order to compare MDPs, we introduce the concept of homomorphic MDPs. We first need to define a partition of a set induced by a function. Let B be a partition of a set X. For any x ∈ X, we denote We callP the homomorphic image of P under h. For homomorphic images the optimal Q-values and the optimal policies are the same.</p><p>Lemma A1 (Ravindran and Barto <ref type="bibr" target="#b97">[98]</ref>). IfP is a homomorphic image of P, then the optimal Q-values are the same and a policy that is optimal inP can be transformed to an optimal policy in P by normalizing the number of actions a that are mapped to the same actionã.</p><p>Consequently, the original MDP can be solved by solving a homomorphic image. Similar results have been obtained by Givan et al. using stochastically bisimilar MDPs: "Any stochastic bisimulation used for aggregation preserves the optimal value and action sequence properties as well as the optimal policies of the model" <ref type="bibr" target="#b33">[34]</ref>.  <ref type="bibr" target="#b72">[73]</ref>. A Markov decision processP is state-enriched compared to an MDP P ifP has the same states, actions, transition probabilities, and reward probabilities as P but with additional information in its states. We define state-enrichment as follows: Definition A2. A Markov decision processP is state-enriched compared to a Markov decision process P if P is a homomorphic image ofP, where gs is the identity and f (s) = s is not bijective.</p><p>Being not bijective means that there exists ands with f (s ) = f (s ), that is,S has more elements than S. In particular, state-enrichment does not change the optimal policies nor the Q-values in the sense of Lemma A1. Proposition A1. If an MDPP is state-enriched compared to an MDP P, then both MDPs have the same optimal Q-values and the same optimal policies.</p><p>Proof. According to the definition P is a homomorphic image ofP. The statements of Proposition A1 follow directly from Lemma A1.</p><p>Optimal policies of the state-enriched MDPP can be transformed to optimal policies of the original MDP P and, vice versa, each optimal policy of the original MDP P corresponds to at least one optimal policy of the state-enriched MDPP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.2 Return-Equivalent Sequence-Markov Decision Processes (SDPs)</head><p>Our goal is to compare Markov decision processes (MDPs) with delayed rewards to decision processes (DPs) without delayed rewards. The DPs without delayed rewards can but need not to be Markov in the rewards. Toward this end, we consider two DPsP and P which differ only in their (non-Markov) reward distributions. However for each policy π the DPsP and P have the same expected return at t = 0, that is,ṽ π 0 = v π 0 , or they have the same expected return for every episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.2.1 Sequence-Markov Decision Processes (SDPs)</head><p>We first define decision processes that are Markov except for the reward, which is not required to be Markov. Definition A3. A sequence-Markov decision process (SDP) is defined as a finite decision process which is equipped with a Markov policy and has Markov transition probabilities but a reward distribution that is not required to be Markov. Proposition A2. Markov decision processes are sequence-Markov decision processes.</p><p>Proof. MDPs have Markov transition probabilities and are equipped with Markov policies.</p><p>Definition A4. We call two sequence-Markov decision processes P andP that have the same Markov transition probabilities and are equipped with the same Markov policy sequence-equivalent. Lemma A2. Two sequence-Markov decision processes that are sequence-equivalent have the same probability to generate state-action sequences (s 0 , a 0 , . . . , s t , a t ), 0 t T .</p><p>Proof. Sequence generation only depends on transition probabilities and policy. Therefore the probability of generating a particular sequences is the same for both SDPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.2.2 Return-Equivalent SDPs</head><p>We define return-equivalent SDPs which can be shown to have the same optimal policies. Definition A5. Two sequence-Markov decision processesP and P are return-equivalent if they differ only in their reward but for each policy π have the same expected returnṽ π 0 = v π 0 .P and P are strictly return-equivalent if they have the same expected return for every episode and for each policy π:</p><formula xml:id="formula_26">E π G 0 | s 0 , a 0 , . . . , s T , a T = E π [G 0 | s 0 , a 0 , . . . , s T , a T ] .<label>(A31)</label></formula><p>The definition of return-equivalence can be generalized to strictly monotonic functions f for which v π 0 = f (v π 0 ). Since strictly monotonic functions do not change the ordering of the returns, maximal returns stay maximal after applying the function f . Strictly return-equivalent SDPs are return-equivalent as the next proposition states. Proposition A3. Strictly return-equivalent sequence-Markov decision processes are returnequivalent.</p><p>Proof. The expected return at t = 0 given a policy is the sum of the probability of generating a sequence times the expected reward for this sequence. Both expectations are the same for two strictly return-equivalent sequence-Markov decision processes. Therefore the expected return at time t = 0 is the same.</p><p>The next proposition states that return-equivalent SDPs have the same optimal policies. Proposition A4. Return-equivalent sequence-Markov decision processes have the same optimal policies.</p><p>Proof. The optimal policy is defined as maximizing the expected return at time t = 0. For each policy the expected return at time t = 0 is the same for return-equivalent decision processes. Consequently, the optimal policies are the same.</p><p>Two strictly return-equivalent SDPs have the same expected return for each state-action sub-sequence (s 0 , a 0 , . . . , s t , a t ), 0 t T .</p><p>Lemma A3. Two strictly return-equivalent SDPsP and P have the same expected return for each state-action sub-sequence (s 0 , a 0 , . . . , s t , a t ), 0 t T :</p><formula xml:id="formula_27">E π G 0 | s 0 , a 0 , . . . , s t , a t = E π [G 0 | s 0 , a 0 , . . . , s t , a t ] .<label>(A32)</label></formula><p>Proof. Since the SDPs are strictly return-equivalent, we have</p><formula xml:id="formula_28">E π G 0 | s 0 , a 0 , . . . , s t , a t (A33) = st+1,at+1,...,s T ,a T p π (s t+1 , a t+1 , . . . , s T , a T | s t , a t ) E π G 0 | s 0 , a 0 , . . . , s T , a T = st+1,at+1,...,s T ,a T p π (s t+1 , a t+1 , . . . , s T , a T | s t , a t ) E π [G 0 | s 0 , a 0 , . . . , s T , a T ] = E π [G 0 | s 0 , a 0 , . . . , s t , a t ] .</formula><p>We used the marginalization of the full probability and the Markov property of the state-action sequence.</p><p>We now give the analog definitions and results for MDPs which are SDPs.</p><p>Definition A6. Two Markov decision processesP and P are return-equivalent if they differ only in p(r | s, a) and p(r | s, a) but have the same expected returnṽ π 0 = v π 0 for each policy π.P and P are strictly return-equivalent if they have the same expected return for every episode and for each policy π:</p><formula xml:id="formula_29">E π G 0 | s 0 , a 0 , . . . , s T , a T = E π [G 0 | s 0 , a 0 , . . . , s T , a T ] .<label>(A34)</label></formula><p>Strictly return-equivalent MDPs are return-equivalent as the next proposition states. Proposition A5. Strictly return-equivalent decision processes are return-equivalent.</p><p>Proof. Since MDPs are SDPs, the proposition follows from Proposition A3.</p><p>Proposition A6. Return-equivalent Markov decision processes have the same optimal policies.</p><p>Proof. Since MDPs are SDPs, the proposition follows from Proposition A4.</p><p>For strictly return-equivalent MDPs the expected return is the same if a state-action sub-sequence is given.</p><p>Proposition A7. Strictly return-equivalent MDPsP and P have the same expected return for a given state-action sub-sequence (s 0 , a 0 , . . . , s t , a t ), 0 t T :</p><formula xml:id="formula_30">E π G 0 | s 0 , a 0 , . . . , s t , a t = E π [G 0 | s 0 , a 0 , . . . , s t , a t ] .<label>(A35)</label></formula><p>Proof. Since MDPs are SDPs, the proposition follows from Lemma A3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.3 Reward Redistribution for Strictly Return-Equivalent SDPs</head><p>Strictly return-equivalent SDPsP and P can be constructed by a reward redistribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.3.1 Reward Redistribution</head><p>We define reward redistributions for SDPs. Definition A7. A reward redistribution given an SDPP is a fixed procedure that redistributes for each state-action sequence s 0 , a 0 , . . . , s T , a T the realization of the associated return variableG 0 = T t=0R t+1 or its expectation E G 0 | s 0 , a 0 , . . . , s T , a T along the sequence. The redistribution creates a new SDP P with redistributed reward R t+1 at time (t + 1) and return variable G 0 = T t=0 R t+1 . The redistribution procedure ensures for each sequence eitherG 0 = G 0 or</p><formula xml:id="formula_31">E π G 0 | s 0 , a 0 , . . . , s T , a T = E π [G 0 | s 0 , a 0 , . . . , s T , a T ] .<label>(A36)</label></formula><p>Reward redistributions can be very general. A special case is if the return can be deduced from the past sequence, which makes the return causal. Definition A8. A reward redistribution is causal if for the redistributed reward R t+1 the following holds:</p><formula xml:id="formula_32">E [R t+1 | s 0 , a 0 , . . . , s T , a T ] = E [R t+1 | s 0 , a 0 , . . . , s t , a t ] .<label>(A37)</label></formula><p>For our approach we only need reward redistributions that are second order Markov.</p><formula xml:id="formula_33">Definition A9. A causal reward redistribution is second order Markov if E [R t+1 | s 0 , a 0 , . . . , s t , a t ] = E [R t+1 | s t−1 , a t−1 , s t , a t ] .<label>(A38)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.4 Reward Redistribution Constructs Strictly Return-Equivalent SDPs</head><p>Theorem A1. If the SDP P is obtained by reward redistribution from the SDPP, thenP and P are strictly return-equivalent.</p><p>Proof. For redistributing the reward we have for each state-action sequence s 0 , a 0 , . . . , s T , a T the same returnG 0 = G 0 , therefore</p><formula xml:id="formula_34">E π G 0 | s 0 , a 0 , . . . , s T , a T = E π [G 0 | s 0 , a 0 , . . . , s T , a T ] .<label>(A39)</label></formula><p>For redistributing the expected return the last equation holds by definition. The last equation is the definition of strictly return-equivalent SDPs.</p><p>The next theorem states that the optimal policies are still the same when redistributing the reward.</p><p>Theorem A2. If the SDP P is obtained by reward redistribution from the SDPP, then both SDPs have the same optimal policies.</p><p>Proof. According to Theorem A1, the SDP P is strictly return-equivalent to the SDPP. According to Proposition A3 and Proposition A4 the SDP P and the SDPP have the same optimal policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.4.1 Special Cases of Strictly Return-Equivalent Decision Processes: Reward Shaping,</head><p>Look-Ahead Advice, and Look-Back Advice Redistributing the reward via reward shaping <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b142">143]</ref>, look-ahead advice, and look-back advice <ref type="bibr" target="#b143">[144]</ref> is a special case of reward redistribution that leads to MDPs which are strictly return-equivalent to the original MDP. We show that reward shaping is a special case of reward redistributions that lead to MDPs which are strictly return-equivalent to the original MDP. First, we subtract from the potential the constant c = (Φ(s 0 , a 0 ) − γ T Φ(s T , a T ))/(1 − γ T ), which is the potential of the initial state minus the discounted potential in the last state divided by a fixed divisor. Consequently, the sum of additional rewards in reward shaping, look-ahead advice, or look-back advice from 1 to T is zero. The original sum of additional rewards is</p><formula xml:id="formula_35">T i=1 γ i−1 (γΦ(s i , a i ) − Φ(s i−1 , a i−1 )) = γ T Φ(s T , a T ) − Φ(s 0 , a 0 ) .<label>(A40)</label></formula><p>If we assume γ T Φ(s T , a T ) = 0 and Φ(s 0 , a 0 ) = 0, then reward shaping does not change the return and the shaping reward is a reward redistribution leading to an MDP that is strictly return-equivalent to the original MDP. For T → ∞ only Φ(s 0 , a 0 ) = 0 is required. The assumptions can always be fulfilled by adding a single new initial state and a single new final state to the original MDP. Without the assumptions γ T Φ(s T , a T ) = 0 and Φ(s 0 , a 0 ) = 0, we subtract c = (Φ(s 0 , a 0 ) − γ T Φ(s T , a T ))/(1 − γ T ) from all potentials Φ, and obtain</p><formula xml:id="formula_36">T i=1 γ i−1 (γ(Φ(s i , a i ) − c) − (Φ(s i−1 , a i−1 ) − c)) = 0 .<label>(A41)</label></formula><p>Therefore, the potential-based shaping function (the additional reward) added to the original reward does not change the return, which means that the shaping reward is a reward redistribution that leads to an MDP that is strictly return-equivalent to the original MDP. Obviously, reward shaping is a special case of reward redistribution that leads to a strictly return-equivalent MDP. Reward shaping does not change the general learning behavior if a constant c is subtracted from the potential function Φ. The Q-function of the original reward shaping and the Q-function of the reward shaping, which has a constant c subtracted from the potential function Φ, differ by c for every Q-value <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b142">143]</ref>. For infinite horizon MDPs with γ &lt; 1, the terms γ T and γ T Φ(s T , a T ) vanish, therefore it is sufficient to subtract c = Φ(s 0 , a 0 ) from the potential function.</p><p>Since TD based reward shaping methods keep the original reward, they can still be exponentially slow for delayed rewards. Reward shaping methods like reward shaping, look-ahead advice, and look-back advice rely on the Markov property of the original reward, while an optimal reward redistribution is not Markov. In general, reward shaping does not lead to an optimal reward redistribution according to Section A2.6.1.</p><p>As discussed in Paragraph A2.9, the optimal reward redistribution does not comply to the Bellman equation. Also look-ahead advice does not comply to the Bellman equation. The return for the look-ahead advice rewardR t+1 is</p><formula xml:id="formula_37">G t = ∞ i=0R t+i+1 (A42)</formula><p>with expectations for the rewardR t+1</p><formula xml:id="formula_38">E π R t+1 | s t+1 , a t+1 , s t , a t =r(s t+1 , a t+1 , s t , a t ) = γΦ(s t+1 , a t+1 ) − Φ(s t , a t ) .</formula><p>(A43) The expected rewardr(s t+1 , a t+1 , s t , a t ) depends on future states s t+1 and, more importantly, on future actions a t+1 . It is a non-causal reward redistribution. Therefore look-ahead advice cannot be directly used for selecting the optimal action at time t. For look-back advice we have</p><formula xml:id="formula_39">E π R t+1 | s t , a t , s t−1 , a t−1 =r(s t , a t , s t−1 , a t−1 ) = Φ(s t , a t ) − γ −1 Φ(s t−1 , a t−1 ) .</formula><p>(A44) Therefore look-back advice introduces a second-order Markov reward like the optimal reward redistribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.5 Transforming an Immediate Reward MDP to a Delayed Reward MDP</head><p>We assume to have a Markov decision process P with immediate reward. The MDP P is transformed into an MDPP with delayed reward, where the reward is given at sequence end. The rewardequivalent MDPP with delayed reward is state-enriched, which ensures that it is an MDP. The state-enriched MDPP has • reward:R</p><formula xml:id="formula_40">t = 0 , for t T T k=0 R k+1 , for t = T + 1 .<label>(A45)</label></formula><p>• state:s</p><formula xml:id="formula_41">t = (s t , ρ t ) ,<label>(A46)</label></formula><formula xml:id="formula_42">ρ t = t−1 k=0 r k+1 , with R k+1 = r k+1 .<label>(A47)</label></formula><p>Here we assume that ρ can only take a finite number of values to assure that the enriched statess are finite. If the original reward was continuous, then ρ can represent the accumulated reward with any desired precision if the sequence length is T and the original reward was bounded. We assume that ρ is sufficiently precise to distinguish the optimal policies, which are deterministic, from sub-optimal deterministic policies. The random variable R k+1 is distributed according to p(r | s k , a k ). We assume that the time t is coded in s in order to know when the episode ends and reward is no longer received, otherwise we introduce an additional state variable τ = t that codes the time. Proposition A8. If a Markov decision process P with immediate reward is transformed by above definedR t ands t to a Markov decision processP with delayed reward, where the reward is given at sequence end, then: (I) the optimal policies do not change, and (II) forπ(a |s) = π(a | s) qπ(s, a) = q π (s, a) +</p><formula xml:id="formula_43">t−1 k=0 r k+1 ,<label>(A48)</label></formula><p>forS t =s, S t = s, and A t = a.</p><p>Proof. For (I) we first perform an state-enrichment of P bys t = (s t , ρ t ) with ρ t = t−1 k=0 r k+1 for R k+1 = r k+1 leading to an intermediate MDP. We assume that the finite-valued ρ is sufficiently precise to distinguish the optimal policies, which are deterministic, from sub-optimal deterministic policies. Proposition A1 ensures that neither the optimal Q-values nor the optimal policies change between the original MDP P and the intermediate MDP. Next, we redistribute the original reward R t+1 according to the redistributed rewardR t . The new MDPP with state enrichment and reward redistribution is strictly return-equivalent to the intermediate MDP with state enrichment but the original reward. The new MDPP is Markov since the enriched state ensures thatR T +1 is Markov. Proposition A5 and Proposition A6 ensure that the optimal policies are the same. For (II) we show a proof without Bellman equation and a proof using the Bellman equation. Equivalence without Bellman equation. We haveG 0 = G 0 . The Markov property ensures that the future reward is independent of the already received reward:</p><formula xml:id="formula_44">E π T k=t R k+1 | S t = s, A t = a, ρ = t−1 k=0 r k+1 = E π T k=t R k+1 | S t = s, A t = a . (A49)</formula><p>We assumeπ(a |s) = π(a | s). We obtainqπ</p><formula xml:id="formula_45">(s, a) = Eπ G 0 |S t =s, A t = a (A50) = Eπ T k=0 R k+1 | S t = s, ρ = t−1 k=0 r k+1 , A t = a = Eπ T k=t R k+1 | S t = s, ρ = t−1 k=0 r k+1 , A t = a + t−1 k=0 r k+1 = E π T k=t R k+1 | S t = s, A t = a + t−1 k=0 r k+1 = q π (s, a) + t−1 k=0 r k+1 .</formula><p>We used Eπ = E π , which is ensured since reward probabilities, transition probabilities, and the probability of choosing an action by the policy correspond to each other in both settings.</p><p>Since the optimal policies do not change for reward-equivalent and state-enriched processes, we havẽ</p><formula xml:id="formula_46">q * (s, a) = q * (s, a) + t−1 k=0 r k+1 .<label>(A51)</label></formula><p>Equivalence with Bellman equation. With q π (s, a) as optimal action-value function for the original Markov decision process, we define a new Markov decision process with action-state functionqπ. ForS t =s, S t = s, and A t = a we havẽ qπ(s, a) := q π (s, a) +</p><formula xml:id="formula_47">t−1 k=0 r k+1 ,<label>(A52)</label></formula><p>π(a |s) := π(a | s) . For t = T we haver = T k=0 r k+1 = ρ and q π (s , a ) = 0 as well asqπ(s , a ) = 0. Both q andq must be zero for t T since after time t = T + 1 there is no more reward. We obtain for t = T and r = r T +1 : qπ(s, a) = q π (s, a) + Sinceqπ(s, a) fulfills the Bellman equation, it is the action-value function forπ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.6 Transforming an Delayed Reward MDP to an Immediate Reward SDP</head><p>Next we consider the opposite direction, where the delayed reward MDPP is given and we want to find an immediate reward SDP P that is return-equivalent toP. We assume an episodic reward forP, that is, reward is only given at sequence end. The realization of final reward, that is the realization of the return,r T +1 is redistributed to previous time steps. Instead of redistributing the realizatioñ r T +1 of the random variableR T +1 , also its expectationr(s T , a T ) = E R T +1 | s T , a T can be redistributed since Q-value estimation considers only the mean. We used the Markov property</p><formula xml:id="formula_48">E π G 0 | s 0 , a 0 , . . . , s T , a T = E π T t=0R t+1 | s 0 , a 0 , . . . , s T , a T (A57) = E R T +1 | s 0 , a 0 , . . . , s T , a T = E R T +1 | s T , a T .</formula><p>Redistributing the expectation reduces the variance of estimators since the variance of the random variable is already factored out. We assume a delayed reward MDPP with reward</p><formula xml:id="formula_49">R t = 0 , for t T R T +1 , for t = T + 1 ,<label>(A58)</label></formula><p>whereR t = 0 means that the random variableR t is always zero. The expected reward at the last time step isr</p><formula xml:id="formula_50">(s T , a T ) = E R T +1 | s T , a T ,<label>(A59)</label></formula><p>which is also the expected return. Given a state-action sequence (s 0 , a 0 , . . . , s T , a T ), we want to redistribute either the realizationr T +1 of the random variableR T +1 or its expectationr(s T , a T ),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.6.1 Optimal Reward Redistribution</head><p>The main goal in this paper is to derive an SDP via reward redistribution that has zero expected future rewards. Consequently the SDP has no delayed rewards. To measure the amount of delayed rewards, we define the expected sum of delayed rewards κ(m, t − 1). Definition A10. For 1 t T and 0 m T − t, the expected sum of delayed rewards at time</p><formula xml:id="formula_51">(t − 1) in the interval [t + 1, t + m + 1] is defined as κ(m, t − 1) = E π m τ =0 R t+1+τ | s t−1 , a t−1 .<label>(A60)</label></formula><p>The Bellman equation for Q-values becomes</p><formula xml:id="formula_52">q π (s t , a t ) = r(s t , a t ) + κ(T − t − 1, t) ,<label>(A61)</label></formula><p>where κ(T − t − 1, t) is the expected sum of future rewards until sequence end given (s t , a t ), that is, in the interval [t + 2, T + 1]. We aim to derive an MDP with κ(T − t − 1, t) = 0, which gives q π (s t , a t ) = r(s t , a t ). In this case, learning the Q-values reduces to estimating the average immediate reward r(s t , a t ) = E [R t+1 | s t , a t ]. Hence, the reinforcement learning task reduces to computing the mean, e.g. the arithmetic mean, for each state-action pair (s t , a t ). Next, we define an optimal reward redistribution.</p><formula xml:id="formula_53">Definition A11. A reward redistribution is optimal, if κ(T − t − 1, t) = 0 for 0 t T − 1.</formula><p>Next theorem states that in general an MDP with optimal reward redistribution does not exist, which is the reason why we will consider SDPs in the following. Theorem A3. In general, an optimal reward redistribution violates the assumption that the reward distribution is Markov, therefore the Bellman equation does not hold.</p><p>Proof. We assume an MDPP withr(s T , a T ) = 0 and which has policies that lead to different expected returns at time t = 0. If all reward is given at time t = 0, all policies have the same expected return at time t = 0. This violates our assumption, therefore not all reward can be given at t = 0. In vector and matrix notation the Bellman equation is</p><formula xml:id="formula_54">q π t = r t + P t→t+1 q π t+1 ,<label>(A62)</label></formula><p>where P t→t+1 is the row-stochastic matrix with p(s t+1 | s t , a t )π(a t+1 | s t+1 ) at positions ((s t , a t ), (s t+1 , a t+1 )). An optimal reward redistribution requires the expected future rewards to be zero:</p><formula xml:id="formula_55">P t→t+1 q π t+1 = 0<label>(A63)</label></formula><p>and, since optimality requires q π t+1 = r t+1 , we have</p><formula xml:id="formula_56">P t→t+1 r t+1 = 0 ,<label>(A64)</label></formula><p>where r t+1 is the vector with componentsr(s t+1 , a t+1 ). Since (i) the MDPs are return-equivalent, (ii)r(s T , a T ) = 0, and (iii) not all reward is given at t = 0, an (t + 1) exists with r t+1 = 0. We can construct an MDPP which has (a) at least as many state-action pairs (s t , a t ) as pairs (s t+1 , a t+1 ) and (b) the transition matrix P t→t+1 has full rank. P t→t+1 r t+1 = 0 is now a contradiction to r t+1 = 0 and P t→t+1 has full rank. Consequently, simultaneously ensuring Markov properties and ensuring zero future return is in general not possible.</p><p>For a particular π, the next theorem states that an optimal reward redistribution, that is κ = 0, is equivalent to a redistributed reward which expectation is the difference of consecutive Q-values of the original delayed reward. The theorem states that an optimal reward redistribution exists but we have to assume an SDP P that has a second order Markov reward redistribution.</p><p>Theorem A4. We assume a delayed reward MDPP, where the accumulated reward is given at sequence end. An new SDP P is obtained by a second order Markov reward redistribution, which ensures that P is return-equivalent toP. For a specific π, the following two statements are equivalent:</p><formula xml:id="formula_57">(I) κ(T − t − 1, t) = 0, i.e. the reward redistribution is optimal, (II) E [R t+1 | s t−1 , a t−1 , s t , a t ] =q π (s t , a t ) −q π (s t−1 , a t−1 ) . (A65)</formula><p>Furthermore, an optimal reward redistribution fulfills for 1 t T and 0 m T − t:</p><formula xml:id="formula_58">κ(m, t − 1) = 0 . (A66)</formula><p>Proof. PART (I): we assume that the reward redistribution is optimal, that is,</p><formula xml:id="formula_59">κ(T − t − 1, t) = 0 .<label>(A67)</label></formula><p>The redistributed reward R t+1 is second order Markov. We abbreviate the expected R t+1 by h t :</p><formula xml:id="formula_60">E [R t+1 | s t−1 , a t−1 , s t , a t ] = h t .<label>(A68)</label></formula><p>The assumptions of Lemma A3 hold for for the delayed reward MDPP and the redistributed reward SDP P. Therefore for a given state-action sub-sequence (s 0 , a 0 , . . . , s t , a t ), 0 t T :</p><formula xml:id="formula_61">E π G 0 | s 0 , a 0 , . . . , s t , a t = E π [G 0 | s 0 , a 0 , . . . , s t , a t ]<label>(A69)</label></formula><formula xml:id="formula_62">with G 0 = T τ =0 R τ +1 andG 0 =R T +1 .</formula><p>The Markov property of the MDPP ensures that the future reward from t + 1 on is independent of the past sub-sequence s 0 , a 0 , . . . , s t−1 , a t−1 :</p><formula xml:id="formula_63">E π T −t τ =0R t+1+τ | s t , a t = E π T −t τ =0R t+1+τ | s 0 , a 0 , . . . , s t , a t .<label>(A70)</label></formula><p>The second order Markov property of the SDP P ensures that the future reward from t + 2 on is independent of the past sub-sequence s 0 , a 0 , . . . , s t−1 , a t−1 :</p><formula xml:id="formula_64">E π T −t−1 τ =0 R t+2+τ | s t , a t = E π T −t−1 τ =0 R t+2+τ | s 0 , a 0 , . . . , s t , a t .<label>(A71)</label></formula><p>Using these properties we obtaiñ</p><formula xml:id="formula_65">q π (s t , a t ) = E π T −t τ =0R t+1+τ | s t , a t (A72) = E π T −t τ =0R t+1+τ | s 0 , a 0 , . . . , s t , a t = E π R T +1 | s 0 , a 0 , . . . , s t , a t = E π T τ =0R τ +1 | s 0 , a 0 , . . . , s t , a t = E π G 0 | s 0 , a 0 , . . . , s t , a t = E π [G 0 | s 0 , a 0 , . . . , s t , a t ] = E π T τ =0 R τ +1 | s 0 , a 0 , . . . , s t , a t = E π T −t−1 τ =0 R t+2+τ | s 0 , a 0 , . . . , s t , a t + t τ =0 h τ = E π T −t−1 τ =0 R t+2+τ | s t , a t + t τ =0 h τ = κ(T − t − 1, t) + t τ =0 h τ = t τ =0 h τ .</formula><p>We used</p><formula xml:id="formula_66">κ(T − t − 1, t) = E π T −t−1 τ =0 R t+2+τ | s t , a t = 0 .<label>(A73)</label></formula><p>It follows that</p><formula xml:id="formula_67">E [R t+1 | s t−1 , a t−1 , s t , a t ] = h t (A74) =q π (s t , a t ) −q π (s t−1 , a t−1 ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PART (II): we assume that</head><formula xml:id="formula_68">E [R t+1 | s t−1 , a t−1 , s t , a t ] = h t (A75) =q π (s t , a t ) −q π (s t−1 , a t−1 ) . The expectations E π [. | s t−1 , a t−1 ] like E π R T +1 | s t−1 , a t−1</formula><p>are expectations over all episodes starting in (s t−1 , a t−1 ) and ending in some (s T , a T ). First, we consider m = 0 and 1 t T , therefore κ(</p><formula xml:id="formula_69">0, t − 1) = E π [R t+1 | s t−1 , a t−1 ]. Sincẽ r(s t−1 , a t−1 ) = 0 for 1 t T , we havẽ q π (s t−1 , a t−1 ) =r(s t−1 , a t−1 ) + st,at p(s t , a t | s t−1 , a t−1 )q π (s t , a t ) (A76) = st,at p(s t , a t | s t−1 , a t−1 )q π (s t , a t ) .</formula><p>Using this equation we obtain for 1 t T :</p><formula xml:id="formula_70">κ(0, t − 1) = E st,at,Rt+1 [R t+1 | s t−1 , a t−1 ] (A77) = E st,at [q π (s t , a t ) −q π (s t−1 , a t−1 ) | s t−1 , a t−1 ] = st,at p(s t , a t | s t−1 , a t−1 ) (q π (s t , a t ) −q π (s t−1 , a t−1 )) =q π (s t−1 , a t−1 ) − st,at p(s t , a t | s t−1 , a t−1 )q π (s t−1 , a t−1 ) =q π (s t−1 , a t−1 ) −q π (s t−1 , a t−1 ) = 0 .</formula><p>Next, we consider the expectation of</p><formula xml:id="formula_71">m τ =0 R t+1+τ for 1 t T and 1 m T − t (for m &gt; 0) κ(m, t − 1) = E π m τ =0 R t+1+τ | s t−1 , a t−1 (A78) = E π m τ =0 (q π (s τ +t , a τ +t ) −q π (s τ +t−1 , a τ +t−1 )) | s t−1 , a t−1 = E π [q π (s t+m , a t+m ) −q π (s t−1 , a t−1 ) | s t−1 , a t−1 ] = E π E π T τ =t+mR τ +1 | s t+m , a t+m | s t−1 , a t−1 − E π E π T τ =t−1R τ +1 | s t−1 , a t−1 | s t−1 , a t−1 = E π R T +1 | s t−1 , a t−1 − E π R T +1 | s t−1 , a t−1 = 0 .</formula><p>We used thatR t+1 = 0 for t &lt; T .</p><formula xml:id="formula_72">For t = τ + 1 and m = T − t = T − τ − 1 we have κ(T − τ − 1, τ ) = 0 ,<label>(A79)</label></formula><p>which characterizes an optimal reward redistribution.</p><p>Thus, an SDP with an optimal reward redistribution has a expected future rewards that are zero. Equation κ(T − t − 1, t) = 0 means that the new SDP P has no delayed rewards as shown in next corollary. Corollary A1. An SDP with an optimal reward redistribution fulfills for</p><formula xml:id="formula_73">0 τ T − t − 1 E π [R t+1+τ | s t−1 , a t−1 ] = 0 . (A80)</formula><p>The SDP has no delayed rewards since no state-action pair can increase or decrease the expectation of a future reward.</p><p>Proof. For τ = 0 we use κ(m, t − 1) = 0 from Theorem A4 with m = 0:</p><formula xml:id="formula_74">E π [R t+1 | s t−1 , a t−1 ] = κ(0, t − 1) = 0 .<label>(A81)</label></formula><p>For τ &gt; 0, we also use κ(m, t − 1) = 0 from Theorem A4:</p><formula xml:id="formula_75">E π [R t+1+τ | s t−1 , a t−1 ] = E π τ k=0 R t+1+k − τ −1 k=0 R t+1+k | s t−1 , a t−1 (A82) = E π τ k=0 R t+1+k | s t−1 , a t−1 − E π τ −1 k=0 R t+1+k | s t−1 , a t−1 = κ(τ, t − 1) − κ(τ − 1, t − 1) = 0 − 0 = 0 .</formula><p>A related approach is to ensure zero return by reward shaping if the exact value function is known <ref type="bibr" target="#b113">[114]</ref>. The next theorem states the major advantage of an optimal reward redistribution:q π (s t , a t ) can be estimated with an offset that depends only on s t by estimating the expected immediate redistributed reward. Thus, Q-value estimation becomes trivial and the the advantage function of the MDPP can be readily computed. Theorem A5. If the reward redistribution is optimal, then the Q-values of the SDP P are given by</p><formula xml:id="formula_76">q π (s t , a t ) = r(s t , a t ) =q π (s t , a t ) − E st−1,at−1 [q π (s t−1 , a t−1 ) | s t ] (A83) =q π (s t , a t ) − ψ π (s t ) .</formula><p>The SDP P and the original MDPP have the same advantage function. Using a behavior policyπ the expected immediate reward is</p><formula xml:id="formula_77">Eπ [R t+1 | s t , a t ] =q π (s t , a t ) − ψ π,π (s t ) .</formula><p>(A84)</p><p>Proof. The expected reward r(s t , a t ) is computed for 0 t T , where s −1 , a −1 are states and actions, which are introduced for formal reasons at the beginning of an episode. The expected reward r(s t , a t ) is withq π (s −1 , a −1 ) = 0:</p><formula xml:id="formula_78">r(s t , a t ) = E rt+1 [R t+1 | s t , a t ] = E st−1,at−1 [q π (s t , a t ) −q π (s t−1 , a t−1 ) | s t , a t ] (A85) =q π (s t , a t ) − E st−1,at−1 [q π (s t−1 , a t−1 ) | s t , a t ] .</formula><p>The expectations E π [. | s t , a t ] like E π R T +1 | s t , a t are expectations over all episodes starting in (s t , a t ) and ending in some (s T , a T ).</p><p>The Q-values for the SDP P are defined for 0 t T as:</p><formula xml:id="formula_79">q π (s t , a t ) = E π T −t τ =0 R t+1+τ | s t , a t (A86) = E π [q π (s T , a T ) −q π (s t−1 , a t−1 ) | s t , a t ] = E π [q π (s T , a T ) | s t , a t ] − E π [q π (s t−1 , a t−1 ) | s t , a t ] =q π (s t , a t ) − E st−1,at−1 [q π (s t−1 , a t−1 ) | s t , a t ] = r(s t , a t ) .</formula><p>The second equality uses</p><formula xml:id="formula_80">T −t τ =0 R t+1+τ = T −t τ =0q π (s t+τ , a t+τ ) −q π (s t+τ −1 , a t+τ −1 ) (A87) =q π (s T , a T ) −q π (s t−1 , a t−1 ) . The posterior p(s t−1 , a t−1 | s t , a t ) is p(s t−1 , a t−1 | s t , a t ) = p(s t , a t | s t−1 , a t−1 ) p(s t−1 , a t−1 ) p(s t , a t ) (A88) = p(s t | s t−1 , a t−1 ) p(s t−1 , a t−1 ) p(s t ) = p(s t−1 , a t−1 | s t ) , where we used p(s t , a t | s t−1 , a t−1 ) = π(a t | s t )p(s t | s t−1 , a t−1 ) and p(s t , a t ) = π(a t | s t )p(s t ).</formula><p>The posterior does no longer contain a t . We can express the mean of previous Q-values by the posterior p(s t−1 , a t−1 | s t , a t ):</p><formula xml:id="formula_81">E st−1,at−1 [q π (s t−1 , a t−1 ) | s t , a t ] = st−1,at−1 p(s t−1 , a t−1 | s t , a t )q π (s t−1 , a t−1 ) (A89) = st−1,at−1 p(s t−1 , a t−1 | s t )q π (s t−1 , a t−1 ) = E st−1,at−1 [q π (s t−1 , a t−1 ) | s t ] = ψ π (s t ) , with ψ π (s t ) = E st−1,at−1 [q π (s t−1 , a t−1 ) | s t ] .<label>(A90)</label></formula><p>The SDP P and the MDPP have the same advantage function, since the value functions are the expected Q-values across the actions and follow the equation v π (s t ) =ṽ π (s t ) + ψ π (s t ). Therefore ψ π (s t ) cancels in the advantage function of the SDP P.</p><p>Using a behavior policyπ the expected immediate reward is</p><formula xml:id="formula_82">Eπ [R t+1 | s t , a t ] = E rt+1,π [R t+1 | s t , a t ] = E st−1,at−1,π [q π (s t , a t ) −q π (s t−1 , a t−1 ) | s t , a t ] (A91) =q π (s t , a t ) − E st−1,at−1,π [q π (s t−1 , a t−1 ) | s t , a t ] . The posterior pπ(s t−1 , a t−1 | s t , a t ) is pπ(s t−1 , a t−1 | s t , a t ) = pπ(s t , a t | s t−1 , a t−1 ) pπ(s t−1 , a t−1 ) pπ(s t , a t ) (A92) = p(s t | s t−1 , a t−1 ) pπ(s t−1 , a t−1 ) pπ(s t ) = pπ(s t−1 , a t−1 | s t ) , where we used pπ(s t , a t | s t−1 , a t−1 ) =π(a t | s t )p(s t | s t−1 , a t−1 ) and pπ(s t , a t ) =π(a t | s t )pπ(s t ).</formula><p>The posterior does no longer contain a t . We can express the mean of previous Q-values by the posterior pπ(s t−1 , a t−1 | s t , a t ):</p><formula xml:id="formula_83">E st−1,at−1,π [q π (s t−1 , a t−1 ) | s t , a t ] = st−1,at−1 pπ(s t−1 , a t−1 | s t , a t )q π (s t−1 , a t−1 ) (A93) = st−1,at−1 pπ(s t−1 , a t−1 | s t )q π (s t−1 , a t−1 ) = E st−1,at−1,π [q π (s t−1 , a t−1 ) | s t ] = ψ π,π (s t ) , with ψ π,π (s t ) = E st−1,at−1,π [q π (s t−1 , a t−1 ) | s t ] .<label>(A94)</label></formula><p>Therefore we have</p><formula xml:id="formula_84">Eπ [R t+1 | s t , a t ] =q π (s t , a t ) − ψ π,π (s t ) . (A95)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.7 Novel Learning Algorithms based on Reward Redistributions</head><p>We assume γ = 1 and a finite horizon or absorbing state original MDPP with delayed reward. According to Theorem A5,q π (s t , a t ) can be estimated with an offset that depends only on s t by estimating the expected immediate redistributed reward. Thus, Q-value estimation becomes trivial and the the advantage function of the MDPP can be readily computed. All reinforcement learning methods like policy gradients that use arg max atq π (s t , a t ) or the advantage functioñ q π (s t , a t ) − E atq π (s t , a t ) of the original MDPP can be used. These methods either rely on Theorem A5 and either estimate q π (s t , a t ) according to Eq. (A83) or the expected immediate reward according to Eq. (A84). Both approaches estimateq π (s t , a t ) with an offset that depends only on s t (either ψ π (s t ) or ψ π,π (s t )). Behavior policies like "greedy in the limit with infinite exploration" (GLIE) or "restricted rank-based randomized" (RRR) allow to prove convergence of SARSA <ref type="bibr" target="#b117">[118]</ref>. These policies can be used with reward redistribution. GLIE policies can be realized by a softmax with exploration coefficient on the Q-values, therefore ψ π (s t ) or ψ π,π (s t ) cancels. RRR policies select actions probabilistically according to the ranks of their Q-values, where the greedy action has highest probability. Therefore ψ(s t ) or ψ π,π (s t ) is not required. For function approximation, convergence of the Q-value estimation together with reward redistribution and GLIE or RRR policies can under standard assumptions be proven by the stochastic approximation theory for two time-scale update rules <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b63">64]</ref>. Proofs for convergence to an optimal policy are in general difficult, since locally stable attractors may not correspond to optimal policies. Reward redistribution can be used for • (A) Q-value estimation, • (B) policy gradients, and • (C) Q-learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.7.1 Q-Value Estimation</head><p>Like SARSA, RUDDER learning continually predicts Q-values to improve the policy. Type (A) methods estimate Q-values and are divided into variants (i), (ii), and (iii). Variant (i) assumes an optimal reward redistribution and estimatesq π (s t , a t ) with an offset depending only on s t . The estimates are based on Theorem A5 either by on-policy direct Q-value estimation according to Eq. (A83) or by off-policy immediate reward estimation according to Eq. (A84). Variant (ii) methods assume a non-optimal reward redistribution and correct Eq. (A83) by estimating κ. Variant (iii) methods use eligibility traces for the redistributed reward.</p><p>Variant (i): Estimation ofq π (s t , a t ) with an offset assuming optimality. Theorem A5 justifies the estimation ofq π (s t , a t ) with an offset by on-policy direct Q-value estimation via Eq. (A83) or by off-policy immediate reward estimation via Eq. (A84). RUDDER learning can be based on policies like "greedy in the limit with infinite exploration" (GLIE) or "restricted rank-based randomized" (RRR) <ref type="bibr" target="#b117">[118]</ref>. GLIE policies change toward greediness with respect to the Q-values during learning.</p><p>Variant (ii): TD-learning of κ and correction of the redistributed reward. For non-optimal reward redistributions κ(T − t − 1, t) can be estimated to correct the Q-values. TD-learning of κ.</p><p>The expected sum of delayed rewards κ(T − t − 1, t) can be formulated as</p><formula xml:id="formula_85">κ(T − t − 1, t) = E π T −t−1 τ =0 R t+2+τ | s t , a t (A96) = E π   R t+2 + T −(t+1)−1 τ =0 R (t+1)+2+τ | s t , a t   = E st+1,at+1,rt+2   R t+2 + E π   T −(t+1)−1 τ =0 R (t+1)+2+τ | s t+1 , a t+1   | s t , a t   = E st+1,at+1,rt+2 [R t+2 + κ(T − t − 2, t + 1) | s t , a t ] .</formula><p>Therefore, κ(T − t − 1, t) can be estimated by R t+2 and κ(T − t − 2, t + 1), if the last two are drawn together, i.e. considered as pairs. Otherwise the expectations of R t+2 and κ(T − t − 2, t + 1) given (s t , a t ) must be estimated. We can use TD-learning if the immediate reward and the sum of delayed rewards are drawn as pairs, that is, simultaneously. The TD-error δ κ becomes</p><formula xml:id="formula_86">δ κ (T − t − 1, t) = R t+2 + κ(T − t − 2, t + 1) − κ(T − t − 1, t) .<label>(A97)</label></formula><p>We now define eligibility traces for κ. Let the n-step return samples of κ for 1 n T − t be</p><formula xml:id="formula_87">κ (1) (T − t − 1, t) = R t+2 + κ(T − t − 2, t + 1) (A98) κ (2) (T − t − 1, t) = R t+2 + R t+3 + κ(T − t − 3, t + 2) . . . κ (n) (T − t, t) = R t+2 + R t+3 + . . . + R t+n+1 + κ(T − t − n − 1, t + n) .</formula><p>The λ-return for κ is</p><formula xml:id="formula_88">κ (λ) (T − t − 1, t) = (1 − λ) T −t−1 n=1 λ n−1 κ (n) (T − t − 1, t) + λ T −t−1 κ (T −t) (T − t − 1, t) .<label>(A99)</label></formula><p>We obtain</p><formula xml:id="formula_89">κ (λ) (T − t − 1, t) = R t+2 + κ(T − t − 2, t + 1) (A100) + λ (R t+3 + κ(T − t − 3, t + 2) − κ(T − t − 2, t + 1)) + λ 2 (R t+4 + κ(T − t − 4, t + 3) − κ(T − t − 3, t + 2)) . . . + λ T −1−t (R T +1 + κ(0, T − 1) − κ(1, T − 2)) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>28</head><p>We can reformulate this as</p><formula xml:id="formula_90">κ (λ) (T − t − 1, t) = κ(T − t − 1, t) + T −t−1 n=0 λ n δ κ (T − t − n − 1, t + n) . (A101) The κ error ∆ κ is ∆ κ (T − t − 1, t) = κ (λ) (T − t − 1, t) − κ(T − t − 1, t) = T −t−1 n=0 λ n δ κ (T − t − n − 1, t + n) .<label>(A102)</label></formula><p>The derivative of</p><formula xml:id="formula_91">1/2 ∆ κ (T − t − 1, t) 2 = 1/2 κ (λ) (T − t − 1, t) − κ(T − t − 1, t; w) 2 (A103) with respect to w is − κ (λ) (T − t − 1, t) − κ(T − t − 1, t; w) ∇ w κ(T − t − 1, t; w) (A104) = − T −t−1 n=0 λ n δ κ (T − t − n − 1, t + n) ∇ w κ(T − t − 1, t; w) .</formula><p>The full gradient of the sum of κ errors is</p><formula xml:id="formula_92">1/2 ∇ w T −1 t=0 ∆ κ (T − t − 1, t) 2 (A105) = − T −1 t=0 T −t−1 n=0 λ n δ κ (T − t − n − 1, t + n) ∇ w κ(T − t − 1, t; w) = − T −1 t=0 T −1 τ =t λ τ −t δ κ (T − τ − 1, τ ) ∇ w κ(T − t − 1, t; w) = − T −1 τ =0 δ κ (T − τ − 1, τ ) τ t=0 λ τ −t ∇ w κ(T − t − 1, t; w) .</formula><p>We set n = τ − t, so that n = 0 becomes τ = t and n = T − t − 1 becomes τ = T − 1. The recursion</p><formula xml:id="formula_93">f (t) = λ f (t − 1) + a t , f (0) = 0 (A106)</formula><p>can be written as</p><formula xml:id="formula_94">f (T ) = T t=1 λ T −t a t .<label>(A107)</label></formula><p>Therefore, we can use following update rule for minimizing</p><formula xml:id="formula_95">T −1 t=0 ∆ κ (T, t) 2 with respect to w with 1 τ T − 1: z −1 = 0 (A108) z τ = λ z τ −1 + ∇ w κ(T − τ, τ ; w) (A109) δ κ (T − τ, τ ) = R τ +2 + κ(T − τ − 1, τ + 1; w) − κ(T − τ, τ ; w) (A110) w new = w + α δ κ (T − τ, τ ) z τ . (A111)</formula><p>Correction of the reward redistribution. For correcting the redistributed reward, we apply a method similar to reward shaping or look-back advice. This method ensures that the corrected redistributed reward leads to an SDP that is has the same return per sequence as the SDP P. The reward correction is</p><formula xml:id="formula_96">F (s t , a t , s t−1 , a t−1 ) = κ(m, t) − κ(m, t − 1) ,<label>(A112)</label></formula><p>29 we define the corrected redistributed reward as</p><formula xml:id="formula_97">R c t+1 = R t+1 + F (s t , a t , s t−1 , a t−1 ) = R t+1 + κ(m, t) − κ(m, t − 1) . (A113)</formula><p>We assume that κ(m, −1) = κ(m, T + 1) = 0, therefore</p><formula xml:id="formula_98">T +1 t=0 F (s t , a t , s t−1 , a t−1 ) = T +1 t=0 κ(m, t) − κ(m, t − 1) = κ(m, T + 1) − κ(m, −1) = 0 .<label>(A114)</label></formula><p>Consequently, the corrected redistributed reward R c t+1 does not change the expected return for a sequence, therefore, the resulting SDP has the same optimal policies as the SDP without correction. For a predictive reward of ρ at time t = k, which can be predicted from time t = l &lt; k to time t = k − 1, we have:</p><formula xml:id="formula_99">κ(m, t) =    0 , for t &lt; l , ρ , for l t &lt; k , 0 , for t k . (A115)</formula><p>The reward correction is</p><formula xml:id="formula_100">F (s t , a t , s t−1 , a t−1 ) =              0 , for t &lt; l , ρ , for t = l , 0 , for l &lt; t &lt; k , −ρ , for t = k , 0 ,</formula><p>for t &gt; k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(A116)</head><p>Using κ as auxiliary task in predicting the return for return decomposition. A κ prediction can serve as additional output of the function g that predicts the return and is the basis of the return decomposition. Even a partly prediction of κ means that the reward can be distributed further back. If g can partly predict κ, then g has all information to predict the return earlier in the sequence. If the return is predicted earlier, then the reward will be distributed further back. Consequently, the reward redistribution comes closer to an optimal reward redistribution. However, at the same time, κ can no longer be predicted. The function g must find another κ that can be predicted. If no such κ is found, then optimal reward redistribution is indicated.</p><p>Variant (iii): Eligibility traces assuming optimality. We can use eligibility traces to further distribute the reward back. For an optimal reward redistribution, we have E st+1 [V (s t+1 )] = 0. The new returns R t are given by the recursion</p><formula xml:id="formula_101">R t = r t+1 + λ R t+1 , (A117) R T +2 = 0 .<label>(A118)</label></formula><p>The expected policy gradient updates with the new returns</p><formula xml:id="formula_102">R are E π [∇ θ log π(a t | s t ; θ)R t ].</formula><p>To avoid an estimation of the value function V (s t+1 ), we assume optimality, which might not be valid. However, the error should be small if the return decomposition works well. Instead of estimating a value function, we can use a correction as it is shown in next paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.7.2 Policy Gradients</head><p>Type (B) methods are policy gradients. In the expected updates E π [∇ θ log π(a | s; θ)q π (s, a)] of policy gradients, the value q π (s, a) is replaced by an estimate of r(s, a) or by samples of the redistributed reward. Convergence to optimal policies is guaranteed even with the offset ψ π (s) in Eq. (A83) similar to baseline normalization for policy gradients. With baseline normalization, the baseline b(s) = E a [r(s, a)] = a π(a | s)r(s, a) is subtracted from r(s, a), which gives the policy</p><formula xml:id="formula_103">gradient E π [∇ θ log π(a | s; θ)(r(s, a) − b(s))]. With eligibility traces using λ ∈ [0, 1] for G λ t [128], we have the new returns G t = r t + λG t+1 with G T +2 = 0. The expected updates with the new returns G are E π [∇ θ log π(a t | s t ; θ)G t ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.7.3 Q-Learning</head><p>The type (C) method is Q-learning with the redistributed reward. Here, Q-learning is justified if immediate and future reward are drawn together, as typically done. Also other temporal difference methods are justified when immediate and future reward are drawn together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.8 Return Decomposition to construct a Reward Redistribution</head><p>We now propose methods to construct reward redistributions which ideally would be optimal. Learning with non-optimal reward redistributions does work since the optimal policies do not change according to Theorem A2. However reward redistributions that are optimal considerably speed up learning, since future expected rewards introduce biases in TD-methods and the high variance in MC-methods. The expected optimal redistributed reward is according to Eq. (A65) the difference of Q-values. The more a reward redistribution deviates from these differences, the larger are the absolute κ-values and, in turn, the less optimal is the reward redistribution. Consequently we aim at identifying the largest Q-value differences to construct a reward redistribution which is close to optimal. Assume a grid world where you have to take a key to later open a door to a treasure room. Taking the key increases the chances to receive the treasure and, therefore, is associated with a large positive Q-value difference. Smaller positive Q-value difference are steps toward the key location.</p><p>Reinforcement Learning as Pattern Recognition. We want to transform the reinforcement learning problem into a pattern recognition problem to employ deep learning approaches. The sum of the Q-value differences gives the difference between expected return at sequence begin and the expected return at sequence end (telescope sum). Thus, Q-value differences allow to predict the expected return of the whole state-action sequence. Identifying the largest Q-value differences reduce the prediction error most. Q-value differences are assumed to be associated with patterns in state-action transitions like taking the key in our example. The largest Q-value differences are expected to be found more frequently in sequences with very large or very low return. The resulting task is to predict the expected return from the whole sequence and identify which state-action transitions contributed most to the prediction. This pattern recognition task is utilized to construct a reward redistribution, where redistributed reward corresponds to the contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.8.1 Return Decomposition Idea</head><p>The return decomposition idea is to predict the realization of the return or its expectation by a function g from the state-action sequence (s, a) 0:T := (s 0 , a 0 , s 1 , a 1 , . . . , s T , a T ) .</p><p>(A119)</p><p>The return is the accumulated reward along the whole sequence (s, a) 0:T . The function g depends on the policy π that is used to generate the state-action sequences. Subsequently, the prediction or the realization of the return is distributed over the sequence with the help of g. One important advantage of a deterministic function g is that it predicts with proper loss functions and if being perfect the expected return. Therefore, it removes the sampling variance of returns. In particular the variance of probabilistic rewards is averaged out. Even an imperfect function g removes the variance as it is deterministic. As described later, the sampling variance may be reintroduced when strictly returnequivalent SDPs are ensured. We want to determine for each sequence element its contribution to the prediction of the function g. Contribution analysis computes the contribution of each state-action pair to the prediction, that is, the information of each state-action pair about the prediction. In principle, we can use any contribution analysis method. However, we prefer three methods: (A) Differences in predictions. If we can ensure that g predicts the sequence-wide return at every time step. The difference of two consecutive predictions is a measure of the contribution of the current state-action pair to the return prediction. The difference of consecutive predictions is the redistributed reward. (B) Integrated gradients (IG) <ref type="bibr" target="#b124">[125]</ref>. (C) Layer-wise relevance propagation (LRP) <ref type="bibr" target="#b2">[3]</ref>. The methods (B) and (C) use information later in the sequence for determining the contribution of the current state-action pair. Therefore, they introduce a non-Markov reward. However, the non-Markov reward can be viewed as probabilistic reward. Since probabilistic reward increases the variance, we prefer method (A).</p><p>Explaining Away Problem. We still have to tackle the problem that reward causing actions do not receive redistributed rewards since they are explained away by later states. To describe the problem, assume an MDPP with the only reward at sequence end. To ensure the Markov property, states inP have to store the reward contributions of previous state-actions; e.g. s T has to store all previous contributions such that the expectationr(s T , a T ) is Markov. The explaining away problem is that later states are used for return prediction, while reward causing earlier actions are missed. To avoid explaining away, between the state-action pair (s t , a t ) and its predecessor (s t−1 , a t−1 ), where (s −1 , a −1 ) are introduced for starting an episode. The sequence of differences is defined as</p><formula xml:id="formula_104">∆ 0:T := ∆(s −1 , a −1 , s 0 , a 0 ), . . . , ∆(s T −1 , a T −1 , s T , a T ) .<label>(A120)</label></formula><p>We assume that the differences ∆ are mutually independent <ref type="bibr" target="#b59">[60]</ref>:</p><formula xml:id="formula_105">p (∆(s t−1 , a t−1 , s t , a t ) | ∆(s −1 , a −1 , s 0 , a 0 ), . . . , ∆(s t−2 , a t−2 , s t−1 , a t−1 ), (A121) ∆(s t , a t , s t+1 , a t+1 ) . . . , ∆(s T −1 , a T −1 , s T , a T )) = p (∆(s t−1 , a t−1 , s t , a t )) .</formula><p>The function g predicts the realization of the sequence-wide return or its expectation from the sequence ∆ 0:T :</p><formula xml:id="formula_106">g ∆ 0:T = E R T +1 | s T , a T =r T +1 . (A122) Return decomposition deconstructs g into contributions h t = h(∆(s t−1 , a t−1 , s t , a t ) at time t: g ∆ 0:T = T t=0 h(∆(s t−1 , a t−1 , s t , a t )) =r T +1 .<label>(A123)</label></formula><p>If we can assume that g can predict the return at every time step:</p><formula xml:id="formula_107">g ∆ 0:t = E π R T +1 | s t , a t ,<label>(A124)</label></formula><p>then we use the contribution analysis method "differences of return predictions", where the contributions are defined as:</p><formula xml:id="formula_108">h 0 = h(∆(s −1 , a −1 , s 0 , a 0 )) := g ∆ 0:0 (A125) h t = h(∆(s t−1 , a t−1 , s t , a t )) := g ∆ 0:t − g ∆ 0:(t−1) .<label>(A126)</label></formula><p>We assume that the sequence-wide return cannot be predicted from the last state. The reason is that either immediate rewards are given only at sequence end without storing them in the states or information is removed from the states. Therefore, a relevant event for predicting the final reward must be identified by the function g. The prediction errors at the end of the episode become, in general, smaller since the future is less random. Therefore, prediction errors later in the episode are up-weighted while early predictions ensure that information is captured in h t for being used later. The prediction at time T has the largest weight and relies on information from the past. If g does predict the return at every time step, contribution analysis decomposes g. For decomposing a linear g one can use the Taylor decomposition (a linear approximation) of g with respect to the h <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b82">83]</ref>. A non-linear g can be decomposed by layerwise relevance propagation (LRP) <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b83">84]</ref> or integrated gradients (IG) <ref type="bibr" target="#b124">[125]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.8.2 Reward Redistribution based on Return Decomposition</head><p>We assume a return decomposition</p><formula xml:id="formula_109">g ∆ 0:T = T t=0 h t ,<label>(A127)</label></formula><p>with</p><formula xml:id="formula_110">h 0 = h(∆(s −1 , a −1 , s 0 , a 0 )) , (A128) h t = h(∆(s t−1 , a t−1 , s t , a t )) for 0 &lt; t T .<label>(A129)</label></formula><p>We use these contributions for redistributing the reward. The reward redistribution is given by the random variable R t+1 for the reward at time t + 1. These new redistributed rewards R t+1 must have the contributions h t as mean:</p><formula xml:id="formula_111">E [R t+1 | s t−1 , a t−1 , s t , a t ] = h t (A130)</formula><p>The rewardR T +1 ofP is probabilistic and the function g might not be perfect, therefore neither g(∆ 0:T ) =r T +1 for the return realizationr T +1 nor g(∆ 0:T ) =r(s T , a T ) for the expected return holds. To assure strictly return-equivalent SDPs, we have to compensate for both a probabilistic rewardR T +1 and an imperfect function g. The compensation is given bỹ</p><formula xml:id="formula_112">r T +1 − T τ =0 h t .<label>(A131)</label></formula><p>We compensate with an extra reward R T +2 at time T + 2 which is immediately given after R T +1 at time T + 1 after the state-action pair (s T , a T ).</p><formula xml:id="formula_113">The new redistributed reward R t+1 is E [R 1 | s 0 , a 0 ] = h 0 , (A132) E [R t+1 | s t−1 , a t−1 , s t , a t ] = h t for 0 &lt; t T , (A133) R T +2 =R T +1 − T t=0 h t ,<label>(A134)</label></formula><p>where the realizationr T +1 is replaced by its random variableR T +1 . If the the prediction of g is perfect, then we can set R T +2 = 0 and redistribute the expected return which is the predicted return. R T +2 compensates for both a probabilistic rewardR T +1 and an imperfect function g. Consequently all variance of sampling the return is moved to R T +2 . Only the imperfect function g must be corrected while the variance does not matter. However, we cannot distinguish, e.g. in early learning phases, between errors of g and random reward. A perfect g results in an optimal reward redistribution.</p><p>Next theorem shows that Theorem A4 holds also for the correction R T +2 . Theorem A6. The optimality conditions hold also for reward redistributions with corrections:</p><formula xml:id="formula_114">κ(T − t + 1, t − 1) = 0 . (A135) Proof. The expectation of κ(T − t + 1, t − 1) = T −t+1 τ =0 R t+1+τ , that is κ(m, t − 1) with m = T − t + 1. E π T −t+1 τ =0 R t+1+τ | s t−1 , a t−1 (A136) = E π R T +1 −q π (s T , a T ) + T −t τ =0 (q π (s τ +t , a τ +t ) −q π (s τ +t−1 , a τ +t−1 )) | s t−1 , a t−1 = E π R T +1 −q π (s t−1 , a t−1 ) | s t−1 , a t−1 = E π R T +1 | s t−1 , a t−1 − E π E π T τ =t−1R τ +1 | s t−1 , a t−1 | s t−1 , a t−1 = E π R T +1 | s t−1 , a t−1 − E π R T +1 | s t−1 , a t−1 = 0 .</formula><p>If we substitute t − 1 by t (t one step further and m one step smaller) it follows κ(T − t, t) = 0 .</p><p>Next, we consider the case t = T + 1, that is κ(0, T ), which is the expected correction. We will use following equality for the expected delayed reward at sequence end:</p><formula xml:id="formula_116">q π (s T , a T ) = ER T +1 R T +1 | s T , a T =r T +1 (s T , a T ) ,<label>(A138)</label></formula><formula xml:id="formula_117">sinceq π (s T +1 , a T +1 ) = 0. For t = T + 1 we obtain E R T +2 [R T +2 | s T , a T ] = ER T +1 R T +1 −q π (s T , a T ) | s T , a T (A139) =r T +1 (s T , a T ) −r T +1 (s T , a T ) = 0 .</formula><p>In the experiments we also use a uniform compensation where each reward has the same contribution to the compensation:</p><formula xml:id="formula_118">R 1 = h 0 + 1 T + 1 R T +1 − T τ =0 h(∆(s τ −1 , a τ −1 , s τ , a τ )) (A140) R t+1 = h t + 1 T + 1 R T +1 − T τ =0 h(∆(s τ −1 , a τ −1 , s τ , a τ )) .<label>(A141)</label></formula><p>Consequently all variance of sampling the return is uniformly distributed across the sequence. Also the error of g is uniformly distributed across the sequence. An optimal reward redistribution implies</p><formula xml:id="formula_119">g ∆ 0:t = t τ =0 h(∆(s τ −1 , a τ −1 , s τ , a τ )) =q π (s t , a t ) (A142) since the expected reward is E [R t+1 | s t−1 , a t−1 , s t , a t ] = h(∆(s t−1 , a t−1 , s t , a t )) (A143) =q π (s t , a t ) −q π (s t−1 , a t−1 )</formula><p>according to Eq. (A65) in Theorem A4 and</p><formula xml:id="formula_120">h 0 = h(∆(s −1 , a −1 , s 0 , a 0 )) (A144) = g ∆ 0:0 =q π (s 0 , a 0 ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.9 Remarks on Return Decomposition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.9.1 Return Decomposition for Binary Reward</head><p>A special case is a reward that indicates success or failure by giving a reward of 1 or 0, respectively. The return is equal to the final reward R, which is a Bernoulli variable. For each state s or each state-action pair (s, a) the expected return can be considered as a Bernoulli variable with success</p><formula xml:id="formula_121">probability p R (s) or p R (s, a). The value function is v π (s) = E π (G | s) = p R (s) and the action- value is q π (s) = E π (G | s, a) = p R (s, a)</formula><p>which is in both cases the expectation of success. In this case, the optimal reward redistribution tracks the success probability</p><formula xml:id="formula_122">R 1 = h 0 = h(∆(s −1 , a −1 , s 0 , a 0 )) =q π (s 0 , a 0 ) = p R (s 0 , a 0 ) (A145) R t+1 = h t = h(∆(s t−1 , a t−1 , s t , a t )) =q π (s t , a t ) −q π (s t−1 , a t−1 ) (A146) = p R (s t , a t ) − p R (s t−1 , a t−1 ) for 0 &lt; t T R T +2 =R T +1 −r T +1 = R − p R (s T , a T ) .<label>(A147)</label></formula><p>The redistributed reward is the change in the success probability. A good action increases the success probability and obtains a positive reward while a bad action reduces the success probability and obtains a negative reward.</p><p>A2.9.2 Optimal Reward Redistribution reduces the MDP to a Stochastic Contextual Bandit Problem The new SDP P has a redistributed reward with random variable R t at time t distributed according to p(r | s t , a t ). Theorem A5 states</p><formula xml:id="formula_123">q π (s t , a t ) = r(s t , a t ) .<label>(A148)</label></formula><p>This equation looks like a contextual bandit problem, where r(s t , a t ) is an estimate of the mean reward for action a t for state or context s t . Contextual bandits <ref type="bibr">[72, p. 208</ref>] are characterized by a conditionally σ-subgaussian noise (Def. 5.1 <ref type="bibr">[72, p. 68]</ref>). We define the zero mean noise variable η by</p><formula xml:id="formula_124">η t = η(s t , a t ) = R t − r(s t , a t ) ,<label>(A149)</label></formula><p>where we assume that η t is a conditionally σ-subgaussian noise variable. Therefore, η is distributed according to p(r − r(s t , a t ) | s t , a t ) and fulfills</p><formula xml:id="formula_125">E [η(s t , a t )] = 0 , (A150) E [exp(λη(s t , a t )] exp(λ 2 σ 2 /2) .<label>(A151)</label></formula><p>Subgaussian random variables have tails that decay almost as fast as a Gaussian. If the reward r is bounded by |r| &lt; B, then η is bounded by |η| &lt; B and, therefore, a B-subgaussian. For binary rewards it is of interest that a Bernoulli variable is 0.5-subgaussian <ref type="bibr">[72, p. 71]</ref>. In summary, an optimal reward redistribution reduces the MDP to a stochastic contextual bandit problem.</p><p>A2.9.3 Relation to "Backpropagation through a Model´T he relation of reward redistribution if applied to policy gradients and "Backpropagation through a Model´´is discussed here. For a delayed reward that is only received at the end of an episode, we decompose the returnr T +1 into</p><formula xml:id="formula_126">g(∆ 0:T ) =r T +1 = T t=0 h(∆(s t−1 , a t−1 , s t , a t )) .<label>(A152)</label></formula><p>The policy gradient for an optimal reward redistribution is</p><formula xml:id="formula_127">E π [∇ θ log π(a t | s t ; θ) h(∆(s t−1 , a t−1 , s t , a t ))] .<label>(A153)</label></formula><p>Summing up the gradient for one episode, the gradient becomes</p><formula xml:id="formula_128">E π T t=0 ∇ θ log π(a t | s t ; θ) h(∆(s t−1 , a t−1 , s t , a t )) (A154) = E π [J θ (log π(a | s; θ)) h(∆(s , a , s, a))] ,</formula><p>where a = (a −1 , a 0 , a 1 , . . . , a T −1 ) and a = (a 0 , a 1 , . . . , a T ) are the sequences of actions, s = (s −1 , s 0 , s 1 , . . . , s T −1 ) and s = (s 0 , s 1 , . . . , s T ) are the sequences of states, J θ (log π) is the Jacobian of the log-probability of the state sequence with respect to the parameter vector θ, and h(∆(s , a , s, a)) is the vector with entries h(∆(s t−1 , a t−1 , s t , a t )).</p><p>An alternative approach via sensitivity analysis is "Backpropagation through a Model´´, where g(∆ 0:T ) is maximized, that is, the return is maximized. Continuous actions are directly fed into g while probabilistic actions are sampled before entering g. Analog to gradients used for Restricted Boltzmann Machines, for probabilistic actions the log-likelihood of the actions is used to construct a gradient. The likelihood can also be formulated as the cross-entropy between the sampled actions and the action probability. The gradient for "Backpropagation through a Model´´is</p><formula xml:id="formula_129">E π [J θ (log π(a | s; θ)) ∇ a g(∆ 0:T )] ,<label>(A155)</label></formula><p>where ∇ a g(∆ 0:T ) is the gradient of g with respect to the action sequence a.</p><p>If for "Backpropagation through a Model´´the model gradient with respect to actions is replaced by the vector of contributions of actions in the model, then we obtain redistribution applied to policy gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3 Bias-Variance Analysis of MDP Q-Value Estimators</head><p>Bias-variance investigations have been done for Q-learning. Grünewälder &amp; Obermayer <ref type="bibr" target="#b40">[41]</ref> investigated the bias of temporal difference learning (TD), Monte Carlo estimators (MC), and least-squares temporal difference learning (LSTD). Mannor et al. <ref type="bibr" target="#b76">[77]</ref> and O'Donoghue et al. <ref type="bibr" target="#b87">[88]</ref> derived bias and variance expressions for updating Q-values. The true, but unknown, action-value function q π is the expected future return. We assume to have the data D, which is a set of state-action sequences with return, that is a set of episodes with return. Using data D, q π is estimated byq π =q π (D), which is an estimate with bias and variance. For bias and variance we have to compute the expectation E D [.] over the data D. The mean squared error (MSE) of an estimatorq π (s, a) is</p><formula xml:id="formula_130">mseq π (s, a) = E D q π (s, a) − q π (s, a) 2 .<label>(A156)</label></formula><p>The bias of an estimatorq π (s, a) is</p><formula xml:id="formula_131">biasq π (s, a) = E D [q π (s, a)] − q π (s, a) .<label>(A157)</label></formula><p>The variance of an estimatorq π (s, a) is</p><formula xml:id="formula_132">varq π (s, a) = E D q π (s, a) − E D [q π (s, a)] 2 .<label>(A158)</label></formula><p>The bias-variance decomposition of the MSE of an estimatorq π (s, a) is</p><formula xml:id="formula_133">mseq π (s, a) = varq π (s, a) + biasq π (s, a) 2 .<label>(A159)</label></formula><p>35</p><p>The bias-variance decomposition of the MSE of an estimatorq π as a vector is</p><formula xml:id="formula_134">mseq π = E D s,a q π (s, a) − q π (s, a) 2 = E D q π − q π 2 , (A160) biasq π = E D [q π ] − q π , (A161) varq π = E D s,a q π (s, a) − E D [q π (s, a)] 2 = TrVar D [q π ] ,<label>(A162)</label></formula><formula xml:id="formula_135">mseq π = varq π + biasq π T biasq π . (A163)</formula><p>A3.1 Bias-Variance for MC and TD Estimates of the Expected Return Monte Carlo (MC) computes the arithmetic meanq π (s, a) of G t for (s t = s, a t = a) over the episodes given by the data.</p><p>For temporal difference (TD) methods, like SARSA, with learning rate α the updated estimate of q π (s t , a t ) is:</p><formula xml:id="formula_136">(q π ) new (s t , a t ) =q π (s t , a t ) − α q π (s t , a t ) − R t+1 − γq π (s t+1 , a t+1 ) = (1 − α)q π (s t , a t ) + α R t+1 + γq π (s t+1 , a t+1 ) .<label>(A164)</label></formula><p>Similar updates are used for expected SARSA and Q-learning, where only a t+1 is chosen differently. Therefore, for the estimation ofq π (s t , a t ), SARSA and Q-learning perform an exponentially weighted arithmetic mean of (R t+1 +γq π (s t+1 , a t+1 )). If for the updatesq π (s t+1 , a t+1 ) is fixed on some data, then SARSA and Q-learning perform an exponentially weighted arithmetic mean of the immediate reward R t+1 plus averaging over whichq π (s t+1 , a t+1 ) (which (s t+1 , a t+1 )) is chosen. In summary, TD methods like SARSA and Q-learning are biased viaq π (s t+1 , a t+1 ) and perform an exponentially weighted arithmetic mean of the immediate reward R t+1 and the next (fixed)q π (s t+1 , a t+1 ). Bias-Variance for Estimators of the Mean. Both Monte Carlo and TD methods, like SARSA and Q-learning, respectively, estimate q π (s, a) = E [G t | s, a], which is the expected future return. The expectations are estimated by either an arithmetic mean over samples with Monte Carlo or an exponentially weighted arithmetic mean over samples with TD methods. Therefore, we are interested in computing the bias and variance of these estimators of the expectation. In particular, we consider the arithmetic mean and the exponentially weighted arithmetic mean. We assume n samples for a state-action pair (s, a). However, the expected number of samples depends on the probabilistic number of visits of (s, a) per episode. Arithmetic mean. For n samples {X 1 , . . . , X n } from a distribution with mean µ and variance σ 2 , the arithmetic mean, its bias and and its variance are:</p><formula xml:id="formula_137">µ n = 1 n n i=1 X i , bias(μ n ) = 0 , var(μ n ) = σ 2 n . (A165)</formula><p>The estimation variance of the arithmetic mean is determined by σ 2 , the variance of the distribution the samples are drawn from. Exponentially weighted arithmetic mean. For n samples {X 1 , . . . , X n } from a distribution with mean µ and variance σ, the variance of the exponential mean with initial value µ 0 iŝ</p><formula xml:id="formula_138">µ 0 = µ 0 ,μ k = (1 − α)μ k−1 + α X k ,<label>(A166)</label></formula><p>which givesμ</p><formula xml:id="formula_139">n = α n i=1 (1 − α) n−i X i + (1 − α) n µ 0 . (A167)</formula><p>This is a weighted arithmetic mean with exponentially decreasing weights, since the coefficients sum up to one:</p><formula xml:id="formula_140">α n i=1 (1 − α) n−i + (1 − α) n = α 1 − (1 − α) n 1 − (1 − α) + (1 − α) n (A168) = 1 − (1 − α) n + (1 − α) n = 1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>36</head><p>The estimatorμ n is biased, since:</p><formula xml:id="formula_141">bias(μ n ) = E [μ n ] − µ = E α n i=1 (1 − α) n−i X i + (1 − α) n µ 0 − µ (A169) = α n i=1 (1 − α) n−i E [X i ] + (1 − α) n µ 0 − µ = µ α n−1 i=0 (1 − α) i + (1 − α) n µ 0 − µ = µ (1 − (1 − α) n ) + (1 − α) n µ 0 − µ = (1 − α) n (µ 0 − µ) . Asymptotically (n → ∞) the estimate is unbiased. The variance is var(μ n ) = E μ 2 n − E 2 [μ n ] (A170) = E   α 2 n i=1 n j=1 (1 − α) n−i X i (1 − α) n−j X j   + E 2 (1 − α) n µ 0 α n i=1 (1 − α) n−i X i + (1 − α) 2n µ 2 0 − ((1 − α) n (µ 0 − µ) + µ) 2 = α 2 E   n i=1 (1 − α) 2(n−i) X 2 i + n i=1 n j=1,j =i (1 − α) n−i X i (1 − α) n−j X j   + 2 (1 − α) n µ 0 µ α n i=1 (1 − α) n−i + (1 − α) 2n µ 2 0 − ((1 − α) n µ 0 + (1 − (1 − α) n ) µ) 2 = α 2 n i=1 (1 − α) 2(n−i) σ 2 + µ 2   + n i=1 n j=1,j =i (1 − α) n−i (1 − α) n−j µ 2   + 2 (1 − α) n µ 0 µ (1 − (1 − α) n ) + (1 − α) 2n µ 2 0 − (1 − α) 2n µ 2 0 − 2 (1 − α) n µ 0 (1 − (1 − α) n ) µ − (1 − (1 − α) n ) 2 µ 2 = σ 2 α 2 n−1 i=0 (1 − α) 2 i + µ 2 α 2 n−1 i=0 (1 − α) i 2 − (1 − (1 − α) n ) 2 µ 2 = σ 2 α 2 1 − (1 − α) 2n 1 − (1 − α) 2 = σ 2 α (1 − (1 − α) 2n ) 2 − α .</formula><p>Also the estimation variance of the exponentially weighted arithmetic mean is proportional to σ 2 , which is the variance of the distribution the samples are drawn from. The deviation of random variable X from its mean µ can be analyzed with Chebyshev's inequality.</p><p>Chebyshev's inequality <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b130">131]</ref> states that for a random variable X with expected value µ and varianceσ 2 and for any real number &gt; 0:</p><formula xml:id="formula_142">Pr [|X − µ| σ] 1 2<label>(A171)</label></formula><p>or, equivalently,</p><formula xml:id="formula_143">Pr [|X − µ| ] σ 2 2 .<label>(A172)</label></formula><p>For n samples {X 1 , . . . , X n } from a distribution with expectation µ and variance σ we compute the arithmetic mean 1 n n i=1 X i . If X is the arithmetic mean, thenσ 2 = σ 2 /n and we obtain</p><formula xml:id="formula_144">Pr 1 n n i=1 X i − µ σ 2 n 2 .<label>(A173)</label></formula><p>Following Grünewälder and Obermayer <ref type="bibr" target="#b40">[41]</ref>, Bernstein's inequality can be used to describe the deviation of the arithmetic mean (unbiased estimator of µ) from the expectation µ (see Theorem 6 of Gábor Lugosi's lecture notes <ref type="bibr" target="#b74">[75]</ref>):</p><formula xml:id="formula_145">Pr 1 n n i=1 X i − µ 2 exp − 2 n 2 σ 2 + 2 M 3 ,<label>(A174)</label></formula><p>where |X − µ| &lt; M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.2 Mean and Variance of an MDP Sample of the Return</head><p>Since the variance of the estimators of the expectations (arithmetic mean and exponentially weighted arithmetic mean) is governed by the variance of the samples, we compute mean and variance of the return estimate q π (s, a). We follow <ref type="bibr" target="#b120">[121,</ref><ref type="bibr" target="#b128">129,</ref><ref type="bibr" target="#b129">130]</ref> for deriving the mean and variance.</p><p>We consider an MDP with finite horizon T , that is, each episode has length T . The finite horizon MDP can be generalized to an MDP with absorbing (terminal) state s = E. We only consider proper policies, that is there exists an integer n such that from any initial state the probability of achieving the terminal state E after n steps is strictly positive. T is the time to the first visit of the terminal state:</p><formula xml:id="formula_146">T = min k | s k = E. The return G 0 is: G 0 = T k=0 γ k R k+1 .<label>(A175)</label></formula><p>The action-value function, the Q-function, is the expected return</p><formula xml:id="formula_147">G t = T −t k=0 γ k R t+k+1<label>(A176)</label></formula><p>if starting in state S t = s and action A t = a:</p><formula xml:id="formula_148">q π (s, a) = E π [G t | s, a] .<label>(A177)</label></formula><p>The second moment of the return is:</p><formula xml:id="formula_149">M π (s, a) = E π G 2 t | s, a .<label>(A178)</label></formula><p>The variance of the return is:</p><formula xml:id="formula_150">V π (s, a) = Var π [G t | s, a] = M π (s, a) − q π (s, a) 2 .<label>(A179)</label></formula><p>Using </p><p>For the second moment, we obtain <ref type="bibr" target="#b128">[129]</ref>:</p><formula xml:id="formula_152">M π (s, a) = E π G 2 t | s, a (A184) = E π   T −t k=0 γ k R t+k+1 2 | s, a   = E π   R t+1 + T −t k=1 γ k R t+k+1 2 | s, a   = r 2 (s, a) + 2 r(s, a) E π T −t k=1 γ k R t+k+1 | s, a + E π   T −t k=1 γ k R t+k+1 2 | s, a   = r 2 (s, a) + 2γ r(s, a) s p(s | s, a) a π(a | s ) q π (s , a ) + γ 2 s p(s | s, a) a π(a | s ) M π (s , a ) = r 2 (s, a) + 2γ r(s, a) E s ,a [q π (s , a ) | s, a] + γ 2 E s ,a [M π (s , a ) | s, a] .</formula><p>For the variance, we obtain: For deterministic reward, that is, Var r [r | s, a] = 0, the corresponding result is given as Equation <ref type="formula">(4)</ref> in <ref type="bibr">Sobel 1982 [121]</ref> and as Proposition 3.1 (c) in Tamar et al. 2012 <ref type="bibr" target="#b128">[129]</ref>. For temporal difference (TD) learning, the next Q-values are fixed toq π (s , a ) when drawing a sample. Therefore, TD is biased, that is, both SARSA and Q-learning are biased. During learning with according updates of Q-values,q π (s , a ) approaches q π (s , a ), and the bias is reduced. However, this reduction of the bias is exponentially small in the number of time steps between reward and updated Q-values, as we will see later. The reduction of the bias is exponentially small for eligibility traces, too. The variance recursion Eq. (A180) of sampled returns consists of three parts: For different settings the following parts may be zero: This result is Equation <ref type="formula">(6)</ref> in <ref type="bibr">Sobel 1982 [121]</ref>. Sobel derived these formulas also for finite horizons and an analog formula if the reward depends also on the next state, that is, for p(r | s, a, s ). Monte Carlo uses the accumulated future rewards for updates, therefore its variance is given by the recursion in Eq. (A180). TD, however, fixes q π (s , a ) to the current estimatesq π (s , a ), which do not change in the current episode. Therefore, TD has E s ,a [V π (s , a ) | s, a] = 0 and only the local variance Var s ,a [q π (s , a ) | s, a] is present. For n-step TD, the recursion in Eq. (A180) must be applied (n − 1) times. Then, the expected next variances are zero since the future reward is estimated byq π (s , a ). Delayed rewards. For TD and delayed rewards, information on new data is only captured by the last step of an episode that receives a reward. This reward is used to update the estimates of the Q-values of the last stateq(s T , a T ). Subsequently, the reward information is propagated one step back via the estimatesq for each sample. The drawn samples (state action sequences) determine where information is propagated back. Therefore, delayed reward introduces a large bias for TD over a long period of time, since the estimatesq(s, a) need a long time to reach their true Q-values. </p><p>Temporal difference (TD) methods replace q π (s , a ) byq π (s , a ) which does not depend on the drawn sample. The mean which is used by temporal difference is</p><formula xml:id="formula_154">q π (s, a) = r(s, a) + γ E s ,a [q π (s , a ) | s, a] .<label>(A189)</label></formula><p>This mean is biased by</p><formula xml:id="formula_155">γ (E s ,a [q π (s , a ) | s, a] − E s ,a [q π (s , a ) | s, a]) .<label>(A190)</label></formula><p>The variance used by temporal difference is</p><formula xml:id="formula_156">V π (s, a) = Var r [r | s, a] + γ 2 Var s ,a [q π (s , a ) | s, a] ,<label>(A191)</label></formula><p>since V π (s , a ) = 0 ifq π (s , a ) is used instead of the future reward of the sample. The variance of TD is smaller than for MC, since variances are not propagated back.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.3 TD corrects Bias exponentially slowly with Respect to Reward Delay</head><p>Temporal Difference. We show that TD updates for delayed rewards are exponentially small, fading exponentially with the number of delay steps. Q-learning with learning rates 1/i at the ith update leads to an arithmetic mean as estimate, which was shown to be exponentially slow <ref type="bibr" target="#b8">[9]</ref>. If for a fixed learning rate the agent always travels along the same sequence of states, then TD is superquadratic <ref type="bibr" target="#b8">[9]</ref>. We, however, consider the general case where the agent travels along random sequences due to a random environment or due to exploration. For a fixed learning rate, the information of the delayed reward has to be propagated back either through the Bellman error or via eligibility traces. We first consider backpropagation of reward information via the Bellman error. For each episode the reward information is propagated back one step at visited state-action pairs via the TD update rule. We denote the Q-values of episode i as q i and assume that the state action pairs (s t , a t ) are the most visited ones. We consider the update of q i (s t , a t ) of a state-action pair (s t , a t ) that is visited at time t in the ith episode:</p><formula xml:id="formula_157">q i+1 (s t , a t ) = q i (s t , a t ) + α δ t ,<label>(A192)</label></formula><formula xml:id="formula_158">δ t = r t+1 + max a q i (s t+1 , a ) − q i (s t , a t ) (Q-learning) (A193) δ t = r t+1 + a π(a | s t+1 ) q i (s t+1 , a ) − q i (s t , a t ) (expected SARSA) . (A194)</formula><p>Temporal Difference with Eligibility Traces. Eligibility traces have been introduced to propagate back reward information of an episode and are now standard for TD(λ) <ref type="bibr" target="#b118">[119]</ref>. However, the eligibility traces are exponentially decaying when propagated back. The accumulated trace is defined as <ref type="bibr" target="#b118">[119]</ref>: e t+1 (s, a) = γ λ e t (s, a) for s = s t or a = a t , γ λ e t (s, a) + 1 for s = s t and a = a t ,</p><p>while the replacing trace is defined as <ref type="bibr" target="#b118">[119]</ref>: e t+1 (s, a) = γ λ e t (s, a) for s = s t or a = a t , 1 for s = s t and a = a t .</p><p>With eligibility traces using λ ∈</p><formula xml:id="formula_161">[0, 1], the λ-return G λ t is [128] G λ t = (1 − λ) ∞ n=1 λ n−1 G (n) t ,<label>(A197)</label></formula><formula xml:id="formula_162">G (n) t = r t+1 + γ r t+2 + . . . + γ n−1 r t+n + γ n−1 V (s t+n ) .<label>(A198)</label></formula><p>We obtain</p><formula xml:id="formula_163">G λ t = (1 − λ) ∞ n=1 λ n−1 G (n) t (A199) = (1 − λ) r t+1 + γ V (s t+1 ) + ∞ n=2 λ n−1 G (n) t = (1 − λ) r t+1 + γ V (s t+1 ) + ∞ n=1 λ n G (n+1) t = (1 − λ) r t+1 + γ V (s t+1 ) + λ γ ∞ n=1 λ n−1 G (n) t+1 + ∞ n=1 λ n r t+1 = (1 − λ) ∞ n=0 λ n r t+1 + (1 − λ)γ V (s t+1 ) + λ γ G λ t+1 = r t+1 + (1 − λ)γ V (s t+1 ) + λ γ G λ t+1</formula><p>. We use the naive Q(λ), where eligibility traces are not set to zero. In contrast, Watkins' Q(λ) <ref type="bibr" target="#b139">[140]</ref> zeros out eligibility traces after non-greedy actions, that is, if not the max a is chosen. Therefore, the decay is even stronger for Watkin's Q(λ). Another eligibility trace method is Peng's Q(λ) <ref type="bibr" target="#b89">[90]</ref> which also does not zero out eligibility traces. The next Theorem A8 states that the decay of TD is exponential for Q-value updates in an MDP with delayed reward, even for eligibility traces. Thus, for delayed rewards TD requires exponentially many updates to correct the bias, where the number of updates is exponential in the delay steps. Theorem A8. For initialization q 0 (s t , a t ) = 0 and delayed reward with r t = 0 for t T , q(s T −i , a T −i ) receives its first update not earlier than at episode i via</p><formula xml:id="formula_164">q i (s T −i , a T −i ) = α i+1 r 1 T +1 , where r 1</formula><p>T +1 is the reward of episode 1. Eligibility traces with λ ∈ [0, 1) lead to an exponential decay of (γλ) k when the reward is propagated k steps back.</p><p>Proof. If we assume that Q-values are initialized with zero, then q 0 (s t , a t ) = 0 for all (s t , a t ). For delayed rewards we have r t = 0 for t T . The Q-value q(s T −i , a T −i ) at time T − i can receive an update for the first time at episode i. Since all Q-values have been initialized with zero, the update is</p><formula xml:id="formula_165">q i (s T −i , a T −i ) = α i+1 r 1 T +1 ,<label>(A200)</label></formula><p>where r 1 T +1 is the reward at time T + 1 for episode 1. We move on to eligibility traces, where the update for a state s is</p><formula xml:id="formula_166">q t+1 (s, a) = q t (s, a) + α δ t e t (s, a) , (A201) δ t = r t+1 + max a q t (s t+1 , a ) − q t (s t , a t ) .<label>(A202)</label></formula><p>If states are not revisited, the eligiblity trace at time t + k for a visit of state s t at time t is:</p><formula xml:id="formula_167">e t+k (s t , a t ) = γ λ k .<label>(A203)</label></formula><p>If all δ t+i are zero except for δ t+k , then the update of q(s, a) is</p><formula xml:id="formula_168">q t+k+1 (s, a) = q t+k (s, a) + α δ t+k e t+k (s, a) = q t+k (s, a) + α γ λ k δ t+k .<label>(A204)</label></formula><p>A learning rate of α = 1 does not work since it would imply to forget all previous learned estimates, and therefore no averaging over episodes would exist. Since α &lt; 1, we observe exponential decay backwards in time for online updates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A3.4 MC affects the Variance of Exponentially Many Estimates with Delayed Reward</head><p>The variance for Monte Carlo is (A209) According to the results in Section A7.1, for proper policies π a unique fixed point V π exists:</p><formula xml:id="formula_169">V π (s, a) = Var r [r | s, a] + γ 2 (E s ,a [V π (s ,</formula><formula xml:id="formula_170">V π = T π [V π ] (A210) V π = lim k→∞ (T π ) k V ,<label>(A211)</label></formula><p>where V is any initial variance. In Section A7.1 it was shown that the operator T π is continuous, monotonically increasing (component-wise larger or smaller), and a contraction mapping for a weighted sup-norm. If we define the operator T π as depending on the on-site variance ω, that is T π ω , then it is monotonically in ω. We obtain component-wise for ω &gt;ω:</p><formula xml:id="formula_171">T π ω [q] (s, a) − T π ω [q] (s, a) (A212) = (ω(s, a) + E s ,a [q(s , a )]) − (ω(s, a) + E s ,a [q(s , a )]) = ω(s, a) −ω(s, a)</formula><p>0 .</p><p>It follows for the fixed points V π of T π ω and V π of T π ω : V π (s, a) V π (s, a) . </p><p>Theorem A9. Starting from the sequence end at t = T , as long as ω(s t , a t ) ω(s t , a t ) holds also the following holds:</p><formula xml:id="formula_173">V (s t , a t ) V (s t , a t ) . (A216)</formula><p>If for (s t , a t ) the strict inequality ω(s t , a t ) &gt;ω(s t , a t ) holds, then we have the strict inequality</p><formula xml:id="formula_174">V (s t , a t ) &gt; V (s t , a t ) . (A217) If p(s t , a t | s t−1 , a t−1 ) = 0 for some (s t−1 , a t−1 ) then E st,at [V (s t , a t ) | s t−1 , a t−1 ] &gt; E st,at V (s t , a t ) | s t−1 , a t−1 .<label>(A218)</label></formula><p>Therefore, the strict inequality ω(s t , a t ) &gt;ω(s t , a t ) is propagated back as a strict inequality of variances.</p><p>Proof. Proof by induction: Induction base:</p><formula xml:id="formula_175">V (s T +1 , a T +1 ) = V (s T +1 , a T +1 ) = 0 and ω(s T , a T ) =ω(s T , a T ) = 0. Induction step ((t + 1) → t):</formula><p>The induction hypothesis is that for all (s t+1 , a t+1 ) we have</p><formula xml:id="formula_176">V (s t+1 , a t+1 ) V (s t+1 , a t+1 )<label>(A219)</label></formula><p>and ω(s t , a t ) ω(s t , a t ). It follows that</p><formula xml:id="formula_177">E st+1,at+1 [V (s t+1 , a t+1 )] E st+1,at+1 V (s t+1 , a t+1 ) .<label>(A220)</label></formula><p>We obtain</p><formula xml:id="formula_178">V (s t , a t ) − V (s t , a t ) (A221) = ω(s t , a t ) + E st+1,at+1 [V (s t+1 , a t+1 )] − ω(s t , a t ) + E st+1,at+1 V (s t+1 , a t+1 ) = ω(s t , a t ) −ω(s t , a t ) 0 .</formula><p>If for (s t , a t ) the strict inequality ω(s t , a t ) &gt;ω(s t , a t ) holds, then we have the strict inequality V (s t , a t ) &gt; V (s t , a t ). If p(s t , a t | s t−1 , a t−1 ) = 0 for some (s t−1 , a t−1 ) then</p><formula xml:id="formula_179">E st,at [V (s t , a t ) | s t−1 , a t−1 ] &gt; E st,at V (s t , a t ) | s t−1 , a t−1 .<label>(A222)</label></formula><p>Therefore, the strict inequality ω(s t , a t ) &gt;ω(s t , a t ) is propagated back as a strict inequality of variances as long as p(s t , a t | s t−1 , a t−1 ) = 0 for some (s t−1 , a t−1 ). The induction goes through as long as ω(s t , a t ) ω(s t , a t ).</p><p>In Stephen Patek's PhD thesis, <ref type="bibr" target="#b88">[89]</ref> Lemma 5.1 on page 88-89 and proof thereafter state that if ω(s, a) = ω(s, a) − λ, then the solution V π is continuous and decreasing in λ. From the inequality above it follows that</p><formula xml:id="formula_180">V π (s, a) − V π (s, a) = (T π ω V π ) (s, a) − T π ω V π (s, a) (A223) = ω(s, a) −ω(s, a) + E s ,a V π (s , a ) − V π (s , a ) | s, a ω(s, a) −ω(s, a) .</formula><p>Time-Agnostic States. We defined a Bellman operator as</p><formula xml:id="formula_181">T π [V π ] (s, a) = ω(s, a) + s p(s | s, a) a π(a | s ) V π (s , a ) (A224) = ω(s, a) + (V π ) T p(s, a) ,</formula><p>where V π is the vector with value V π (s , a ) at position (s , a ) and p(s, a) is the vector with value p(s | s, a)π(a | s ) at position (s , a ). The fixed point equation is known as the Bellman equation. In vector and matrix notation the Bellman equation reads</p><formula xml:id="formula_182">T π [V π ] = ω + P V π ,<label>(A225)</label></formula><p>where P is the row-stochastic matrix with p(s | s, a)π(a | s ) at position ((s, a), (s , a )). We assume that the set of state-actions {(s, a)} is equal to the set of next state-actions {(s , a )}, therefore P is a square row-stochastic matrix. This Bellman operator has the same characteristics as the Bellman operator for the action-value function q π .</p><p>Since P is a row-stochastic matrix, the Perron-Frobenius theorem says that (1) P has as largest eigenvalue 1 for which the eigenvector corresponds to the steady state and (2) the absolute value of each (complex) eigenvalue is smaller equal 1. Only the eigenvector to eigenvalue 1 has purely positive real components. Equation 7 of Bertsekas and Tsitsiklis, 1991, <ref type="bibr" target="#b12">[13]</ref> states that</p><formula xml:id="formula_183">(T π ) t [V π ] = t−1 k=0 P k ω + P t V π .<label>(A226)</label></formula><p>Applying the operator T π recursively t times can be written as <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_184">(T π ) t [V π ] = t−1 k=0 P k ω + P t V π .<label>(A227)</label></formula><p>In particular for V π = 0, we obtain</p><formula xml:id="formula_185">(T π ) t [0] = t−1 k=0 P k ω .<label>(A228)</label></formula><p>For finite horizon MDPs, the values V π = 0 are correct for time step T + 1 since no reward for t &gt; T + 1 exists. Therefore, the "backward induction algorithm" <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b95">96]</ref> gives the correct solution:</p><formula xml:id="formula_186">V π = (T π ) T [0] = T −1 k=0 P k ω .<label>(A229)</label></formula><p>The product of square stochastic matrices is a stochastic matrix, therefore P k is a stochastic matrix. Perron-Frobenius theorem states that the spectral radius R(P k ) of the stochastic matrix P k is: R(P k ) = 1. Furthermore, the largest eigenvalue is 1 and all eigenvalues have absolute values smaller or equal one. Therefore, ω can have large influence on V π at every time step.</p><p>Time-Aware States. Next we consider time-aware MDPs, where transitions occur only from states s t to s t+1 . The transition matrix from states s t to s t+1 is denoted by P t . We assume that P t are row-stochastic matrices which are rectangular, that is P t ∈ R m×n . Definition A12. A row-stochastic matrix A ∈ R m×n has non-negative entries and the entries of each row sum up to one. It is known that the product of square stochastic matrices A ∈ R n×n is a stochastic matrix. We show in next theorem that this holds also for rectangular matrices. Lemma A4. The product C = AB with C ∈ R m×k of a row-stochastic matrix A ∈ R m×n and a row-stochastic matrix B ∈ R n×k is row-stochastic.</p><p>Proof. All entries of C are non-negative since they are sums and products of non-negative entries of A and B. The row-entries of C sum up to one:</p><formula xml:id="formula_187">k C ik = k j A ij B jk = j A ij k B jk = j A ij = 1 .<label>(A230)</label></formula><p>We will use the ∞-norm and the 1-norm of a matrix, which are defined based on the ∞-norm x ∞ = max i |x i | and 1-norm x 1 = i |x i | of a vector x. Definition A13. The ∞-norm of a matrix is the maximum absolute row sum:</p><formula xml:id="formula_188">A ∞ = max x ∞ =1 A x ∞ = max i j |A ij | .<label>(A231)</label></formula><p>The 1-norm of a matrix is the maximum absolute column sum:</p><formula xml:id="formula_189">A 1 = max x 1 =1 A x 1 = max j i |A ij | .<label>(A232)</label></formula><p>The statements of next theorem are known as Perron-Frobenius theorem for square stochastic matrices A ∈ R n×n , e.g. that the spectral radius R is R(A) = 1. We extend the theorem to a "∞-norm equals one" property for rectangular stochastic matrices A ∈ R m×n . Lemma A5 (Perron-Frobenius). If A ∈ R m×n is a row-stochastic matrix, then</p><formula xml:id="formula_190">A ∞ = 1 , A T 1 = 1 , and for n = m R(A) = 1 . (A233)</formula><p>Proof. A ∈ R m×n is a row-stochastic matrix, therefore A ij = |A ij |. Furthermore, the rows of A sum up to one. Thus, A ∞ = 1. Since the column sums of A T are the row sums of A, it follows that A T 1 = 1. For square stochastic matrices, that is m = n, Gelfand's Formula (1941) says that for any matrix norm . , for the spectral norm R(A) of a matrix A ∈ R n×n we obtain:</p><formula xml:id="formula_191">R(A) = lim k→∞ A k 1/k .<label>(A234)</label></formula><p>Since the product of row-stochastic matrices is a row-stochastic matrix, A k is a row-stochastic matrix. Consequently A k ∞ = 1 and A k 1/k ∞ = 1. Therefore, the spectral norm R(A) of a row-stochastic matrix A ∈ R n×n is</p><formula xml:id="formula_192">R(A) = 1 .<label>(A235)</label></formula><p>The last statement follows from Perron-Frobenius theorem, which says that the spectral radius of P is 1.</p><p>Using random matrix theory, we can guess how much the spectral radius of a rectangular matrix deviates from that of a square matrix. Let A ∈ R m×n be a matrix whose entries are independent copies of some random variable with zero mean, unit variance, and finite fourth moment. The Marchenko-Pastur quarter circular law for rectangular matrices says that for n = m the maximal singular value is 2 √ m <ref type="bibr" target="#b78">[79]</ref>. Asymptotically we have for the maximal singular value s max (A) ∝ √ m + √ n <ref type="bibr" target="#b103">[104]</ref>. A bound on the largest singular value is given by <ref type="bibr" target="#b121">[122]</ref>:</p><formula xml:id="formula_193">s 2 max (A) ( √ m + √ n) 2 + O( √ n log(n)) a.s.<label>(A236)</label></formula><p>Therefore, a rectangular matrix modifies the largest singular value by a factor of a = 0.5(1 + n/m) compared to a m × m square matrix. In the case that tstates are time aware, transitions only occur from states s t to s t+1 . The transition matrix from states s t to s t+1 is denoted by P t .</p><p>States affected by the on-site variance ω k (reachable states). Typically, states in s t have only few predecessor states in s t−1 compared to N t−1 , the number of possible states in s t−1 . Only for those states in s t−1 the transition probability to the state in s t is larger than zero. That is, each i ∈ s t+1 has only few j ∈ s t for which p t (i | j) &gt; 0. We now want to know how many states have increased variance due to ω k , that is how many states are affected by ω k . In a general setting, we assume random connections. Let N t be the number of all states s t that are reachable after t time steps of an episode.N = 1/k k t=1 N t is the arithmetic mean of N t . Let c t be the average connectivity of a state in s t to states in s t−1 andc = k t=1 c t 1/k the geometric mean of the c t . Let n t be the number of states in s t that are affected by the on-site variance ω k at time k for t k. The number of states affected by ω k is a k = k t=0 n t . We assume that ω k only has one component larger than zero, that is, only one state at time t = k is affected: n k = 1. The number of affected edges from s t to s t−1 is c t n t . However, states in s t−1 may be affected multiple times by different affected states in s t . <ref type="figure" target="#fig_7">Figure A1</ref> shows examples of how affected states affect states in a previous time step. The left panel shows no overlap since affected states in s t−1 connect only to one affected state in s t . The right panel shows some overlap since affected states in s t−1 connect to multiple affected states in s t . The next theorem states that the on-site variance ω k can have large effect on the variance of each previous state-action pair. Furthermore, for small k the number of affected states grows exponentially, while for large k it grows only linearly after some timet. <ref type="figure" target="#fig_1">Figure A2</ref> shows the function which determines how much a k grows with k. Theorem A10. For t k, ω k contributes to V π t by the term P t←k ω k , where P t←k ∞ = 1. The number a k of states affected by the on-site variance ω k is  Proof. The "backward induction algorithm" <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b95">96]</ref> gives with V π T +1 = 0 and on-site variance ω T +1 = 0:</p><formula xml:id="formula_194">a k = k t=0 1 − 1 − c t N t−1 nt N t−1 .<label>(A237</label></formula><formula xml:id="formula_195">V π t = T k=t k−1 τ =t P τ ω k ,<label>(A238)</label></formula><p>where we define t−1 τ =t P τ = I and [ω k ] (s k ,a k ) = ω(s k , a k ). Since the product of two row-stochastic matrices is a row-stochastic matrix according to Lemma A4, P t←k = k−1 τ =t P τ is a row-stochastic matrix. Since P t←k ∞ = 1 according to Lemma A5, each on-site variance ω k with t k can have large effects on V π t . Using the row-stochastic matrices P t←k , we can reformulate the variance:</p><formula xml:id="formula_196">V π t = T k=t P t←k ω k ,<label>(A239)</label></formula><p>with P t←k ∞ = 1. The on-site variance ω k at step k increases all variances V π t with t k. Next we proof the second part of the theorem, which considers the growth of a k . To compute a k we first have to know n t . For computing n t−1 from n t , we want to know how many states are affected in s t−1 if n t states are affected in s t . The answer to this question is the expected coverage when searching a document collection using a set of independent computers <ref type="bibr" target="#b18">[19]</ref>. We follow the approach of Cox et al. <ref type="bibr" target="#b18">[19]</ref>. The minimal number of affected states in s t−1 is c t , where each of the c t affected states in s t−1 connects to each of the n t states in s t (maximal overlap). The maximal number of affected states in s t−1 is c t n t , where each affected state in s t−1 connects to only one affected state in s t (no overlap). We consider a single state in s t . The probability of a state in s t−1 being connected to this single state in s t is c t /N t−1 and being not connected to this state in s t is 1 − c t /N t−1 . The probability of a state in s t−1 being not connected to any of the n t affected states in s t is</p><formula xml:id="formula_197">1 − c t N t−1 nt .<label>(A240)</label></formula><p>The probability of a state in s t−1 being at least connected to one of the n t affected states in s t is Thus, the expected number of distinct states in s t−1 being connected to one of the n t affected states in s t is</p><formula xml:id="formula_198">1 − 1 − c t N t−1 nt .<label>(A241)</label></formula><formula xml:id="formula_199">n t−1 = 1 − 1 − c t N t−1 nt N t−1 .<label>(A242)</label></formula><p>The number a k of affected states by ω k is</p><formula xml:id="formula_200">a k = k t=0 1 − 1 − c t N t−1 nt N t−1 .<label>(A243)</label></formula><p>Corollary A2. For small k, the number a k of states affected by the on-site variance ω k at step k grows exponentially with k by a factor ofc:</p><formula xml:id="formula_201">a k &gt;c k .<label>(A244)</label></formula><p>For large k and after some time t &gt;t, the number a k of states affected by ω k grows linearly with k with a factor ofN :</p><formula xml:id="formula_202">a k ≈ at −1 + (k −t + 1)N .<label>(A245)</label></formula><p>Proof. For small n t with ctnt</p><formula xml:id="formula_203">Nt−1 1, we have 1 − c t N t−1 nt ≈ 1 − c t n t N t−1 ,<label>(A246)</label></formula><formula xml:id="formula_204">thus n t−1 ≈ c t n t .<label>(A247)</label></formula><p>For large N t−1 compared to the number of connections c t of a single state in s t to states in s t−1 , we have the approximation</p><formula xml:id="formula_205">1 − c t N t−1 nt = 1 + −c t N t−1 Nt−1 nt/Nt−1 ≈ exp(−(c t n t )/N t−1 ) .<label>(A248)</label></formula><p>We obtain</p><formula xml:id="formula_206">n t−1 = (1 − exp(−(c t n t )/N t−1 )) N t−1 .<label>(A249)</label></formula><p>For small n t , we again have</p><formula xml:id="formula_207">n t−1 ≈ c t n t .<label>(A250)</label></formula><p>Therefore, for small k − t, we obtain</p><formula xml:id="formula_208">n t ≈ k τ =t c τ ≈c k−t .<label>(A251)</label></formula><p>Thus, for small k the number a k of states affected by ω k is</p><formula xml:id="formula_209">a k = k t=0 n t ≈ k t=0c k−t = k t=0c t =c k+1 − 1 c − 1 &gt;c k .<label>(A252)</label></formula><p>Consequently, for small k the number a k of states affected by ω k grows exponentially with k by a factor ofc. For large k, at a certain time t &gt;t, n t has grown such that c t n t &gt; N t−1 , yielding exp(−(c t n t )/N t−1 ) ≈ 0, and thus</p><formula xml:id="formula_210">n t ≈ N t .<label>(A253)</label></formula><p>Therefore</p><formula xml:id="formula_211">a k − at −1 = k t=t n t ≈ k t=t N t ≈ (k −t + 1)N .<label>(A254)</label></formula><p>Consequently, for large k the number a k of states affected by ω k grows linearly with k by a factor of N .</p><p>Therefore, we aim for decreasing the on-site variance ω k for large k, in order to reduce the variance.</p><p>In particular, we want to avoid delayed rewards and provide the reward as soon as possible in each episode. Our goal is to give the reward as early as possible in each episode to reduce the variance of action-values that are affected by late rewards and their associated immediate and local variances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4 Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.1 Artificial Tasks</head><p>This section provides more details for the artificial tasks (I), (II) and (III) in the main paper. Additionally, we include artificial task (IV) characterized by deterministic reward and state transitions, and artificial task (V) which is solved using policy gradient methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.1.1 Task (I): Grid World</head><p>This environment is characterized by probabilistic delayed rewards. It illustrates a situation, where a time bomb explodes at episode end. The agent has to defuse the bomb and then run away as far as possible since defusing fails with a certain probability. Alternatively, the agent can immediately run away, which, however, leads to less reward on average since the bomb always explodes. The Grid World is a quadratic 31 × 31 grid with bomb at coordinate <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b14">15]</ref> and start at <ref type="bibr">[30 − d, 15]</ref>, where d is the delay of the task. The agent can move in four different directions (up, right, left, and down). Only moves are allowed that keep the agent on the grid. The episode finishes after 1.5d steps. At the end of the episode, with a given probability of 0.5, the agent receives a reward of 1000 if it has visited bomb. At each time step the agent receives an immediate reward of c · t · h, where the factor c depends on the chosen action, t is the current time step, and h is the Hamming distance to bomb. Each move of the agent, which reduces the Hamming distance to bomb, is penalized by the immediate reward via c = −0.09. Each move of the agent, which increases the Hamming distance to bomb, is rewarded by the immediate reward via c = 0.1. The agent is forced to learn the Q-values precisely, since the immediate reward of directly running away hints at a sub-optimal policy. For non-deterministic reward, the agent receives the delayed reward for having visited bomb with probability p(r T +1 = 100 | s T , a T ). For non-deterministic transitions, the probability of transiting to next state s is p(s | s, a). For the deterministic environment these probabilities were either 1 or zero.</p><p>Policy evaluation: learning the action-value estimator for a fixed policy. First, the theoretical statements on bias and variance of estimating the action-values by TD in Theorem A8 and by MC in Theorem A10 are experimentally verified for a fixed policy. Secondly, we consider the bias and variance of TD and MC estimators of the transformed MDP with optimal reward redistribution according to Theorem A5. The new MDP with an optimal reward redistribution has advantages over the original MDP both for TD and MC. For TD, the new MDP corrects the bias exponentially faster and for MC it has fewer number of action-values with high variance. Consequently, estimators for the new MDP learn faster than the same estimators in the original MDP.</p><p>Since the bias-variance analysis is defined for a particular number of samples drawn from a fixed distribution, we need to fix the policy for sampling. We use an -greedy version of the optimal policy, where is chosen such that on average in 10% of the episodes the agent visits bomb. For the analysis, the delay ranges from 5 to 30 in steps of 5. The true Q-table for each delay is computed by backward induction and we use 10 different action-value estimators for computing bias and variance. For the TD update rule we use the exponentially weighted arithmetic mean that is sample-updates, with initial value q 0 (s, a) = 0. We only monitor the mean and the variance for action-value estimators at the first time step, since we are interested in the time required for correcting the bias. 10 different estimators are run for 10,000 episodes. <ref type="figure" target="#fig_20">Figure A3a</ref> shows the bias correction for different delays, normalized by the first error. For the MC update rule we use the arithmetic mean for policy evaluation (later we will use constantα MC for learning the optimal policy). For each delay, a test set of state-actions for each delay is generated by drawing 5,000 episodes with the -greedy optimal policy. For each action-value estimator the mean and the variance is monitored every 10 visits. If every action-value has 500 updates (visits), learning is stopped. Bias and variance are computed based on 10 different action-value estimators. As expected from Section A3.1, in <ref type="figure" target="#fig_20">Figure A3b</ref> the variance decreases by 1/n, where n is the number of samples. <ref type="figure" target="#fig_20">Figure A3b</ref> shows that the number of state-actions with a variance larger than a threshold increases exponentially with the delay. This confirms the statements of Theorem A10.</p><p>Learning the optimal policy. For finding the optimal policy for the Grid World task, we apply Monte Carlo Tree Search (MCTS), Q-learning, and Monte Carlo (MC). We train until the greedy policy reaches 90% of the return of the optimal policy. The learning time is measured by the number of episodes. We use sample updates for Q-learning and MC <ref type="bibr" target="#b127">[128]</ref>. For MCTS the greedy policy uses 0 for the exploration constant in UCB1 <ref type="bibr" target="#b67">[68]</ref>. The greedy policy is evaluated in 100 episodes intervals. The MCTS selection step begins in the start state, which is the root of the game tree that is traversed using UCB1 <ref type="bibr" target="#b67">[68]</ref> as the tree policy. If a tree-node gets visited the first time, it is expanded with an initial value obtained by 100 simulated trajectories that start at this node. These simulations use a uniform random policy whose average Return is calculated. The backpropagation step uses the MCTS(1) update rule <ref type="bibr" target="#b65">[66]</ref>. The tree policies exploration constant is √ 2. Q-learning and MC use a learning rate of 0.3 and an -greedy policy with = 0.3. For RUDDER the optimal reward redistribution using a return decomposition as stated in Section A2.6.1 is used. For each delay and each method, 300 runs with different seeds are performed to obtain statistically relevant results.</p><p>Estimation of the median learning time and quantiles. The performance of different methods is measured by the median learning time in terms of episodes. We stop training at 100 million episodes. Some runs, especially for long delays, have taken too long and have thus been stopped. To resolve this bias the quantiles of the learning time are estimated by fitting a distribution using right censored data <ref type="bibr" target="#b32">[33]</ref> .The median is still robustly estimated if more than 50% of runs have finished, which is the case for all plotted datapoints. We find that for delays where all runs have finished the learning time follows a Log-normal distribution. Therefore, we fit a Log-normal distribution on the right censored data. We estimate the median from the existing data, and use maximum likelihood estimation to obtain the second distribution parameter σ 2 . The start value of the σ 2 estimation is calculated by the measured variance of the existing data which is algebraically transformed to get the σ parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.1.2 Task (II): The Choice</head><p>In this experiment we compare RUDDER, temporal difference (TD) and Monte Carlo (MC) in an environment with delayed deterministic reward and probabilistic state transitions to investigate how reward information is transferred back to early states. This environment is a variation of our introductory pocket watch example and reveals problems of TD and MC, while contribution analysis excels. In this environment, only the first action at the very beginning determines the reward at the end of the episode. or s ⊕ to s − or s or vice versa are zero. Thus, the first action determines whether that agent stays in "+"-states or "−"-states. The reward is determined by how many times the agent visits charged states plus a bonus reward depending on the agent's first action. The accumulative reward is given at sequence end and is deterministic. After T time steps, the agent is in the final state s f , in which the reward R T +1 is provided. R T +1 is the sum of 3 deterministic terms: 1. R 0 , the baseline reward associated to the first action; 2. R C , the collected reward across states, which depends on the number of visits n to the charged states; 3. R b , a bonus if the first action a 0 = +. The expectations of the accumulative rewards for R 0 and R C have the same absolute value but opposite signs, therefore they cancel in expectation over episodes. Thus, the expected return of an episode is the expected reward R b : p(a 0 = +)b. The rewards are defined as follows:</p><formula xml:id="formula_212">c 0 = 1 if a 0 = + −1 if a 0 = − ,<label>(A255)</label></formula><formula xml:id="formula_213">R b = b if a 0 = + 0 if a 0 = − ,<label>(A256)</label></formula><formula xml:id="formula_214">R C = c 0 C n , (A257) R 0 = − c 0 C p C T , (A258) R T +1 = R C + R 0 + R b ,<label>(A259)</label></formula><p>where C is the baseline reward for charged states, and p C the probability of staying in or transiting to charged states. The expected visits of charged states is E</p><formula xml:id="formula_215">[n] = p C T and E[R T +1 ] = E[R b ] = p(a 0 = +)b.</formula><p>Methods compared: The following methods are compared: 1. Q-learning with eligibility traces according to Watkins <ref type="bibr" target="#b139">[140]</ref>, 2. Monte Carlo, 3. RUDDER with reward redistribution. For RUDDER, we use an LSTM without lessons buffer and without safe exploration. Contribution analysis is realized by differences of return predictions. For MC, Q-values are the exponential moving average of the episode return. For RUDDER, the Q-values are estimated by an exponential moving average of the reward redistribution.</p><p>Performance evaluation and results. The task is considered as solved when the exponential moving average of the selection of the desired action at time t = 0 is equal to 1 − , where is the exploration rate. The performances of the compared methods are measured by the average learning time in the number of episodes required to solve the task. A Wilcoxon signed-rank test is performed between the learning time of RUDDER and those of the other methods. Statistical significance p-values are obtained by Wilcoxon signed-rank test. RUDDER with reward redistribution is significantly faster than all other methods with p-values &lt; 10 −8 . <ref type="table" target="#tab_4">Table A1</ref> reports the number of episodes required by different methods to solve the task. RUDDER with reward redistribution clearly outperforms all other methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.1.3 Task(III): Trace-Back</head><p>This section supports the artificial task (III) -Trace-Back -in the main paper. RUDDER is compared to potential-based reward shaping methods. In this experiment, we compare reinforcement learning methods that have to transfer back information about a delayed reward. These methods comprise RUDDER, TD(λ) and potential-based reward shaping approaches. For potential-based reward shaping we compare the original reward shaping <ref type="bibr" target="#b86">[87]</ref>, look-forward advice, and look-back advice <ref type="bibr" target="#b142">[143]</ref> with three different potential functions. Methods that transfer back reward information are characterized by low variance estimates of the value function or the action-value function, since they use an estimate of the future return instead of the future return itself. To update the estimates of the future returns, reward information has to be transferred back. The task in this experiment can be solved by Monte Carlo estimates very fast, which do not transfer back information but use samples of the future return for the estimation instead. However, Monte Carlo methods have high variance, which is not considered in this experiment.</p><p>The environment is a 15×15 grid, where actions move the agent from its current position in 4 adjacent positions (up, down, left, right), except the agent would be moved outside the grid. The number of steps (moves) per episode is T = 20. The starting position is <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b6">7)</ref> in the middle of the grid. The maximal return is a combination of negative immediate reward and positive delayed reward.</p><p>To obtain the maximum return, the policy must move the agent up in the time step t = 1 and right in the following time step t = 2. In this case, the agent receives an immediate reward of -50 at t = 2 and a delayed reward of 150 at the end of the episode at t = 20, that is, a return of 100. Any other combination of actions gives the agent immediate reward of 50 at t = 2 without any delayed reward, that is, a return of 50. To ensure Markov properties the position of the agent, the time, as well as the delayed reward are coded in the state. The future reward discount rate γ is set to 1. The state transition probabilities are deterministic for the first two moves. For t &gt; 2 and for each action, state transition probabilities are equal for each possible next state (uniform distribution), meaning that actions after t = 2 do not influence the return. For comparisons of long delays, both the size of the grid and the length of the episode are increased. For a delay of n, a (3n/4) × (3n/4) grid is used with an episode length of n, and starting position (3n/8, 3n/8).</p><p>Compared methods. We compare different TD(λ) and potential-based reward shaping methods. For TD(λ), the baseline is Q(λ), with eligibility traces λ = 0.9 and λ = 0 and Watkins' implementation <ref type="bibr" target="#b139">[140]</ref>. The potential-based reward shaping methods are the original reward shaping, look-ahead advice as well as look-back advice. For look-back advice, we use SARSA(λ) <ref type="bibr" target="#b104">[105]</ref> instead of Q(λ) as suggested by the authors <ref type="bibr" target="#b142">[143]</ref>. Q-values are represented by a state-action table, that is, we consider only tabular methods. In all experiments an -greedy policy with = 0.2 is used. All three reward shaping methods require a potential function φ, which is based on the reward redistribution (r t ) in three different ways: (I) The Potential function φ is the difference of LSTM predictions, which is the redistributed reward</p><formula xml:id="formula_216">R t : φ(s t ) = E [R t+1 | s t ] or (A260) φ(s t , a t ) = E [R t+1 | s t , a t ] .</formula><p>(A261) (II) The potential function φ is the sum of future redistributed rewards, i.e. the q-value of the redistributed rewards. In the optimal case, this coincides with implementation (I):</p><formula xml:id="formula_217">φ(s t ) = E T τ =t R τ +1 | s t or (A262) φ(s t , a t ) = E T τ =t R τ +1 | s t , a t .<label>(A263)</label></formula><p>(III) The potential function φ corresponds to the LSTM predictions. In the optimal case this corresponds to the accumulated reward up to t plus the q-value of the delayed MDP:</p><formula xml:id="formula_218">φ(s t ) = E T τ =0R τ +1 | s t or (A264) φ(s t , a t ) = E T τ =0R τ +1 | s t , a t .<label>(A265)</label></formula><p>The following methods are compared: 1. Q-learning with eligibility traces according to Watkins (Q(λ)), 2. SARSA with eligibility traces (SARSA(λ)), 3. Reward Shaping with potential functions (I), (II), or (III) according to Q-learning and eligibility traces according to <ref type="bibr">Watkins,</ref><ref type="bibr" target="#b3">4</ref>. Look-ahead advise with potential functions (I), (II), or (III) with Q(λ), 5. Look-back advise with potential functions (I), (II), or (III) with SARSA(λ), 6. RUDDER with reward redistribution for Q-value estimation and RUDDER applied on top of Q-learning. RUDDER is implemented with an LSTM architecture without output gate nor forget gate. For this experiments, RUDDER does not use lessons buffer nor safe exploration. For contribution analysis we use differences of return predictions. For RUDDER, the Q-values are estimated by an exponential moving average (RUDDER Q-value estimation) or alternatively by Q-learning. Performance evaluation: The task is considered solved when the exponential moving average of the return is above 90, which is 90% of the maximum return. Learning time is the number of episodes required to solve the task. The first evaluation criterion is the average learning time. The Q-value differences at time step t = 2 are monitored. The Q-values at t = 2 are the most important ones, since they have to predict whether the maximal return will be received or not. At t = 2 the immediate reward acts as a distraction since it is -50 for the action leading to the maximal return (a + ) and 50 for all other actions (a − ). At the beginning of learning, the Q-value difference between a + and a − is about -100, since the immediate reward is -50 and 50, respectively. Once the Q-values converge to the optimal policy, the difference approaches 50. However, the task will already be correctly solved as soon as this difference is positive. The second evaluation criterion is the Q-value differences at time step t = 2, since it directly shows to what extend the task is solved.</p><p>Results: <ref type="table" target="#tab_4">Table A1</ref> reports the number of episodes required by different methods to solve the task. The mean and the standard deviation over 100 trials are given. A Wilcoxon signed-rank test is performed between the learning time of RUDDER and those of the other methods. Statistical significance p-values are obtained by Wilcoxon signed-rank test. RUDDER with reward redistribution is significantly faster than all other methods with p-values &lt; 10 −17 . Tables A2,A3 report the results for all methods.   <ref type="figure">Figure A5</ref> is characterized by deterministic reward and state transitions. The environment consists of two states: charged C / discharged D and two actions charge c / discharge d. The deterministic reward is r(D, d) = 1, r(C, d) = 10, r(D, c) = 0, and r(C, c) = 0. The reward r <ref type="figure">(C, d)</ref> is accumulated for the whole episode and given only at time T + 1, where T corresponds to the maximal delay of the reward. The optimal policy alternates between charging and discharging to accumulate a reward of 10 every other time step. The smaller immediate reward of 1 distracts the agent from the larger delayed reward. The distraction forces the agent to learn the value function well enough to distinguish between the contribution of the immediate and the delayed reward to the final return.  <ref type="figure">Figure A5</ref>: The Charge-Discharge task with two basic states: charged C and discharged D. In each state the actions charge c leading to the charged state C and discharge d leading to discharged state D are possible. Action d in the discharged state D leads to a small immediate reward of 1 and in the charged state C to a delayed reward of 10. After sequence end T = 4, the accumulated delayed reward r T +1 = r 5 is given.</p><p>For this task, the RUDDER backward analysis is based on monotonic LSTMs and on layer-wise relevance propagation (LRP). The reward redistribution provided by RUDDER uses an LSTM which consists of 5 memory cells and is trained with Adam and a learning rate of 0.01. The reward redistribution is used to learn an optimal policy by Q-learning and by MC with a learning rate of 0.1 and an exploration rate of 0.1. Again, we use sample updates for Q-learning and MC <ref type="bibr" target="#b127">[128]</ref>. The learning is stopped either if the agent achieves 90% of the reward of the optimal policy or after a maximum number of 10 million episodes. For each T and each method, 100 runs with different seeds are performed to obtain statistically relevant results. For delays with runs which did not finish within 100m episodes we estimate parameters like described in Paragraph A4.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.1.5 Task (V): Solving Trace-Back using policy gradient methods</head><p>In this experiment, we compare policy gradient methods instead of Q-learning based methods. These methods comprise RUDDER on top of PPO with and without GAE, and a baseline PPO using GAE. The environment and performance evaluation are the same as reported in Task III. Again, RUDDER is exponentially faster than PPO. RUDDER on top of PPO is slightly better with GAE than without.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.2 Atari Games</head><p>In this section we describe the implementation of RUDDER for Atari games. The implementation is largely based on the OpenAI baselines package <ref type="bibr" target="#b20">[21]</ref> for the RL components and our package for the LSTM reward redistribution model, which will be announced upon publication. If not specified otherwise, standard input processing, such as skipping 3 frames and stacking 4 frames, is performed by the OpenAI baselines package. We consider the 52 Atari games that were compatible with OpenAI baselines, Arcade Learning Environment (ALE) <ref type="bibr" target="#b10">[11]</ref>, and OpenAI Gym <ref type="bibr" target="#b17">[18]</ref>. Games are divided into episodes, i.e. the loss of a life or the exceeding of 108k frames trigger the start of a new episode without resetting the environment. Source code will be made available at upon publication.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.2.1 Architecture</head><p>We use a modified PPO architecture and a separate reward redistribution model. While parts of the two could be combined, this separation allows for better comparison between the PPO baseline with and without RUDDER.</p><p>PPO architecture. The design of the policy and the value network relies on the ppo2 implementation <ref type="bibr" target="#b20">[21]</ref>, which is depicted in <ref type="figure">Figure A7</ref> and summarized in <ref type="table" target="#tab_9">Table A4</ref>. The network input, 4 stacked Atari game frames <ref type="bibr" target="#b81">[82]</ref>, is processed by 3 convolution layers with ReLU activation functions, followed by a fully connected layer with ReLU activation functions. For PPO with RUDDER, 2 output units, for the original and redistributed reward value function, and another set of output units for the policy prediction are applied. For the PPO baseline without RUDDER, the output unit for the redistributed reward value function is omitted.</p><p>Reward redistribution model. Core of the reward redistribution model is an LSTM layer containing 64 memory cells with sigmoid gate activations, tanh input nonlinearities, and identity output activation functions, as illustrated in <ref type="figure">Figure A7</ref> and summarized in <ref type="table" target="#tab_9">Table A4</ref>. This LSTM implementation omits output gate and forget gate to simplify the network dynamics. Identity output activation functions were chosen to support the development of linear counting dynamics within the LSTM layer, as is required to count the reward pieces during an episode chunk. Furthermore, the input gate is only connected recurrently to other LSTM blocks and the cell input is only connected to forward connections from the lower layer. For the vision system the same architecture was used as with the PPO network, with the first convolution layer being doubled to process ∆ frames and full frames separately in the first layer. Additionally, the memory cell layer receives the vision feature activations of the PPO network, the current action, and the approximate in-game time as inputs. No gradients from the reward redistribution network are propagated over the connections to the PPO network. After the LSTM layer, the reward redistribution model has one output node for the prediction g of the return realization g of the return variable G 0 . The reward redistribution model has 4 additional output nodes for the auxiliary tasks as described in Section A4. <ref type="bibr" target="#b1">2</ref>  <ref type="figure">Figure A7</ref>: RUDDER architecture for Atari games as described in Section A4.2.1. Left: The ppo2 implementation <ref type="bibr" target="#b20">[21]</ref>. Right: LSTM reward redistribution architecture. The reward redistribution network has access to the PPO vision features (dashed lines) but no gradient is propagated between the networks. The LSTM layer receives the current action and an approximate in-game-time as additional input. The PPO outputs v for value function prediction and π for policy prediction each represent multiple output nodes: the original and redistributed reward value function prediction for v and the outputs for all of the available actions for π. Likewise, the reward redistribution network output g represents multiple outputs, as described in Section A4.2.3 Details on layer configuration are given in <ref type="table" target="#tab_9">Table A4</ref>.   <ref type="figure">Figure A7</ref>. Truncated normal initialization has the default values mean= 0, stddev= 1 and is optionally multiplied by a factor scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.2.2 Lessons Replay Buffer</head><p>The lessons replay buffer is realized as a priority-based buffer containing up to 128 samples. New samples are added to the buffer if (i) the buffer is not filled or if (ii) the new sample is considered more important than the least important sample in the buffer, in which case the new sample replaces the least important sample. Importance of samples for the buffer is determined based on a combined ranking of (i) the reward redistribution model error and (ii) the difference of the sample return to the mean return of all samples in the lessons buffer. Each of these two rankings contributes equally to the final ranking of the sample. Samples with higher loss and greater difference to the mean return achieve a higher ranking. Sampling from the lessons buffer is performed as a sampling from a softmax function on the samplelosses in the buffer. Each sample is a sequence of 512 consecutive transitions, as described in the last paragraph of Section A4.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.2.3 Game Processing, Update Design, and Target Design</head><p>Reward redistribution is performed in an online fashion as new transitions are sampled from the environment. This allows to keep the original update schema of the PPO baseline, while still using the redistributed reward for the PPO updates. Training of the reward redistribution model is done separately on the lessons buffer samples from Section A4.2.2. These processes are described in more detail in the following paragraphs. (A266)</p><p>Goal of this scaling is to normalize the reward r to range [−10, 10] with a linear scaling, suitable for training the PPO and reward redistribution model. Since the scaling is linear, the original proportions between rewards are kept. Downside to this approach is that if a new maximum return is encountered, the scaling factor is updated, and the models have to readjust.</p><p>Reward redistribution. Reward redistribution is performed using differences of return predictions of the LSTM network. That is, the differences of the reward redistribution model prediction g at time step t and t−1 serve as contribution analysis and thereby give the redistributed reward r t = g t − g t−1 . This allows for online reward redistribution on the sampled transitions before they are used to train the PPO network, without waiting for the game sequences to be completed. To assess the current quality of the reward redistribution model, a quality measure based on the relative absolute error of the prediction g T at the last time step T is introduced:</p><formula xml:id="formula_219">quality = 1 − |g − g T | µ 1 1 − ,<label>(A267)</label></formula><p>with as quality threshold of = 80% and the maximum possible error µ as µ = 10 due to the reward scaling applied. quality is furthermore clipped to be within range [0, 1].</p><p>PPO model. The ppo2 implementation <ref type="bibr" target="#b20">[21]</ref> samples from the environment using multiple agents in parallel. These agents play individual environments but share all weights, i.e. they are distinguished by random effects in the environment or by exploration. The value function and policy network is trained online on a batch of transitions sampled from the environment. Originally, the policy/value function network updates are adjusted using a policy loss, a value function loss, and an entropy term, each with dedicated scaling factors <ref type="bibr" target="#b114">[115]</ref>. To decrease the number of hyperparameters, the entropy term scaling factor is adjusted automatically using Proportional Control to keep the policy entropy in a predefined range. We use two value function output units to predict the value functions of the original and the redistributed reward. For the PPO baseline without RUDDER, the output unit for the redistributed reward is omitted. Analogous to the ppo2 implementation, these two value function predictions serve to compute the advantages used to scale the policy gradient updates. For this, the advantages for original reward a o and redistributed reward a r are combined as a weighted sum a = a o (1 − qualityv) + a r quality. The PPO value function loss term L v is replaced by the sum of the value function v o loss L o for the original reward and the scaled value function v r loss L r for the redistributed reward, such that L v = L o + L r quality. Parameter values were taken from the original paper <ref type="bibr" target="#b114">[115]</ref> and implementation <ref type="bibr" target="#b20">[21]</ref>. Additionally, a coarse hyperparameter search was performed with value function coefficients {0.1, 1, 10} and replacing the static entropy coefficient by a Proportional Control scaling of the entropy coefficient. The Proportional Control target entropy was linearly decreased from 1 to 0 over the course of training. PPO baseline hyperparamters were used for PPO with RUDDER without changes. Parameter values are listed in <ref type="table" target="#tab_10">Table A5</ref>.</p><p>Reward redistribution model. The loss of the reward redistribution model for a sample is composed of four parts. (i) The main loss L m , which is the squared prediction loss of g at the last time step T of the episode</p><formula xml:id="formula_220">L m = (g − g T ) 2 ,<label>(A268)</label></formula><p>(ii) the continuous prediction loss L c of g at each time step</p><formula xml:id="formula_221">L c = 1 T + 1 T t=0 (g − g t ) 2 ,<label>(A269)</label></formula><p>(iii) the loss L e of the prediction of the output at t + 10 at each time step t</p><formula xml:id="formula_222">L e = 1 T − 9 T −10 t=0 g t+10 − ( g t+10 ) t 2 ,<label>(A270)</label></formula><p>as well as (iv) the loss on 3 auxiliary tasks. At every time step t, these auxiliary tasks are (1) the prediction of the action-value function q t , (2) the prediction of the accumulated original rewardr in the next 10 frames </p><formula xml:id="formula_223">L a1 = 1 T + 1 T t=0 (q t − q t ) 2 ,<label>(A271)</label></formula><formula xml:id="formula_224">L a2 = 1 T − 9 T −10 t=0   t+10 i=tr i − t+10 i=tr i t   2 ,</formula><p>(A272)</p><formula xml:id="formula_225">L a3 = 1 T − 49 T −50 t=0   t+50 i=tr i − t+50 i=tr i t   2 ,</formula><p>(A273)</p><formula xml:id="formula_226">L a = 1 3 (L a1 + L a2 + L a3 ) .<label>(A274)</label></formula><p>The final loss for the reward redistribution model is then computed as</p><formula xml:id="formula_227">L = L m + 1 10 (L c + L e + L a ) .<label>(A275)</label></formula><p>The continuous prediction and earlier prediction losses L c and L e push the reward redistribution model toward performing an optimal reward redistribution. This is because important events that are redundantly encoded in later states are stored as early as possible. Furthermore, the auxiliary loss L  <ref type="table" target="#tab_10">Table A5</ref>.</p><p>PPO RUDDER learning rate 2.5 · 10 −4 learning rate 10 −4 policy coefficient 1.0 L 2 weight decay 10 −7 initial entropy coefficient 0.01 gradient clipping 0.5 value function coefficient 1.0 optimization ADAM Sequence chunking and Truncated Backpropagation Through Time (TBPTT). Ideally, RUD-DER would be trained on completed game sequences, to consequently redistribute the reward within a completed game. To shorten computational time for learning the reward redistribution model, the model is not trained on completed game sequences but on sequence chunks consisting of 512 time steps. The beginning of such a chunk is treated as beginning of a new episode for the model and ends of episodes within this chunk reset the state of the LSTM, so as to not redistribute rewards between episodes. To allow for updates on sequence chunks even if the game sequence is not completed, the PPO value function prediction is used to estimate the expected future reward at the end of the chunk. Utilizing TBPTT to further speed up LSTM learning, gradients for the reward redistribution LSTM are cut after every 128 time steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.2.4 Exploration</head><p>Safe exploration to increase the likelihood of observing delayed rewards is an important feature of RUDDER. We use a safe exploration strategy, which is realized by normalizing the output of the policy network to range [0, 1] and randomly picking one of the actions that is above a threshold θ. Safe exploration is activated once per sequence at a random sequence position for a random duration between 0 and the average game lengthl. Thereby we encourage long but safe off-policy trajectories within parts of the game sequences. Only 2 of the 8 parallel actors use safe exploration with θ 1 = 0.001 and θ 1 = 0.5, respectively. All actors sample from the softmax policy output. To avoid policy lag during safe exploration transitions, we use those transitions only to update the reward redistribution model but not the PPO model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A4.2.5 Results</head><p>Training curves for 3 random seeds for PPO baseline and PPO with RUDDER are shown in <ref type="figure" target="#fig_22">Figure A8</ref> and scores are listed in <ref type="table" target="#tab_12">Table A6</ref> for all 52 Atari games. Training was conducted over 200M game frames (including skipped frames), as described in the experiments section of the main paper. We investigated failures and successes of RUDDER in different Atari games. RUDDER failures were observed to be mostly due to LSTM failures and comprise e.g. slow learning in Breakout, explaining away in Double Dunk, spurious redistributed rewards in Hero, overfitting to the first levels in Qbert, and exploration problems in MontezumaRevenge. RUDDER successes were observed to be mostly due to redistributing rewards to important key actions that would otherwise not receive reward, such as moving towards the built igloo in Frostbite, diving up for refilling oxygen in Seaquest, moving towards the treasure chest in Venture, and shooting at the shield of the enemy boss UFO, thereby removing its shield.    Visual Confirmation of Detecting Relevant Events by Reward Redistribution. We visually confirm a meaningful and helpful redistribution of reward in both Bowling and Venture during training. As illustrated in <ref type="figure">Figure A9</ref>, RUDDER is capable of redistributing a reward to key events in a game, drastically shortening the delay of the reward and quickly steering the agent toward good policies. Furthermore, it enriches sequences that were sparse in reward with a dense reward signal. Video demonstrations are available at https://goo.gl/EQerZV. <ref type="figure">Figure A9</ref>: Observed return decomposition by RUDDER in two Atari games with long delayed rewards. Left: In the game Bowling, reward is only given after a turn which consist of multiple rolls. RUDDER identifies the actions that guide the ball in the right direction to hit all pins. Once the ball hit the pins, RUDDER detects the delayed reward associated with striking the pins down. In the figure only 100 frames are represented but the whole turn spans more than 200 frames. In the original game, the reward is given only at the end of the turn. Right: In the game Venture, reward is only obtained after picking the treasure. RUDDER guides the agent (red) towards the treasure (golden) via reward redistribution. Reward is redistributed to entering a room with treasure. Furthermore, the redistributed reward gradually increases as the agent approaches the treasure. For illustration purposes, the green curve shows the return redistribution before applying lambda. The environment only gives reward at the event of collecting treasure (blue).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A5 Discussion and Frequent Questions</head><p>RUDDER and reward rescaling. RUDDER works with no rescaling, various rescalings, and sign function as we have confirmed in additional experiments. Rescaling ensures similar reward magnitudes across different Atari games, therefore the same hyperparameters can be used for all games. For LSTM and PPO, we only scale the original return by a constant factor, therefore do not change the problem and do not simplify it. The sign function, in contrast, may simplify the problem but may change the optimal policy.</p><p>RUDDER for infinite horizon: Continual Learning. RUDDER assumes a finite horizon problem. For games and for most tasks in real world these assumptions apply: did you solve the task? (make tax declaration, convince a customer to buy, design a drug, drive a car to a location, assemble a car, build a building, clean the room, cook a meal, pass the Turing test). In general our approach can be extended to continual learning with discounted reward. Only the transformation of an immediate reward MDP to an MDP with episodic reward is no longer possible. However the delayed reward problem becomes more obvious and also more serious when not discounting the reward.</p><p>Is the LSTM in RUDDER a state-action value function? For reward redistribution we assume an MDP with one reward (=return) at sequence end which can be predicted from the last state-action pair. When introducing the ∆-states, the reward cannot be predicted from the last ∆ and the task is no longer Markov. However the return can be predicted from the sequence of ∆s. Since the ∆s are mutually independent, the contribution of each ∆ to the return must be stored in the hidden states of the LSTM to predict the final reward. The ∆ can be generic as states and actions can be numbered and then the difference of this numbers can be used for ∆.</p><p>In the applications like Atari with immediate rewards we give the accumulated reward at the end of the episode without enriching the states. This has a similar effect as using ∆. We force the LSTM to build up an internal state which tracks the already accumulated reward. True, the LSTM is the value function at time t based on the ∆ sub-sequence up to t. The LSTM prediction can be decomposed into two sub-predictions. The first sub-prediction is the contribution of the already known ∆ sub-sequence up to t to the return (backward view). The second sub-prediction is the expected contribution of the unknown future sequence from t+1 onwards to the return (forward view). However, we are not interested in the second sub-prediction but only in the contribution of ∆ t to the prediction of the expected return. The second sub-prediction is irrelevant for our approach. We cancel the second sub-prediction via the differences of predictions. The difference at time t gives the contribution of ∆ t to the expected return. Empirical confirmation: Four years ago, we started this research project with using LSTM as a value function, but we failed. This was the starting point for RUDDER. In the submission, we used LSTM predictions in artificial task (IV) as potential function for reward shaping, look-ahead advice, and look-back advice. Furthermore, we investigated LSTM as a value function for artificial task (II) but these results have not been included. At the time where RUDDER already solved the task, the LSTM error was too large to allow learning via a value function. Problem is the large variance of the returns at the beginning of the sequence which hinders LSTM learning (forward view). RUDDER LSTM learning was initiated by propagating back prediction errors at the sequence end, where the variance of the return is lower (backward view). These late predictions initiated the storing of key events at the sequence beginning even with high prediction errors. The redistributed reward at the key events led RUDDER solve the task. Concluding: at the time RUDDER solved the task, the early predictions are not learned due to the high variance of the returns. Therefore using the predictions as value function does not help (forward view). Example: The agent has to take a key to open the door. Since it is an MDP, the agent is always aware to have the key indicated by a key bit to be on. The reward can be predicted in the last step. Using differences ∆ the key bit is zero, except for the step where the agent takes the key. Thus, the LSTM has to store this event and will transfer reward to it.</p><p>Compensation reward. The compensation corrects for prediction errors of g (g is the sum of h).</p><p>The prediction error of g can have two sources: (1) the probabilistic nature of the reward, (2) an approximation error of g for the expected reward. We aim to make (2) small and then the correction is only for the probabilistic nature of the reward. The compensation error depends on g, which, in turn, depends on the whole sequence. The dependency on state-action pairs from t = 0 to T − 1 is viewed as random effect, therefore the compensation reward only depends on the last state-action pair.</p><p>That h t and R t+1 depends only on (s t , a t , s t−1 , a t−1 ) is important to prove Theorem 3. Then a t−1 cancels and the advantage function remains the same.</p><p>Connection theory and algorithms. Theorem 1 and Theorem 2 ensure that the algorithms are correct since the optimal policies do not change even for non-optimal return decompositions. In contrast to TD methods which are biased, Theorem 3 shows that the update rule Q-value estimation is unbiased when assuming optimal decomposition. Theorem 4 explicitly derives optimality conditions for the expected sum of delayed rewards "kappa" and measures the distance to optimality. This "kappa" is used for learning and is explicitly estimated to correct learning if an optimal decomposition cannot be assured. The theorems are used to justify following learning methods (A) and (B): (A) Q-value estimation: (i) Direct Q-value estimation (not Q-learning) according to Theorem 3 is given in Eq. (9) when an optimal decomposition is assumed. (ii) Q-value estimation with correction by kappa according to Theorem 4, when optimal decomposition is not assumed. Here kappa is learned by TD as given in Eq. We also shows variants (not in the main paper) on page 31 and 32 of using kappa "Correction of the reward redistribution" by reward shaping with kappa and "Using kappa as auxiliary task in predicting the return for return decomposition".</p><p>Optimal Return Decomposition, contributions and policy. The Q-value q π depends on a particular policy π. The function h depends on policy π since h predicts the expected return (E π [R T +1 ]) which depends on π. Thus, both return decomposition and optimal return decomposition are defined for a particular policy π. A reward redistribution from a return decomposition leads to a return equivalent MDP. Return equivalent MDPs are defined via all policies even if the reward redistribution was derived from a particular policy. A reward redistribution depends only on the state-action sequence but not on the policy that generated this sequence. Also ∆ does not depend on a policy.</p><p>Optimal policies are preserve for every state. We assume all states are reachable via at least one non-zero transition probability to each state and policies that have a non-zero probability for each action due to exploration. For an MDP being optimal in the initial state is the same as being optimal in every reachable state. This follows from recursively applying the Bellman optimality equation to the initial value function. The values of the following states must be optimal otherwise the initial value function is smaller. Only states to which the transition probability is zero the Bellman optimality equation does not determine the optimality. All RL algorithms are suitable. For example we applied TD, Monte Carlo, Policy Gradient, which all work faster with the new MDP.</p><p>Limitations. In all of the experiments reported in this manuscript, we show that RUDDER significantly outperforms other methods for delayed reward problems. However, RUDDER might not be effective when the reward is not delayed since LSTM learning takes extra time and has problems with very long sequences. Furthermore, reward redistribution may introduce disturbing spurious reward signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A6 Additional Related Work</head><p>Delayed Reward. To learn delayed rewards there are three phases to consider: (i) discovering the delayed reward, (ii) keeping information about the delayed reward, (iii) learning to receive the delayed reward to secure it for the future. Recent successful reinforcement learning methods provide solutions to one or more of these phases. Most prominent are Deep Q-Networks (DQNs) <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b81">82]</ref>, which combine Q-learning with convolutional neural networks for visual reinforcement learning <ref type="bibr" target="#b68">[69]</ref>. The success of DQNs is attributed to experience replay <ref type="bibr" target="#b73">[74]</ref>, which stores observed statereward transitions and then samples from them. Prioritized experience replay <ref type="bibr" target="#b108">[109,</ref><ref type="bibr" target="#b57">58]</ref> advanced the sampling from the replay memory. Different policies perform exploration in parallel for the Ape-X DQN and share a prioritized experience replay memory <ref type="bibr" target="#b57">[58]</ref>. DQN was extended to double DQN (DDQN) <ref type="bibr" target="#b133">[134,</ref><ref type="bibr" target="#b134">135]</ref> which helps exploration as the overestimation bias is reduced. Noisy DQNs <ref type="bibr" target="#b25">[26]</ref> explore by a stochastic layer in the policy network (see <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b109">110]</ref>). Distributional Q-learning <ref type="bibr" target="#b9">[10]</ref> profits from noise since means that have high variance are more likely selected. The dueling network architecture <ref type="bibr" target="#b137">[138,</ref><ref type="bibr" target="#b138">139]</ref> separately estimates state values and action advantages, which helps exploration in unknown states. Policy gradient approaches <ref type="bibr" target="#b144">[145]</ref> explore via parallel policies, too. A2C has been improved by IMPALA through parallel actors and correction for policy-lags between actors and learners <ref type="bibr" target="#b23">[24]</ref>. A3C with asynchronous gradient descent <ref type="bibr" target="#b79">[80]</ref> and Ape-X DPG <ref type="bibr" target="#b57">[58]</ref> also rely on parallel policies. Proximal policy optimization (PPO) extends A3C by a surrogate objective and a trust region optimization that is realized by clipping or a Kullback-Leibler penalty <ref type="bibr" target="#b114">[115]</ref>.</p><p>Recent approaches aim to solve learning problems caused by delayed rewards. Function approximations of value functions or critics <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b79">80]</ref> bridge time intervals if states associated with rewards are similar to states that were encountered many steps earlier. For example, assume a function that has learned to predict a large reward at the end of an episode if a state has a particular feature. The function can generalize this correlation to the beginning of an episode and predict already high reward for states possessing the same feature. Multi-step temporal difference (TD) learning <ref type="bibr" target="#b126">[127,</ref><ref type="bibr" target="#b127">128]</ref> improved both DQNs and policy gradients <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b79">80]</ref>. AlphaGo and AlphaZero learned to play Go and Chess better than human professionals using Monte Carlo Tree Search (MCTS) <ref type="bibr" target="#b115">[116,</ref><ref type="bibr" target="#b116">117]</ref>. MCTS simulates games from a time point until the end of the game or an evaluation point and therefore captures long delayed rewards. Recently, world models using an evolution strategy were successful <ref type="bibr" target="#b41">[42]</ref>. These forward view approaches are not feasible in probabilistic environments with a high branching factor of state transition.</p><p>Backward View. We propose learning from a backward view, which either learns a separate model or analyzes a forward model. Examples of learning a separate model are to trace back from known goal states <ref type="bibr" target="#b22">[23]</ref> or from high reward states <ref type="bibr" target="#b35">[36]</ref>. However, learning a backward model is very challenging. When analyzing a forward model that predicts the return then either sensitivity analysis or contribution analysis may be utilized. The best known backward view approach is sensitivity analysis (computing the gradient) like "Backpropagation through a Model´´ <ref type="bibr" target="#b85">[86,</ref><ref type="bibr" target="#b100">101,</ref><ref type="bibr" target="#b101">102,</ref><ref type="bibr" target="#b141">142,</ref><ref type="bibr" target="#b4">5]</ref>. Sensitivity analysis has several drawbacks: local minima, instabilities, exploding or vanishing gradients, and proper exploration <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b109">110]</ref>. The major drawback is that the relevance of actions is missed since sensitivity analysis does not consider their contribution to the output but only their effect on the output when slightly perturbing them. We use contribution analysis since sensitivity analysis has serious drawbacks. Contribution analysis determines how much a state-action pair contributes to the final prediction. To focus on stateactions which are most relevant for learning is known from prioritized sweeping for model-based reinforcement learning <ref type="bibr" target="#b84">[85]</ref>. Contribution analysis can be done by computing differences of return predictions when adding another input, by zeroing out an input and then compute the change in the prediction, by contribution-propagation <ref type="bibr" target="#b70">[71]</ref>, by a contribution approach <ref type="bibr" target="#b93">[94]</ref>, by excitation backprop <ref type="bibr" target="#b146">[147]</ref>, by layer-wise relevance propagation (LRP) <ref type="bibr" target="#b2">[3]</ref>, by Taylor decomposition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b82">83]</ref>, or by integrated gradients (IG) <ref type="bibr" target="#b124">[125]</ref>.</p><p>LSTM. LSTM was already used in reinforcement learning <ref type="bibr" target="#b111">[112]</ref> for advantage learning <ref type="bibr" target="#b3">[4]</ref>, for constructing a potential function for reward shaping by representing the return by a sum of LSTM outputs across an episode <ref type="bibr" target="#b123">[124]</ref>, and learning policies <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b44">45]</ref>.</p><p>Reward Shaping, Look-Ahead Advice, Look-Back Advice. Redistributing the reward is fundamentally different from reward shaping <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b142">143]</ref>, look-ahead advice and look-back advice <ref type="bibr" target="#b143">[144]</ref>. However, these methods can be viewed as a special case of reward redistribution that result in an MDP that is return-equivalent to the original MDP as is shown in Section A2.2. On the other hand every reward function can be expressed as look-ahead advice <ref type="bibr" target="#b42">[43]</ref>. In contrast to these methods, reward redistribution is not limited to potential functions, where the additional reward is the potential difference, therefore it is a more general concept than shaping reward or look-ahead/look-back advice. The major difference of reward redistribution to reward shaping, look-ahead advice, and look-back advice is that the last three keep the original rewards. Both look-ahead advice and look-back advice have not been designed for replacing for the original rewards. Since the original reward is kept, the reward redistribution is not optimal according to Section A2.6.1. The original rewards may have long delays that cause an exponential slow-down of learning. The added reward improves sampling but a delayed original reward must still be transferred to the Q-values of early states that caused the reward. The concept of return-equivalence of SDPs resulting from reward redistributions allows to eliminate the original reward completely. Reward shaping can replace the original reward. However, it only depends on states but not on actions, and therefore, it cannot identify relevant actions without the original reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A7 Markov Decision Processes with Undiscounted Rewards</head><p>We focus on Markov Decision Processes (MDPs) with undiscounted rewards, since the relevance but also the problems of a delayed reward can be considerably decreased by discounting it. Using discounted rewards both the bias correction in TD as well as the variance of MC are greatly reduced. The correction amount decreases exponentially with the delay steps, and also the variance contribution to one state decreases exponentially with the delay of the reward. MDPs with undiscounted rewards are either finite horizon or process absorbing states without reward. The former can always be described by the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A7.1 Properties of the Bellman Operator in MDPs with Undiscounted Rewards</head><p>At each time t the environment is in some state s = s t ∈ S. The agent takes an action a = a t ∈ A according to policy π, which causes a transition of the environment to state s = s t+1 ∈ S and a reward r = r t+1 ∈ R for the agent with probability p(s , r | s, a).</p><p>The Bellman operator maps a action-value function q = q(s, a) to another action-value function. We do not require that q are Q-values and that r is the actual reward. We define the Bellman operator T π for policy π as:</p><formula xml:id="formula_228">T π [q] (s, a) = s ,r p(s , r | s, a) r + a π(a | s ) q(s , a ) .<label>(A276)</label></formula><p>We often rewrite the operator as </p><p>We did not explicitly express the dependency on the policy π and the state-action pair (s, a) in the expectation E s ,a . A more precise way would be to write E π s ,a [. | s, a]. More generally, we have (A280)</p><p>In the following we show properties for this general formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A7.1.1 Monotonically Increasing and Continuous</head><p>We assume the general formulation Eq. (A280) of the Bellman operator. Proposition 2.1 on pages 22-23 in Bertsekas and Tsitsiklis, 1996, <ref type="bibr" target="#b13">[14]</ref> shows that a fixed point q π of the Bellman operator exists and that for every q:</p><formula xml:id="formula_230">q π = T π [q π ] (A281) q π = lim k→∞ (T π ) k q .<label>(A282)</label></formula><p>The fixed point equation </p><formula xml:id="formula_231">q π = T π [q π ]<label>(A283)</label></formula><p>whereḡ is the long term average reward or the expected value of the reward for the stationary distribution:ḡ</p><formula xml:id="formula_233">= lim T →∞ 1 T + 1 T t=0 g(s t , a t ) .<label>(A285)</label></formula><p>We assumeḡ = 0 since after some time the agent does no longer receive reward in MDPs with finite time horizon or in MDPs with absorbing states that have zero reward. T π is monotonically increasing in its arguments <ref type="bibr" target="#b13">[14]</ref>. For q 1 and q 2 with the component-wise condition q 1 q 2 , we have</p><formula xml:id="formula_234">T π [q 1 ] (s, a) − T π [q 2 ] (s, a) (A286) = (g(s, a) + E s ,a [q 1 (s , a )]) − (g(s, a) + E s ,a [q 2 (s , a )]) = E s ,a [q 1 (s , a ) − q 2 (s , a )] 0 ,</formula><p>where " " is component-wise. The last inequality follows from the component-wise condition</p><formula xml:id="formula_235">q 1 q 2 .</formula><p>We define the norm . ∞ , which gives the maximal difference of the Q-values:</p><formula xml:id="formula_236">q 1 − q 2 ∞ = max s,a |q 1 (s, a) − q 2 (s, a)| . (A287)</formula><p>T is a non-expansion mapping for q 1 and q 2 : The first inequality is valid since the absolute value is moved into the sum. The second inequality is valid since the expectation depending on (s, a) is replaced by a maximum that does not depend on (s, a). Consequently, the operator T π is continuous.</p><formula xml:id="formula_237">T π [q 1 ] − T π [q 2 ] ∞ = max</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A7.1.2 Contraction for Undiscounted Finite Horizon</head><p>For time-aware states, we can define another norm with 0 &lt; η &lt; 1 which allows for a contraction mapping:</p><formula xml:id="formula_238">q 1 − q 2 ∞,t = T max t=0 η T −t+1 max st,a |q 1 (s t , a) − q 2 (s t , a)| .<label>(A289)</label></formula><p>T π is a contraction mapping for q 1 and q 2 <ref type="bibr" target="#b13">[14]</ref>:</p><formula xml:id="formula_239">T π [q 1 ] − T π [q 2 ] ∞,t = T max t=0 η T −t+1 max st,a |T[q 1 ](s t , a) − T[q 2 ](s t , a)| (A290) = T max t=0 η T −t+1 max st,a   g(s t , a) + st+1 p(s t+1 | s t , a) a π(a | s ) q 1 (s t+1 , a )   −   g(s t , a) + st+1 p(s t+1 | s t , a) a π(a | s ) q 2 (s t+1 , a )   = T max t=0 η T −t+1 max st,a st+1 p(s t+1 | s t , a) a π(a | s ) [q 1 (s t+1 , a ) − q 2 (s t+1 , a )] T max t=0 η T −t+1 max st,a st+1 p(s t+1 | s t , a) a π(a | s ) |q 1 (s t+1 , a ) − q 2 (s t+1 , a )| T max t=0 η T −t+1 max st+1,a |q 1 (s t+1 , a ) − q 2 (s t+1 , a )| T max t=0 η η T −(t+1)+1 max st+1,a |q 1 (s t+1 , a ) − q 2 (s t+1 , a )| = η T +1 max t=1 η T −t+1 max st,a |q 1 (s t , a ) − q 2 (s t , a )| = η T max t=0 η T −t+1 max st,a |q 1 (s t , a ) − q 2 (s t , a )| = η q 1 − q 2 ∞,t .</formula><p>The equality in the last but one line stems from the fact that all Q-values at t = T + 1 are zero and that all Q-values at t = 1 have the same constant value. Furthermore, all q values are equal to zero for additionally introduced states at t = T + 1 since for t &gt; T + 1 all rewards are zero. We have</p><formula xml:id="formula_240">q π = T T [q] ,<label>(A291)</label></formula><p>which is correct for additionally introduced states at time t = T + 1 since they are zero. Then, in the next iteration Q-values of states at time t = T are correct. After iteration i, Q-values of states at time t = T − i + 1 are correct. This iteration is called the "backward induction algorithm" <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b95">96]</ref>. If we perform this iteration for a policy π instead of the optimal policy, then this procedure is called "policy evaluation algorithm" <ref type="bibr" target="#b94">[95,</ref><ref type="bibr" target="#b95">96]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A7.1.3 Contraction for Undiscounted Infinite Horizon With Absorbing States</head><p>A stationary policy is proper if there exists an integer n such that from any initial state x the probability of achieving the terminal state after n steps is strictly positive. If all terminal states are absorbing and cost/reward free and if all stationary policies are proper the Bellman operator is a contraction mapping with respect to a weighted sup-norm. The fact that the Bellman operator is a contraction mapping with respect to a weighted sup-norm has been proved in <ref type="bibr" target="#b131">Tseng, 1990</ref>, in Lemma 3 with equation <ref type="formula" target="#formula_0">(13)</ref> and text thereafter <ref type="bibr" target="#b131">[132]</ref>. Also Proposition 1 in Bertsekas and Tsitsiklis, 1991, <ref type="bibr" target="#b12">[13]</ref>, Theorems 3 and 4(b) &amp; 4(c) in <ref type="bibr">Tsitsiklis, 1994, [133]</ref>, and Proposition 2.2 on pages 23-24 in Bertsekas and Tsitsiklis, 1996, <ref type="bibr" target="#b13">[14]</ref> have proved the same fact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A7.1.4 Fixed Point of Contraction is Continuous wrt Parameters</head><p>The mean q π and variance V π are continuous with respect to π, that is π(a | s ), with respect to the reward distribution p(r | s, a) and with respect to the transition probabilities p(s | s, a). A complete metric space or a Cauchy space is a space where every Cauchy sequence of points has a limit in the space, that is, every Cauchy sequence converges in the space. The Euclidean space R n with the usual distance metric is complete. Lemma 2.5 in Jachymski, 1996, is <ref type="bibr" target="#b61">[62]</ref>: Theorem A11 (Jachymski: complete metric space). Let (X, d) be a complete metric space, and let (P, d P ) be a metric space. Let F : P × X → X be continuous in the first variable and contractive in the second variable with the same Lipschitz constant α &lt; 1. For p ∈ P , let x * (p) be the unique fixed point of the map x → F (p, x). Then the mapping x * is continuous. This theorem is Theorem 2.3 in Frigon, 2007, <ref type="bibr" target="#b26">[27]</ref>. Corollary 4.2 in Feinstein, 2016, generalized the theorem to set valued operators, that is, these operators may have more than one fixed point <ref type="bibr" target="#b24">[25]</ref> (see also <ref type="bibr" target="#b66">[67]</ref>). All mappings F (p, .) must have the same Lipschitz constant α &lt; 1.</p><p>A locally compact space is a space where every point has a compact neighborhood. R n is locally compact as a consequence of the Heine-Borel theorem. Proposition 3.2 in Jachymski, 1996, is <ref type="bibr" target="#b61">[62]</ref>: Theorem A12 (Jachymski: locally compact complete metric space). Let (X, d) be a locally compact complete metric space, and let (P, d P ) be a metric space. Let F : P × X → X be continuous in the first variable and contractive in the second variable with not necessarily the same Lipschitz constant. For p ∈ P , let x * (p) be the unique fixed point of the map x → F (p, x). Then the mapping x * is continuous. This theorem is Theorem 2.5 in <ref type="bibr">Frigon, 2007, [27]</ref> and Theorem 2 in <ref type="bibr" target="#b69">Kwiecinski, 1992</ref>, <ref type="bibr" target="#b69">[70]</ref>. The mappings F (p, .) can have different Lipschitz constants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A7.1.5 t-fold Composition of the Operator</head><p>We define the Bellman operator as  <ref type="figure">(s , a )</ref>). The Poisson equation is q π +ḡ1 = g + P q , (A294) where 1 is the vector of ones andḡ is the long term average reward or the expected value of the reward for the stationary distribution:</p><formula xml:id="formula_241">g = lim T →∞ 1 T + 1 T t=0 g(s t , a t ) .<label>(A295)</label></formula><p>We assumeḡ = 0 since after some time the agent does no longer receive reward for MDPs with finite time horizon or MDPs with absorbing states that have zero reward.</p><p>Since P is a row-stochastic matrix, the Perron-Frobenius theorem says that (1) P has as largest eigenvalue 1 for which the eigenvector corresponds to the steady state and (2) the absolute value of each (complex) eigenvalue is smaller or equal 1. Only the eigenvector to the eigenvalue 1 has purely positive real components. Equation 7 of Bertsekas and Tsitsiklis, 1991, <ref type="bibr" target="#b12">[13]</ref> states</p><formula xml:id="formula_242">(T π ) t [q] = t−1 k=0 P k g + P t q .<label>(A296)</label></formula><p>If p is the stationary distribution vector for P , that is, lim The expected return at time t = 0 is:</p><formula xml:id="formula_243">k→∞ P k = 1 p T (A297) lim k→∞ p T 0 P k = p T (A298) then lim k→∞ 1 k k−1 i=0 P i = 1 p T (A299) lim k→∞ 1 k k−1 i=0 p T 0 P i = p T .<label>(A300</label></formula><formula xml:id="formula_244">v 0 = s0 p(s 0 ) v(s 0 ) .<label>(A302)</label></formula><p>As introduced for the REINFORCE algorithm, we can subtract a baseline v 0 from the return. We subtract the baseline v 0 from the last reward. Therefore, for the new rewardR we haveR t = R t for</p><formula xml:id="formula_245">t T andR T +1 = R T +1 − v 0 . Consequently,q(s t , a t ) = q(s t , a t ) − v 0 for t T .</formula><p>The TD update rules are:</p><formula xml:id="formula_246">q(s t , a t ) ←− q(s t , a t ) + α r t + a π(a | s t+1 ) q(s t+1 , a) − q(s t , a t ) .<label>(A303)</label></formula><p>The δ-errors are</p><formula xml:id="formula_247">R t+1 + a π(a | s t+1 ) q(s t+1 , a) − q(s t , a t ) = R t+1 + a π(a | s t+1 ) (q(s t+1 , a) − v 0 ) − (q(s t , a t ) − v 0 ) =R t+1 + a π(a | s t+1 )q(s t+1 , a) −q(s t , a t )<label>(A304)</label></formula><p>and for the last step</p><formula xml:id="formula_248">R T +1 − q(s T , a T ) = (R T +1 − v 0 ) − (q(s T , a T ) − v 0 ) (A305) =R T +1 −q(s T , a T ) .</formula><p>If we setq (s t , a t ) = q(s t , a t ) − v 0 , for t T .</p><p>(A306)</p><formula xml:id="formula_249">R t = R t , for t T R T +1 − v 0 , for t = T + 1 ,<label>(A307)</label></formula><p>then the δ-errors and the updates remain the same for q andq. We are equally far away from the optimal solution in both cases.</p><p>Removing the offset v 0 at the end byR T +1 = R T +1 − v 0 , can also be derived via reward shaping. However, the offset has to be added at the beginning:R 1 = R 1 + v 0 . Reward shaping requires for the shaping reward F and a potential function Φ <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b142">143]</ref>:</p><formula xml:id="formula_250">F (s t , a t , s t+1 ) = Φ(s t+1 ) − Φ(s t ) .<label>(A308)</label></formula><p>For introducing a reward of c at time t = k and removing it from time t = m &lt; k we set:</p><formula xml:id="formula_251">Φ(s t ) =    0 , for t m , −c , for m + 1 t k , 0 , for t &gt; k ,<label>(A309)</label></formula><p>then the shaping reward is</p><formula xml:id="formula_252">F s t , a t , s t+1 =              0 , for t &lt; m , −c , for t = m , 0 , for m + 1 t &lt; k , c , for t = k , 0 , for t &gt; k .<label>(A310)</label></formula><p>For k = T , m = 0, and c = −v 0 we obtain above situation but withR 1 = R 1 + v 0 andR T +1 = R T +1 − v 0 , that is, v 0 is removed at the end and added at the beginning. All Q-values except q(s 0 , a 0 ) are decreased by v 0 . In the general case, all Q-values q(s t , a t ) with m + 1 t k are increased by c. Q-value normalization: We apply reward shaping <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b142">143]</ref> for normalization of the Q-values. The potential Φ(s) defines the shaping reward F (s t , a t , s t+1 ) = Φ(s t+1 ) − Φ(s t ). The optimal policies do not change and the Q-values become</p><formula xml:id="formula_253">q new (s t , a t ) = q(s t , a t ) − Φ(s t ) .<label>(A311)</label></formula><p>We change the Q-values for all 1 t T , but not for t = 0 and t = T + 1. The first and the last Q-values are not normalized. All the shaped reward is added/subtracted to/from the initial and the last reward.</p><p>• The maximal Q-values are zero and the non-optimal Q-values are negative for all 1 t T :</p><formula xml:id="formula_254">Φ(s t ) = max a q(s t , a) .<label>(A312)</label></formula><p>• The minimal Q-values are zero and all others Q-values are positive for all 1 t T − 1:</p><formula xml:id="formula_255">Φ(s t ) = min a q(s t , a) . (A313)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A7.3 Alternative Definition of State Enrichment</head><p>Next, we define state-enriched processesP compared to P. The states ofP is enriched with a deterministic information compared to a state s of P. The enriched information ins can be computed from the state-action pair (s, a) and the reward r. Enrichments may be the accumulated reward, count of the time step, a count how often a certain action has been performed, a count how often a certain state has been visited, etc. Givan et al. have already shown that state-enriched Markov decision processes (MDPs) preserve the optimal action-value and action sequence properties as well as the optimal policies of the model <ref type="bibr" target="#b33">[34]</ref>. </p><formula xml:id="formula_256">p(s , r |s, a) = p(f (s ), r | f (s), a) , (A314) p 0 (s 0 ) = p 0 (f (s 0 )) ,<label>(A315)</label></formula><p>wherep 0 and p 0 are the probabilities of the initial states ofP and P, respectively.</p><p>If the reward is deterministic, thenp(s , r |s, a) =p(s |s, a) andp 0 (s 0 , r) =p 0 (s 0 ). We proof the following theorem, even if it has been proved several times as mention above.</p><p>Theorem A13. If decision processP is state-enriched compared to P, then for each optimal policỹ π * ofP there exists an equivalent optimal policy π * of P, and vice versa, withπ * (s) = π * (f (s)).</p><p>The optimal return is the same forP and P.</p><p>Proof. We proof by induction thatqπ(s, a) = q π (f (s), a) ifπ(s) = π(f (s)).</p><p>Basis: The end of the sequence. For t T we haveqπ(s, a) = q π (f (s), a) = 0, since no policy receives reward for t T . </p><p>Using Bellman's optimality equation would give the same result, where in above equation both a π(a | f (s )) and a π(a |s ) are replaced by max a . Theorem A14. If a Markov decision processP is state-enriched compared to the MDP P, then for each optimal policyπ * ofP there exists an equivalent optimal policy π * of P, and vice versa, with π * (f (s)) = π * (s). The optimal return is the same forP and P.</p><p>Proof. The MDPP is a homomorphic image of P. For state-enrichment, the mapping g is bijective, therefore the optimal policies inP and P are equal according to Lemma A1. The optimal return is also equal since it does not change via state-enrichment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A7.4 Variance of the Weighted Sum of a Multinomial Distribution</head><p>State transitions are multinomial distributions and the future expected reward is a weighted sum of multinomial distributions. Therefore, we are interested in the variance of the weighted sum of a multinomial distribution. Since we have</p><formula xml:id="formula_258">E s ,a [q π (s , a ) | s, a] = s p(s | s, a) a π(a | s ) q π (s , a ) ,<label>(A318)</label></formula><p>the variance of E s ,a [q π (s , a )] is determined by the variance of the multinomial distribution p(s | s, a). In the following we derive the variance of the estimation of a linear combination of variables of a multinomial distribution like s p(s | s, a)f (s ). A multinomial distribution with parameters (p 1 , . . . , p N ) as event probabilities satisfying N i=1 p i = 1 and support x i ∈ {0, . . . , n}, i ∈ {1, . . . , N } for n trials, that is x i = n, has pdf n!</p><formula xml:id="formula_259">x 1 ! · · · x k ! p x1 1 · · · p x k k ,<label>(A319)</label></formula><formula xml:id="formula_260">mean E[X i ] = n p i , (A320) variance Var[X i ] = n p i (1 − p i ) , (A321) covariance Cov[X i , X j ] = − n p i p j , (i = j) ,<label>(A322)</label></formula><p>where X i is the random variable and x i the actual count. A linear combination of random variables has variance</p><formula xml:id="formula_261">Var N i=1 a i X i = N i,j=1 a i a j Cov [X i , X j ] (A323) = N i=1 a 2 i Var [X i ] + i =j a i a j Cov [X i , X j ] .</formula><p>The variance of estimating the mean X of independent random variables (X 1 , . . . , X n ) that all have variance σ 2 is:</p><formula xml:id="formula_262">Var [X] = Var 1 n n i=1 X i (A324) = 1 n 2 n i=1</formula><p>Var</p><formula xml:id="formula_263">[X i ] = 1 n 2 n i=1 σ 2 = σ 2 n .</formula><p>When estimating the meanȳ over n samples of a linear combination of variables of a multinomial distribution y = N i=1 a i X i , where each y has n y trials, we obtain:</p><formula xml:id="formula_264">Var [ȳ] = σ 2 y n = 1 n   N i=1 a 2 i n y p i (1 − p i ) − i =j a i a j n y p i p j   (A325) = n y n   N i=1 a 2 i p i (1 − p i ) − i =j a i a j p i p j   = n y n   N i=1 a 2 i p i − (N,N ) (i,j)=(1,1) a i a j p i p j   = n y n   N i=1 a 2 i p i − N i=1 a i p i 2   .</formula><p>A8 Long Short-Term Memory (LSTM)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A8.1 LSTM Introduction</head><p>Recently, Long Short-Term Memory (LSTM; <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55]</ref>) networks have emerged as the bestperforming technique in speech and language processing. LSTM networks have been overwhelming successful in different speech and language applications, including handwriting recognition <ref type="bibr" target="#b36">[37]</ref>, generation of writings <ref type="bibr" target="#b37">[38]</ref>, language modeling and identification <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b145">146]</ref>, automatic language translation <ref type="bibr" target="#b125">[126]</ref>, speech recognition <ref type="bibr" target="#b106">[107,</ref><ref type="bibr" target="#b28">29]</ref> analysis of audio data <ref type="bibr" target="#b77">[78]</ref>, analysis, annotation, and description of video data <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b135">136,</ref><ref type="bibr" target="#b122">123]</ref>. LSTM has facilitated recent benchmark records in TIMIT phoneme recognition (Google), optical character recognition, text-to-speech synthesis (Microsoft), language identification (Google), large vocabulary speech recognition (Google), Englishto-French translation (Google), audio onset detection, social signal classification, image caption generation (Google), video-to-text description, end-to-end speech recognition (Baidu), and semantic representations. In the proceedings of the flagship conference ICASSP 2015 (40 th IEEE International Conference on Acoustics, Speech and Signal Processing, Brisbane, Australia, April 19-24, 2015), 13 papers had "LSTM" in their title, yet many more contributions described computational approaches that make use of LSTM.</p><p>The key idea of LSTM is the use of memory cells that allow for constant error flow during training. Thereby, LSTM avoids the vanishing gradient problem, that is, the phenomenon that training errors are decaying when they are back-propagated through time <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b51">52]</ref>. The vanishing gradient problem severely impedes credit assignment in recurrent neural networks, i.e. the correct identification of relevant events whose effects are not immediate, but observed with possibly long delays. LSTM, by its constant error flow, avoids vanishing gradients and, hence, allows for uniform credit assignment,</p><p>i.e. all input signals obtain a similar error signal. Other recurrent neural networks are not able to assign the same credit to all input signals, therefore they are very limited concerning the solutions they will find. Uniform credit assignment enabled LSTM networks to excel in speech and language tasks: if a sentence is analyzed, then the first word can be as important as the last word. Via uniform credit assignment, LSTM networks regard all words of a sentence equally. Uniform credit assignment enables to consider all input information at each phase of learning, no matter where it is located in the input sequence. Therefore, uniform credit assignment reveals many more solutions to the learning algorithm which would otherwise remain hidden.   <ref type="figure" target="#fig_7">Figure A10</ref>: LSTM memory cell without peepholes. z is the vector of cell input activations, i is the vector of input gate activations, f is the vector of forget gate activations, c is the vector of memory cell states, o is the vector of output gate activations, and y is the vector of cell output activations. The activation functions are g for the cell input, h for the cell state, and σ for the gates. Data flow is either "feed-forward" without delay or "recurrent" with an one-step delay. "Input" connections are from the external input to the LSTM network, while "recurrent" connections take inputs from other memory cells and hidden units of the LSTM network with a delay of one time step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A8.2 LSTM in a Nutshell</head><p>The central processing and storage unit for LSTM recurrent networks is the memory cell. As already mentioned, it avoids vanishing gradients and allows for uniform credit assignment. The most commonly used LSTM memory cell architecture in the literature <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b111">112]</ref> contains forget gates <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32]</ref> and peephole connections <ref type="bibr" target="#b29">[30]</ref>. In our previous work <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b52">53]</ref>, we found that peephole connections are only useful for modeling time series, but not for language, meta-learning, or biological sequences. That peephole connections can be removed without performance decrease, was recently confirmed in a large assessment, where different LSTM architectures have been tested <ref type="bibr" target="#b39">[40]</ref>. While LSTM networks are highly successful in various applications, the central memory cell architecture was not modified since 2000 <ref type="bibr" target="#b111">[112]</ref>. A memory cell architecture without peepholes is depicted in <ref type="figure" target="#fig_7">Figure A10</ref>. In our definition of a LSTM network, all units of one kind are pooled to a vector: z is the vector of cell input activations, i is the vector of input gate activations, f is the vector of forget gate activations, c is the vector of memory cell states, o is the vector of output gate activations, and y is the vector of cell output activations. We assume to have an input sequence, where the input vector at time t is x t . The matrices W z , W i , W f , and W o correspond to the weights of the connections between inputs and cell input, input gate, forget gate, and output gate, respectively. The vectors b z , b i , b f , and b o are the bias vectors of cell input, input gate, forget gate, and output gate, respectively. The activation functions are g for the cell input, h for the cell state, and σ for the gates, where these functions are evaluated in a component-wise manner if they are applied to vectors. Typically, either the sigmoid 1 1+exp(−x) or tanh are used as activation functions. denotes the point-wise multiplication of two vectors. Without peepholes, the LSTM memory cell forward pass rules are (see <ref type="figure" target="#fig_7">Figure A10</ref>):</p><formula xml:id="formula_265">z t = g W z x t + b z cell input (A326) i t = σ W i x t + b i input gate (A327) f t = σ W f x t + b f forget gate (A328) c t = i t z t + f t c t−1 cell state (A329) o t = σ W o x t + b o output gate (A330) y t = o t h c t cell output (A331)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A8.3 Long-Term Dependencies vs. Uniform Credit Assignment</head><p>The LSTM network has been proposed with the aim to learn long-term dependencies in sequences which span over long intervals <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b50">51]</ref>. However, besides extracting long-term dependencies, LSTM memory cells have another, even more important, advantage in sequence learning: as already described in the early 1990s, LSTM memory cells allow for uniform credit assignment, that is, the propagation of errors back to inputs without scaling them <ref type="bibr" target="#b48">[49]</ref>. For uniform credit assignment of current LSTM architectures, the forget gate f must be one or close to one. A memory cell without an input gate i just sums up all the squashed inputs it receives during scanning the input sequence. Thus, such a memory cell is equivalent to a unit that sees all sequence elements at the same time, as has been shown via the "Ersatzschaltbild" <ref type="bibr" target="#b48">[49]</ref>. If an output error occurs only at the end of the sequence, such a memory cell, via backpropagation, supplies the same delta error at the cell input unit z at every time step. Thus, all inputs obtain the same credit for producing the correct output and are treated on an equal level and, consequently, the incoming weights to a memory cell are adjusted by using the same delta error at the input unit z.</p><p>In contrast to LSTM memory cells, standard recurrent networks scale the delta error and assign different credit to different inputs. The more recent the input, the more credit it obtains. The first inputs of the sequence are hidden from the final states of the recurrent network. In many learning tasks, however, important information is distributed over the entire length of the sequence and can even occur at the very beginning. For example, in language-and text-related tasks, the first words are often important for the meaning of a sentence. If the credit assignment is not uniform along the input sequence, then learning is very limited. Learning would start by trying to improve the prediction solely by using the most recent inputs. Therefore, the solutions that can be found are restricted to those that can be constructed if the last inputs are considered first. Thus, only those solutions are found that are accessible by gradient descent from regions in the parameter space that only use the most recent input information. In general, these limitations lead to sub-optimal solutions, since learning gets trapped in local optima. Typically, these local optima correspond to solutions which efficiently exploit the most recent information in the input sequence, while information way back in the past is neglected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A8.4 Special LSTM Architectures for contribution Analysis A8.4.1 LSTM for Integrated Gradients</head><p>For Integrated Gradients contribution analysis with LSTM, we make following assumptions:</p><p>(A1) f t = 1 for all t. That is the forget gate is always 1 and nothing is forgotten. We assume uniform credit assignment, which is ensured by the forget gate set to one.</p><p>(A2) o t = 1 for all t. That is the output gate is always 1 and nothing is forgotten.</p><p>(A3) We set h = a h tanh with a h = 1, 2, 4.</p><p>(A4) We set g = a g tanh with a g = 1, 2, 4.</p><p>(A5) The cell input gate z is only connected to the input but not to other memory cells. W z has only connections to the input.</p><p>(A6) The input gate i is not connected to the input, that is, W i has only connections to other memory cells. This ensures that LRP assigns relevance only via z to the input.</p><p>(A7) The input gate i has a negative bias, that is, b i &lt; 0. The negative bias reduces the drift effect, that is, the memory content c either increases or decreases over time. Typical values are b i = −1, −2, −3, −4, −5.</p><p>(A8) The memory cell content is initialized with zero at time t = 0, that is, c 0 = 0. The resulting LSTM forward pass rules for Integrated Gradients are:</p><formula xml:id="formula_266">z t = a g σ W z x t + b z cell input (A332) i t = σ W i x t + b i input gate (A333) c t = i t z t + c t−1 cell state (A334) y t = a h tanh c t cell output (A335)</formula><p>See <ref type="figure" target="#fig_7">Figure A11</ref> which depicts these forward pass rules for Integrated Gradients.  <ref type="figure" target="#fig_7">Figure A11</ref>: LSTM memory cell used for Integrated Gradients (IG). Forget gates and output gates are set to 1 since they can modify all cell inputs at times after they have been observed, which can make the dynamics highly nonlinear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A8.4.2 LSTM for LRP</head><p>LRP has already been used for LSTM in order to identify important terms in sentiment analysis <ref type="bibr" target="#b0">[1]</ref>. In texts, positive and negative terms with respect to the topic could be identified. For LRP contribution analysis with LSTM, we make following assumptions:</p><p>(A1) f t = 1 for all t. That is the forget gate is always 1 and nothing is forgotten. We assume uniform credit assignment, which is ensured by the forget gate set to one. (A2) g &gt; 0, that is, g is positive. For example we can use a sigmoid σ(x) = a g 1 1+exp(−x) : g(x) = a g σ(x), with a g = 2, 3, 4. Methods like LRP have problems with negative contributions which cancel with positive contributions <ref type="bibr" target="#b83">[84]</ref>. With a positive g all contributions are positive. The cell input z (the function g) has a negative bias, that is, b z &lt; 0. This is important to avoid the drift effect. The drift effect is that the memory content only gets positive contributions which lead to an increase of c over time. Typical values are b z = −1, −2, −3, −4, −5. (A3) We want to ensure that h(0) = 0. If the memory content is zero then nothing is transferred to the next layer. Therefore we set h = a h tanh with a h = 1, 2, 4.</p><p>(A4) The cell input gate z is only connected to the input but not to other memory cells. W z has only connections to the input. This ensures that LRP assigns relevance z to the input and z is not disturbed by redistributing relevance to the network. (A5) The input gate i is not connected to the input, that is, W i has only connections to other memory cells. This ensures that LRP assigns relevance only via z to the input. (A6) The output gate o is not connected to the input, that is, W o has only connections to other memory cells. This ensures that LRP assigns relevance only via z to the input. (A7) The input gate i has a negative bias, that is, b i &lt; 0. Like with the cell input the negative bias avoids the drift effect. Typical values are b i = −1, −2, −3, −4. (A8) The output gate o may also have a negative bias, that is, b o &lt; 0. This allows to bring in different memory cells at different time points. It is related to resource allocation. (A9) The memory cell content is initialized with zero at time t = 0, that is, c 0 = 0. The memory cell content c t is non-negative c t 0 since z 0 and i 0. The resulting LSTM forward pass rules for LRP are:</p><formula xml:id="formula_267">z t = a g σ W z x t + b z cell input (A336) i t = σ W i x t + b i input gate (A337) c t = i t z t + c t−1 cell state (A338) o t = σ W o x t + b o output gate (A339) y t = o t a h tanh c t cell output (A340)</formula><p>See <ref type="figure" target="#fig_1">Figure A12</ref> which depicts these forward pass rules for LRP. However, gates may be used while no relevance is given to them which may lead to inconsistencies.</p><p>LRP and Contribution Propagation for LSTM. We analyze Layer-wise Relevance Propagation (LRP) and Contribution Propagation for LSTM networks. A single memory cell can be described by:</p><formula xml:id="formula_268">c t = i t z t + c t−1 .<label>(A341)</label></formula><p>Here we treat i t like a weight for z t and c t−1 has weight 1.</p><p>For positive values of i t , z t , and c t−1 , both LRP and contribution propagation leads to</p><formula xml:id="formula_269">R c t ←y t = R y t (A342) R c t = R c t ←c t+1 + R c t ←y t (A343) R c t−1 ←c t = c t−1 c t R c t (A344) R z t ←c t = i t z t c t R c t .<label>(A345)</label></formula><p>Since we predict only at the last step t = T , we have R y t = 0 for t &lt; T . For t = T we obtain</p><formula xml:id="formula_270">R c T = R y T , since R c T ←c T +1 = 0.</formula><p>We obtain for t = 1 . . . T :</p><formula xml:id="formula_271">R c T = R y T (A346) R c t−1 = c t−1 c t R c t<label>(A347)</label></formula><p>which gives</p><formula xml:id="formula_272">R c t = R y T T τ =t+1 c τ −1 c τ = c t c T R y T<label>(A348)</label></formula><p>and consequently as c 0 = 0 we obtain  . Data flow is either "feed-forward" without delay or "recurrent" with an one-step delay. External input reaches the LSTM network only via the cell input z. All gates only receive recurrent input, that is, from other memory cells.</p><formula xml:id="formula_273">R c 0 = 0 ,<label>(A349)</label></formula><formula xml:id="formula_274">R z t = i t z t c T R y T .<label>(A350</label></formula><p>Since we assume c 0 = 0, we have</p><formula xml:id="formula_275">c T = T t=1 i t z t<label>(A351)</label></formula><p>and therefore</p><formula xml:id="formula_276">R z t = i t z t T τ =1 i τ z τ R y T .<label>(A352)</label></formula><p>Therefore the relevance R y T is distributed across the inputs z t for t = 1 . . . T − 1, where input z t obtains relevance R z t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A8.4.3 LSTM for Nondecreasing Memory Cells</head><p>contribution analysis is made simpler if memory cells are nondecreasing since the contribution of each input to each memory cells is well defined. The problem that a negative and a positive input cancels each other is avoided. For nondecreasing memory cells and contribution analysis with LSTM, we make following assumptions:</p><p>(A1) f t = 1 for all t. That is the forget gate is always 1 and nothing is forgotten. We assume uniform credit assignment, which is ensured by the forget gate set to one. (A2) g &gt; 0, that is, g is positive. For example we can use a sigmoid σ(x) = a g cell input z (the function g) has a negative bias, that is, b z &lt; 0. This is important to avoid the drift effect. The drift effect is that the memory content only gets positive contributions which lead to an increase of c over time. Typical values are b z = −1, −2, −3, −4, −5. (A3) We want to ensure that h(0) = 0. If the memory content is zero then nothing is transferred to the next layer. Therefore we set h = a h tanh with a h = 1, 2, 4. (A4) The cell input gate z is only connected to the input but not to other memory cells. W z has only connections to the input. (A5) The input gate i is not connected to the input, that is, W i has only connections to other memory cells. (A6) The output gate o is not connected to the input, that is, W o has only connections to other memory cells. (A7) The input gate i has a negative bias, that is, b i &lt; 0. Like with the cell input the negative bias avoids the drift effect. Typical values are b i = −1, −2, −3, −4. (A8) The output gate o may also have a negative bias, that is, b o &lt; 0. This allows to bring in different memory cells at different time points. It is related to resource allocation. (A9) The memory cell content is initialized with zero at time t = 0, that is, c 0 = 0. We ensured via the architecture that c t 0 and c t+1 c t , that is, the memory cells are positive and nondecreasing. The resulting LSTM forward pass rules for nondecreasing memory cells are:</p><formula xml:id="formula_277">z t = a g σ W z x t + b z cell input (A353) i t = σ W i x t + b i input gate (A354) c t = i t z t + c t−1 cell state (A355) o t = σ W o x t + b o output gate (A356) y t = o t a h tanh c t cell output (A357)</formula><p>See <ref type="figure" target="#fig_7">Figure A13</ref> for a LSTM memory cell that is nondecreasing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A8.4.4 LSTM without Gates</head><p>The most simple LSTM architecture for contribution analysis does not use any gates. Therefore complex dynamics that have to be treated in the contribution analysis are avoided. For LSTM without gates, we make following assumptions:</p><p>(A1) f t = 1 for all t. That is the forget gate is always 1 and nothing is forgotten.</p><p>(A2) o t = 1 for all t. That is the output gate is always 1.</p><p>(A3) i t = 1 for all t. That is the input gate is always 1.</p><p>(A4) g &gt; 0, that is, g is positive. For example we can use a sigmoid σ(x) = a g 1 1+exp(−x) : g(x) = a g σ(x), with a g = 2, 3, 4. With a positive g all contributions are positive. The cell input z (the function g) has a negative bias, that is, b z &lt; 0. This is important to avoid the drift effect. The drift effect is that the memory content only gets positive contributions which lead to an increase of c over time. Typical values are b z = −1, −2, −3, −4, −5. (A5) We want to ensure that h(0) = 0. If the memory content is zero then nothing is transferred to the next layer. Therefore we set h = a h tanh with a h = 1, 2, 4. (A6) The memory cell content is initialized with zero at time t = 0, that is, c 0 = 0.</p><p>The resulting LSTM forward pass rules are: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A9 Contribution Analysis</head><p>A9.1 Difference of Consecutive Predictions for Sequences General Approach. The idea is to assess the information gain that is induced by an input at a particular time step. This information gain is used for predicting the target at sequence end by determining the change in prediction. The input to a recurrent neural network is the sequence x = (x 1 , . . . , x d ) with target y d , which is only given at sequence end. The prefix sequence x t of length t d is x t = (x 1 , . . . , x t ). F predicts the target y d at every time step t:</p><formula xml:id="formula_278">F (x t ) = y d .<label>(A361)</label></formula><p>We can define the decomposition of F through contributions at different time steps</p><formula xml:id="formula_279">h 0 = F (x 0 ) , (A362) h t = F (x t ) − F (x t−1 ) for t &gt; 0 ,<label>(A363)</label></formula><p>where F (x 0 ) is a predefined constant. We have</p><formula xml:id="formula_280">F (x t ) = t τ =0 h τ .<label>(A364)</label></formula><p>We assume a loss function for F that is minimal if F ≡ F min predicts the expected y d</p><formula xml:id="formula_281">F min (x t ) = E [y d | x t ] .<label>(A365)</label></formula><p>Then</p><formula xml:id="formula_282">h 0 = E [y d ] , (A366) h t = E [y d | x t ] − E [y d | x t−1 ] for t &gt; 0 .<label>(A367)</label></formula><p>In this case, the contributions are the change in the expectation of the target that will be observed at sequence end. The contribution can be viewed as the information gain in time step t for predicting the target. If we cannot ensure that F predicts the target at every time step, then other contribution analysis methods must be employed. For attributing the prediction of a deep network to its input features several contribution analysis methods have been proposed. We consider Input Zeroing, Integrated Gradients (IG), and Layer-Wise Relevance Propagation (LRP).</p><p>Linear Models and Coefficient of Determination. We consider linear models and the average gain of information about the reward at sequence end if we go one time step further in the input sequence. By adding a variable, that is, another sequence element, the mean squared error (MSE) decreases, which is the amount by which the expectation improves due to new information. But by what amount does the MSE decrease in average? Here, we consider linear models. For linear models we are interested in how much the coefficient of determination increases if we add another variable, that is, if we see another input. We consider the feature vector x = (x 1 , x 2 , . . . , x k ) T from which the target y (the reward at sequence end) has to be predicted. We assume to have n pairs (x i , y i ), 1 i n, as training set. The prediction or estimation of y i from x i isŷ i withŷ i = F (x i ). The vector of all training labels is y = (y 1 , . . . , y n ) and the training feature matrix is X = (x 1 , . . . , x n ). We define the mean squared error (MSE) as</p><formula xml:id="formula_283">mse(y, X) = 1 n − 1 n i=1 (ŷ i − y i ) 2 .<label>(A368)</label></formula><p>The coefficient of determination R 2 is equal to the correlation between the target y and its prediction y. R 2 is given by:</p><formula xml:id="formula_284">R 2 = 1 − 1 n−1 n i=1 (ŷ i − y i ) 2 1 n−1 n i=1 (y i −ȳ) 2 = 1 − mse(y, X) s 2 y .<label>(A369)</label></formula><p>Therefore, R 2 is one minus the ratio of the mean squared error divided by the mean total sum of squares. R 2 is a strict monotonically decreasing function of the mean squared error. We will give a breakdown of the factors that determine how much each variable adds to R 2 [100, chapter 10.6, p. 263]. The feature vector x is expanded by one additional feature z: w = (x 1 , x 2 , . . . , x k , z) T = (x T , z) T . We want to know the increase in R 2 due to adding z. Therefore, we decompose w into x and z. The difference in coefficients of determination is the difference of the according MSEs divided by the empirical variance of y:</p><formula xml:id="formula_285">R 2 yw − R 2 yx = mse(y, W ) − mse(y, X) s 2 y .<label>(A370)</label></formula><p>We further need definitions:</p><p>• x = (x 1 , x 2 , . . . , x k ) T .</p><p>• w = (x 1 , x 2 , . . . , x k , z) T = (x T , z) T .</p><p>• The sample covariance between y and x is s yx = n i=1 (x i −x)(y i −ȳ)/(n − 1), wherē x = n i=1 x i /n andȳ = n i=1 y i /n are the sample means. The variance of x is s xx often written as s 2</p><p>x , the standard deviation squared: s x := √ s xx .</p><p>• The correlation between y and x is r yx = s yx /(s x s y ).</p><p>• The covariance matrix S xx of a vector x is the matrix with entries [S xx ] ij = s xixj .</p><p>• The covariance matrix R xx of a vector x is the matrix with entries [R xx ] ij = r xixj .</p><p>• The diagonal matrix D x = [diag(S xx )] 1/2 has a ith diagonal entry √ s xi and is the diagonal matrix of standard deviations of the components of x.</p><p>• R 2 yw is the squared multiple correlation between y and w. • R 2 yx is the squared multiple correlation between y and x. • R 2 zx = s T zx S −1 xx s zx /s 2 z = r T zx R −1 xx r zx is the squared multiple correlation between z and x. • r yz is the simple correlation between y and z: r yz = s yz /(s y s z ).</p><p>• r yx = (r yx1 , r yx2 , . . . , r yx k ) T = s −1 y D −1 x S yx is the vector of correlations between y and x.</p><p>• r zx = (r zx1 , r zx2 , . . . , r zx k ) T = s −1 z D −1 x S zx is the vector of correlations between z and x.</p><p>•β * zx = R −1 xx r zx is the vector of standardized regression coefficients (beta weights) of z regressed on x.</p><p>• The parameter vector is partitioned into the constant β 0 and β 1 via β = (β 0 , β 1 , . . . , β m ) T = (β 0 , β T 1 ) T . We have for the maximum likelihood estimatê</p><formula xml:id="formula_286">β 0 =ȳ − s T yx S −1 xxx ,<label>(A371)</label></formula><formula xml:id="formula_287">β 1 = S −1 xx s yx .<label>(A372)</label></formula><p>The offsetβ 0 guaranteesȳ =ȳ, therefore, y Tȳ =ŷ Tȳ , sinceȳ =ȳ1:</p><formula xml:id="formula_288">y = 1 n n i=1ŷ i = 1 n n i=1 β 0 +β T 1 x i (A373) =ȳ − s T yx S −1 xxx + 1 n n i=1β T 1 x i =ȳ − s T yx S −1 xxx + s T yx S −1 xxx =ȳ .</formula><p>• The vector of standardized coefficientsβ * 1 arê</p><formula xml:id="formula_289">β * 1 = 1 s y D xβ1 = R −1 xx r yx .<label>(A374)</label></formula><p>The next theorem is Theorem 10.6 in Rencher and Schaalje <ref type="bibr" target="#b99">[100]</ref> and gives a breakdown of the factors that determine how much each variable adds to R 2 [100, Chapter 10.6, p. 263].</p><p>The information gain can now be expressed by the correlation r ey between the target y and the error e:</p><formula xml:id="formula_290">R 2 yw − R 2 yx = (r yz − r yz ) 2 1 − R 2 zx = 1 s 2 z s 2 y 1 (n−1) 2 ( n i=1 (ẑ i − z i )(y i −ȳ)) 2 1 n−1 n i=1 (ẑi − zi) 2 1 n−1 n i=1 (zi −z) 2 (A382) = 1 s 2 y 1 (n−1) 2 ( n i=1 (ẑ i − z i )(y i −ȳ)) 2 1 n−1 n i=1 (ẑ i − z i ) 2 = r 2</formula><p>ey . The information gain is the squared correlation r 2 ey between the target y and the error e. The information gain is the information in z about y, which is not contained in x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A9.2 Input Zeroing</head><p>The simplest contribution analysis method is Input Zeroing, where just an input is set to zero to determine its contribution to the output. Input Zeroing sets a particular input x i to zero and then computes the network's output. For the original input x = (x 1 , . . . , x d ) and the input with x i = 0, i.e. x i = (x 1 , . . . , x i−1 , 0, x i+1 , . . . , x d ), we compute ∆x i = F (x) − F (x i ) to obtain the contribution of x i . We obtain for the difference of F (x) to the baseline of average zeroing 1</p><formula xml:id="formula_291">d d i=1 F (x i ): F (x) − 1 d d i=1 F (x i ) = 1 d d i=1 ∆x i .<label>(A383)</label></formula><p>The problem is that the F (x i ) have to be computed d-times, that is, for each input component zeroed out.</p><p>Input Zeroing does not recognize redundant inputs, i.e. each one of the inputs is sufficient to produce the output but if all inputs are missing at the same time then the output changes. In contrast, Integrated Gradients (IG) and Layer-Wise Relevance Propagation (LRP) detect the relevance of an input even if it is redundant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A9.3 Integrated Gradients</head><p>Integrated gradients is a recently introduced method <ref type="bibr" target="#b124">[125]</ref>. Integrated gradients decomposes the difference F (x) − F (x) between the network output F (x) and a baseline F (x):</p><formula xml:id="formula_292">F (x) − F (x) = d i=1 (x i −x i ) 1 t=0 ∂F ∂x i (x + t(x −x)) dt (A384) ≈ d i=1 (x i −x i ) 1 m m k=1</formula><p>∂F ∂x i (x + (k/m)(x −x)) .</p><p>In contrast to previous approaches, we have F and its derivative to evaluate only m-times, where m &lt; d.</p><p>The equality can be seen if we define h = x −x and g : [0, 1] → R g(t) = F (x + th) .</p><p>Consequently, we have F (x + h) − F (x) = g(1) − g(0) = 1 0 g (t) dt (A386)</p><formula xml:id="formula_294">= 1 0 d i=1 ∂F ∂x i (x + th) h i dt = d i=1 1 0 ∂F ∂x i (x + th) dt h i .<label>(A387)</label></formula><p>For the final reward decomposition, we obtain</p><formula xml:id="formula_295">F (x) = d i=1 (x i −x i ) 1 m m k=1 ∂F ∂x i (x + (k/m)(x −x)) + 1 d F (x) .<label>(A388)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A9.4 Layer-Wise Relevance Propagation</head><p>Layer-Wise Relevance Propagation (LRP) <ref type="bibr" target="#b2">[3]</ref> has been introduced to interpret machine learning models. LRP is an extension of the contribution-propagation algorithm <ref type="bibr" target="#b70">[71]</ref> based on the contribution approach <ref type="bibr" target="#b93">[94]</ref>. Recently "excitation backprop" was proposed <ref type="bibr" target="#b146">[147]</ref>, which is like LPR but uses only positive weights and shifts the activation function to have non-negative values. Both algorithms assign a relevance or importance value to each node of a neural network which describes how much it contributed to generating the network output. The relevance or importance is recursively propagated back: A neuron is important to the network output if it has been important to its parents, and its parents have been important to the network output. LRP moves through a neural network like backpropagation: it starts at the output, redistributes the relevance scores of one layer to the previous layer until the input layer is reached. The redistribution procedure satisfies a local relevance conservation principle. All relevance values that a node obtains from its parents will be redistributed to its children. This is analog to Kirchhoff's first law for the conservation of electric charge or the continuity equation in physics for transportation in general form. LRP has been used for deep neural networks (DNN) <ref type="bibr" target="#b83">[84]</ref> and for recurrent neural networks like LSTM <ref type="bibr" target="#b0">[1]</ref>. We consider a neural network with activation x i for neuron i. The weight from neuron l to neuron i is denoted by w il . The activation function is g and net i is the netinput to neuron i with bias b i . We have following forward propagation rules:</p><formula xml:id="formula_296">net i = l w il x l ,<label>(A389)</label></formula><p>x i = f i (net i ) = g(net i + b i ) .</p><p>Let R i be the relevance for neuron i and R i←k the share of relevance R k that flows from neuron k in the higher layer to neuron i in the lower layer. The parameter z ik is a weighting for the share of R k of neuron k that flows to neuron i. We define R i←k as</p><formula xml:id="formula_298">R i←k = z ik l z lk R k .<label>(A391)</label></formula><p>The relative contributions z ik are previously defined as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b0">1]</ref>:</p><formula xml:id="formula_299">z ik = w ik x k .<label>(A392)</label></formula><p>Here, z ik is the contribution of x k to the netinput value net i . If neuron k is removed from the network, then z ik will be the difference to the original net i . The relevance R i of neuron i is the sum of relevances it obtains from its parents k from a layer above:</p><formula xml:id="formula_300">R i = k R i←k .<label>(A393)</label></formula><p>Furthermore, a unit k passes on all its relevance R k to its children, which are units i of the layer below:</p><formula xml:id="formula_301">R k = i R i←k .<label>(A394)</label></formula><p>It follows the conservation of relevance. The sum of relevances R k of units k in a layer is equal to the sum of relevances R i of units i of a layer below:</p><formula xml:id="formula_302">k R k = k i R i←k = i k R i←k = i R i .<label>(A395)</label></formula><p>The scalar output g(x) of a neural network with input x = (x 1 , . . . , x d ) is considered as relevance R which is decomposed into contributions R i of the inputs x i :</p><formula xml:id="formula_303">i R i = R = g(x) .<label>(A396)</label></formula><p>The decomposition is valid for recurrent neural networks, where the relevance at the output is distributed across the sequence elements of the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>90</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A9.4.1 New Variants of LRP</head><p>An alternative definition of z ik is</p><formula xml:id="formula_304">z ik = w ik (x k −x k ) ,<label>(A397)</label></formula><p>wherex k is the mean of x k across samples. Therefore, (x k −x k ) is the contribution of the actual sample to the variance of x k . This in turn is related to the information carried by x k . Here, z ik is the contribution of x k to the variance of net i . However, we can have negative values of (x k −x k ) which may lead to negative contributions even if the weights are positive. Another alternative definition of z ik is</p><formula xml:id="formula_305">z ik = f i (net i ) − f i (net i − w ik x k ) .<label>(A398)</label></formula><p>Here, z ik is the contribution of x k to the activation value x i = f i (net i ). If neuron k is removed from the network, then z ik will be the difference to the original x i . If f i is strict monotone increasing and x k &gt; 0, then positive weights w ik will lead to positive values and negative weights w ik to negative values. Preferred Solution:</p><p>A definition of z ik is</p><formula xml:id="formula_306">z ik = w ik (x k − x min ) ,<label>(A399)</label></formula><p>where x min is the minimum of x k either across samples (mini-batch) or across time steps. The difference (x k − x min ) is always positive. Using this definition, activation functions with negative values are possible, like for excitation backprop <ref type="bibr" target="#b146">[147]</ref>. The minimal value is considered as default off-set, which can be included into the bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A9.4.2 LRP for Products</head><p>Here we define relevance propagation for products of two units. We assume that z = x 1 x 2 with x 1 &gt; 0 and x 2 &gt; 0. We view x 1 and x 2 as units of a layer below the layer in which z is located. Consequently, R z has to be divided between x 1 and x 2 , which gives the conservation rule</p><formula xml:id="formula_307">R z = R x1←z + R x2←z .<label>(A400)</label></formula><p>Alternative 1:</p><formula xml:id="formula_308">R x1←z = 0.5 R z (A401) R x2←z = 0.5 R z .<label>(A402)</label></formula><p>The relevance is equally distributed. Preferred Solution: Alternative 2: The contributions according to the deep Taylor decomposition around (a, a) are ∂z ∂x 1 (a,a) (x 1 − a) = (x 1 − a) a ,</p><p>∂z ∂x 2 (a,a) (x 2 − a) = a (x 2 − a) .</p><p>We compute the relative contributions:</p><formula xml:id="formula_311">(x 1 − a) a (x 1 − a) a + a (x 2 − a) = x 1 − a (x 1 + x 2 − 2 a) ,<label>(A405)</label></formula><formula xml:id="formula_312">(x 2 − a) a (x 1 − a) a + a (x 2 − a) = x 2 − a (x 1 + x 2 − 2 a) .<label>(A406)</label></formula><p>For lim a→0 we obtain x 1 /(x 1 + x 2 ) and x 2 /(x 1 + x 2 ) as contributions.</p><p>We use this idea but scale x 1 and x 2 to the range [0, 1]: </p><formula xml:id="formula_313">R x1←z =</formula><p>The relevance is distributed according to how close the maximal value is achieved and how far away it is from the minimal value. Alternative 3: </p><formula xml:id="formula_315">R</formula><p>All ln-values are negative, therefore the fraction in front of R z is positive. x 1 = x min leads to a zero relevance for x 1 . The ratio of the relevance for x 1 increases to 1 when x 1 approaches x max . The relevance is distributed according to how close the maximal value is achieved. We assume that the maximal value is a saturating value, therefore we use ln, the natural logarithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A9.5 Variance Considerations for contribution Analysis</head><p>We are interested how the redistributed reward affects the variance of the estimators. We consider (A) the difference of consecutive predictions is the redistributed reward, (B) integrated gradients (IG), and (C) layer-wise relevance propagation (LRP). For (A) the difference of consecutive predictions is the redistributed reward, all variance is moved to the final correction. However imperfect g and variance cannot be distinguished. For (B) integrated gradients (IG) the redistributed rewards depend on future values. Therefore the variance can even be larger than in the original MDP. For (C) layer-wise relevance propagation (LRP) the variance is propagated back without decreasing or increasing if the actual return is used as relevance. If the prediction is used as relevance and a final correction is used then the variance is moved to the final prediction but new variance is injected since rewards depend on the future path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A10 Reproducibility Checklist</head><p>We followed the reproducibility checklist <ref type="bibr" target="#b91">[92]</ref> and point to relevant sections.</p><p>For all models and algorithms presented, check if you include:</p><p>• A clear description of the mathematical setting, algorithm, and/or model. Description of mathematical settings starts at paragraph MDP Definitions and Return-Equivalent Sequence-Markov Decision Processes (SDPs). Description of novel learning algorithms starts at paragraph Novel Learning Algorithms Based on Reward Redistributions.</p><p>• An analysis of the complexity (time, space, sample size) of any algorithm. Plots in <ref type="figure" target="#fig_7">Figure 1</ref> show the number of episodes, i.e. the sample size, which are needed for convergence to the optimal policies. They are evaluated for different algorithms and delays in all artificial tasks. For Atari games, the number of samples corresponds to the number of game frames. See paragraph Atari Games. We further present a bias-variance analysis of TD and MC learning in Section A3.1 and Section A3.2 in the appendix.</p><p>• A link to a downloadable source code, with specification of all dependencies, including external libraries. https://github.com/ml-jku/baselines-rudder</p><p>For any theoretical claim, check if you include:</p><p>• A statement of the result. The main theorems:</p><p>-Theorem 1 -Theorem 2 -Theorem 3</p><p>Additional supporting theorems can be found in the proof section of the appendix A2.</p><p>• A clear explanation of any assumptions. The proof section A2 in the appendix covers all the assumptions for the main theorems.</p><p>• A complete proof of the claim. Proof of the main theorems are moved to the appendix.</p><p>-Proof of Theorem 1 can be found after Theorem A2 in the appendix.</p><p>-Proof of Theorem 2 can be found after Theorem A4 in the appendix.</p><p>-Proof of Theorem 3 can be found after Theorem A5 in the appendix.</p><p>Proofs for additional theorems can also be found in this appendix.</p><p>For all figures and tables that present empirical results, check if you include:</p><p>• A complete description of the data collection process, including sample size. For artificial tasks the environment descriptions can be found in section Artificial Tasks in the main paper. For Atari games, we use the standard sampling procedures as in OpenAI Gym <ref type="bibr" target="#b17">[18]</ref> (description can be found in paragraph Atari Games).</p><p>• A link to a downloadable version of the dataset or simulation environment. Link to our repository: https://github.com/ml-jku/rudder</p><p>• An explanation of any data that were excluded, description of any pre-processing step For Atari games, we use the standard pre-processing described in <ref type="bibr" target="#b79">[80]</ref>.</p><p>• An explanation of how samples were allocated for training / validation / testing. For artificial tasks, description of training and evaluation are included in section A4.1 . For Atari games, description of training and evaluation are included Section A4.1.</p><p>• The range of hyper-parameters considered, method to select the best hyper-parameter configuration, and specification of all hyper-parameters used to generate results. A description can be found at paragraph PPO model in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>93</head><p>• The exact number of evaluation runs. For artificial tasks evaluation was performed during training runs. See <ref type="figure" target="#fig_7">Figure 1</ref>. For Atari games see paragraph Atari Games. We also provide a more detailed description in Section A4.1 and Section A4.2 in the appendix. • A description of how experiments were run. For artificial task, description can be found at 4. For Atari games, description starts at paragraph Atari Games. We also provide a more detailed description in Section A4.1 and Section A4.2 in the appendix. • A clear definition of the specific measure or statistics used to report results.</p><p>For artificial tasks, see section 4. For Atari games, see section A4.2 and the caption of <ref type="table">Table  1</ref>. We also provide a more detailed description in Section A4.1 and Section A4.2 in the appendix. • Clearly defined error bars.</p><p>For artificial tasks, see caption of <ref type="figure" target="#fig_7">Figure 1</ref>, second line. For Atari games we show all runs in <ref type="figure" target="#fig_22">Figure A8</ref> in the appendix. • A description of results with central tendency (e.g. mean) &amp; variation (e.g. stddev).</p><p>An exhaustive description of the results including mean, variance and significant test, is included in <ref type="table" target="#tab_4">Table A1</ref>, <ref type="table" target="#tab_5">Table A2</ref> and <ref type="table" target="#tab_6">Table A3</ref> in Section A4.1 in the appendix. • A description of the computing infrastructure used.</p><p>We distributed all runs across 2 CPUs per run and 1 GPU per 4 runs for Atari experiments. We used various GPUs including GTX 1080 Ti, TITAN X, and TITAN V. Our algorithm takes approximately 10 days.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2displays how RUDDER redistributes rewards to key events in Bowling. At delayed reward Atari games, RUDDER considerably increases the scores compared to the PPO baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2: RUDDER redistributes rewards to key events in the Atari game Bowling. Originally, rewards are delayed and only given at episode end. The first 120 out of 200 frames of the episode are shown. RUDDER identifies key actions that steer the ball to hit all pins.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>[x] B the block of B to which x belongs. Any function f from a set X to a set Y induces a partition (or equivalence relation) on X, with [x] f = [x ] f if and only if f (x) = f (x ). We now can define homomorphic MDPs. Definition A1 (Ravindran and Barto [98, 99]). An MDP homomorphism h from an MDP P = (S, A, R, p, γ) to an MDPP = (S,Ã,R,p,γ) is a a tuple of surjections (f, g 1 , g 2 , . . . , g n ) (n is number of states), with h(s, a) = (f (s), g s (a)), where f : S →S and g s : A s →Ã f (s) for s ∈ S (A s are the admissible actions in state s andÃ f (s) are the admissible actions in states). Furthermore, for all s, s ∈ S, a ∈ A s :p (f (s ) | f (s), g s (a)) = s ∈[s ] f p(s | s, a) , (A29) p(r | f (s), g s (a)) = p(r | s, a) . (A30) We use [s] f = [s ] f if and only if f (s) = f (s ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>= (s , ρ ), ρ = r + ρ, andr is constant, the valuesS t+1 =s andR t+1 =r can be computed from R t+1 = r, ρ, and S t+1 = s . Therefore, we havẽ p(s ,r | s, ρ, a) =p(s , ρ ,r | s, ρ, a) = p(s , r | s, a) . (A54) For t &lt; T , we haver = 0 and ρ = r + ρ, where we set r = r t+1 : qπ(s, a) = q π (s, a) + r | s, a) r + a π(a | s ) q π (s , a ) + , ρ ,r | s, ρ, a) r + a π(a | s ) q π (s , a ) + r |s, a) r + a π(a | s ) q π (s , a ) + r |s, a) r + a π(a | s ) q π (s , a ) + (s ,r |s, a) r + a π(a |s )qπ(s , a ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>r | s, a) r + a π(a | s ) q π (s , a ) + ρ | s, ρ, a) r + a π(a | s ) q π (s , a ) + | s ) q π (s , a ) = s ,ρ p(s |s, a) ρ + a π(a | s ) q π (s , a ) = s ,ρ p(s |s, a) [ρ + 0] = s ,rp (s |s, a) r + a π(a |s )qπ(s , a ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>E s ,a (f (s , a )) = s p(s | s, a) a π(a | s )f (s , a ), and analogously Var s ,a and Var r , the next Theorem A7 gives mean and variance V π (s, a) = Var π [G t | s, a] of sampling returns from an MDP. Theorem A7. The mean q π and variance V π of sampled returns from an MDP are q π (s, a) = s ,r p(s , r | s, a) r + γ a π(a | s )q π (s , a ) = r(s, a) + γE s ,a [q π (s , a ) | s, a] , V π (s, a) = Var r [r | s, a] + γ 2 (E s ,a [V π (s , a ) | s, a] + Var s ,a [q π (s , a ) | s, a]) . (A180) Proof. The Bellman equation for Q-values is q π (s, a) = s ,r p(s , r | s, a) r + γ a π(a | s ) q π (s , a ) (A181) = r(s, a) + γ E s ,a [q π (s , a ) | s, a] . This equation gives the mean if drawing one sample. We use r(s, a) = r r p(r | s, a) , (A182) r 2 (s, a) = r r 2 p(r | s, a) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>V= r 2</head><label>2</label><figDesc>π (s, a) = M π (s, a) − q π (s, a) (s, a) − (r(s, a)) 2 + γ 2 E s ,a [M π (s , a ) | s, a] − γ 2 E 2 s ,a [q π (s , a ) | s, a] = Var r [r | s, a] + γ 2 E s ,a M π (s , a ) − q π (s , a ) 2 | s, a − E 2 s ,a [q π (s , a ) | s, a] + E s ,a q π (s , a ) 2 | s, a = Var r [r | s, a] + γ 2 (E s ,a [V π (s , a ) | s, a] + Var s ,a [q π (s , a ) | s, a]) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>• ( 1 )</head><label>1</label><figDesc>the immediate variance Var r [r | s, a] of the immediate reward stemming from the probabilistic reward p(r | s, a), • (2) the local variance γ 2 Var s ,a [q π (s , a ) | s, a] from state transitions p(s | s, a) and new actions π(a | s ), • (3) the expected variance γ 2 E s ,a [V π (s , a ) | s, a] of the next Q-values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>• ( 1 )</head><label>1</label><figDesc>the immediate variance Var r [r | s, a] is zero for deterministic immediate reward, • (2) the local variance γ 2 Var s ,a [q π (s , a ) | s, a] is zero for (i) deterministic state transitions and deterministic policy and for (ii) γ = 0 (only immediate reward),• (3) the expected variance γ 2 E s ,a [V π (s , a ) | s,a] of the next Q-values is zero for (i) temporal difference (TD) learning, since the next Q-values are fixed and set to their current estimates (if just one sample is drawn) and for (ii) γ = 0 (only immediate reward). The local variance Var s ,a [q π (s , a ) | s, a] is the variance of a linear combination of Q-values weighted by a multinomial distribution s p(s | s, a) a π(a | s ). The local variance is Var s ,a [q π (s , a ) | s, a] = s p(s | s, a) a π(a | s ) q π (s , a ) | s, a) a π(a | s ) q π (s , a ) 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>For Monte Carlo and delayed rewards, the immediate variance Var r [r | s, a] = 0 except for the last step of the episode. The delayed reward increases the variance of Q-values according to Eq. (A180). Sample Distribution Used by Temporal Difference and Monte Carlo. Monte Carlo (MC) sampling uses the true mean and true variance, where the true mean is q π (s, a) = r(s, a) + γ E s ,a [q π (s , a ) | s, a] (A187) and the true variance is V π (s, a) = Var r [r | s, a] + γ 2 (E s ,a [V π (s , a ) | s, a] + Var s ,a [q π (s , a ) | s, a]) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>a ) | s, a] + Var s ,a [q π (s , a ) | s, a]) . (A205) This is a Bellman equation of the variance. For undiscounted reward γ = 1, we obtain V π (s, a) = Var r [r | s, a] + E s ,a [V π (s , a ) | s, a] + Var s ,a [q π (s , a ) | s, a] . (A206) If we define the "on-site" variance ω as ω(s, a) = Var r [r | s, a] + Var s ,a [q π (s , a ) | s, a] , (A207) we get V π (s, a) = ω(s, a) + E s ,a [V π (s , a ) | s, a] . (A208) This is the solution of the general formulation of the Bellman operator. The Bellman operator is defined component-wise for any variance V as T π [V ] (s, a) = ω(s, a) + E s ,a [V (s , a ) | s, a] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>ω(s, a) = Var r [r | s, a] + Var s ,a [q π (s , a ) | s, a] (A214) ω(s, a) = Var r [r | s, a] + Var s ,a [q π (s , a ) | s, a] then V π (s, a) V π (s, a) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure A1 :</head><label>A1</label><figDesc>Examples of how affected states (cyan) affect states in a previous time step (indicated by cyan edges) starting with n 5 = 1 (one affected state). The left panel shows no overlap since affected states in s t−1 connect only to one affected state in s t . The right panel shows some overlap since affected states in s t−1 connect to multiple affected states in s t .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure A2 :</head><label>A2</label><figDesc>The function 1 − 1 − ct Nt−1 nt which scales N t−1 in Theorem A10. This function determines the growth of a k , which is exponentially at the beginning, and then linearly when the function approaches 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure</head><label></label><figDesc>A3: (a) Experimental evaluation of bias and variance of different Q-value estimators on the Grid World. (b) Normalized bias reduction for different delays. Right: Average variance reduction for the 10th highest values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>1 Figure A4 :</head><label>1A4</label><figDesc>The environment is an MDP consisting of two actions a ∈ A = {+, −}, an initial state s 0 , two charged states s + , s − , two neutral states s ⊕ , s , and a final state s f . After the first action a 0 ∈ A = {+, −} in state s 0 , the agent transits to state s + for action a 0 = + and to s − for action a 0 = −. Subsequent state transitions are probabilistic and independent on actions. With probability p C the agent stays in the charged states s + or s − , and with probability (1 − p C ) it transits from s + or s − to the neutral states s ⊕ or s , respectively. The probability to go from neutral states to charged states is p C , and the probability to stay in neutral states is (1 − p C ). Probabilities to transit from s + State transition diagram for The Choice task. The diagram is a simplification of the actual MDP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure A6 :</head><label>A6</label><figDesc>Comparison of performance of RUDDER with GAE (RUDDER+GAE) and without GAE (RUDDER) and PPO with GAE (PPO) on artificial task V with respect to the learning time in episodes (median of 100 trials) in log scale vs. the delay of the reward. The shadow bands indicate the 40% and 60% quantiles. Again, RUDDER significantly outperforms all other methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>Reward Scaling. As described in the main paper, rewards for the PPO baseline and RUDDER are scaled based on the maximum return per sample encountered during training so far. With i samples sampled from the environment and a maximum return of g max i = max 1 j i {|g j |} encountered, the scaled reward r new is r new = 10 r g max i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>t+10 i=tr i , and ( 3 )</head><label>3</label><figDesc>the prediction of the accumulated reward in the next 50 frames t+50 i=tr i , resulting in the final auxiliary loss L a as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>a speeds up learning by adding more information about the original immediate rewards to the updates. The reward redistribution model is only trained on the lessons buffer. Training epochs on the lessons buffer are performed every 10 4 PPO updates or if a new sample was added to the lessons buffer. For each such training epoch, 8 samples are sampled from the lessons buffer. Training epochs are repeated until the reward redistribution quality is sufficient (quality &gt; 0) for all replayed samples in the last 5 training epochs. The reward redistribution model is not trained or used until the lessons buffer contains at least 32 samples and samples with different return have been encountered. Parameter values are listed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Figure A8 :</head><label>A8</label><figDesc>Training curves for PPO baseline and PPO with RUDDER over 200M game frames, 3 runs with different random seeds each. Curves show scores during training of a single agent that does not use safe exploration, smoothed using Locally Weighted Scatterplot Smoothing (y-value estimate using 20% of data with 10 residual-based re-weightings).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>average</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head></head><label></label><figDesc>(10). (iii) Q-value estimation using eligibility traces. (B) Policy gradient: Theorems are used as for Q-value estimation as in (A) but now the Q-values serve for policy gradient. (C) Q-learning: Here the properties in Theorem 3 and Theorem 4 are ignored.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>T</head><label></label><figDesc>π [q] (s, a) = r(s, a) + E s ,a [q(s , a )] , (A277) where r(s, a) = r r p(r | s, a) , (A278) E s ,a [q(s , a )] = s p(s | s, a) a π(a | s ) q(s , a ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>T</head><label></label><figDesc>π [q] (s, a) = g(s, a) + E s ,a [q(s , a )] .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>|T[q 1 ]</head><label>1</label><figDesc>(s, a) − T[q 2 ](s, | s ) q 1 (s , a ) − g(s, a) + s p(s | s, a) a π(a | s ) q 2 (s , a ) = max s,a s p(s | s, a) a π(a | s ) (q 1 (s , a ) − q 2 (s , a )) max s,a s p(s | s, a) a π(a | s ) |q 1 (s , a ) − q 2 (s , a )| max s ,a |q 1 (s , a ) − q 2 (s , a )| = q 1 − q 2 ∞ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>T</head><label></label><figDesc>π [q] (s, a) = g(s, a) + s p(s | s, a) a π(a | s ) q(s , a ) (A292) = g(s, a) + q T p(s, a) , where q is the vector with value q(s , a ) at position (s , a ) and p(s, a) is the vector with value p(s | s, a)π(a | s ) at position (s , a ). In vector notation we obtain the Bellman equation or Poisson equation. For the Poisson equation see Equation 33 to Equation 37 for the undiscounted case and Equation 34 and Equation 43 for the discounted case in Alexander Veretennikov, 2016, [137]. This form of the Poisson equation describes the Dirichlet boundary value problem. The Bellman equation or Poisson equation is T π [q] = g + P q , (A293) where P is the row-stochastic matrix with p(s | s, a)π(a | s ) at position ((s, a),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>)A7. 2 Q</head><label>2</label><figDesc>-value Transformations: Shaping Reward, Baseline, and Normalization The Bellman equation for the action-value function q π is q π (s, a) = s ,r p(s , r | s, a) r + a π(a | s ) q π (s , a ) .(A301)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head></head><label></label><figDesc>Inductive step (t → t − 1): Assumeqπ(s , a ) = q π (f (s ), a ) for the next states and next action a .qπ(s, a) = Eπ G t |s t =s, A t = a = s ,rp (s , r |s, a) r + a π(a |s )qπ(s , a ) = f (s ),g(s ),rp (s , r |s, a) r + a π(a |s )qπ(s , a ) (A316) = f (s ),G(r,s,a),rp (s , r |s, a) r + a π(a |s )qπ(s , a ) = f (s ),rp (s , r |s, a) r + a π(a |s )qπ(s , a ) = f (s ),r p(f (s ), r | f (s), a) r + a π(a | f (s ))qπ(s , a ) = f (s ),r p(f (s ), r | f (s), a) r + a π(a | f (s )) q π (f (s ), a ) = q π (f (s), a) . For the induction step 1 → 0 we usep 0 (s 0 , r) = p 0 (f (s 0 ), r) instead ofp(s , r |s, a) = p(f (s ), r | f (s), a). It follows thatq * (s, a) = q * (f (s), a), and thereforẽ π * (s) = argmax aq * (s, a) = argmax a q * (f (s), a) = π * (f (s)) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>Figure A12 :</head><label>A12</label><figDesc>LSTM memory cell used for Layer-Wise Relevance Propagation (LRP). z is the vector of cell input activations, i is the vector of input gate activations, c is the vector of memory cell states, o is the vector of output gate activations, and y is the vector of cell output activations. The activation functions are the sigmoid σ(x) = a g 1 1+exp(−x) and the cell state activation h(x) = a h tanh(x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure A14 :</head><label>A14</label><figDesc>z t = a g σ W z x t + b z cell input (A358) c t = z t + c t−1 cell state (A359) y t = a h tanh c t cell output(A360)SeeFigure A14for a LSTM memory cell without gates which perfectly distributes the relevance across the input. LSTM memory cell without gates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head></head><label></label><figDesc>xmax−xmin + x2−xmin xmax−xmin R z (A407) R x2←z = x2−xmin xmax−xmin x1−xmin xmax−xmin + x2−xmin xmax−xmin R z .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Theorem 7 and Corollary 9.1 in Givan et al. show the facts of Lemma A1. Li et al. give an overview over state abstraction and state aggregation for Markov decision processes, which covers homomorphic MDPs</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table A1 :</head><label>A1</label><figDesc>Number of episodes required by different methods to solve the grid world task with delayed reward. Numbers give the mean and the standard deviation over 100 trials. RUDDER with reward redistribution clearly outperforms all other TD methods.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Delay 10</cell><cell></cell><cell></cell><cell>Delay 15</cell><cell></cell><cell>Delay 20</cell></row><row><cell>RUDDER</cell><cell>3520.06</cell><cell cols="2">± 2343.79 p = 5.00E-01</cell><cell>3062.07</cell><cell cols="2">± 1278.92 p = 5.00E-01</cell><cell>3813.96</cell><cell>± 2738.18 p = 5.00E-01</cell></row><row><cell>MC</cell><cell>10920.64</cell><cell cols="2">± 7550.04 p = 5.03E-24</cell><cell>17102.89</cell><cell cols="2">± 12640.09 p = 1.98E-30</cell><cell>22910.85</cell><cell>± 19149.02 p = 1.25E-28</cell></row><row><cell>Q</cell><cell>66140.76</cell><cell cols="2">± 1455.33 p = 1.28E-34</cell><cell>115352.25</cell><cell cols="2">± 1962.20 p = 1.28E-34</cell><cell>171571.94</cell><cell>± 2436.25 p = 1.28E-34</cell></row><row><cell>Method</cell><cell></cell><cell>Delay 25</cell><cell></cell><cell></cell><cell>Delay 30</cell><cell></cell><cell>Delay 35</cell></row><row><cell>MC</cell><cell>39772</cell><cell cols="2">± 47460 p &lt; 1E-29</cell><cell>41922</cell><cell cols="2">± 36618 p &lt; 1E-30</cell><cell>50464</cell><cell>± 60318 p &lt; 1E-30</cell></row><row><cell>Q</cell><cell>234912</cell><cell cols="2">± 2673 p &lt; 1E-33</cell><cell>305894</cell><cell cols="2">± 2928 p &lt; 1E-33</cell><cell>383422</cell><cell>± 4346 p &lt; 1E-22</cell></row><row><cell>RUDDER</cell><cell>4112</cell><cell>± 3769</cell><cell></cell><cell>3667</cell><cell>± 1776</cell><cell></cell><cell>3850</cell><cell>± 2875</cell></row><row><cell>Method</cell><cell></cell><cell>Delay 40</cell><cell></cell><cell></cell><cell>Delay 45</cell><cell></cell><cell>Delay 50</cell></row><row><cell>MC</cell><cell>56945</cell><cell cols="2">± 54150 p &lt; 1E-30</cell><cell>69845</cell><cell cols="2">± 79705 p &lt; 1E-31</cell><cell>73243</cell><cell>± 70399 p = 1E-31</cell></row><row><cell>Q</cell><cell>466531</cell><cell cols="2">± 3515 p = 1E-22</cell><cell></cell><cell></cell><cell></cell></row><row><cell>RUDDER</cell><cell>3739</cell><cell>± 2139</cell><cell></cell><cell>4151</cell><cell>± 2583</cell><cell></cell><cell>3884</cell><cell>± 2188</cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell cols="2">Delay 100</cell><cell></cell><cell cols="2">Delay 500</cell></row><row><cell>MC</cell><cell></cell><cell>119568</cell><cell cols="2">± 110049 p &lt; 1E-11</cell><cell>345533</cell><cell cols="2">± 320232 p &lt; 1E-16</cell></row><row><cell>RUDDER</cell><cell></cell><cell>4147</cell><cell cols="2">± 2392</cell><cell>5769</cell><cell cols="2">± 4309</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table A2 :</head><label>A2</label><figDesc>Number of episodes required by different methods to solve the Trace-Back task with delayed reward. The numbers represent the mean and the standard deviation over 100 trials. RUDDER with reward redistribution significantly outperforms all other methods.</figDesc><table><row><cell>Method</cell><cell></cell><cell>Delay 6</cell><cell></cell><cell>Delay 8</cell><cell></cell><cell>Delay 10</cell></row><row><cell>Look-back I</cell><cell>6074</cell><cell>± 952 p = 1E-22</cell><cell>13112</cell><cell>± 2024 p = 1E-22</cell><cell>21715</cell><cell>± 4323 p = 1E-06</cell></row><row><cell>Look-back II</cell><cell>4584</cell><cell>± 917 p = 1E-22</cell><cell>9897</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A3 :</head><label>A3</label><figDesc>Cont. Number of episodes required by different methods to solve the Trace-Back task with delayed reward. The numbers represent the mean and the standard deviation over 100 trials. RUDDER with reward redistribution significantly outperforms all other methods.</figDesc><table><row><cell>A4.1.4 Task (IV): Charge-Discharge</cell><cell></cell><cell></cell></row><row><cell>The Charge-Discharge task depicted in</cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Delay 20</cell><cell>Delay 25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>.3.</figDesc><table><row><cell>v</cell><cell>π</cell><cell>g</cell></row><row><cell></cell><cell></cell><cell>a t</cell></row><row><cell cols="2">Dense Layer n=512</cell><cell cols="2">LSTM Layer</cell></row><row><cell cols="2">Conv.Layer2</cell><cell cols="2">Conv.Layer6</cell></row><row><cell cols="2">3x3x64, strides=1</cell><cell cols="2">3x3x64, strides=1</cell></row><row><cell cols="2">Conv.Layer1</cell><cell cols="2">Conv.Layer5</cell></row><row><cell cols="2">4x4x64, strides=2</cell><cell cols="2">4x4x64, strides=2</cell></row><row><cell cols="2">Conv.Layer0</cell><cell>Conv.Layer3</cell><cell>Conv.Layer4</cell></row><row><cell cols="2">8x8x32, strides=4</cell><cell>8x8x32, strides=4</cell><cell>8x8x32, strides=4</cell></row><row><cell cols="2">Stacked</cell><cell>Single</cell><cell>Delta-</cell></row><row><cell cols="2">Frames</cell><cell>Frame</cell><cell>Frame</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A4 :</head><label>A4</label><figDesc></figDesc><table /><note>Specifications of PPO and RUDDER architectures as shown in</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A5 :</head><label>A5</label><figDesc>Left: Update parameters for PPO model. Entropy coefficient is scaled via Proportional Control with the target entropy linearly annealed from 1 to 0 over the course of learning. Unless stated otherwise, default parameters of ppo2 implementation [21] are used. Right: Update parameters for reward redistribution model of RUDDER.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table A6 :</head><label>A6</label><figDesc>Scores on all 52 considered Atari games for the PPO baseline and PPO with RUDDER and the improvement by using RUDDER in percent (%). Agents are trained for 200M game frames (including skipped frames) with no-op starting condition, i.e. a random number of up to 30 no-</figDesc><table /><note>operation actions at the start of each game. Episodes are prematurely terminated if a maximum of 108K frames is reached. Scoring metrics are (a) average, the average reward per completed game throughout training, which favors fast learning [115] and (b) final, the average over the last 10 consecutive games at the end of training, which favors consistency in learning. Scores are shown for one agent without safe exploration.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>is called Bellman equation or Poisson equation. For the Poisson equation see Equation 33 to Equation 37 for the undiscounted case and Equation 34 and Equation 43 for the discounted case in Alexander Veretennikov, 2016, [137]. This form of the Poisson equation describes the Dirichlet boundary value problem. The Poisson equation is</figDesc><table><row><cell>q π (s, a) +ḡ = g(s, a) + E s ,a [q(s , a ) | s, a] ,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Theorem 7 and Corollary 9.1 in Givan et al. proved these properties<ref type="bibr" target="#b33">[34]</ref> by bisimulations (stochastically bisimilar MDPs). A homomorphism between MDPs maps a MDP to another one with corresponding reward and transitions probabilities. Ravindran and Barto have shown that solving the original MDP can be done by solving a homomorphic image<ref type="bibr" target="#b98">[99]</ref>. Therefore, Ravindran and Barto have also shown that state-enriched MDPs preserve the optimal action-value and action sequence properties. Li et al. give an overview over state abstraction or state aggregation for MDPs, which covers state-enriched MDPs<ref type="bibr" target="#b72">[73]</ref>.Definition A14. A decision processP is state-enriched compared to a decision process P if following conditions hold. Ifs is the state ofP, then there exists a function f :s → s with f (s) = s, where s is the state of P. There exists a function g :s → R, where g(s) gives the additional information of states compared to f (s). There exists a function ν with ν(f (s), g(s)) =s, that is, the states can be constructed from the original state and the additional information. There exists a function H with h(s ) = H(r,s, a), wheres is the next state and r the reward. H ensures that h(s ) of the next statẽ s can be computed from reward r, actual states, and the actual action a. Consequently,s can be computed from (r,s, a). For alls ands following holds:</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">1+exp(−x) : g(x) = a g σ(x), with a g = 2, 3, 4. With a positive g all contributions are positive. The</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was supported by NVIDIA Corporation, Merck KGaA, Audi.JKU Deep Learning Center, Audi Electronic Venture GmbH, Janssen Pharmaceutica (madeSMART), TGW Logistics Group, ZF Friedrichshafen AG, UCB S.A., FFG grant 871302, LIT grant DeepToxGen and AI-SNN, and FWF grant P 28660-N31.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Theorem 1 <ref type="bibr">(Rencher Theorem 10.6)</ref>. The increase in R 2 due to z can be expressed as</p><p>wherer yz = (β * zx ) T r yx is a "predicted" value of r yz based on the relationship of z to the x's. The following equality shows thatr yz = (β * zx ) T r yx is indeed a prediction of r yz :</p><p>If z is orthogonal to x (i.e., if r zx = 0), thenβ * zx = 0, which implies thatr yz = 0 and R 2 zx = 0. In this case, Eq. (A375) can be written as</p><p>Consequently, if all x i are independent from each other, then</p><p>The contribution of z to R 2 can either be less than or greater than r yz . If the correlation r yz can be predicted from x, thenr yz is close to r yz and, therefore, z has contributes less to R 2 than r 2 yz . Next, we compute the contribution of z to R 2 explicitly. The correlation between y and z is</p><p>We assume thatz =z. We want to express the information gain using the mean squared error (MSE)</p><p>We define the error e i :=ẑ i − z i at sample i withē =z −z = 0. Therefore, the MSE is equal to the empirical variance s 2 e = 1/(n − 1) n i=1 e 2 i . The correlation r ey between the target y and the error e is</p><p>Using Eq. (A376) and Eq. (A379), we can express the difference between the estimater yz and the true correlation r yz by:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Explaining recurrent neural network predictions in sentiment analysis. CoRR, abs/1706</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Arras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7206</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Playing hard exploration games by watching YouTube</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Le</forename><surname>Paine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reinforcement learning with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bakker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>T. G. Dietterich, S. Becker, and Z. Ghahramani</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1475" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reinforcement learning by backpropagation through an lstm model/critic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bakker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Approximate Dynamic Programming and Reinforcement Learning</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="127" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Transfer in deep reinforcement learning using successor features and generalised policy improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mankowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zídek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">35th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1901" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="501" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Successor features for transfer in reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barreto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<idno>1606.05312</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="4055" to="4065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Handbook of Learning and Approximate Dynamic Programming, chapter Reinforcement Learning and Its Relationship to Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="page" from="45" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Comparing value-function estimation algorithms in undiscounted problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beleznay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grobler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<idno>TR-99-02</idno>
		<imprint>
			<date type="published" when="1999" />
			<publisher>Mindmaker Ltd</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A distributional perspective on reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>D. Precup and Y. W. Teh</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="449" to="458" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research (ICML)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Arcade learning environment: An evaluation platform for general agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="253" to="279" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">BEGAN: boundary equilibrium generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An analysis of stochastic shortest path problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Math. Oper. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neuro-dynamic programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Considérations àl&apos;appui de la découverte de laplace</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I.-J</forename><surname>Bienaymé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comptes Rendus de l&apos;Académie des Sciences</title>
		<imprint>
			<date type="published" when="1853" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="309" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Instrumentation and Control Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Process Controllers</title>
		<meeting>ess Controllers<address><addrLine>Newnes</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="99" to="121" />
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stochastic approximation with two time scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Borkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems &amp; Control Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="291" to="294" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<title level="m">Openai gym. ArXiv</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Probably approximately correct search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Information Retrieval Theory</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The convergence of TD(λ) for general λ</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">341</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sidor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://github.com/openai/baselines" />
		<title level="m">Openai baselines</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Forward-backward reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Davidson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">IMPALA: Scalable distributed Deep-RL with importance weighted actor-learner architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Doron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Firoiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>ArXiv: 1802.01561</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>J. Dy and A. Krause</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Continuity properties and sensitivity analysis of parameterized fixed points and approximate fixed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feinstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>Operations Research and Financial Engineering Laboratory, Washington University in St. Louis</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Noisy networks for exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fortunato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fixed point and continuation results for contractions in metric and Gauge spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Frigon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Banach Center Publications</publisher>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="page" from="89" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning robust rewards with adversarial inverse reinforcement learning. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Robust speech recognition using long short-term memory recurrent neural networks for hybrid acoustic modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rigoll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Annual Conf. of the Int. Speech Communication Association (INTERSPEECH 2014)</title>
		<meeting>15th Annual Conf. of the Int. Speech Communication Association (INTERSPEECH 2014)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09" />
			<biblScope unit="page" from="631" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recurrent nets that time and count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Joint Conf. on Neural Networks (IJCNN 2000)</title>
		<meeting>Int. Joint Conf. on Neural Networks (IJCNN 2000)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Artificial Neural Networks (ICANN &apos;99)</title>
		<meeting>Int. Conf. on Artificial Neural Networks (ICANN &apos;99)<address><addrLine>Edinburgh, Scotland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="850" to="855" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to forget: Continual prediction with LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Gers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cummins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2451" to="2471" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Irène Gijbels. Censored data. Wiley Interdisciplinary Reviews: Computational Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="178" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Equivalence notions and model minimization in Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Givan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Greig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">147</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="223" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automatic language identification using long short-term memory recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Dominguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lopez-Moreno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzalez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moreno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Annual Conf. of the Int. Speech Communication Association (INTERSPEECH 2014)</title>
		<meeting>15th Annual Conf. of the Int. Speech Communication Association (INTERSPEECH 2014)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09" />
			<biblScope unit="page" from="2155" to="2159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Recall traces: Backtracking models for efficient reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A novel connectionist system for improved unconstrained handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="855" to="868" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A.-R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech and Signal essing<address><addrLine>Vancouver, BC</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
	<note>ICASSP 2013</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional LSTM and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">LSTM: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The optimal unbiased value estimator and its relation to LSTD, TD and MC</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grünewälder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="289" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">World models. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Expressing arbitrary reward functions as potential-based advice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vrancx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Now&amp;apos;e</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI&apos;15)</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI&apos;15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2652" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep recurrent Q-Learning for partially observable MDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Learning and transfer of modulated locomotor controllers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Is multiagent deep reinforcement learning the answer or the question? A brief survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hernandez-Leal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kartal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Taylor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Modayil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dabney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<title level="m">Rainbow: Combining improvements in deep reinforcement learning. ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Implementierung und Anwendung eines &apos;neuronalen&apos; Echtzeit-Lernalgorithmus für reaktive Umgebungen. Practical work</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<editor>J. Schmidhuber</editor>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
		<respStmt>
			<orgName>Institut für Informatik, Technische Universität München</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<title level="m">Untersuchungen zu dynamischen neuronalen Netzen. Master&apos;s thesis</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
		<respStmt>
			<orgName>Technische Universität München</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Recurrent neural net learning and vanishing gradient</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fuzzy-Neuro-Systeme &apos;97</title>
		<editor>C. Freksa</editor>
		<meeting>Fuzzy-Neuro-Systeme &apos;97<address><addrLine>Sankt Augustin</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="page" from="130" to="137" />
		</imprint>
	</monogr>
	<note>INFIX</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The vanishing gradient problem during learning recurrent neural nets and problem solutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Internat. J. Uncertain. Fuzziness Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="116" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Field Guide to Dynamical Recurrent Networks</title>
		<editor>J. F. Kolen and S. C. Kremer</editor>
		<imprint>
			<publisher>IEEE Press</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Fast model-based protein homology detection without alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Obermayer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="1728" to="1736" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>FKI-207-95</idno>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>Fakultät für Informatik, Technische Universität München</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">LSTM can solve hard long time lag problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 9</title>
		<editor>M. C. Mozer, M. I. Jordan, and T. Petsche</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="473" to="479" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Steven</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">R</forename><surname>Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Artificial Neural Networks (ICANN 2001)</title>
		<editor>G. Dorffner, H. Bischof, and K. Hornik</editor>
		<meeting>Int. Conf. on Artificial Neural Networks (ICANN 2001)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Distributed prioritized experience replay. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Optimizing agent behavior over long time scales by transporting value</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Carnevale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Independent Component Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">On the convergence of stochastic iterative dynamic programming algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1185" to="1201" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Continuous dependence of attractors of iterated function systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jachymski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Of Mathematical Analysis And Applications</title>
		<imprint>
			<biblScope unit="volume">198</biblScope>
			<biblScope unit="issue">0077</biblScope>
			<biblScope unit="page" from="221" to="226" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">When the best move isn&apos;t optimal: q-learning with exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>John</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th Tenth National Conference on Artificial Intelligence</title>
		<meeting>the 10th Tenth National Conference on Artificial Intelligence<address><addrLine>Menlo Park, CA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1994" />
			<biblScope unit="page">1464</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Two time-scale stochastic approximation with controlled Markov noise and off-policy temporal-difference learning. Mathematics of Operations Research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karmakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Sparse attentive backtracking: Temporal credit assignment through reminding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7640" to="7651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">On the analysis of complex backup strategies in Monte Carlo Tree Search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Liebman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Niekum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1319" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Continuous dependence on parameters of the fixed point set for some set-valued operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kirr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Petrusel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discussiones Mathematicae Differential Inclusions</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="29" to="41" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Bandit based Monte-Carlo planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Evolving large-scale neural networks for vision-based reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cuccu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation, GECCO &apos;13</title>
		<meeting>the 15th Annual Conference on Genetic and Evolutionary Computation, GECCO &apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1061" to="1068" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A note on continuity of fixed points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kwiecinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Universitatis Iagellonicae Acta Mathematica</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="19" to="24" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Interpreting individual classifications of hierarchical networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Landecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Thomure</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M A</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Kenyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Brumby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Symposium on Computational Intelligence and Data Mining (CIDM)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="32" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Bandit Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Draft of 28th July</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Revision 1016</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Towards a unified theory of state abstraction for MDPs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ninth International Symposium on Artificial Intelligence and Mathematics (ISAIM)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Reinforcement Learning for Robots Using Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<pubPlace>Pittsburgh</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Concentration-of-measure inequalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Summer School on Machine Learning at the Australian National University</title>
		<meeting><address><addrLine>Canberra</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Time delays, competitive interdependence, and firm performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luoma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruutu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tikkanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Strategic Management Journal</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="506" to="525" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Bias and variance approximation in value function estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="308" to="322" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Multi-resolution linear prediction based features for audio onset detection with bidirectional LSTM neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ferroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gabrielli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Squartini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing</title>
		<meeting>IEEE Int. Conf. on Acoustics, Speech and Signal essing<address><addrLine>Florence</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-05" />
			<biblScope unit="page" from="2164" to="2168" />
		</imprint>
	</monogr>
	<note>ICASSP 2014</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Distribution of eigenvalues or some sets of random matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Marȏenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Pastur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of the USSR-Sbornik</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">457</biblScope>
			<date type="published" when="1967" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Asynchronous methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML)</title>
		<editor>M. F. Balcan and K. Q. Weinberger</editor>
		<meeting>the 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Playing Atari with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Riedmiller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Explaining nonlinear classification decisions with deep Taylor decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="211" to="222" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Methods for interpreting and understanding deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Digital Signal Processing</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Prioritized sweeping: Reinforcement learning with less data and less time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Atkeson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="103" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">A dual back-propagation scheme for scalar reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Munro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Annual Conference of the Cognitive Science Society</title>
		<meeting>the Ninth Annual Conference of the Cognitive Science Society<address><addrLine>Seattle, WA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="165" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Policy invariance under reward transformations: Theory and application to reward shaping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth International Conference on Machine Learning (ICML&apos;99)</title>
		<meeting>the Sixteenth International Conference on Machine Learning (ICML&apos;99)</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="278" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">The uncertainty Bellman equation and exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>O&amp;apos;donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Stochastic and shortest path games: theory and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Patek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology. Dept. of Electrical Engineering and Computer Science</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Incremental multi-step q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="283" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Reinforcement learning by reward-weighted regression for operational space control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schaal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="745" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">The machine learning reproducibility checklist</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">99</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Observe and look further: Achieving consistent performance on Atari</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Horgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Budden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Barth-Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Večerík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Visual explanation of evidence in additive classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eisner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Szafron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Wishart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fyshe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pearcy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Macdonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Anvik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Innovative Applications of Artificial Intelligence (IAAI)</title>
		<meeting>the 18th Conference on Innovative Applications of Artificial Intelligence (IAAI)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1822" to="1829" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Models</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1990" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="331" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Markov Decision Processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>John Wiley &amp; Sons, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Effects of feedback delay on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmandad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Repenning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sterman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">System Dynamics Review</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="338" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<monogr>
		<title level="m" type="main">Symmetries and model minimization in Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<pubPlace>Amherst, MA, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">SMDP homomorphisms: An algebraic approach to abstraction in semi-Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI&apos;03)</title>
		<meeting>the 18th International Joint Conference on Artificial Intelligence (IJCAI&apos;03)<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1011" to="1016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Linear Models in Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Rencher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Schaalje</surname></persName>
		</author>
		<idno>978-0-471-75498-5</idno>
		<imprint>
			<date type="published" when="2008" />
			<publisher>John Wiley &amp; Sons</publisher>
			<pubPlace>Hoboken, New Jersey</pubPlace>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Dynamic Error Propagation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Robinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
		<respStmt>
			<orgName>Trinity Hall and Cambridge University Engineering Department</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Dynamic reinforcement driven error propagation networks with application to game playing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fallside</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Conference of the Cognitive Science Society</title>
		<meeting>the 11th Conference of the Cognitive Science Society<address><addrLine>Ann Arbor</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="page" from="836" to="843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Reward estimation for variance reduction in deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Francois-Lavet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Non-asymptotic theory of random matrices: extreme singular values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rudelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vershynin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">On-line q-learning using connectionist systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Rummery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niranjan</surname></persName>
		</author>
		<idno>TR 166</idno>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
		<respStmt>
			<orgName>Cambridge University Engineering Department</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b105">
	<monogr>
		<title level="m" type="main">Reinforcement learning never worked, and &apos;deep&apos; only helped a bit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sahni</surname></persName>
		</author>
		<ptr target="himanshusahni.github.io/2018/02/23/reinforcement-learning-never-worked.html" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 15th Annual Conf. of the Int. Speech Communication Association (INTERSPEECH 2014)</title>
		<meeting>15th Annual Conf. of the Int. Speech Communication Association (INTERSPEECH 2014)<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-09" />
			<biblScope unit="page" from="338" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Is imitation learning the route to humanoid robots?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="233" to="242" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<monogr>
		<title level="m" type="main">Prioritized experience replay</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title level="m" type="main">Making the world differentiable: On using fully recurrent self-supervised neural networks for dynamic reinforcement learning and planning in non-stationary environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno>FKI-126-90</idno>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
		<respStmt>
			<orgName>Institut für Informatik, Technische Universität München</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
	<note>Experiments by Sepp Hochreiter</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Reinforcement learning in markovian and non-markovian environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R. P. Lippmann, J. E. Moody, and D. S. Touretzky</editor>
		<meeting><address><addrLine>San Mateo, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1991" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="500" to="506" />
		</imprint>
	</monogr>
	<note>Pole balancing experiments by Sepp Hochreiter</note>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32st International Conference on Machine Learning (ICML)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">High-dimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth International Conference on Learning Representations (ICLR&apos;16)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Klimov</surname></persName>
		</author>
		<title level="m">Proximal policy optimization algorithms. ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Mastering Chess and Shogi by self-play with a general reinforcement learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Graepel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<title level="m">Convergence results for single-step on-policy reinforcement-learning algorithms. Machine Learning</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="287" to="308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Reinforcement learning with replacing eligibility traces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="123" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Reinforcement today</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Skinner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Psychologist</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="94" to="99" />
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">The variance of discounted Markov decision processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Sobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="794" to="802" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">A note on universality of the distribution of the largest eigenvalues in certain sample covariance matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soshnikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Statist. Phys</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1033" to="1056" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">Reward shaping with recurrent neural networks for speeding up on-line policy learning in spoken dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mrksic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="417" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27 (NIPS&apos;13)</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Learning to predict by the methods of temporal differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="9" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<monogr>
		<title level="m" type="main">Reinforcement Learning: An Introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Policy gradients with variance related risk criteria</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dicastro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML&apos;12)</title>
		<editor>J. Langford and J. Pineau</editor>
		<meeting>the 29th International Conference on Machine Learning (ICML&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Learning the variance of the reward-to-go</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dicastro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Des valeurs moyennes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tchebichef</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal de mathématiques pures et appliquées</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="177" to="184" />
			<date type="published" when="1867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Solving h-horizon, stationary Markov decision problems in time proportional to log(h)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="287" to="297" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">Asynchronous stochastic approximation and q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="185" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Double q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 23</title>
		<editor>J. D. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. S. Zemel, and A. Culotta</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2613" to="2621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with double q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Van Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2094" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<monogr>
		<title level="m" type="main">Translating videos to natural language using deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<monogr>
		<title level="m" type="main">Ergodic Markov processes and poisson equations (lecture notes)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veretennikov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<monogr>
		<title level="m" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ArXiv</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Dueling network architectures for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hasselt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning (ICML)</title>
		<editor>M. F. Balcan and K. Q. Weinberger</editor>
		<meeting>the 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1995" to="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<monogr>
		<title level="m" type="main">Learning from Delayed Rewards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<pubPlace>King&apos;s College</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b140">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C H</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Q-Learning</surname></persName>
		</author>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">A menu of designs for reinforcement learning over time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Werbos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks for Control</title>
		<editor>W. T. Miller, R. S. Sutton, and P. J. Werbos</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1990" />
			<biblScope unit="page" from="67" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Potential-based shaping and q-value initialization are equivalent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="205" to="208" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Principled methods for advising reinforcement learning agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Elkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth International Conference on International Conference on Machine Learning (ICML&apos;03)</title>
		<meeting>the Twentieth International Conference on International Conference on Machine Learning (ICML&apos;03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="792" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<title level="m">Recurrent neural network regularization. ArXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Top-down neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th European Conference on Computer Vision (ECCV)</title>
		<meeting>the 14th European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="543" to="559" />
		</imprint>
	</monogr>
	<note>part IV</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
