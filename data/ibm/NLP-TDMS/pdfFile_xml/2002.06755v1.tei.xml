<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unifying Graph Convolutional Neural Networks and Label Propagation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
						</author>
						<title level="a" type="main">Unifying Graph Convolutional Neural Networks and Label Propagation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node is spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Consider the problem of node classification in a graph, where the goal is to learn a mapping M : V → L from node set V to label set L. Solution to this problem is widely applicable to various scenarios, e.g., inferring income of users in a social network or classifying scientific articles in a citation network. Different from a generic machine 1 Computer Science Department, Stanford University, Stanford, CA 94305, United States. Correspondence to: Hongwei Wang &lt;hongweiw@cs.stanford.edu&gt;. learning problem where samples are independent from each other, nodes are connected by edges in the graph, which provide additional information and require more delicate modeling. To capture the graph information, researchers have mainly designed models on the assumption that labels and features vary smoothly over the edges of the graph. In particular, on the label side L, node labels are propagated and aggregated along edges in the graph, which is known as Label Propagation Algorithm (LPA) <ref type="bibr">(Zhu et al., 2005;</ref><ref type="bibr">Zhou et al., 2004;</ref><ref type="bibr">Zhang &amp; Lee, 2007;</ref><ref type="bibr" target="#b20">Wang &amp; Zhang, 2008;</ref><ref type="bibr" target="#b4">Karasuyama &amp; Mamitsuka, 2013;</ref><ref type="bibr" target="#b1">Gong et al., 2017;</ref><ref type="bibr" target="#b11">Liu et al., 2019a)</ref>; On the node side V, node features are propagated along edges and transformed through neural network layers, which is known as Graph Convolutional Neural Networks (GCN) <ref type="bibr" target="#b6">(Kipf &amp; Welling, 2017;</ref><ref type="bibr" target="#b2">Hamilton et al., 2017;</ref><ref type="bibr" target="#b9">Li et al., 2018;</ref><ref type="bibr" target="#b21">Xu et al., 2018;</ref><ref type="bibr" target="#b10">Liao et al., 2019;</ref><ref type="bibr" target="#b22">Xu et al., 2019;</ref><ref type="bibr" target="#b13">Qu et al., 2019)</ref>.</p><p>GCN and LPA are related in that they propagate features and labels on the two sides of the mapping M, respectively. However, the relationship between GCN and LPA has not yet been investigated. Specifically, what is the theoretical relationship between GCN and LPA, and how can they be combined to develop a more accurate model for node classification in graphs?</p><p>Here we study the theoretical relationship between GCN and LPA from two viewpoints: (1) Feature/label smoothing, where we show that the intuition behind GCN/LPA is smoothing features/labels of nodes across the edges of the graph, i.e., one node's feature/label equals the weighted average of features/labels of its neighbors. We prove that if the weights of edges in a graph smooth the node features with high precision, they also smooth the node labels with guaranteed upper bound on the smoothing error. And, (2) feature/label influence, where we quantify how much the initial feature/label of node v b influences the output feature/label of node v a in GCN/LPA by studying the Jacobian/gradient of node v b with respect to node v a . We also prove the quantitative relationship between feature influence and label influence.</p><p>Based on the above theoretical analysis, we propose a unified model GCN-LPA for node classification. We show that the key to improving the performance of GCN is to enable nodes within the same class/label to connect more strongly arXiv:2002.06755v1 <ref type="bibr">[cs.</ref>LG] 17 Feb 2020 with each other by making edge weights/strengths trainable. Then we prove that increasing the strength of edges between the nodes of the same class is equivalent to increasing the accuracy of LPA's predictions. Therefore, we can first learn the optimal edge weights by minimizing the loss of predictions in LPA, then plug the optimal edge weights into a GCN to learn node representations and do final classification. In GCN-LPA, we further combine the two steps together and train the whole model in an end-to-end fashion, where the LPA part serves as regularization to assist the GCN part in learning proper edge weights that benefit the separation of different node classes. It is worth noticing that GCN-LPA can also be seen as learning attention weights for edges based on node label information, which requires less handcrafting and is more task-oriented than existing work that learns attention weights based on node feature similarity <ref type="bibr" target="#b19">(Veličković et al., 2018;</ref><ref type="bibr" target="#b18">Thekumparampil et al., 2018;</ref><ref type="bibr" target="#b24">Zhang et al., 2018;</ref><ref type="bibr" target="#b12">Liu et al., 2019b)</ref>.</p><p>We conduct extensive experiments on five datasets, and the results indicate that our model outperforms state-ofthe-art methods in terms of classification accuracy. The experimental results also show that combining GCN and LPA together is able to learn more informative edge weights thereby leading to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Unifying GCN and LPA</head><p>In this section, we first formulate the node classification problem and briefly introduce LPA and GCN. We then prove their relationship from the viewpoints of smoothing and influence. Based on the theoretical findings, we propose a unified model GCN-LPA, and analyze why our model is theoretically superior to vanilla GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Problem Formulation and Preliminaries</head><p>We begin by describing the problem of node classification on graphs and introducing notation. Consider a graph G = (V, A, X, Y ), where V = {v 1 , · · · , v n } is the set of nodes, A ∈ R n×n is the adjacency matrix (self-loops are included), X is the feature matrix of nodes and Y is labels of nodes. a ij (the ij-th entry of A) is the weight of the edge connecting v i and v j . N (v) denotes the set of immediate neighbors of node v in graph G. Each node v i has a feature vector x i which is the i-th row of X, while only the first m nodes have labels y 1 , · · · , y m from a label set L = {1, · · · , c}. The goal is to learn a mapping M : V → L and predict labels of unlabeled nodes.</p><p>Label Propagation Algorithm. LPA assumes that two connected nodes are likely to have the same label, and thus it propagates labels iteratively along the edges. Let</p><formula xml:id="formula_0">Y (k) = [y (k) 1 , · · · , y (k)</formula><p>n ] ∈ R n×c be the soft label matrix in iteration k &gt; 0, in which the i-th row y (k) i denotes the predicted label distribution for node v i in iteration k. When k = 0, the initial label matrix Y (0) = [y (0) 1 , · · · , y (0) n ] consists of one-hot label indicator vectors y (0) i for i = 1, · · · , m (i.e., labeled nodes) or zero vectors otherwise (i.e., unlabeled nodes). Let D be the diagonal degree matrix for A with entries d ii = j a ij . Then <ref type="bibr">LPA (Zhu et al., 2005)</ref> in iteration k is formulated as the following two steps:</p><formula xml:id="formula_1">Y (k+1) = D −1 A Y (k) ,<label>(1)</label></formula><formula xml:id="formula_2">y (k+1) i = y (0) i , ∀ i ≤ m.<label>(2)</label></formula><p>In Eq.</p><p>(1), all nodes propagate labels to their neighbors according to normalized edge weights. Then in Eq.</p><p>(2), labels of all labeled nodes are reset to their initial values, because LPA wants to persist labels of nodes which are labeled so that unlabeled nodes do not overpower the labeled ones as the initial labels would otherwise fade away.</p><p>Graph Convolutional Neural Network. GCN is a multi-layer feedforward neural network that propagates and transforms node features across the graph. The layer-wise propagation rule of GCN is</p><formula xml:id="formula_3">X (k+1) = σ(D − 1 2 AD − 1 2 X (k) W (k) ), where W (k) is trainable weight matrix in the k-th layer, σ(·)</formula><p>is an activation function such as ReLU, and</p><formula xml:id="formula_4">X (k) = [x (k) 1 , · · · , x (k)</formula><p>n ] are the k-th layer node representations with X (0) = X. To align with the above LPA, we use D −1 A as the normalized adjacency matrix instead of the symmetric one D − 1 2 AD − 1 2 proposed by <ref type="bibr" target="#b6">(Kipf &amp; Welling, 2017)</ref>. Therefore, the feature propagation scheme of GCN in layer k is:</p><formula xml:id="formula_5">X (k+1) = σ D −1 AX (k) W (k) .<label>(3)</label></formula><p>Notice similarity between Eqs.</p><p>(1) and (3). Next we shall study and uncover the relationship between the two equations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Feature Smoothing and Label Smoothing</head><p>The intuition behind both LPA and GCN is smoothing <ref type="bibr">(Zhu et al., 2003;</ref><ref type="bibr" target="#b9">Li et al., 2018)</ref>: In LPA, the final label of a node is the weighted average of labels of its neighbors:</p><formula xml:id="formula_6">y (∞) i = 1 d ii j∈N (i) a ij y (∞) j .<label>(4)</label></formula><p>In GCN, the final node representation is also the weighted average of representations of its neighbors if we assume σ is identity function and W (·) are identity matrices:</p><formula xml:id="formula_7">x (∞) i = 1 d ii j∈N (i) a ij x (∞) j .<label>(5)</label></formula><p>Next we show the relationship between feature smoothing and label smoothing:</p><p>Theorem 1 (Relationship between feature smoothing and label smoothing) Suppose that the latent ground-truth mapping M : x → y from node features to node labels is differentiable and satisfies L-Lipschitz constraint, i.e., |M(x 1 ) − M(x 2 )| ≤ L x 1 − x 2 2 for any x 1 and x 2 (L is a constant). If the edge weights {a ij } approximately smooth x i over its immediate neighbors with error i , i.e.,</p><formula xml:id="formula_8">x i = 1 d ii j∈N (i) a ij x j + i ,<label>(6)</label></formula><p>then the edge weights {a ij } also approximately smooth y i over its immediate neighbors with the following approximation error:</p><formula xml:id="formula_9">y i − 1 d ii j∈N (i) a ij y j ≤ L i 2 +o max j∈N (i) ( x j −x i 2 ) ,<label>(7)</label></formula><p>where o(α) denotes a higher order infinitesimal than α.</p><p>Proof of Theorem 1 is in Appendix A. Theorem 1 indicates that label smoothing is theoretically guaranteed by feature smoothing. Note that if we treat edge weights {a ij } learnable, then feature smoothing (i.e., i → 0) can be directly achieved by keeping node features x i fixed while setting {a ij } appropriately, without resorting to feature propagation in a multi-layer GCN. Therefore, a simple approach to exploit this theorem would be to learn {a ij } by reconstructing node feature x i from its neighbors, then use the learned {a ij } to reconstruct node labels y i <ref type="bibr" target="#b4">(Karasuyama &amp; Mamitsuka, 2013)</ref>.</p><p>As shown in Theorem 1, the approximation error of labels is dominated by L i 2 . However, this error could be fairly large in practice because: (1) The number of immediate neighbors for a given node may be too small to reconstruct its features perfectly, especially in the case where node features are high-dimensional and sparse. For example, in a citation network where node features are one-hot bagof-words vectors, the feature of one article can never be precisely reconstructed if none of its neighboring articles contains the specific word that appears in this article. As a result, i 2 will be non-neglibible. This explains why it is beneficial to apply LPA and GCN for multiple iterations/layers in order to include information from farther away neighbors. (2) The ground-truth mapping M may not be sufficiently smooth due to the complex structure of latent manifold and possible noise, which fails to satisfy L-Lipschitz constraint. In other words, the constant L will be extremely large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Feature Influence and Label Influence</head><p>To address the above concerns and extend our analysis, we next consider GCN and LPA with multiple layers/iterations, and do not impose any constraint on the ground-truth mapping M.</p><p>Consider two nodes v a and v b in a graph. Inspired by <ref type="bibr" target="#b7">(Koh &amp; Liang, 2017)</ref> and <ref type="bibr" target="#b21">(Xu et al., 2018)</ref>, we study the relationship between GCN and LPA in terms of influence, i.e., how the output feature/label of v a will change if the initial feature/label of v b is varied slightly. Technically, the feature/label influence is measured by the Jacobian/gradient of the output feature/label of v a with respect to the initial fea-</p><formula xml:id="formula_10">ture/label of v b . Denote x (k)</formula><p>a as the k-th layer representation vector of v a in GCN, and x b as the initial feature vector of v b . We quantify the feature influence of v b on v a as follows:</p><p>Definition 1 (Feature influence) The feature influence of node v b on node v a after k layers of GCN is the L1-norm of the expected Jacobian matrix ∂x</p><formula xml:id="formula_11">(k) a /∂x b : I f (v a , v b ; k) = E ∂x (k) a /∂x b 1 .<label>(8)</label></formula><p>The normalized feature influence is then defined as</p><formula xml:id="formula_12">I f (v a , v b ; k) = I f (v a , v b ; k) vi∈V I f (v a , v i ; k) .<label>(9)</label></formula><p>We also consider the label influence of node v b on node v a in LPA (this implies that v a is unlabeled and v b is labeled).</p><p>Since different label dimensions of y (·) i do not interact with each other in LPA, we assume that all y i and y <ref type="bibr">(·)</ref> i are scalars within [0, 1] (i.e., a binary classification) for simplicity. Label influence is defined as follows:</p><p>Definition 2 (Label influence) The label influence of labeled node v b on unlabeled node v a after k iterations of LPA is the gradient of y (k) a with respect to y b :</p><formula xml:id="formula_13">I l (v a , v b ; k) = ∂y (k) a /∂y b .<label>(10)</label></formula><p>The following theorem shows the relationship between feature influence and label influence:</p><p>Theorem 2 (Relationship between feature influence and label influence) Assume the activation function used in GCN is ReLU. Denote v a as an unlabeled node, v b as a labeled node, and β as the fraction of unlabeled nodes.</p><p>Then the label influence of v b on v a after k iterations of LPA equals, in expectation, to the cumulative normalized feature influence of v b on v a after k layers of GCN:</p><formula xml:id="formula_14">E I l (v a , v b ; k) = k j=1 β jĨ f (v a , v b ; j).<label>(11)</label></formula><p>Proof of Theorem 2 is in Appendix B. Intuitively, Theorem 2 shows that if v b has high label influence on v a , then the initial feature vector of v b will also affect the output feature vector of v a to a large extent. Theorem 2 provides the theoretical guideline for designing our unified model in the next subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">The Unified Model</head><p>Before introducing the proposed model, we first rethink the GCN method and see what an ideal node representation should be like. Since we aim to classify nodes, the perfect node representation would be such that nodes with the same label are embedded close together, which would give a large separation between different classes. Intuitively, the key to achieve this goal is to enable nodes within the same class to connect more strongly with each other, so that they are pushed together by the GCN. We can therefore make edge strengths/weights trainable, then learn to increase the intraclass feature influence for each class i:</p><formula xml:id="formula_15">va,v b :ya=i,y b =iĨ f (v a , v b )<label>(12)</label></formula><p>by adjusting edge weights. However, this requires operating on Jacobian matrices with the size of d (0) × d (K) (d (0) and d (K) are the dimensions of initial and output features, respectively), which is impractical if initial node features are high-dimensional. Fortunately, we can turn to optimizing the intra-class label influence instead of Eq. <ref type="formula" target="#formula_1">(12)</ref></p><formula xml:id="formula_16">, i.e., va,v b :ya=i,y b =i I l (v a , v b ),<label>(13)</label></formula><p>according to Theorem 2. We further show that, by the following theorem, the total intra-class label influence on a given node v a is proportional to the probability that v a is classified correctly by LPA:</p><p>Theorem 3 (Relationship between label influence and LPA's prediction) Consider a given node v a and its label y a . If we treat node v a as unlabeled, then the total label influence of nodes with label y a on node v a is proportional to the probability that node v a is classified as y a by LPA:</p><formula xml:id="formula_17">v b :y b =ya I l (v a , v b ; k) ∝ Pr ŷ lpa a = y a ,<label>(14)</label></formula><p>whereŷ lpa a is the predicted label of v a using a k-iteration LPA.</p><p>Proof of Theorem 3 is in Appendix C. Theorem 3 indicates that, if edge weights {a ij } maximize the probability that v a is correctly classified by LPA, then they also maximize the intra-class label influence on node v a . We can therefore first learn the optimal edge weights A * by minimizing the loss of predicted labels by LPA: 1</p><formula xml:id="formula_18">A * = arg min A L lpa (A) = arg min A 1 m va:a≤m J(ŷ lpa a , y a ),<label>(15)</label></formula><p>1 Here the optimal edge weights A * share the same topology as the original graph G, meaning that we do not add or remove edges from G but only learning the weights of existing edges. See the end of this subsection for more discussion.  <ref type="figure">(Figure 1a</ref>). To ease the separation of the two classes, our model will increase the connecting strength among nodes within the same class (i.e., within one dotted circle), thereby increasing their feature/label influence on each other. In this way, our model is able to identify potential intra-class edges (bold links in <ref type="figure">Figure 1b</ref>) and strengthen their weights.</p><p>where J is the cross-entropy loss,ŷ lpa a and y a are the predicted label distribution of v a using LPA and the true onehot label of v a , respectively. 2 a ≤ m means v a is labeled. The optimal A * maximize the probability that each node is correctly labeled by LPA, thus also increasing the intraclass label influence (by Theorem 3) and intra-class feature influence (by Theorem 2). Then we can apply A * and the corresponding D * to a GCN to predict labels:</p><formula xml:id="formula_19">X (k+1) = σ(D * −1 A * X (k) W (k) ), k = 0, 1, · · · , K − 1.</formula><p>(16) We useŷ gcn a , the a-th row of X (K) , to denote the predicted label distribution of v a using the GCN specified in Eq. (16). The the optimal transformation matrices in the GCN can be learned by minimizing the loss of predicted labels by GCN:</p><formula xml:id="formula_20">W * = arg min W L gcn (W, A * ) = arg min W 1 m va:a≤m J(ŷ gcn a , y a ),<label>(17)</label></formula><p>In practice, it is generally better to combine the above two steps together and train the whole model in an end-to-end fashion:</p><formula xml:id="formula_21">W * , A * = arg min W,A L gcn (W, A) + λL lpa (A),<label>(18)</label></formula><p>where λ is the balancing hyper-parameter. In this way, L lpa (A) serves as a regularization term that assists the learning of edge weights A, since it is hard for the GCN to learn  <ref type="figure" target="#fig_1">Figure 2a</ref> visualizes the graph. Node coordinates in <ref type="figure" target="#fig_1">Figure 2b</ref>-2e are the embedding coordinates. Notice that GCN does not produce linearly separable embeddings <ref type="figure" target="#fig_1">(Figure 2b</ref> vs. <ref type="figure" target="#fig_1">Figure 2c</ref>), while GCN-LPA performs much better even in the presence of noisy edges <ref type="figure" target="#fig_1">(Figure 2d</ref> vs. <ref type="figure" target="#fig_1">Figure 2e</ref>). Additional visualizations are included in Appendix E.</p><p>both W and A simultaneously due to overfitting. The proposed GCN-LPA approach can also be seen as learning the importance of edges that can be used to reconstruct node labels accurately by LPA, then transferring this knowledge from label space to feature space for the GCN. From this perspective, GCN-LPA also connects to Theorem 1 except that the knowledge transfer is in the other direction.</p><p>It is also worth noticing how the optimal A * is configured. The principle here is that we do not modify the basic structure of the original graph (i.e., not adding or removing edges) but only adjusting weights of existing edges. This is equivalent to learning a positive mask matrix M for the adjacency matrix A and taking the Hadamard product M • A = A * . Each element M ij can be set as either a free variable or a function of the nodes at edge endpoints, for example, M ij = log exp(x i Hx j ) + 1 where H is a learnable kernel matrix for measuring feature similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5.">Analysis of GCN-LPA Model Behavior</head><p>In this subsection, we show benefits of our unified model compared with GCN by analyzing properties of embeddings produced by the two models. We first analyze the update rule of GCN for node v i :</p><formula xml:id="formula_22">x (k+1) i = σ   vj ∈N (vi)ã ij x (k) j W (k)   ,<label>(19)</label></formula><formula xml:id="formula_23">whereã ij = a ij /d ii is the normalized weight of edge (j, i).</formula><p>This formula can be decomposed into the following two steps:</p><p>(1) In aggregation step, we calculate the aggregated representation h</p><formula xml:id="formula_24">(k) i of all neighborhoods N (v i ): h (k) i = vj ∈N (vi)ã ij x (k) j .<label>(20)</label></formula><p>(2) In transformation step, the aggregated representation h (k) i is mapped to a new space by a transformation matrix and nonlinear function:</p><formula xml:id="formula_25">x (k+1) i = σ h (k) i W (k) .<label>(21)</label></formula><p>We show by the following theorem that the aggregation step reduces the overall distance in the embedding space between the nodes that are connected in the graph:</p><p>Theorem 4 (Shrinking property in GCN) Let D(x) = 1 2 vi,vjã ij x i − x j 2 2 be a distance metric over node embeddings x. Then we have</p><formula xml:id="formula_26">D(h (k) ) ≤ D(x (k) ).</formula><p>Proof of Theorem 4 is in Appendix D. Theorem 4 indicates that the overall distance among connected nodes is reduced after taking one aggregation step, which implies that connected components in the graph "shrink" and nodes within each connected component get closer to each other in the embedding space. In an ideal case where edges only connect nodes with the same label, the aggregation step will push nodes within the same class together, which greatly benefits the transformation step that acts like a hyperplane W (k) for classification. However, two connected nodes may have different labels. These "noisy" edges will impede the formation of clusters and make the inter-class boundary less clear.</p><p>Fortunately, in GCN-LPA, edge weights are learned by minimizing the difference between ground-truth labels and labels reconstructed from multi-hop neighbors. This will force the model to increase weight/bandwidth of possible paths that connect nodes with the same label, so that labels can "flow" easily along these paths for the purpose of label reconstruction. In this way, GCN-LPA is able to identify potential intra-class edges and increase their weights to assist learning clustering structures. <ref type="figure">Figure 1</ref> gives a toy example illustrating how our model works intuitively.</p><p>To empirically justify our claim, we apply a two-layer untrained GCN with randomly initialized transformation matrices to the well-known Zachary's karate club network  <ref type="bibr" target="#b23">(Zachary, 1977)</ref> as shown in <ref type="figure" target="#fig_1">Figure 2a</ref>, which contains 34 nodes of 2 classes and 78 unweighted edges (grey solid lines). We then increase the weights of intra-class edges by ten times to simulate GCN-LPA. We find that GCN works well on this network <ref type="figure" target="#fig_1">(Figure 2b</ref>), but GCN-LPA performs even better than GCN because the node embeddings are completely linearly separable as shown in <ref type="figure" target="#fig_1">Figure 2c</ref>. To further justify our claim, we randomly add 20 "noisy" interclass edges (grey dotted lines) to the original network, from which we observe that GCN is misled by noise and mixes nodes of two classes together <ref type="figure" target="#fig_1">(Figure 2d</ref>), but GCN-LPA still distinguishes the two clusters <ref type="figure" target="#fig_1">(Figure 2e</ref>) because it is better at "denoising" undesirable edges based on the supervised signal of labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Connection to Existing Work</head><p>Edge weights play a key role in graph-based node classification as well as representation learning. In this section, we discuss three lines of related work that learn edge weights adaptively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Locally Linear Embedding</head><p>Locally linear embedding (LLE) <ref type="bibr" target="#b14">(Roweis &amp; Saul, 2000)</ref> and its variants <ref type="bibr">(Zhang &amp; Wang, 2007;</ref><ref type="bibr" target="#b8">Kong et al., 2012)</ref> learn edge weights by constructing a linear dependency between a node and its neighbors, then use the learned edge weights to embed high-dimensional nodes into a low-dimensional space. Our work is similar to LLE in the aspect of transferring the knowledge of edge importance from one space to another, but the difference is that LLE is an unsupervised dimension reduction method that learns the graph structure based on local proximity only, while our work is semi-supervised and explores high-order relationship among nodes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Label Propagation Algorithm</head><formula xml:id="formula_27">− d (x id − x jd ) 2 /σ 2 d )</formula><p>where d is dimensionality of features), minimizing neighborhood reconstruction error <ref type="bibr" target="#b20">(Wang &amp; Zhang, 2008;</ref><ref type="bibr" target="#b4">Karasuyama &amp; Mamitsuka, 2013)</ref>, using leave-one-out loss <ref type="bibr">(Zhang &amp; Lee, 2007)</ref>, or imposing sparseness on edge weights <ref type="bibr" target="#b3">(Hong et al., 2009</ref>). However, in these LPA variants, node features are only used to assist learning the graph structure rather than explicitly mapped to node labels, which limits their capability in node classification. Another notable difference is that adaptive LPA learns edge weights by introducing the regularizations above, while our work takes LPA itself as regularization to learn edge weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attention Mechanism on Graphs</head><p>Our method is also conceptually connected to attention mechanism on graphs <ref type="bibr" target="#b19">(Veličković et al., 2018;</ref><ref type="bibr" target="#b18">Thekumparampil et al., 2018;</ref><ref type="bibr" target="#b24">Zhang et al., 2018;</ref><ref type="bibr" target="#b12">Liu et al., 2019b)</ref>, in which an attention weight α ij is learned between node v i and v j . For example, α ij = LeakyReLU(a [W x i ||W x j ]) in GAT <ref type="bibr" target="#b19">(Veličković et al., 2018)</ref>, α ij = a · cos(W x i , W x j ) in AGNN (Thekumparampil et al., 2018), α ij = (W 1 x i ) W 2 x j in GaAN <ref type="bibr" target="#b24">(Zhang et al., 2018)</ref>, and α ij = a tanh(W 1 x i + W 2 x j ) in Ge-niePath <ref type="bibr" target="#b12">(Liu et al., 2019b)</ref>, where a and W are trainable variables. A significant difference between these attention mechanisms and our work is that attention weights are learned based merely on feature similarity, while we propose that edge weights should be consistent with the distribution of labels on the graph, which requires less handcrafting of the attention function and is more task-oriented. Nevertheless, all the above formulas for calculating attentions can also be used in our model as the implementation of edge weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our model and present its performance on five datasets including citation networks and coauthor networks. We also study the hyper-parameter sensitivity and provide training time analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We use the following five datasets in our experiments: datasets <ref type="bibr" target="#b15">(Sen et al., 2008)</ref>: Cora, Citeseer, and Pubmed. In these datasets, nodes correspond to documents, edges correspond to citation links, and each node has a sparse bag-of-words feature vector as well as a class label.</p><p>Coauthor networks: We also use two co-authorship networks <ref type="bibr" target="#b16">(Shchur et al., 2018)</ref>, Coauthor-CS and Coauthor-Phy, based on Microsoft Academic Graph from the KDD Cup 2016 challenge. Here nodes are authors and an edge indicates that two authors co-authored a paper. Node features represent paper keywords for each author's papers, and class labels indicate most active fields of study for each author.</p><p>Statistics of the five datasets are shown in <ref type="table">Table 1</ref>. We also calculate the intra-class edge rate (the fraction of edges that connect two nodes within the same class), which is significantly higher than inter-class edge rate in all networks. The finding supports our claim in Section 2.5 that node classification benefits from intra-class edges in a graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines</head><p>We compare against the following baselines in our experiments. The first two baselines only utilize node features, the third baseline only utilizes graph structure, while the rest of baselines are GNN-based methods utilizing both node features and graph structure as input. Hyper-parameters of baselines are set as default in Python packages or their open-source codes unless otherwise stated.</p><p>• Multi-layer Perceptron (MLP) and Logistic Regression (LR) are feature-based methods that do not consider the graph structure. We set solver='lbfgs' for LR and hidden layer sizes=50 for MLP using Python sklearn package.</p><p>• Label Propagation (LPA) <ref type="figure" target="#fig_1">(Zhu et al., 2005)</ref>, on the other hand, only consider the graph structure and ignore node features. We set the iteration of LPA as 20 in our implementation.</p><p>• Graph Convolutional Network (GCN) <ref type="bibr" target="#b6">(Kipf &amp; Welling, 2017)</ref> proposes a first-order approximation to spectral graph convolutions.</p><p>• Graph Attention Network (GAT) <ref type="bibr" target="#b19">(Veličković et al., 2018)</ref> propose an attention mechanism to treat neighbors differently in the aggregation step.</p><p>• Jumping Knowledge Networks (JK-Net) <ref type="bibr" target="#b21">(Xu et al., 2018)</ref> leverages different neighborhood ranges for each node to enable structure-aware representation. We use concat as the aggregator for JK-Net.</p><p>• Graph Sampling and Aggregation (GraphSAGE) <ref type="bibr" target="#b2">(Hamilton et al., 2017</ref>) is a mini-batch implementation of GCN that uses neighborhood sampling strategy and different aggregation schemes. We use mean as the aggregator for GraphSAGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Experimental Setup</head><p>Our experiments focus on the transductive setting where we only know labels of part of nodes but have access to the entire graph as well as features of all nodes. 3 The ratio of training, validation, and test set are set as 6 : 2 : 2. The weight of each edge is treated as a free variable during training. We train our model for 200 epochs using Adam <ref type="bibr" target="#b5">(Kingma &amp; Ba, 2015)</ref> and report the test set accuracy when validation set accuracy is maximized. Each experiment is repeated three times and we report the mean and the 95% confidence interval. We initialize weights according to <ref type="bibr" target="#b0">(Glorot &amp; Bengio, 2010)</ref> and row-normalize input features. During training, we apply L2 regularization to the transformation matrices and use the dropout technique <ref type="bibr" target="#b17">(Srivastava et al., 2014)</ref>. The settings of all other hyper-parameters can be found in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results</head><p>The results of node classification are summarized in <ref type="table">Table 2</ref>. <ref type="table">Table 2</ref> indicates that only using node features (MLP, LR) or graph structure (LPA) will lead to information loss and can-   not fully exploit datasets in general. The results demonstrate that our proposed GCN-LPA model surpasses state-of-theart GCN/GNN baselines. We note that JK-Net is a strong baseline on Cora, but it does not perform consistently well on other datasets.</p><p>We investigate the influence of the number of LPA iterations and the training weight of LPA loss term λ on the performance of classification. The results on Citeseer dataset are plotted in <ref type="figure">Figures 3 and 4</ref>, respectively, where each line corresponds to a given number of GCN layers in GCN-LPA. From <ref type="figure">Figure 3</ref> we observe that the performance is boosted at first when the number of LPA iterations increases, then the accuracy stops increasing and decreases since a large number of LPA iterations will include more noisy nodes. <ref type="figure">Figure 4</ref> shows that training without the LPA loss term (i.e., λ = 0) is more difficult than the case where λ = 1 ∼ 5, which justifies our aforementioned claim that it is hard for the GCN part to learn both transformation matrices W and edge weights A simultaneously without the assistance of LPA regularization.</p><p>To further show how much the LPA impacts the performance, we vary the ratio of labeled nodes in LPA from 100% to 0% during training, and report the result of acuracy on Citeseer dataset in <ref type="table" target="#tab_4">Table 3</ref>. From <ref type="table" target="#tab_4">Table 3</ref> we observe that the performance of GCN-LPA gets worse when the ratio of labeled nodes in LPA decreases. In addition, using more labeled nodes in LPA also helps improve the model stability.</p><p>Note that a ratio of 0% does not mean that GCN-LPA is equivalent to GCN <ref type="bibr" target="#b6">(Kipf &amp; Welling, 2017)</ref> because the edge weights in GCN-LPA is still trainable, which increases the risk of overfitting the training data.</p><p>We study the training time of GCN-LPA on random graphs. We use the one-hot identity vector as feature and 0 as label for each node. The size of training set and validation set is 100 and 200, respectively, while the rest is test set. The average number of neighbors for each node is set as 5, and the number of nodes is varied from one thousand to one million. We run GCN-LPA and GCN for 100 epochs on a Microsoft Azure virtual machine with 1 NVIDIA Tesla M60 GPU, 12 Intel Xeon CPUs (E5-2690 v3 @2.60GHz), and 128GB of RAM, using the same hyper-parameter setting as in Cora. The training time per epoch of GCN-LPA and GCN is presented in <ref type="figure" target="#fig_2">Figure 5</ref>. Our result shows that GCN-LPA requires only 9.2% extra training time on average compared to GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>We studied the theoretical relationship between two types of well-known graph-based models for node classification, Label Propagation Algorithm and Graph Convolutional Neural Networks, from the perspectives of feature/label smoothing and feature/label influence. We then propose a unified model GCN-LPA, which learns transformation matrices and edge weights simultaneously in GCN with the assistance of LPA regularizer. We also analyze why our unified model performs better than traditional GCN in node classification. Experiments on five datasets demonstrate that our model outperforms state-of-the-art baselines, and it is also highly time-efficient with respect to the size of a graph.</p><p>We point out two avenues of possible directions for future work. First, our proposed model focuses on transductive setting where all node features and the entire graph structure are given. An interesting problem is how the model performs in inductive setting where we have no access to test nodes during training. Second, the question of how to generalize the idea of our model to GNNs with different aggregation functions (e.g., concatenation or max-pooling) is also a promising direction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Theorem 1</head><p>Proof. Denoteã ij = a ij /d ii as the normalized weight of edge (j, i). It is clear that j∈N (i)ã ij = 1. Given that M is differentiable, we perform a first-order Taylor expansion with Peano's form of remainder at x i for j∈N (i)ã ij y j :</p><formula xml:id="formula_28">j∈N (i)ã ij y j = j∈N (i)ã ij M(x j ) = j∈N (i)ã ij M(x i ) + ∂M(x i ) ∂x (x j − x i ) + o( x j − x i 2 ) = M(x i ) + ∂M(x i ) ∂x j∈N (i)ã ij (x j − x i ) + j∈N (i)ã ij o( x j − x i 2 ) = y i − ∂M(x i ) ∂x i + j∈N (i)ã ij o( x j − x i 2 ).<label>(22)</label></formula><p>According to Cauchy-Schwarz inequality and L-Lipschitz property, we have</p><formula xml:id="formula_29">∂M(x i ) ∂x i ≤ ∂M(x i ) ∂x 2 i 2 ≤ L i 2 .<label>(23)</label></formula><p>Therefore, the approximation of y i is bounded by</p><formula xml:id="formula_30">y i − j∈N (i)ã ij y j = ∂M(x i ) ∂x i − j∈N (i)ã ij o( x j − x i 2 ) ≤ ∂M(x i ) ∂x i + j∈N (i)ã ij o( x j − x i 2 ) ≤ L i 2 + o max j∈N (i) ( x j − x i 2 ) .<label>(24)</label></formula><p>B. Proof of Theorem 2</p><p>Before proving Theorem 2, we first give two lemmas that demonstrate the exact form of feature influence and label influence defined in this paper. The relationship between feature influence and label influence can then be deduced from their exact forms.</p><p>Lemma 1 Assume that the nonlinear activation function in GCN is ReLU.</p><formula xml:id="formula_31">Let P a→b k be a path [v (k) , v (k−1) , · · · , v (0) ] of length k from node v a to node v b , where v (k) = v a , v (0) = v b , and v (i−1) ∈ N (v (i) ) for i = k, · · · , 1. Then we havẽ I f (v a , v b ; k) = P a→b k 1 i=kã v (i−1) ,v (i) ,<label>(25)</label></formula><p>whereã v (i−1) ,v (i) is the normalized weight of edge (v (i) , v (i−1) ).</p><p>Proof. See <ref type="bibr" target="#b21">(Xu et al., 2018)</ref> for the detailed proof.</p><p>The product term in Eq. <ref type="formula" target="#formula_2">(25)</ref> is the probability of a given path P a→b k . Therefore, the right hand side in Eq. (25) is the sum over probabilities of all possible paths of length k from v a to v b , which is the probability that a random walk starting at v a ends at v b after taking k steps. Note that the propagation of v a 's label to v 3 is cut off since v 3 is labeled thus absorbing v a 's label. (b) v a 's label that propagated to v 1 further propagates to v 2 and v b (yellow arrows). Meanwhile, v a 's label is reset to its initial value then propagates from v a again (green arrows). (c) Label propagation in iteration 3. Purple arrows denote the propagation of v a 's label starting from v a for the third time. (d) All possible paths of length no more than three from v a to v b containing unlabeled nodes only. Note that there is no path of length one from v a to v b .</p><formula xml:id="formula_32">Lemma 2 Let U a→b j be a path [v (j) , v (j−1) , · · · , v (0) ] of length j from node v a to node v b , where v (j) = v a , v (0) = v b , v (i−1) ∈ N (v (i)</formula><p>) for i = j, · · · , 1, and all nodes along the path are unlabeled except v (0) . Then we have</p><formula xml:id="formula_33">I l (v a , v b ; k) = k j=1 U a→b j 1 i=jã v (i−1) ,v (i) ,<label>(26)</label></formula><p>whereã</p><formula xml:id="formula_34">v (i−1) ,v (i) is the normalized weight of edge (v (i) , v (i−1) ).</formula><p>To intuitively understand this lemma, note that there are two differences between Lemma 1 and Lemma 2: (1) In Lemma 1, I f (v a , v b ; k) sums over all paths from v a to v b of length k, but in Lemma 2, I l (v a , v b ; k) sums over all paths from v a to v b of length no more than k. The is because in LPA, v b 's label is reset to its initial value after each iteration, which means that the label of v b serves as a constant signal that begins propagating in the graph again and again after each iteration. <ref type="formula" target="#formula_2">(2)</ref> In Lemma 1 we consider all possible paths from v a to v b , but in Lemma 2, the paths are restricted to contain unlabeled nodes only. The reason here is the same as above: Since the labels of labeled nodes are reset to their initial values after each iteration in LPA, the influence of v b 's label will be absorbed in labeled nodes, and the propagation of v b 's label will be cut off at these nodes. Therefore, v b 's label can only flow to v a along the paths with unlabeled nodes only. See <ref type="figure" target="#fig_3">Figure 6</ref> for an illustrating example showing the label propagation in LPA.</p><formula xml:id="formula_35">1 i=k−jã v (i−1) ,v (i) = k j=1 U a→b j 1 i=jã v (i−1) ,v (i) .<label>(32)</label></formula><p>Now Theorem 2 can be proved by combining Lemma 1 and Lemma 2:</p><p>Proof. Suppose that whether a node is labeled or not is independent of each other for the given graph. Then we have</p><formula xml:id="formula_36">E I l (v a , v b ; k) =E   k j=1 U a→b j 1 i=jã v (i−1) ,v (i)   = k j=1 E   U a→b j 1 i=jã v (i−1) ,v (i)   = k j=1 P a→b j Pr P a→b j is an unlabeled-nodes-only path 1 i=jã v (i−1) ,v (i) = k j=1 P a→b j β j 1 i=jã v (i−1) ,v (i) = k j=1 β jĨ f (v a , v b ; j).<label>(33)</label></formula><p>C. Proof of Theorem 3</p><p>Proof. Denote the set of labels as L. Since different label dimensions in y (·) a do not interact with each other when running LPA, the value of the y a -th dimension in y (·) a (denoted by y (·) a [y a ]) comes only from the nodes with initial label y a . It is clear that</p><formula xml:id="formula_37">y (k) a [y a ] = v b :y b =ya k j=1 U a→b j 1 i=jã v (i−1) ,v (i) ,<label>(34)</label></formula><p>which equals v b :y b =ya I l (v a , v b ; k) according to Lemma 2. Therefore, we have</p><formula xml:id="formula_38">Pr(ŷ a = y a ) = y (k) a [y a ] i∈L y (k) a [i] ∝ y (k) a [y a ] = v b :y b =ya I l (v a , v b ; k)<label>(35)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof of Theorem 4</head><p>In this proof we assume that the dimension of node representations is one, but note that the conclusion can be easily generalized to the case of multi-dimensional representations since the function D(x) can be decomposed into the sum of one-dimensional cases. In the following of this proof, we still use bold notations x i to denote node representations, but keep in mind that they are scalars rather than vectors.</p><p>We give two lemmas before proving Theorem 4. The first one is about the gradient of D(x):</p><formula xml:id="formula_39">Lemma 3 h (k) i = x (k) i − ∂D(x (k) ) ∂x (k) i . Proof. x (k) i − ∂D(x (k) ) ∂x (k) i = x (k) i − vj ∈N (vi)ã ij (x (k) i − x (k) j ) = vj ∈N (vi)ã ij x (k) j = h (k) i .</formula><p>It is interesting to see from Lemma 3 that the aggregation step in GCN is equivalent to running gradient descent for one step with a step size of one. However, this is not able to guarantee that D(h (k) ) ≤ D(x (k) ) because the step size may be too large to reduce the value of D.</p><p>The second lemma is about the Hessian of D(x):</p><p>Lemma 4 ∇ 2 D(x) 2I, or equivalently, 2I − ∇ 2 D(x) is a positive semidefinite matrix.</p><p>Proof. We first calculate the Hessian of D(x) = 1 2 vi,vjã ij x i − x j 2 2 :</p><formula xml:id="formula_40">∇ 2 D(x) =      1 −ã 11 −ã 12 · · · −ã 1n −ã 21 1 −ã 22 · · · −ã 2n . . . . . . . . . . . . −ã n1 −ã n2 · · · 1 −ã nn      = I − D −1 A.<label>(36)</label></formula><p>Therefore, 2I − ∇ 2 D(x) = I + D −1 A. Since D −1 A is Markov matrix (i.e., each entry is non-negative and the sum of each row is one), its eigenvalues are within the range [-1, 1], so the eigenvalues of I + D −1 A are within the range [0, 2]. Therefore, I + D −1 A is a positive semidefinite matrix, and we have ∇ 2 D(x) 2I.</p><p>We can now prove Theorem 4:</p><p>Proof. Since D is a quadratic function, we perform a second-order Taylor expansion of D around x (k) and obtain the following inequality:</p><formula xml:id="formula_41">D(h (k) ) =D(x (k) ) + ∇D(x (k) ) (h (k) − x (k) ) + 1 2 (h (k) − x (k) ) ∇ 2 D(x)(h (k) − x (k) )</formula><p>=D(x (k) ) − ∇D(x (k) ) ∇D(x (k) ) + 1 2 ∇D(x (k) ) ∇ 2 D(x)∇D(x (k) )</p><p>≤D(x (k) ) − ∇D(x (k) ) ∇D(x (k) ) + ∇D(x (k) ) ∇D(x (k) ) = D(x (k) ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(37)</head><p>Unifying Graph Convolutional Neural Networks and Label Propagation E. More Visualization Results on Karate Club Network <ref type="figure" target="#fig_5">Figure 7</ref> illustrates more visualization of GCN and GCN-LPA on karate club network. In each subfigure, we vary the number of layers from 1 to 4 to examine how the learned representations evolve. The initial node features are one-hot identity vectors, and the dimension of hidden layers and output layer is 2. The transformation matrices are uniformly initialized within range <ref type="bibr">[-1, 1]</ref>. We use sigmoid function as the nonlinear activation function. Comparing the four figures in each row, we conclude that the aggregation step and transformation step in GCN and GCN-LPA do benefit the separation of different classes. Comparing <ref type="figure" target="#fig_5">Figure 7a</ref> and 7c (or <ref type="figure" target="#fig_5">Figure 7b and 7d)</ref>, we conclude that more inter-class edges will make the separation harder for GCN (or GCN-LPA). Comparing <ref type="figure" target="#fig_5">Figure 7a</ref> and 7b (or <ref type="figure" target="#fig_5">Figure 7c and 7d)</ref>, we conclude that GCN-LPA is more noise-resistant than GCN, therefore, GCN-LPA can better differentiate classes and identify clustering substructures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Hyper-parameter Settings</head><p>The detailed hyper-parameter settings for all datasets are listed in <ref type="table" target="#tab_7">Table 4</ref>. In GCN-LPA, we use the same dimension for all hidden layers. Note that the number of GCN layers and the number of LPA iterations can actually be different since GCN and LPA are implemented as two independent modules. We use grid search to determine hyper-parameters on Cora, and perform fine-tuning on other datasets, i.e., varying one hyper-parameter per time to see if the performance can be further improved. The search spaces for hyper-parameters are as follows:</p><p>• Dimension of hidden layers: {8, 16, 32};</p><p>• # GCN layers: {1, 2, 3, 4, 5, 6};</p><p>• # LPA iterations: {1, 2, 3, 4, 5, 6, 7, 8, 9};</p><p>• L2 weight: {10 −7 , 2 × 10 −7 , 5 × 10 −7 , 10 −6 , 2 × 10 −6 , 5 × 10 −6 , 10 −5 , 2 × 10 −5 , 5 × 10 −5 , 10 −4 , 2 × 10 −4 , 5 × 10 −4 , 10 −3 };</p><p>• LPA weight (λ): {0, 1, 2, 5, 10, 15, 20};</p><p>•   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) A graph with two classes of nodes (red vs. blue) (b) Potential intra-class edges (bold links) Figure 1: A graph with two classes of nodes, while white nodes are unlabeled</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Node embeddings of Zachary's karate club network trained on a node classification task (red vs. blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Training time per epoch on random graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Paths from va to v b An illustrating example of label propagation in LPA. Suppose labels are propagated for three iterations, and no self-loop exists. Blue nodes are labeled while white nodes are unlabeled. (a) v a 's label propagates to v 1 (yellow arrows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Visualization of GCN and GCN-LPA with 1 ∼ 4 layers on karate club network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Dataset statistics after removing self-loops and duplicate edges.</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Coauthor-CS</cell><cell>Coauthor-Phy</cell></row><row><cell># nodes</cell><cell>2,708</cell><cell>3,327</cell><cell>19,717</cell><cell>18,333</cell><cell>34,493</cell></row><row><cell># edges</cell><cell>5,278</cell><cell>4,552</cell><cell>44,324</cell><cell>81,894</cell><cell>247,962</cell></row><row><cell># features</cell><cell>1,433</cell><cell>3,703</cell><cell>500</cell><cell>6,805</cell><cell>8,415</cell></row><row><cell># classes</cell><cell>7</cell><cell>6</cell><cell>3</cell><cell>15</cell><cell>5</cell></row><row><cell>Intra-class edge rate</cell><cell>81.0%</cell><cell>73.6%</cell><cell>80.2%</cell><cell>80.8%</cell><cell>93.1%</cell></row><row><cell>Table 1:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Mean and the 95% confidence intervals of test set accuracy for all methods and datasets.</figDesc><table><row><cell>Method</cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Coauthor-CS</cell><cell>Coauthor-Phy</cell></row><row><cell>MLP</cell><cell>64.6 ± 1.7</cell><cell>62.0 ± 1.8</cell><cell>85.9 ± 0.3</cell><cell>91.7 ± 1.4</cell><cell>94.1 ± 1.2</cell></row><row><cell>LR</cell><cell>77.3 ± 1.8</cell><cell>71.2 ± 1.8</cell><cell>86.0 ± 0.6</cell><cell>91.1 ± 0.6</cell><cell>93.8 ± 1.1</cell></row><row><cell>LPA</cell><cell>85.3 ± 0.9</cell><cell>70.0 ± 1.7</cell><cell>82.6 ± 0.6</cell><cell>91.3 ± 0.2</cell><cell>94.9 ± 0.4</cell></row><row><cell>GCN</cell><cell>88.2 ± 0.8</cell><cell>77.3 ± 1.5</cell><cell>87.2 ± 0.4</cell><cell>93.6 ± 1.5</cell><cell>96.2 ± 0.2</cell></row><row><cell>GAT</cell><cell>87.7 ± 0.3</cell><cell>76.2 ± 0.9</cell><cell>86.9 ± 0.5</cell><cell>93.8 ± 0.4</cell><cell>96.3 ± 0.7</cell></row><row><cell>JK-Net</cell><cell>89.1 ± 1.2</cell><cell>78.3 ± 0.9</cell><cell>85.8 ± 1.1</cell><cell>92.4 ± 0.4</cell><cell>94.8 ± 0.4</cell></row><row><cell>GraphSAGE</cell><cell>86.8 ± 1.9</cell><cell>75.2 ± 1.1</cell><cell>84.7 ± 1.6</cell><cell>92.6 ± 1.6</cell><cell>94.5 ± 1.1</cell></row><row><cell>GCN-LPA</cell><cell>88.5 ± 1.5</cell><cell>78.7 ± 0.6</cell><cell>87.8 ± 0.6</cell><cell>94.8 ± 0.4</cell><cell>96.9 ± 0.2</cell></row><row><cell>Table 2:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Citation networks: We consider three citation network</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Result of GCN-LPA on Citeseer dataset with differet ratio of labeled nodes in LPA.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Zhang, X. and Lee, W. S. Hyperparameter learning for graph based semi-supervised learning algorithms. In Advances in Neural Information Processing Systems, 2007.Zhang, Z. and Wang, J. Mlle: Modified locally linear embedding using multiple weights. In Advances in Neural Information Processing Systems, 2007.</figDesc><table><row><cell>Zhou, D., Bousquet, O., Lal, T. N., Weston, J., and</cell></row><row><cell>Schölkopf, B. Learning with local and global consis-</cell></row><row><cell>tency. In Advances in Neural Information Processing</cell></row><row><cell>Systems, 2004.</cell></row><row><cell>Zhu, X., Ghahramani, Z., and Lafferty, J. D. Semi-</cell></row><row><cell>supervised learning using gaussian fields and harmonic</cell></row><row><cell>functions. In Proceedings of the 20th International Con-</cell></row><row><cell>ference on Machine Learning, 2003.</cell></row><row><cell>Zhu, X., Lafferty, J., and Rosenfeld, R. Semi-supervised</cell></row><row><cell>learning with graphs. PhD thesis, Carnegie Mellon Uni-</cell></row><row><cell>versity, school of language technologies institute, 2005.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Dropout rate: {0, 0.1, 0.2, 0.3, 0.4, 0.5}; • Learning rate: {0.01, 0.02, 0.05, 0.1, 0.2, 0.5};</figDesc><table><row><cell></cell><cell>Cora</cell><cell>Citeseer</cell><cell>Pubmed</cell><cell>Coauthor-CS</cell><cell>Coauthor-Phy</cell></row><row><cell>Dimension of hidden layers</cell><cell>32</cell><cell>16</cell><cell>32</cell><cell>32</cell><cell>32</cell></row><row><cell># GCN layers</cell><cell>5</cell><cell>2</cell><cell>2</cell><cell>2</cell><cell>2</cell></row><row><cell># LPA iterations</cell><cell>5</cell><cell>5</cell><cell>1</cell><cell>2</cell><cell>3</cell></row><row><cell>L2 weight</cell><cell>1 × 10 −4</cell><cell>5 × 10 −4</cell><cell>2 × 10 −4</cell><cell>1 × 10 −4</cell><cell>1 × 10 −4</cell></row><row><cell>LPA weight (λ)</cell><cell>10</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>1</cell></row><row><cell>Dropout rate</cell><cell>0.2</cell><cell>0</cell><cell>0</cell><cell>0.2</cell><cell>0.2</cell></row><row><cell>Learning rate</cell><cell>0.05</cell><cell>0.2</cell><cell>0.1</cell><cell>0.1</cell><cell>0.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Hyper-parameter settings for all datasets.</figDesc><table><row><cell></cell><cell cols="2">Unifying Graph Convolutional Neural Networks and Label Propagation</cell><cell></cell></row><row><cell>1-layer</cell><cell>2-layer</cell><cell>3-layer</cell><cell>4-layer</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Here we somewhat abuse the notations for simplicity, since in Theorem 3 the two notations represent label category rather than label distribution. But the subtle difference can be easily distinguished based on context.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The experimental setting here is the same as GCN<ref type="bibr" target="#b6">(Kipf &amp; Welling, 2017)</ref>. But note that our method can be easily generalized to inductive case if implemented in a way similar to GraphSAGE<ref type="bibr" target="#b2">(Hamilton et al., 2017)</ref>.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Proof. As mentioned above, a significant difference between LPA and GCN is that all labeled nodes are reset to its original labels after each iteration in LPA. This implies that the initial label y b of node v b appears not only as y <ref type="bibr">(0)</ref> b , but also as every y</p><p>a for j = 0, 1, · · · , k − 1:</p><p>According to the updating rule of LPA, we have</p><p>In the above equation, the derivative</p><p>is decomposed into the weighted average of</p><p>, where v z traverses all neighbors of v a . For those v z 's that are initially labeled, y (k−1) z is reset to their initial labels in each iteration. Therefore, they are always constant and independent of y consider the terms where v z is an unlabeled node:</p><p>where z &gt; m means v z is unlabeled. To intuitively understand Eq. (29), one can imagine that we perform a random walk starting from node v a for one step, where the "transition probability" is the edge weightsã, and all nodes in this random walk are restricted to unlabeled nodes only. Note that we can further decompose every y (k−1) z in Eq. (29) in the way similar to what we do for y (k) a in Eq. (28). So the expansion in Eq. (29) can be performed iteratively until the index k decreases to j. This is equivalent to performing all possible random walks for k − j steps starting from v a , where all nodes but the last in the random walk are restricted to be unlabeled nodes:</p><p>where v z in the first summation term is the end node of a random walk, U a→z k−j in the second summation term is an unlabeled-nodes-only path from v a to v z of length k − j, and the product term is the probability of a given path U a→z k−j . Consider the last term</p><p>in Eq. (30). We know that = 1 for z = b, which means that only those random-walk paths that end exactly at v b (i.e., the end node v z is exactly v b ) count for the computation in Eq. (30). Therefore, we have ∂y</p><p>where U a→b k−j is a path from v a to v b of length k − j containing only unlabeled nodes except v b . Substituting the right hand term of Eq. (27) with Eq. (31), we obtain that</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 13th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Label propagation via teaching-to-learn and learning-to-teach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Lystems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sparsity induced similarity measure for label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th IEEE International Conference on Computer Vision</title>
		<meeting>the 12th IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Manifold-based similarity adaptation for label propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karasuyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mamitsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conference on Learning Representations</title>
		<meeting>the 3rd International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations</title>
		<meeting>the 5th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Understanding black-box predictions via influence functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An iterative locally linear embedding algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Coference on International Conference on Machine Learning</title>
		<meeting>the 29th International Coference on International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-M</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 32nd AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lanczosnet: Multi-scale deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
		<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to propagate labels: Transductive propagation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
		<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geniepath</surname></persName>
		</author>
		<title level="m">Graph neural networks with adaptive receptive paths</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>The 33rd AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph markov neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gmnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Nonlinear dimensionality reduction by locally linear embedding. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">290</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pitfalls of graph neural network evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems Workshop on Relational Representation Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Attention-based graph neural network for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Thekumparampil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.03735</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Label propagation through linear neighborhoods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Representation learning on graphs with jumping knowledge networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sonobe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-I</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations</title>
		<meeting>the 7th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An information flow model for conflict and fission in small groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Zachary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of anthropological research</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07294</idno>
		<title level="m">Gated attention networks for learning on large and spatiotemporal graphs</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
