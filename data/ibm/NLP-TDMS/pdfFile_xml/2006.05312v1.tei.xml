<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Feature Interaction based Neural Network for Click-Through Rate Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dafang</forename><surname>Zou</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leiming</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiafa</forename><surname>Mao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Weiguo</forename><surname>Sheng</surname></persName>
						</author>
						<title level="a" type="main">Feature Interaction based Neural Network for Click-Through Rate Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Click-Through Rate</term>
					<term>Neural Network</term>
					<term>journal</term>
					<term>Feature Interaction</term>
					<term>paper</term>
					<term>template</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Click-Through Rate (CTR) prediction is one of the most important and challenging in calculating advertisements and recommendation systems. To build a machine learning system with these data, it is important to properly model the interaction among features. However, many current works calculate the feature interactions in a simple way such as inner product and element-wise product. This paper aims to fully utilize the information between features and improve the performance of deep neural networks in the CTR prediction task. In this paper, we propose a Feature Interaction based Neural Network (FINN) which is able to model feature interaction via a 3-dimention relation tensor. FINN provides representations for the feature interactions on the the bottom layer and the non-linearity of neural network in modelling higher-order feature interactions. We evaluate our models on CTR prediction tasks compared with classical baselines and show that our deep FINN model outperforms other state-of-the-art deep models such as PNN and DeepFM. Evaluation results demonstrate that feature interaction contains significant information for better CTR prediction. It also indicates that our models can effectively learn the feature interactions, and achieve better performances in real-world datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>M ANY industrial applications scuh as Online advertising <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, recommender system <ref type="bibr">[3]</ref>, and web search <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> are Cost per Click(CPC) for most of Internet companies. In the CPC advertising system, the RankSore of advertisements is generally determined by the product of bid price and measurement of advertising items <ref type="bibr" target="#b5">[6]</ref>. In advertising systems, click-through rate (CTR) prediction is generally used as a measure of advertising items.</p><formula xml:id="formula_0">RankScore = bid × CT R<label>(1)</label></formula><p>Therefore, Correctly predicting CTR of ads is a prerequisite for ensuring reve-nue and user experience. The CTR prediction problem is a typical supervised machine learning problem whose goal is to accurately predict the probability of users behavior under the premise of a given advertising item, user and query context. The features of this supervised learning problem are denoted as x, and the target is denoted as y. By collecting online advertisements and click logs, we can obtain a large number of labeled samples (x i , y i ) as training data for supervised learning. We use a parametric w to model this probability:</p><p>CT R = P robalility(click|AD, U ser, Query) = f (x, w)</p><p>This supervised learning is an optimization problem where the search a proper w minimizes the objective loss function L(y, f (x, w)). The negative Log-Likelihood function is generally used as the loss function in the CTR prediction problem. Many classical machine learning models, including logistic regression (LR) <ref type="bibr" target="#b6">[7]</ref>, Bayesian models <ref type="bibr" target="#b7">[8]</ref>, polynomial-2 (Poly2) <ref type="bibr" target="#b8">[9]</ref>, gradient boosting decision tree <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, tensor-based models <ref type="bibr" target="#b13">[13]</ref>, and factorization machines (FM) <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref>, <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref>, have been proposed in this field. In order to improve the fitting ability of complex relationships, in feature en-gineering, first-order discrete features are often combined in pairs to form higher-order combined features. To build an effective machine learning (ML) model with CTR prediction, it is crucial to model interactions between features. It has been proved in the Kaggle competition that crafting combinatorial features is an effective way for CTR prediction <ref type="bibr" target="#b18">[18]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b20">[20]</ref>, <ref type="bibr" target="#b21">[21]</ref>. For example, in the CTR predic-tion task, the row data has two discrete features: language and type. In order to improve the fitting ability, we can cross feature language={Chinese, English} with type={movie, teleplay} and get a new feature language type={Chinese movie, Chinese teleplay, English movie, English teleplay}. However, the performance improvement comes with high cost, since it requires a lot of manual feature engineering as well as domain knowledge to crafting effective feature interactions, and the construction process is not universal.</p><p>Instead of designing new featues manually, another way is to apply machine learning algorithms to learn feature interactions from raw data automatically. Factorization machines (FM) <ref type="bibr" target="#b14">[14]</ref>, proposed to solve the automatic feature combination problem via inner product of feature embedding vectors, which is regarded as one of the most successful embedding models <ref type="bibr" target="#b22">[22]</ref> in this field. Deep neural networks (DNN) have achieved success in image classification <ref type="bibr" target="#b23">[23]</ref>, <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, natural language processing (NLP) <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref> and speech recognition <ref type="bibr" target="#b28">[28]</ref> over the past years. DNN can automatically capture feature representations and dependencies for prediction purpose compared with linear models. As a result, several DNN based methods for CTR prediction has been proposed in this field. One challenge of applying DNN in CTR prediction is sparsity. Most of these data (such as user ID, gender and city etc.) in CTR problem are non-contiguous and discrete, which are typically converted to a set of highdimensional sparse fea-tures via the one-hot encoding <ref type="bibr" target="#b29">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b18">[18]</ref>. For those spare data, they firstly need to be converted into dense feature embedding vectors before inputing to the neural networks. Besides the transformation for the raw features, we also need to provide representations arXiv:2006.05312v1 [cs.LG] 7 Jun 2020 for the feature interactions in the neural networks structure because the representing feature interactions is crucial for CTR prediction tasks. Intuitively, we can directly utilize the feature interactions vector to represent interactions of two features. However, the direct representation of two features usually leads to little sufficient value whose parameters cannot obtain adequate training since row data are sparse. Therefore, we could utilize two feature embedding vectors to caculate feature interactions, such as inner <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b21">[21]</ref> and element-wise product <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref>.</p><p>In this work, we propose a novel model for CTR prediction named Feature Interaction based Neural Network (FINN), which enhances DNNs by modelling 2-order feature interactions after feature embedding. By employing a new feature interaction operation in neural network modelling, We have improved the ability of neural networks to learn feature cross information. In order to improve performance further, we also deepen the shallow model combining a classical deep neural network(DNN) component, modelling higherorder and nonlinear feature interactions effectively to improve expressiveness. In contrast to traditional methods that simply perform inner product or element-wise on embedding vectors to model feature interaction in the low level, our proposed feature mechanism encodes more informative feature interactions, greatly facilitating the following deep layers to learn meaningful information. We conduct extensive experiments on two real-world datasets and show that our model out-performs other models such as factorization machine(FM) and the other state-of-the-art deep models such as PNN and DeepFM.</p><p>The remainder of the paper is organized as follows. Related works are introduced in Section 2. The model details are described in Section 3. Section 4 exhib-its experiments analysis. Finally, we conclude the paper in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Many methods have been proposed to process high dimensional sparse data in literature. The section will review word embedding and neural network based methods, which are related to our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Shallow Methods</head><p>Logistic regression (LR) <ref type="bibr" target="#b34">[34]</ref> is a linear model, which is a widely used model in CTR task. Its corresponding optimization problem has very good properties, which is an unconstrained convex optimization problem with a globally unique optimal solution. It supports large-scale features and can quickly converge to the optimal solution through the commonly used gradient descent method. The interpretability of the logistic regression model is very good. Through the weights corresponding to the features, we can analyze the importance of each feature and their influence on the click rate. However, its expression ability is relatively weak as a linear model, and improve the expression ability of the model through a large number of feature engineering, such as feature combination.</p><p>Factorization machine(FM) <ref type="bibr" target="#b14">[14]</ref> proposed to learn the feature interactions using inner product, which is one of the most successful CTR model.</p><formula xml:id="formula_2">y F M (x) = w 0 + n i=1 w i x i + n i=1 n j=i+1 v i , v j · x i x j (3) where w 0 ∈ R is the global bias, w i ∈ R is the weight of i-th feature, v i ∈ R k is k-dimensional vector,</formula><p>and v i , v j defines inner product. FM performers well on large sparse data, and it has a low time complexity. Compared with the FM model, the Field-aware factorization machine (FFM) <ref type="bibr" target="#b16">[16]</ref>, <ref type="bibr" target="#b17">[17]</ref> introduces the concept of field. In FM, feature i using the same vector makes interactions with other features, while in FFM, feature i using different vectors makes interactions with features of different fields, which further improves expressiveness of the model. However, FFM was limmited by the need of large memory and cannot easily be applied in real CTR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Neural Network based Methods</head><p>Deep learning has achieved great success in many research fields such as computer vision <ref type="bibr" target="#b35">[35]</ref>, <ref type="bibr" target="#b36">[36]</ref> and natural language processing <ref type="bibr" target="#b37">[37]</ref>. As a result, many deep learning based CTR models have also been proposed in recent years <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref>. How to effectively model the feature interactions is the key factor for most of these neural network based models.</p><p>In traditional DNN model, the input of network is usually dense and numberical, while the case of CTR task data whose dimension could over one million after one-hot encoding is not directly applicable. To deal with such an issue, factorization machine supported neural networks (FNN) has also been proposed, which combines an embedding layer pretrained by FM to convert sparse features to a low dimension and dense space and a DNN component to capture high-order feature interactions <ref type="bibr" target="#b22">[22]</ref>. To strengthen models capacity of feature interactions, product-based neural network (PNN) <ref type="bibr" target="#b21">[21]</ref> and its extension product-network in network (PIN) <ref type="bibr" target="#b41">[41]</ref> introduces product operations performed on the embedding layer before applying full-connected DNN. Wide &amp; Deeps model trains both the shallow component and deep component <ref type="bibr" target="#b42">[42]</ref>  Similarly, deepFM jointly train the FM part and the DNN part, which is regarded as one state-of-the-art model. As for neural factorization machines (NFM) <ref type="bibr" target="#b32">[32]</ref>, it is an approach used DNN to improve FM. As mentioned, FM models the same weights of feature interactions over to the CTR result prediction. Supposing the contribution of each feature interactions to CTR prediction result is different, attention neural factorization machines (AFM) <ref type="bibr" target="#b33">[33]</ref> utilized an attention mechanism originated from Neural Machine Translation (NMT) field <ref type="bibr" target="#b44">[44]</ref> to learn the weights of feature interactions. Deep Interest Network (DIN) <ref type="bibr" target="#b5">[6]</ref> designs an a local activation unit structure to adaptively capture user diverse interests from historical behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>Our methods main purpose is to model the feature interaction representation in a more effective way. To this end, we propose Feature Interaction based Neural Network for CTR prediction tasks.</p><p>In this section, we will present the proposed model from the following parts: sparse input layer, embedding layer, Feature-Interaction layer, combination layer, multiple hidden layers and output layer. The architecture of the FINN model as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>. The logistic regression part is not shown for clarity purpose. The sparse input layer and embedding layer are the same with DeepFM <ref type="bibr" target="#b43">[43]</ref>, which adopts a sparse representation for input features and embeds the raw feature input into a dense vector. The following Feature-Interaction layer models second order feature interactions on the original embedding. Subsequently, these cross features are concatenated by a combination layer which merges the outputs of Feature-Interaction layer. At last, we feed the cross features into a deep neural network and the network outputs the prediction score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sparse Input and Embedding layer</head><p>Unlike image classification or speech recognition, the input data in CTR task are usually non-contiguous and categorical. To represent for raw input features, they are normally converted to a high-dimensional sparse features via the onehot encoding. For example, user id = {001, 002, }, goods = {book, basketball, } and gender = {male, female}. By employing the one-hot encoding, one input instance can be transformed into: The dimension of above features, especially the user ID and goods type, will become huge after converting. For example, if the number of goods is 550, the dimension of goods feature will increase to 550 after coding, and only one of the 550 values is effective. The sparseness of the coded feature suggest that DNNs is not directly applicable, a solution to utilize the neural network in these case is embed sparse feature into a continuous, dense real-value vector space with a low dimension. The architecture of embedding layer is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. The result of embedding layer is a wide concatenated field embedding vector:</p><formula xml:id="formula_3">[1, 0, 0, 0, ..., 0] user id=001 [0, 1, 0, 0, ..., 0] goods=book [0, 1] gender=f emale</formula><formula xml:id="formula_4">E = [e 1 , e 2 , , e i , , e m ]</formula><p>where m denotes the number of fields, e i ∈ R k denotes the embedding vector of i-th field, and k is the dimension of embedding vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Feature-Interaction Layer</head><p>To improve prediction accuracy for CTR task, it is useful to provide representations for the feature interactions after raw features embedding layer. The Feature-Interaction layer aims to model the second order feature relations in a precise and effective way. Intuitively, we can directly utilize the feature interactions vector p ij to represent interaction of i-th feature and j-th feature. The number of the feature interaction vector is n(n − 1)/2 , where n denotes the number of coded features. However, it is difficult to adequately train the vector p ij in practical application scenarios where data sparsity is widespread. The reason is that the training of each parameter pij requires a large number of samples with non-zero x i and x j . Because the sample data is inherently sparse, there will be very few samples that satisfy both x i and x j . Insufficient training samples can easily lead to inaccurate p ij parameters, which will ultimately seriously affect the performance of the model. To deal with such an issue, one of solutions is using embedding vector x to caculate the interaction vector p. Inner product and element-wise product are currently the most classical methods for calculate feature interaction. Shallow models such as FM and FFM commonly employ inner product, deep model such as PNN and NFM widely adopt the elementwise product. The terms of inner product and element-wise product can be respectively defined as:</p><formula xml:id="formula_5">f inner (ε) = {(v i · v j )x i x j } (i,j)∈R X (4) f element−wise (ε) = {(v i v j )x i x j } (i,j)∈R X<label>(5)</label></formula><p>where R X = {(i, j)} i∈X ,j∈X ,j&gt;i }, v i is the i-th embedding vector, · defines the inner product, and defines the elementwise product. () k defines k-th dimension value of the vector, that is,</p><formula xml:id="formula_6">(v i v j ) k = v ik v jk .</formula><p>A major problem with inner product and element-wise product methods in interaction representation is that they are too simple to effectively calculate the interactions of feature vectors. Therefore, we propose a method to represent feature interaction vector. We take the i-th feature vector and the j-th feature vector as an example, the feature interaction vector p ij of the two feature vectors can be defined as:</p><formula xml:id="formula_7">p ij = [p 1 ij , ..., p u ij , ..., p l ij ]<label>(6)</label></formula><p>where p u ij is the u-th dimension value of interaction vector, l is the dimension of interaction vector p u ij can be expressed as:</p><formula xml:id="formula_8">p u ij = v i · W u · v T j<label>(7)</label></formula><p>where W ∈ R k×k×l is the 3-dimensional tensor. Each slice W u,u∈{1,2,...,l} of the tensor W represents the i-th relation matrix. <ref type="figure" target="#fig_3">Figure 3</ref> shows the representation of different feature interaction methods. Based on the original embedding E, we get the result of the Feature-Interaction layer, which denotes as {p 1 , ..., p i , ..., p n }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Deep Network</head><p>These interaction vectors p are concatenated and then feeds into the deep component which is a feed-forward neural network. The definition of result of combination layer is as follows:</p><formula xml:id="formula_9">F concat (p 1 , ..., p i , ..., p n ) = [c 1 , ..., c i , ..., c k ]<label>(8)</label></formula><p>The deep network is used to captures high-order interaction between features and generate the model result. Let h (0) = [c 1 , c 2 , ..., c n ] denotes the inputs of the deep network, where n is the total size of interaction vectors. Formally, the definition of each full-connected neural network layer is as follow:</p><formula xml:id="formula_10">h (l) = σ(W (l) h (l−1) + b (l) )<label>(9)</label></formula><p>where l is the layer number of deep network and σ is the activation function. W l ∈ R D l+1 ×D l , b (l) , and h l ∈ R D l are the model weight, bias and output , respectively, of the l-th layer. The deep network is allowed to capture higher-order feature interactions by non-linear activation functions, such as sigmoid, tanh, and ReLU. At last, the output vector of the last neural network layer is generated which is used to caculate the final CTR prediction:</p><formula xml:id="formula_11">y d = σ(W |L|+1 h |L| + b |L|+1 )<label>(10)</label></formula><p>where |L| is the depth of DNN, σ is the sigmoid function, defined as σ(x) = 1/(1 + e −x ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Output Layer and Learning</head><p>To summarize, we show the overall formulation of INN models output as:ŷ</p><formula xml:id="formula_12">= σ(w 0 + n i=1 w i x i + y d )<label>(11)</label></formula><p>where y ∈ (0, 1) is the value of CTR prediction, σ is the sigmoid function, n is the total size of feature, x is a sparse coded input and wi is the i-th weight of sparse feature. In the experiments, we evaluate the performance of our method on CTR task and aim to minimize the loss function as follow:</p><formula xml:id="formula_13">loss = x∈X −y i (x) log(ŷ i (x)) + (1 − y i (x)) log(1 −ŷ i (x))<label>(12)</label></formula><p>where y i (x) is the ground truth of instance x,ŷ i is the prediction value of CTR, and X denotes the set of instances for training.</p><p>It should be noted that if we remove the neural network part of our model, the relationship dimension u in the relationship tensor is set to 1, and the relationship matrix W is set to      1 0 · · · 0 0 1 · · · 0 . . . . . . . . . . . .</p><formula xml:id="formula_14">0 0 · · · 1      ∈ R k×k ,</formula><p>where k is the dimension of feature vector, so our method is equivalent to FM. If we sum each element in vector c and then use a sigmoid function to output a prediction value, we have a shallow CTR model. In practice, the size of the training data is usually relatively large when training deep neural networks, especially in CTR task. If it is necessary to calculate the gradient on the entire training data in each iteration during the training process, more computing resources are required. Therefore, Mini-Batch Gradient Descent is often used to train deep neural networks. Learning rate is an important hy-perparameter in neural network optimization. The value of the learning rate α is very critical in the gradient descent methods. If it is too large, it will not converge. If it is too small, the convergence rate is too slow. Therefore, we often use some methods to adaptively adjust the learning rate in practice, such as AdaGrad <ref type="bibr" target="#b45">[45]</ref>, RMSprop, AdaDelta, Adam <ref type="bibr" target="#b46">[46]</ref>, etc. Adam algorithm is a combination of RMSProp and momentum method. In our experiment, we use Adam as the optimizer to learn the parameters, which is denoted as:</p><formula xml:id="formula_15">M t = β 1 M t−1 + (1 − β 1 )g t<label>(13)</label></formula><formula xml:id="formula_16">G t = β 2 G t−1 + (1 − β 2 )g t g t<label>(14)</label></formula><formula xml:id="formula_17">∆θ t = −α M t /(1 − β t 1 ) (G t /(1 − β t 2 )) + ε<label>(15)</label></formula><p>where, β 1 and β 2 are the decay rates of the two moving averages, and the values are usually β 1 = 0.9, β 2 = 0.99, ε is a small constant for numerical stability and the value is empirically setted to 10 −8 , M t and G t is first moment and second moment, g t is the real gradient at training step t. 1) Dropout: While neural networks have achieved great performance, they are also easy to overfit the training data. When training a deep neural network, we can randomly discard a part of neurons (at the same time discard their corresponding connected edges) to avoid overfitting. This method is called the Dropout <ref type="bibr" target="#b47">[47]</ref>. The neurons chosen to be discarded each time are random. For each neuron, there is a probability p to determine whether to keep it. The average number of activated neurons is the original p percent during train, while all neurons can be activated during test. In the INN model, to avoid feature interactions representations overfit the data, we apply dropout in the feature-interaction layer. Specifically, after feture-interaction layer, we randomly drop of concatenated vector with a certain drop-out ratio p. Moreover, we also apply dropout in neural networks to prevent the learning from overfitting.</p><p>2) Batch Normalization: In the training of deep neural network, the input of a middle layer is the output of the previous neural layer. Therefore, changes in the parameters of the neural layer will cause a large difference in the distribution of its output. From the perspective of machine learning, if the input distribution of a neural layer changes, then its parameters need to be relearned. This phenomenon is called Internal Covariate Shift. To deal with this problem, it is necessary to make the distribution of the input of each neural layer consistent during the training process. The simplest and most direct method is to normalize each neural layer to make its distribution stable. The Batch Normalization (BN) <ref type="bibr" target="#b48">[48]</ref> method is an effective layer-by-layer normalization method that can normalize any intermediate layer in the neural network. In INN, to avoid the update of feature interactions changing the input distribution to neural networks layers, we apply BN on the output of the feature-interaction layer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Difference with Other Neural Network based Methods</head><p>The neural networks based methods can be summarized into the following steps as: (1) An embedding layer maps highdimensional sparse feature into low-dimensional distributed representations. (2) Several operations are applied on the embedding vectors to get the medial features.</p><p>(3) A multi-layer perceptron (M LP ) is applied after the fusion of embedding vector to learn nonlinear relations among features.</p><p>The fusion of embedding vector is the most intensive part of deep learning model. This layer is the input layer to enter the deep learning model. The quality of embedding fusion will affect the learning of DNN model. There are basically three operations: concatenate <ref type="bibr" target="#b42">[42]</ref>, <ref type="bibr" target="#b21">[21]</ref>, product <ref type="bibr" target="#b21">[21]</ref> or weight sum <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b5">[6]</ref> embedding vectors. We define neural networks based methods uniformly as:</p><formula xml:id="formula_18">y = net(f usion(embed(x)))<label>(16)</label></formula><p>FNN simply concatenates the embedding vector together as input to a multilayer neural network, thus lacking the intersection of features. FNN is defined as:</p><formula xml:id="formula_19">y F N N = net(concatenate(embed(x)))<label>(17)</label></formula><p>NFM method simply accumulates the feature intersection vectors directly after the feature intersection, which is defined as:ŷ P N N = net(concatenate(interaction(embed(x)))) (18)</p><p>Wide &amp; Deep can be considered to combine FNN and Linear method, which is defined as:</p><formula xml:id="formula_20">y W ide&amp;Deep =ŷ Linear +ŷ F N N<label>(19)</label></formula><p>DeepFM learns the weight of the feature interaction through FM component. Finally, the output of the FM and the output of the Deep part are used as the final result to participate in the fitting of the final target. The FM part and the deep neural network part share the same Embedding layer.</p><formula xml:id="formula_21">y DeepF M =ŷ F M +ŷ F N N<label>(20)</label></formula><p>In contrast to PNN and DeepFM, which directly perform inner product processing after embedding of two features, we use proposed feature interaction operation to learn feature interaction and retain more information of feature interaction, which extended the inner product and is considered as an endto-end method.</p><p>y F IN N =ŷ Linear + net(concatenate(F I(embed(x)))) (21) Finally, the proposed INN can capture high-order latent feature patterns with multi-layer neural network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Performance Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we present our experiments in detail and compare our proposed INN model with related methods, including datasets, baseline methods, evaluation eetricsdata processing, experimental setups, performance comparison, and the analyses of result. In our experiments, the INN model outperforms major state-of-the-art models in the CTR prediction task on two real-world datasets.</p><p>A. Dataset 1) Criteo: Criteo 1 includes one month of click records with 98 millions of data examples. There are 13 continuous features and 26 categorical ones, and and there is no feature description released. We split the dataset into two parts: day6-12 for training, and day13 for test. For numerical features, we discretized them by equalsize buckets. For categorical features, we removed long-tailed data appearing less than 20 times. Nagetive sown sampling is used since the enormous data volume and serious label unbalance (only 3% samples are positive), and the re-sulting positive sample ratio is about 0.5. After one-hot encoding, the feature space approximates 1M.</p><p>2) Avazu: Avazu 2 includes several days of click-through data which is ordered chronolog-ically. It includes 40 millions click records. For each click instance, there are 24 data fields.We randomly split the public dataset into two parts: 80% is for train-ing and 20% is for testing, and remove categories appearing less than 20 times to reduce dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline Methods</head><p>We compare INN with 6 models in our experiments, which are implemented with TensorFlow and trained with the Adam optimization algorithm.</p><p>LR LR is the classical model in CTR task, which treats the recommendation problem as a classification problem and ranks items by predicting the probability of positive samples.</p><p>FM FM learns a feature vector for each feature, and the inner product of the two feature vectors is used as feature interactions.</p><p>FNN FNN initializes the Embedding layer with the latent vector of FM as the input of the neural network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Evaluation Metrics</head><p>The evaluation metrics are AUC, and log loss. AUC refers to area under ROC cure, which is a widely used metric in binary classification. AUC is insensitive to the classification threshold and the positive ratio, and it can quantitatively reflect the model performance measured based on the ROC curve. The larger the AUC, the more likely the classifier is to put the true positive samples first, and the better the classification performance.</p><p>Log loss can measure the distance between two distributions, which is another widely used metric for binary classification. The lower bound of log loss is 0, indicating the two distributions perfectly match, and a smaller value indicates better performance. <ref type="table" target="#tab_0">Table II</ref> shows the overall performance on Criteo and Avazu datasets, respectively. We implement all the models with Tensorflow 3 in our experiments. For a fair comparison, the size of embedding vector is set to 30 for Criteo dataset and 50 for Avazu dataset. For the optimization method, we use the Adam <ref type="bibr" target="#b46">[46]</ref> with a mini-batch size of 1000 for Criteo and 500 for Avazu datasets, and the learning rate is set to 0.0001. For all deep models, the depth of layers is set to 5, all activation functions are RELU, the number of neurons per layer is 700 for Criteo dataset and 500 for Avazu dataset. In terms of initialization, we initialize DNN hidden layers with xavier <ref type="bibr" target="#b49">[49]</ref>, and we initialize the embedding vectors from uniform distributions. We conduct our experiments with 2 GTX 1080Ti GPUs. Comparing FM with LR on the AUC and Logloss performance, FM outperforms LR on two dataset, illustrating the effectiveness of feature interactions. Neural networks based models achieve better performance than FM which only models two-order feature patterns on    compared with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Parameter Study</head><p>In this subsection, we will conduct some hyper-parameter investigations in our model. We focus on hyper-parameters in the following two components in FINN: the embedding part, the DNN part and feature interaction part. Specifically, we change the following hyper-parameters:(1) the dimension of embeddings; (2) the depth of DNN. Unless specially mentioned in our paper; (3)the dimension feature interaction vector. the default parameter of our network is set as the Section 4.4.</p><p>Embedding Part. We change the embedding sizes from 10 to 50 and summarize the experimental results in <ref type="figure" target="#fig_5">Figure 4</ref>. We can find some observations as follows. As the dimension is expanded from 10 to 50, our model can obtain a substantial improvement on Avazu dataset. We find 30 are the best for our model when we increase the embedding size on Criteo dataset. Enlarging embedding size indicates increasing the number of parameters in embedding layer and DNN part. We guess that it may be the much more features in Criteo dataset as opposed to Avazu dataset that leads to optimization difficulties. DNN Part. In deep part, we can change the number of neurons per layer, the depths of DNN, the activation functions. For brevity, we just study the impact different depths in DNN part. As a matter of fact, increasing the number of layers can increase the model complexity. We can observe from <ref type="figure" target="#fig_6">Figure 5</ref> that increasing number of layers improves model performance at the beginning. However, the performance is degraded if the number of layers keeps increasing. This is be-cause an overcomplicated model is easy to overfit. Its a good choice that the number of hidden layers is set to 5 for Avazu dataset and Criteo dataset.</p><p>We find that BN does not improve the performance for FINN. In order to make a choice between the two techniques, we compare models with BN layer and models with 0.5 dropout rate on network hidden layers. <ref type="table" target="#tab_0">Table III</ref> illustrates the results. We observe that FINN outperforms FINN+BN for on two metrics on Criteo and Avazu datasets. BN solves internal covariate shift <ref type="bibr" target="#b48">[48]</ref> and accelerates DNN training. However, BN may fail when the input is sparse, because BN relies on the statistics of a mini-batch.</p><p>Feature interaction part. In feature layer, we change the feature interaction vector sizes from 10 to 40. As is show in <ref type="figure" target="#fig_7">Figure 6</ref>, we can find 10 are the best feature interaction vector size on the two metrics on the Criteo and Avazu dataset. In addition, the performance is stable when we increase the feature interaction vector size on Avazu dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work, we proposed a novel neural network model FINN, which brings together the eectiveness of feature interactions machines with the strong representation ability of non-linear neural networks for CTR prediction. The key of FINNs architecture is the newly proposed feature interaction operation rather than calculating the feature interactions with Hadamard product or inner product, based on which we allow a neural network model to learn more informative feature interactions at the lower level. In order to improve performance further, we combine a classical deep neural network(DNN) component with the shallow model to be a deep model. Extensive experiments on two real-world datasets show that with one hidden layer only, FINN signicantly outperforms LR, FM, and state-of-the-artdeep learning approaches Wide &amp; Deep and DeepFM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>at same time. The shallow component based on linear model such LR has the benefits of memorization for low order features while the deep component based on DNN improves the generalization of model. However, features of the linear component used directly for final prediction require manual design to ensure a good result in the Wide &amp; Deep. Therefore, the DeepFM [43] tries to replace the linear part of wide &amp; deep model with FM to learn feature interactions and avoid manual feature engineering and employ the same feature embedding vectors between the shallow component and DNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Architecture of our model. Note: Yellow square denotes input by employing the one-hot encoding; Letter 'e' denotes k-dimension embedding vector; Symbol '×' denotes element-wise product of two vectors; Letter 'w' denotes the weighted accumulation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture of embedding layer. The dimension of embedding vector is 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>The different methods to calculate feature interaction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1</head><label></label><figDesc>Criteo http://labs.criteo.com/downloads/download-terabyte-click-logs/ 2 Avazu http://www.kaggle.com/c/avazu-ctr-prediction PNN The embedding vectors of different features are no longer simple concat-enating, but use product operations to perform pairwise interactions to obtain the interaction information between features more targetedly. Wide &amp; Deep The Wide &amp; Deep model is a hybrid model consisting of a single layer Wide part and a multilayer deep part. DeepFM DeepFM improves the Wide &amp; Deep model by replacing the original Wide part with FM. INN INN is the proposed model of this paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>The performance of different embedding sizes on Criteo and Avazu datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>The performance of different number of layers in DNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>The performance of different number of layers in DNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I</head><label>I</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>DATASET STATISTICS</cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>#instance</cell><cell>#categories</cell><cell>#fields</cell><cell>pos ratio</cell></row><row><cell>Criteo</cell><cell>1 × 10 8</cell><cell>1 × 10 6</cell><cell>39</cell><cell>0.5</cell></row><row><cell>Avazu</cell><cell>4 × 10 7</cell><cell>6 × 10 5</cell><cell>24</cell><cell>0.17</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II OVERALL</head><label>II</label><figDesc>PERFORMANCE ON THE CRITEO AND AVAZU DATASETS</figDesc><table><row><cell>Method</cell><cell>Criteo</cell><cell></cell><cell>Avazu</cell><cell></cell></row><row><cell></cell><cell>AUC</cell><cell>Log loss</cell><cell>AUC</cell><cell>Log loss</cell></row><row><cell>LR</cell><cell>0.7742</cell><cell>0.5742</cell><cell>0.7545</cell><cell>0.3996</cell></row><row><cell>FM</cell><cell>0.7922</cell><cell>0.5509</cell><cell>0.7765</cell><cell>0.3820</cell></row><row><cell>FNN</cell><cell>0.7987</cell><cell>0.5431</cell><cell>0.7802</cell><cell>0.3801</cell></row><row><cell>PNN</cell><cell>0.7994</cell><cell>0.5425</cell><cell>0.7807</cell><cell>0.3797</cell></row><row><cell>Wide &amp; Deep</cell><cell>0.7986</cell><cell>0.5432</cell><cell>0.7806</cell><cell>0.3800</cell></row><row><cell>DeepFM</cell><cell>0.7986</cell><cell>0.5428</cell><cell>0.7804</cell><cell>0.3797</cell></row><row><cell>INN</cell><cell>0.8020</cell><cell>0.5409</cell><cell>0.7818</cell><cell>0.3785</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III PERFORMANCE</head><label>III</label><figDesc>COMPARISON BETWEEN BN AND DROPOUT. which demonstrates the im-portance of highorder feature interactions. PNN perform better than FNN. A possible reason is the FNN directly e concatenates the feature vectors as the in-put of the neural network makes it hard to explore all possible feature interac-tions. Therefore, we can infer from experimental results that providing representations for the feature interactions on the bottom layer help the neural networks model to gain more expressive power.The im-provement of the DeepFM model mainly aims to improve automatic feature com-bination capabilities of wide part. Unlike PNN, the DeepFM model uses FM for feature interactions. Among all the compared methods, our proposed deep FINN achieves the best result on all metrics on both Criteo and Avazu datasets. The performance of FINN verifie that our proposed feature interaction mechanism to model the feature interaction in bottom DNN for prediction is effective</figDesc><table><row><cell>Method</cell><cell>Criteo</cell><cell></cell><cell>Avazu</cell><cell></cell></row><row><cell></cell><cell>AUC</cell><cell>Log loss</cell><cell>AUC</cell><cell>Log loss</cell></row><row><cell>FINN</cell><cell>0.8020</cell><cell>0.5409</cell><cell>0.7818</cell><cell>0.3785</cell></row><row><cell>FINN+drop</cell><cell>0.7983</cell><cell>0.5435</cell><cell>0.7834</cell><cell>0.3780</cell></row><row><cell>FINN+BN</cell><cell>0.8003</cell><cell>0.5412</cell><cell>0.7796</cell><cell>0.3812</cell></row><row><cell>both datasets,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">TensorFlow: https://www.tensorflow.org/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ad click prediction: a view from the trenches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">B</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Holt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sculley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davydov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Golovin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 19th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1222" to="1230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">User response learning for directly optimizing campaign performance in display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM International on Conference on Information and Knowledge Management</title>
		<meeting>the 25th ACM International on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="679" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning user interaction models for predicting web search result preferences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agichtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ragno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval</title>
		<meeting>the 29th annual international ACM SIGIR conference on Research and development in information retrieval</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A dynamic bayesian network click model for web search ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th international conference on World wide web</title>
		<meeting>the 18th international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep interest network for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06978</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting clicks: estimating the click-through rate for new ads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ragno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th international conference on World Wide Web</title>
		<meeting>the 16th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Idiot&apos;s bayesnot so stupid after all?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International statistical review</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="385" to="398" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast context-aware recommendations with factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval</title>
		<meeting>the 34th international ACM SIGIR conference on Research and development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="635" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Practical lessons from predicting clicks on ads at facebook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Atallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herbrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bowers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Workshop on Data Mining for Online Advertising</title>
		<meeting>the Eighth International Workshop on Data Mining for Online Advertising</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Lightgbm: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Matrix factorization techniques for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="30" to="37" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="995" to="1000" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Factorization machines with libfm</title>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">57</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Field-aware factorization machines for ctr prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Field-aware factorization machines in a real-world online advertising system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lefortier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web Companion. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 26th International Conference on World Wide Web Companion. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="680" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep crossing: Web-scale modeling without manually crafted combinatorial features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">R</forename><surname>Hoens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simple and scalable response prediction for display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Manavoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Response prediction using collaborative filtering with hierarchies and side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-P</forename><surname>Chitrapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kota</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="141" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Productbased neural networks for user response prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 16th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1149" to="1154" />
		</imprint>
	</monogr>
	<note>Data Mining (ICDM)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep learning over multi-field categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on information retrieval</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="45" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, speech and signal processing (icassp), 2013 ieee international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A generic coordinate descent framework for learning from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kanagal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1341" to="1350" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ispir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural factorization machines for sparse predictive analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="355" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Attentional factorization machines: Learning the weight of feature interactions via attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04617</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Kleinbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Klein</surname></persName>
		</author>
		<title level="m">Logistic regression</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ł</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">xdeepfm: Combining explicit and implicit feature interactions for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1754" to="1763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Factorized weight interaction neural networks for sparse feature prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computing and Applications</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Fibinet: combining feature importance and bilinear feature interaction for click-through rate prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM Conference on Recommender Systems</title>
		<meeting>the 13th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="169" to="177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Product-based neural networks for user response prediction over multi-field categorical data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Wide &amp; deep learning for recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ispir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</title>
		<meeting>the 1st Workshop on Deep Learning for Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="7" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deepfm: a factorizationmachine based neural network for ctr prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1725" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
