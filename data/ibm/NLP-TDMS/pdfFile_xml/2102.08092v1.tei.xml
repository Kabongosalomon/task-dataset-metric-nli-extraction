<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An AutoML-based Approach to Multimodal Image Sentiment Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasco</forename><surname>Lopes</surname></persName>
							<email>vasco.lopes@ubi.pt</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NOVA LINCS</orgName>
								<orgName type="institution" key="instit2">Universidade da Beira Interior † LIAAD</orgName>
								<orgName type="institution" key="instit3">INESC TEC -Institute for Systems and Computer Engineering, Technology and Science ‡ HULTIG -Centre of Human Language Technology and Bioinformatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">António</forename><surname>Gaspar</surname></persName>
							<email>antonio.pedro.gaspar@ubi.pt</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NOVA LINCS</orgName>
								<orgName type="institution" key="instit2">Universidade da Beira Interior † LIAAD</orgName>
								<orgName type="institution" key="instit3">INESC TEC -Institute for Systems and Computer Engineering, Technology and Science ‡ HULTIG -Centre of Human Language Technology and Bioinformatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luís</forename><forename type="middle">A</forename><surname>Alexandre</surname></persName>
							<email>luis.alexandre@ubi.pt</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NOVA LINCS</orgName>
								<orgName type="institution" key="instit2">Universidade da Beira Interior † LIAAD</orgName>
								<orgName type="institution" key="instit3">INESC TEC -Institute for Systems and Computer Engineering, Technology and Science ‡ HULTIG -Centre of Human Language Technology and Bioinformatics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Cordeiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">NOVA LINCS</orgName>
								<orgName type="institution" key="instit2">Universidade da Beira Interior † LIAAD</orgName>
								<orgName type="institution" key="instit3">INESC TEC -Institute for Systems and Computer Engineering, Technology and Science ‡ HULTIG -Centre of Human Language Technology and Bioinformatics</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">An AutoML-based Approach to Multimodal Image Sentiment Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sentiment analysis is a research topic focused on analysing data to extract information related to the sentiment that it causes. Applications of sentiment analysis are wide, ranging from recommendation systems, and marketing to customer satisfaction. Recent approaches evaluate textual content using Machine Learning techniques that are trained over large corpora. However, as social media grown, other data types emerged in large quantities, such as images. Sentiment analysis in images has shown to be a valuable complement to textual data since it enables the inference of the underlying message polarity by creating context and connections. Multimodal sentiment analysis approaches intend to leverage information of both textual and image content to perform an evaluation. Despite recent advances, current solutions still flounder in combining both image and textual information to classify social media data, mainly due to subjectivity, inter-class homogeneity and fusion data differences. In this paper, we propose a method that combines both textual and image individual sentiment analysis into a final fused classification based on AutoML, that performs a random search to find the best model. Our method achieved state-of-the-art performance in the B-T4SA dataset, with 95.19% accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract-Sentiment analysis is a research topic focused on analysing data to extract information related to the sentiment that it causes. Applications of sentiment analysis are wide, ranging from recommendation systems, and marketing to customer satisfaction. Recent approaches evaluate textual content using Machine Learning techniques that are trained over large corpora. However, as social media grown, other data types emerged in large quantities, such as images. Sentiment analysis in images has shown to be a valuable complement to textual data since it enables the inference of the underlying message polarity by creating context and connections. Multimodal sentiment analysis approaches intend to leverage information of both textual and image content to perform an evaluation. Despite recent advances, current solutions still flounder in combining both image and textual information to classify social media data, mainly due to subjectivity, inter-class homogeneity and fusion data differences. In this paper, we propose a method that combines both textual and image individual sentiment analysis into a final fused classification based on AutoML, that performs a random search to find the best model. Our method achieved state-of-the-art performance in the B-T4SA dataset, with 95.19% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Sentiment analysis is an ever-growing research topic, where the focus is to analyze the underlying sentiment of a given source of data, based on its subjectivity and context. Sentiment analysis is mostly performed using textual data, where the goal is to, based on a sentence or a text, determine the author's message polarity. The classification is generally binary -either negative or positive, or n-class classification, wherein, the most used one is a 3-class classification -negative, neutral and positive, using either machine-learning approaches, where a classifier is trained using a labelled corpus, or using lexiconbased approaches, where the textual information is classified based on its semantics or by using statistical approaches <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. Applications of sentiment analysis can be seen in many contexts, such as brand awareness <ref type="bibr" target="#b2">[3]</ref>, political voting intentions <ref type="bibr" target="#b0">[1]</ref>, customer satisfaction <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref> and in disaster relief <ref type="bibr" target="#b5">[6]</ref>.</p><p>The proliferation of social media opened doors for massive collection of data for sentiment analysis <ref type="bibr" target="#b6">[7]</ref>. These are important channels of human communication, allowing for instantaneous spread of information. Twitter emerged as one focal point to capture and analyze data, in which users express their opinions, feelings and thoughts regarding entities or events. Detecting sentiments in Twitter differs from detecting sentiments in conventional text such as blogs and forums, due to the reduced size of the textual data, and because of the information context, addition of symbols in the form of emojis and irony and subjectivity. However, social media networks, such as Twitter, also provide the opportunity to congregate textual data with more information, usually in the form of images, videos or audio. The coupling of multiple data sources, allows the development of multimodal classification, in which a method leverages more than one type of data to perform classification <ref type="bibr" target="#b7">[8]</ref>. This is significantly harder in the context of sentiment analysis, as extracting sentiments solely from textual information is easier than combining information from text and, for example, images. Even though a multimodal approach can improve the performance when compared to a sole text-approach <ref type="bibr" target="#b8">[9]</ref>, this is a challenging task, especially when using data acquired from social media, as the different data types are sparse and can have different contexts, present irony, different intentions and their combined evaluation is not trivial.</p><p>In this paper, we propose a novel multimodal sentiment analysis method, that uses both textual data and images from social media to perform 3-class classification regarding polarity. The proposed method consists of initial individual classification of the textual and image components, and then, based on Automated Machine Learning (AutoML), fuse both classifications into a final one. To perform the individual classifications, we leverage the power of deep neural networks. For this, we evaluated the performance of multiple networks and then selected the best for both the text and the image part. Then, to build the fusing method, we used AutoML to perform a random search to determine the best model to perform the final classification. We evaluated the proposed method in the task of multimodal sentiment analysis using a dataset containing over 470 thousand tweets, where each tweet is composed of both textual and image content.</p><p>The contributions of this paper, can be summarized as follows: 1) we conduct a comparison regarding the performance of sentiment analysis in textual data from Twitter; 2) we compare different state-of-the-art deep learning models in image sentiment analysis, and 3), we propose a novel fusion method that combines the individual classifications into a final one, by levering an optimal model generated with AutoML mechanisms, which resulted in state-of-the-art accuracy on the</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Positive</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neutral Negative</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pre-processing Classification</head><p>Thank you Phuket Sunset Weddings for using Wedding Flowers Phuket. Waiting the happy couple. <ref type="figure">Fig. 1</ref>. Proposed multimodal architecture. The first container represents the pre-processing component, that receives an image and associated text, and preprocesses it to remove noise and non-important data. The second container shows both classification components, where the image and the text are classified individually using CNNs. The third container receives the concatenation of the individual classifications, and performs a final classification using the optimal model searched -represented by a Gradient Boosting Machine (GBM) in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AutoML Fusion</head><p>B-T4SA dataset. The remainder of this work is organized as follows. Section II, contextualizes the related work in multimodal sentiment analysis, and the use and application of AutoML methods. Section III, details the proposed method. Presenting the architecture of our proposal, including a detailed description of the text and image analysis methods, as well as the AutoML component. In Section IV, we introduce the datasets used, a description of the conducted experiments, and discuss the results. Finally, Section V presents a conclusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multimodal Sentiment Analysis</head><p>Even though the vast majority of the sentiment analysis proposals focus on single model sentiment analysis, mainly using textual information <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref>, there are interesting proposals that try to fuse more than one source of information to perform multimodal sentiment analysis. The method proposed in <ref type="bibr" target="#b10">[11]</ref>, combines features from audio and video, and fuses them with text features to estimate the sentiment of youtube movie reviews. In <ref type="bibr" target="#b11">[12]</ref>, the authors propose Tensor Fusion Network, which is a network capable of fusing features extracted from different sources of data, into a single tensor, allowing sentiment analysis both in separate and conjoined. Taking a different approach <ref type="bibr" target="#b8">[9]</ref>, performs multimodal sentiment analysis by conducting hierarchical fusion of the different features by first fusing them into pairs, and then combining them into one. In <ref type="bibr" target="#b12">[13]</ref>, the authors proposed Visual Aspect Attention Network, in which the goal is to use images as attention mechanisms to aid in detecting important sentences in documents. To perform such operation, images are analyzed using a CNN, and the output is used as weights in a word encoder. In <ref type="bibr" target="#b13">[14]</ref>, the authors propose a multimodal method that performs individual analysis of both the image and the text, and then performs a weighted average over the individual predictions to perform a final classification. However, while producing the individual predictions, the authors also introduce Image Content Analysis, a second method to classify the images, which is based on detecting the most predominant object on the image and classifying the image based on the probability of that object appearing in a given class in the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. AutoML</head><p>AutoML <ref type="bibr" target="#b14">[15]</ref>, focuses on developing approaches that provide efficient methods to design machine learning workflows without extensive need for human intervention or optimization processes <ref type="bibr" target="#b15">[16]</ref>. There have been proposals for solving the optimization problem of designing machine learning workflows using random search <ref type="bibr" target="#b16">[17]</ref>, evolutionary strategies <ref type="bibr" target="#b17">[18]</ref>, bayesian optimization <ref type="bibr" target="#b18">[19]</ref> and reinforcement learning <ref type="bibr" target="#b19">[20]</ref>. In <ref type="bibr" target="#b20">[21]</ref>, the authors extended Auto-Weka in order to use AutoML to design methods to detect railway track defects. In <ref type="bibr" target="#b21">[22]</ref>, AutoML is leveraged to improve the classification component of CNNs. This method was built by training CNNs until convergence and then partially removing their classification component and replacing it by a searched model, resulting in performance improvements both in accuracy and inference time. Finally, in <ref type="bibr" target="#b22">[23]</ref>, AutoML is used to forecasting bank failures by performing automated feature extraction.</p><p>Our work has similarities with Neural Architecture Search (NAS) <ref type="bibr" target="#b23">[24]</ref>, in the sense that the focus is in building the classifier and not the entire process of a machine learning workflow, but the main difference is that we do not limit our search to neural networks, rather, we allow more machine learning models to be searched.</p><p>Some of the differentiating points of our proposal are the fact that it performs a simple and efficient fusion, based on the individual analysis of the text and image components, and that it takes advantage of AutoML for finding the final classifier that works on top of the fused features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD A. Architecture</head><p>The proposed method is composed of three stages: preprocessing, individual classifications, and the fusion stage. The entire architecture of the method can be seen in <ref type="figure">Fig. 1</ref>. For both individual classification components, we implemented several state-of-the-art methods to perform a comparison and select the most performant one on the validation set, to be integrated into the proposed architecture.</p><p>In the following sections, we present the implementation details of both the image (III-B) and text classification (III-C), as well as the implementation details for the fusion mechanism (III-D). Moreover, in each section, we further explain the preprocessing components of the image and the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Image Sentiment Analysis</head><p>To classify the post's polarity using image data, we explored the use of state-of-the-art CNNs that perform feature extraction and classification over the inner representations created. For this, we focused on using two architectures that are known to do well in different image analysis tasks: ResNet and DenseNet. The remainder of this section details the CNNs used, as well as the pre-processing steps for cleaning the input images.</p><p>1) Preprocessing:</p><p>We first apply a resize operation, changing the image size to 224 * 224, to uniformize the dataset, as images from the Twitter dataset have different dimensions. Then, each image is normalized using the mean and standard deviation of each channel in the whole dataset. This is applied by subtracting the mean and dividing by the standard deviation in each channel of each image:</p><formula xml:id="formula_0">img i = (o i − µ i )/σ i , i = 1, .</formula><p>., c, where i represents the channel, o the original image, µ is the mean of the dataset and σ is the standard deviation of the dataset.</p><p>2) Models: ResNet: Residual Neural Networks (ResNet) <ref type="bibr" target="#b24">[25]</ref>, introduced the idea of skip connections on CNNs. The "identity shortcut connection" present in ResNets, usually skipping two or three layers in the model, allowing for efficient training of deep CNNs, which are known to suffer from the vanishing gradient problem. This problem is present in the back-propagation of the calculated gradients to earlier layers. As the gradient is propagated backwards, repeated multiplications might produce very small gradients, resulting in performance degradation. By having residual connections, residual nets learn residual functions concerning the layer inputs. Furthermore, instead of learning a direct mapping using stacked-layers, residual connections let these layers learn residual mappings. This means that instead of having a layer learning a desired mapping H(x), regarding to the input x, residual connections allow to reframe this mapping as F (x) := H(x) − x, which can then be reframed to H(x) := F (x) + x. More, if the identity mapping is optimal, residuals can be pushed to zero, meaning that residual networks will, at least, have the same performance of networks using stacked-layers without residual connections.</p><p>The ResNet architectures, with different depths, implemented were: ResNet18, ResNet34, ResNet50, ResNet101 and ResNet152, using the parameters defined in the original paper <ref type="bibr" target="#b24">[25]</ref>.</p><p>DenseNet: Densely Connected Convolutional Networks (DenseNet) <ref type="bibr" target="#b25">[26]</ref> improved upon ResNet by proposing a type of CNN that utilizes dense connections between layers, using Dense Blocks. In this networks, every layer obtains additional inputs from all preceding layers, instead of traditional layers, in which their input is the output of the last layer or the output of the last layer plus a short connection. By allowing multiple parallel connections, DenseNets preserve the feed-forward network scheme, by having all layers obtaining concatenated outputs of all preceding layers, and passing its own featuremaps to subsequent layers. This allows every layer to receive a "collective knowledge" from all past layers, mitigating the vanishing-gradient problem, encouraging feature reuse and allowing for a reduced number of parameters in each layer, since the feature maps continuously expand by concatenation with previous feature maps.</p><p>DenseNet models achieved state-of-the-art results in many image tasks while requiring less computation and fewer model parameters than previous state-of-the-art CNNs. In this work, we have implemented DenseNet161 using the parameters detailed in the original paper <ref type="bibr" target="#b25">[26]</ref>.</p><p>For both ResNet and DenseNet models implemented, we have also applied transfer learning and conducted an experiment, in which the initial layer of each model was modified to have 4 input channels, instead of the original 3. A detailed explanation of these experiments is presented in Section IV-C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Text Sentiment Analysis</head><p>This section presents the operations performed over textual data and the models used to do so. For every deep learning model with an Embedding Layer, a GloVe 200 dimension pretrained on Twitter Embedding was used <ref type="bibr" target="#b26">[27]</ref>.</p><p>1) Pre-processing:</p><p>To effectively classify text, before inputting the text into a classifier, a pre-processing step takes place. This is done because: 1) the type of text obtained from social-media varies substantially and contains noise: syntactic, semantic and grammar errors, mainly due to size constraints, typing speed and slang; 2) standardize data, so that classifiers can more easily learn patterns; 3) be in concordance with the input constraints of Word Embeddings layers and different classifiers.</p><p>So, the steps to clean the textual data were: 1) transform HTML codes into words and symbols; 2) remove stop words using NLTK functionalities; 3) transform every word to lower case; 4) remove occurrences of more than three equal sequential characters into a maximum of two (e.g., "sooo happppyy" becomes "so happy"); 5) remove links and specific social media user-mentions (both the mention and the "RT" word from Twitter); and finally, 6) punctuation was removed.</p><p>2) Models: VADER: Valence Aware Dictionary for Sentiment Reasoning (VADER) <ref type="bibr" target="#b27">[28]</ref>, is a lexicon and rule-based method designed to classify the polarity of text from social media. It uses a list of lexical features, such as words, which are labelled accordingly to their polarity. The output of VADER is based on the probability of the sentence belonging to either the positive, negative or neutral class. This method was created having as basis a set of human-curated lexicon sentiment analysis.</p><p>TextBlob: TextBlob is a library that implements methods for processing textual data <ref type="bibr" target="#b28">[29]</ref>. Like VADER, TexBlob sentiment analysis is based on a set of lexicons that are labelled accordingly to their polarity. This method performs an average over all the lexicons that represent the words in the input sentence and outputs a polarity between −1 and 1. Using this polarity, we defined that values under −0.1 are classified as having a negative meaning, over 0.1 are classified as positive, and the remainder is classified as neutral.</p><p>FastText: FastText is a shallow network architecture that can be trained in a reduced time, when compared to deeper architectures, whilst achieving competitive results in multiple text processing tasks <ref type="bibr" target="#b29">[30]</ref>. The idea behind FastText is to have an architecture, based on the Continuous Bag of Words (CBOW) model <ref type="bibr" target="#b30">[31]</ref> to represent words, being then followed by linear classifiers that classify the input.</p><p>The FastText architecture designed in this work consists on an Embedding layer followed by two Linear layers, the first having input size equal to the embedding size and output equal to 256, whilst the second one has input size of 256 and outputs the classification vector.</p><p>LSTM: The Long-Short Term Memory architecture (LSTM) <ref type="bibr" target="#b31">[32]</ref>, is a variation of the traditional Recurrent Neural Networks (RNNs), and was created to mitigate the exploding and vanishing gradient problems found in simple RNNs. An LSTM layer consists of a predefined set of recurrent blocks, each one of them containing cells. These cells have three types: the input, output and forget gates and serve the purpose of updating, or not, the cell state, erasing its memory and deciding if the cell output should be available. With this architecture, an LSTM layer can store information for later use, preventing gradients from vanishing during the learning process, and can also determine what information to ignore, therefore, allowing the network to remember important information for a longer period of time.</p><p>Formally, an LSTM layer computes for each element in the input sequence:</p><formula xml:id="formula_1">i t = σ(W ii x t + b ii + W hi h (t−1) + b hi ) (1) g t = tanh(W ig x t + b ig + W hg h (t−1) + b hg ) (2) o t = σ(W io x t + b io + W ho h (t−1) + b ho ) (3) c t = f t • c (t−1) + i t • g t (4) h t = o t • tanh(c t )<label>(5)</label></formula><p>where h t is the hidden state at time t; c t is the cell state at time t; x t is the input at time t; h (t−1) is the hidden state of the layer at time t − 1 or the initial hidden state at time 0, and i t , f t , g t , o t are the input, forget, cell, and output gates, respectively. σ is the sigmoid function, and • is the elementwise product.</p><p>The architecture implemented consist of an Embedding layer, followed by an LSTM layer with an input size equal to the dimension of the Embedding and with the number of features in the hidden state equal to 256. This is then followed by a Linear layer with input size equal to the number of features in the LSTM (256) and output size equal to the number of classes.</p><p>LSTM-Attn: The second LSTM architecture we implemented, LSTM-Attn, to perform sentiment analysis in the text is based on the aforementioned LSTM architecture, but with the addition of an Attention layer <ref type="bibr" target="#b32">[33]</ref> between the LSTM layer and the Linear layer. The idea behind the Attention mechanism is to increase the importance of specific parts of the input sentence <ref type="bibr" target="#b33">[34]</ref>. There are two types of attention mechanisms, the global and the local ones. The difference is that in the global mechanisms, all the hidden states of the previous layer are considered for deriving the context vector, whilst on the local attention mechanisms, only some hidden states are considered <ref type="bibr" target="#b34">[35]</ref>. In this implementation, the Attention layer is a global attention mechanism that computes the soft alignment score between the output of the LSTM and its final hidden state.</p><p>The final architecture for LSTM-Attn is: an Embedding layer, followed by an LSTM layer with input size equal to the embedding dimension and 256 as the number of hidden features. Following this, comes the Attention layer that receives the output from the LSTM and the last hidden LSTM state, and outputs a new hidden state with the same size as the output of the LSTM. This then serves as input to the Linear layer, which outputs the classification vector.</p><p>Bi-LSTM: Bi-directional LSTMs (Bi-LSTM) were created as an improvement over the vanilla LSTMs in tasks where full sequences are present, and context is important <ref type="bibr" target="#b36">[36]</ref>. The idea behind Bi-LSTM is to present the information forward and backwards to two separate LSTM networks that are both connected to the same output layer. Having such bi-directional processing on the input information means that the network has, for each input (embedding or word), sequential and context information about it.</p><p>The architecture implemented starts with an Embedding layer, followed by a bi-directional LSTM layer with input size equal to the embedding dimension and 256 as hidden features. Then, we use the output from the Bi-LSTM layer and perform both an average pool and a max pool, which are concatenated together and fed into a Linear layer of input size 256 * 4 and output of 64. Then, a ReLU operation is performed, followed by a dropout, with p = 0.1. The result of this goes to a Linear layer that outputs the classification vector.</p><p>RNN: Recurrent Neural Networks were designed to allow neural networks to have temporal information, which simple neural networks cannot have <ref type="bibr" target="#b37">[37]</ref>. Basically, RNNs form a chain structure in which each node receives as input the output from the predecessor node and one part of the input sequence (e.g., a word or a vector). Each node outputs a value, both to the successor node and to the next layer.</p><p>So, what an RNN layer does is, for each input element, it computes:</p><formula xml:id="formula_2">h t = tanh(W ih x t + b ih + W hh h (t−1) + b hh )<label>(6)</label></formula><p>where h t is the hidden state at time t; x t is the input at time t, and h (t−1) is the hidden state of the previous layer at time t − 1 or the initial hidden state at time 0.</p><p>The architecture implemented is an Embedding Layer followed by a multi-layer Elman RNN with 2 layers, input size equal to the dimension of the embedding and with a hidden size of 256. The output of this layer is then inserted into a Linear layer that outputs the classification vector.</p><p>RCNN: The idea behind Recurrent Convolutional Neural Network (RCNN) <ref type="bibr" target="#b38">[38]</ref>, is to apply a recurrent network structure to text classification that requires no human-designed features. The recurrent structure captures contextual information as far as possible when learning word representations whilst having less noise when compared to methods that use neural networks that rely on window-based processing. To store the context, the RCNN uses a bi-directional recurrent structure and employs a max-pooling layer to capture key features present in the text automatically.</p><p>The architecture implemented consists of an Embedding Layer, a bi-directional LSTM Layer with input size equal to the dimension of the embedding, hidden size of 256 and a dropout of 0.8. The final embedding vector is the concatenation of its embedding and left and right contextual embeddings, which in this case is the hidden vector of the LSTM. This concatenated vector is then passed to a Linear Layer which maps the input vector back to a vector with a size equal to the hidden size of the LSTM, 256. This is passed through a 1D Max Pooling Layer, and finally, the output from this layer is sent to a Linear Layer that maps the input to a classification vector.</p><p>TextCNN: The Convolutional Neural Networks for Sentence Classification (TextCNN), performs convolutions, of different kernel sizes, on textual data <ref type="bibr" target="#b39">[39]</ref>. This is done by performing convolutions over the embedding matrix that represents the input sentences. The convolutions performed are parallel and independent of one another. The architecture of TextCNN implemented is an Embedding Layer followed by 5 convolutional blocks. Each one of those blocks consists of a 2D Convolutional Layer followed by a ReLU activation function and a 1D Max Pooling Layer. The output of all blocks is concatenated into a single vector that goes through a dropout phase with p = 0.8, and the result then goes to a Linear Layer that returns the classification vector.</p><p>Based on TextCNN, we defined a second CNN-based network to perform text classification, which we named sCNN. The architecture is similar, but instead of having 5 blocks of Convolutions-ReLU-Max Pooling, it has only 3 with kernel sizes of 1, 3 and 5 respectively. VDCNN: Inspired by the results that deep convolutional networks have on image classification tasks <ref type="bibr" target="#b40">[40]</ref>, the authors of Very Deep Convolutional Networks for Text Classification (VDCNN) <ref type="bibr" target="#b41">[41]</ref> designed a similar deep neural network for the problem of classifying text. In VDCNN, many convolutions with small kernel sizes (size 3) are stacked to form a deep network, where shortcuts are present for keeping contextual information and solving the vanishing gradient problem. VD-CNN employs convolutional blocks that consist of a sequence of two convolutional layers, each one followed by batch normalization and a ReLU activation.</p><p>We implemented 4 VDCNN architectures with different depths: 9, 17, 29 and 49. Every architecture starts with an Embedding layer, followed by a 1D Conv layer with input size equal to embedding size and output size of 64. Then, they have a set of Convolution Blocks. The number of Convolution Blocks depends on the depth of the architecture, but can be seen in <ref type="bibr" target="#b41">[41]</ref>- <ref type="table">Table 2</ref>. After the Convolution Blocks, comes a K-Max Pooling layer, a Linear layer with input of 512 * k, where k is the number selected for the pooling layer, (2), and output of 2048. Following it, a Linear Layer with 2048 as input and output is inserted and finally, a Linear Layer with an input size of 2048, outputs the classification vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fusion Method</head><p>The focus of the fusion method is to use the individual classifications of the text and image components to perform a final classification. The goal with the fusion approach is to leverage context knowledge of both sources, to outperform individual classifications.</p><p>To create the classifier, we based our approach on AutoML <ref type="bibr" target="#b14">[15]</ref>, where the goal is to create an optimal model to classify a given dataset, without requiring extensive human modelling. First, we can define a machine learning model L, as a mapping from the space of datasets, D, and architectures, A, to the space of models M, L : D × A → M . For any given dataset d ∈ D, and architecture a ∈ A, the mapping returns the solution to the problem, which consists of minimizing a loss function, L, with regularization mechanisms, R, with respect to the model, m, with parameters θ, architecture a, and using the training data, d (train) <ref type="bibr" target="#b42">[42]</ref>: So, we can define our problem as a nested optimization problem, where the goal is to find an optimal model to classify the sentiment based on the individual classifications, d, and a search space A: a * ∈ A, that maximizes the objective function O, on the validation set:</p><formula xml:id="formula_3">L</formula><formula xml:id="formula_4">a * = arg max a∈A O(L(a, d (train) ), d (valid) )<label>(8)</label></formula><p>Since our problem relies on fusing individual classifications of both text and image into a final one, the first step is to get d based on Y img and Y text , where Y represents the classification vector, and img and text represent the classifiers, in order to fuse them into a unique feature map: X = Y img Y text , where X will be the input for the optimization problem (final classifier). Note that in our problem, O is defined as accuracy in the task of 3-class sentiment classification.</p><p>To search for the optimal machine learning model, we based our solution on <ref type="bibr" target="#b43">[43]</ref>, by performing an automatic random search <ref type="bibr" target="#b16">[17]</ref> over a set of several machine learning algorithms and their inner parameters. So, in this work, to search the optimal model and its inner parameters, we performed a random search over the space that includes the following models: a random forest, an extremely-randomized forest, a random grid of generalized linear models, a random grid of XGboost, a random grid of gradient boosting machines (GBM), a random grid of deep neural networks. After searching these models, 2 stacked ensembles were created, the first one comprised of all models evaluated, and the other one, containing the best model of each type. In the end, the model with the best performance on the validation set is the one selected to be in the architecture of the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>As aforementioned, our approach tackles the problem of multimodal sentiment analysis, using both textual and image information. For this, we focused on using the B-T4SA dataset, which is a dataset comprised of Twitter information, in which every sample has both text and image. Furthermore, to conduct experiments using transfer-learning <ref type="bibr" target="#b45">[44]</ref>, we incorporated two more datasets: Stanford Sentiment Treebank (SST-5), for the text classification, and Flickr and Instagram Dataset, for the image classification component.</p><p>Following is a detailed explanation of all the datasets used. 1) B-T4SA: B-T4SA is a subset of T4SA, consisting of 470 thousand samples, each one containing both text and image information. All classes are balanced, and the splits are stratified. The train set consists of approximately 80% of the dataset, while both the validation and test sets have 10% each. B-T4SA was created to solve the problems of T4SA, such as duplicated entries, small sentences, malformed images, and unbalanced classes <ref type="bibr" target="#b46">[45]</ref>. In <ref type="figure" target="#fig_1">Figure 2</ref>, we show an example of an image and the corresponding text, for each class (negative, neutral, positive).</p><p>2) Stanford Sentiment Treebank: The Stanford Sentiment Treebank (SST) consists of sentiment labels for 215,154 phrases in the parse trees of 11855 sentences <ref type="bibr" target="#b47">[46]</ref>. The dataset can be presented in the form of binary classification, either negative or positive, or in a fine-grained way, using a 5-class classification: very negative, negative, neutral, positive, and very positive. The latter is denominated SST-5, and is widely used to evaluate text sentiment classifiers. SST-5 was used in this work as a way to initially train a model, before training it on a final dataset. This allows performing fine-tuning on the model, by transferring the knowledge from the first dataset to the second one, avoiding initializing the model weights randomly <ref type="bibr" target="#b48">[47]</ref>. Denote that, as this dataset has 5 classes, when the models were transferred to B-T4SA, the last layer was removed and substituted by a new one with only 3 output classes.</p><p>3) Flickr and Instagram: To perform transfer-learning on the image classifier, we have also used the Flickr and Instagram dataset <ref type="bibr" target="#b49">[48]</ref>, which is composed of 23308 labelled images of 8 different classes of emotions -amusement, anger, awe, contentment, disgust, excitement, fear and sadness. The goal with this dataset was to pre-train the image classifiers, which upon convergence, are transferred to B-T4SA, by re-placing the last classification layer to an identical one with only 3 output classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Text Analysis</head><p>To select the best text sentiment analysis model to use in the multimodal architecture, we conducted a set of experiments with all the models implemented. We evaluated each model three times in the task of classifying sentiments in the B-T4SA dataset, using the Adam optimizer <ref type="bibr" target="#b50">[49]</ref>, and the Cross-Entropy loss.</p><p>The mean accuracy and standard deviation in each one of the sets are shown in <ref type="table" target="#tab_0">Table I</ref>, where the first block represents the results for two models that can be used as a library in python (VADER and TextBlob), the second block represents the deep learning models, and the third block represents the best model of the previous block with fine-tuning. In the first block, we show two results for both methods, including with and without the pre-processing step. The "-PP" represents the results with clean data, which achieved better results than without any data cleaning, showing that cleaning textual data to remove noise will allow methods to yield better results. In the second and third blocks, all experiments were conducted using pre-processed data. Here, it is possible to see that, except for FastText that achieved approximately 42% and LSTM that was incapable of learning to solve the task (even with different learning rates, hidden features and optimizers), all models achieved a mean accuracy of over 90%. This is well above the plug-and-play methods, and the previous state-of-the-art <ref type="bibr" target="#b13">[14]</ref>, which used TextBlob methods to achieve 64.27% accuracy. The best result from these methods was obtained with the RCNN, which achieved a mean accuracy of 94.61%, outperforming all other methods. This can be justified due to the strong capability of RCNNs to evaluate a word, based on its embeddings, coupled by the right and left contexts, which are extracted using recurrent structures. This combination allows features to be extracted more accurately when working in problems that require context, of which sentiment analysis is heavily dependent on. On the third block of the table, the results of fine-tuning the RCNN model are presented. In this case, the fine-tuning was performed by initially training the model on the SST-5 dataset, and then replacing its final classification layer to output three values instead of five. More, "RCNN-sst ft B-T4SA FC" represents training initially on SST-5 and then train only on the Linear Layers of the model using B-T4SA, whereas "RCNN-sst ft B-T4SA" represents fine-tuning the entire model on B-T4SA, after training it on SST-5. By conducting such experiments, it is possible to see that fine-tuning the entire model yields better results when compared to only transfer learning and finetune the last classification layers. However, these results do not improve upon the normal model, with weights initialized randomly.</p><p>From this experiment, we selected RCNN as the text classifier to be used in the proposed method, since it presented the best results in the validation set. Even though the validation set cannot be seen as a surrogate of the test set, it is the best  way to evaluate how a model will perform in unseen data, without introducing biases by using the test set, which is only used at the end of the entire training process (when all the methods for the proposed method are selected).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image Analysis</head><p>Regarding the selection of the model to perform the image classification, we have evaluated the performance of multiple resnet architectures and densenet161 in the task of image sentiment analysis. In <ref type="table" target="#tab_0">Table II</ref>, the results for our experiments are shown. Note that every network was evaluated using the same learning rate (1e−3), Adam optimizer and cross-entropy loss. The first row of the table represents if the experiment was done using transfer-learning, meaning that the models  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fusion Analysis</head><p>Based on the text classifier, RCNN, and the image classifier, ResNet34, we then evaluated the performance of the proposed method as a whole. For this, we initially searched for the optimal model, using the method described in III-D, allowing the search for a maximum of two hours. By doing this, the most performant model in the validation set was a GBM. By evaluating the entire proposed method on the test set, it achieved an accuracy of 95.19%, which the result is synthesized in <ref type="table" target="#tab_0">Table III</ref>. To evaluate the performance of the proposed method, but with a different fusion classifier, we have also evaluated the use of a Support Vector Machine (SVM), which achieved a performance of 95.16% using all the settings of the proposed method. The difference between the GBM and the SVM classifier is small, 0.03%, but the AutoML searched method has several advantages. The first one is the time required to train, while the AutoML method of searching for methods only required two hours, SVM required several hours to train. More, SVMs tend not to scale well, as there are more features (in the order on thousands), they tend to become extremely slow to fit the data, whilst our proposed methodology to search for a classifier is extremely robust by comprising methods that can handle a large number of features without becoming untenable. However, the SVM baseline consolidates the proposed method, by showing that the proposed architecture works, even in the presence of different fusion classifiers.</p><p>To further validate our proposal, we compare our results with state-of-the-art methods, in which the results are present in <ref type="table" target="#tab_0">Table IV</ref>. In this, it is possible to see that our proposal outperforms others. More, to further evaluate the effectiveness of our proposal, we have further tested the proposed method in <ref type="bibr" target="#b13">[14]</ref>, by replacing its text classifier by our RCNN, resulting in a 15.9% accuracy improvement, (represented in the table by Information Fusion <ref type="bibr" target="#b13">[14]</ref> (TM)), but is still 18.8% below our proposed method. This consolidates that our proposed method of fusing the individual classifications and then performing a random search to find the optimal fusion classifier, is an efficient method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>This paper proposes a novel method to perform multimodal sentiment classification of social media content. The proposed method consists of performing individual text and image classifications, which are then fused by an AutoML-generated model to perform a final classification. We explored several state-of-the-art classifiers for both text and image. More, with the proposed AutoML approach, our method was capable of </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>B-T4SA Test Set Accuracy (%) Random Classifier 33.33% Hybrid-T4SA FT-F <ref type="bibr" target="#b46">[45]</ref> 49.90% Hybrid-T4SA FT-A <ref type="bibr" target="#b46">[45]</ref> 49.10% VGG-T4SA FT-F <ref type="bibr" target="#b46">[45]</ref> 50.60% VGG-T4SA FT-A <ref type="bibr" target="#b46">[45]</ref> 51.30% Information Fusion <ref type="bibr" target="#b13">[14]</ref> 60.42% Information Fusion <ref type="bibr" target="#b13">[14]</ref> (TM) 76.35% SVM-fusion (ours) 95.16% AutoML-based Fusion (ours) <ref type="bibr">95.19%</ref> finding an optimal model that outperformed the state-of-theart in the B-T4SA dataset, which, due to its natural content, is very challenging and contains intra and inter-class subjectivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was supported by 'FCT -Fundação para a Ciência e Tecnologia' throught the research grant '2020.04588.BD', partially supported by NOVA LINCS under grant UID/EEA/50008/2019, by the project MOVES-Monitoring Virtual Crowds in Smart Cities (PTDC/EEI-AUT/28918/2017) financed by FCT-Fundação para a Ciência e a Tecnologia, and partially supported by project 026653 (POCI-01-0247-FEDER-026653) INDTECH 4.0 -New technologies for smart manufacturing, cofinanced by the Portugal 2020 Program (PT 2020), Compete 2020 Program and the European Union through the European Regional Development Fund (ERDF).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a, d (train) ) = arg min m (a,θ) ∈M (a)L(m (a,θ) , d (train) ) + R(θ)<ref type="bibr" target="#b6">(7)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Negative: "His eyes speak of the horror of war that no one should go through. War doesn't help, it only kills. Pls Stop." (b) Neutral: "And they say it's grim up north..." (c) Positive: "Thank you Phuket Sunset Weddings for using Wedding Flowers Phuket. Waiting the happy couple." Examples of images and the correspondent texts of the three different classes presented in the dataset. (a), presents a negative example; (b), a neutral one, and (c), a positive sample.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I MEAN</head><label>I</label><figDesc>ACCURACY AND STANDARD DEVIATION ON THE TRAIN AND VALIDATION SET OF THE B-T4SA DATASET WITH DIFFERENT METHODS FOR LANGUAGE PROCESSING. THE FIRST BLOCK SHOWS THE RESULTS USING METHODS THAT ARE AVAILABLE AS PYTHON LIBRARIES. THE SECOND BLOCK SHOWS THE RESULT OF OUR IMPLEMENTATIONS OF DIFFERENT DEEP LEARNING METHODS. IN THE THIRD BLOCK, WE SHOW THE RESULTS OF THE BEST METHOD FROM THE SECOND BLOCK (RCNN), PRE-TRAINING ON SST AND FINE-TUNED ON B-T4SA. EACH MODEL WAS EVALUATED THREE TIMES, UNDER THE SAME CONDITIONS.</figDesc><table><row><cell></cell><cell cols="2">Mean Accuracy (%)</cell></row><row><cell>Method</cell><cell>Train</cell><cell>Validation</cell></row><row><cell>VADER</cell><cell>41.04 ± 0</cell><cell>41.02 ± 0</cell></row><row><cell>VADER-PP</cell><cell>56.84 ± 0</cell><cell>56.82 ± 0</cell></row><row><cell>Textblob</cell><cell>64.22 ± 0</cell><cell>64.27 ± 0</cell></row><row><cell>Textblob-PP</cell><cell>64.88 ± 0</cell><cell>64.78 ± 0</cell></row><row><cell>FastText</cell><cell>42.86 ± 0.03</cell><cell>42.76 ± 0.05</cell></row><row><cell>LSTM</cell><cell>33.33 ± 0.04</cell><cell>33.13 ± 0.00</cell></row><row><cell>LSTM-Attn</cell><cell>97.36 ± 0.03</cell><cell>93.48 ± 0.57</cell></row><row><cell>BI-LSTM</cell><cell>96.56 ± 0.97</cell><cell>94.35 ± 0.07</cell></row><row><cell>RNN</cell><cell>90.72 ± 0.78</cell><cell>91.24 ± 0.48</cell></row><row><cell>RCNN</cell><cell>98.12 ± 1.10</cell><cell>94.61 ± 0.03</cell></row><row><cell>TextCNN</cell><cell>95.47 ± 0.86</cell><cell>93.73 ± 0.00</cell></row><row><cell>sCNN</cell><cell>90.69 ± 0.10</cell><cell>92.69 ± 0.02</cell></row><row><cell>VDCNN9</cell><cell>94.18 ± 0.81</cell><cell>93.33 ± 0.23</cell></row><row><cell>VDCNN17</cell><cell>88.29 ± 2.28</cell><cell>92.05 ± 0.52</cell></row><row><cell>VDCNN29</cell><cell>93.90 ± 0.65</cell><cell>93.19 ± 0.18</cell></row><row><cell>VDCNN49</cell><cell>92.61 ± 0.33</cell><cell>92.81 ± 0.11</cell></row><row><cell>RCNN-sst ft B-T4SA FC</cell><cell>86.73 ± 0.69</cell><cell>86.51 ± 0.67</cell></row><row><cell>RCNN-sst ft B-T4SA</cell><cell>98.60 ± 1.29</cell><cell>94.60 ± 0.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II ACCURACY</head><label>II</label><figDesc>(%) OF SEVERAL DEEP LEARNING NETWORKS IN THE TASK OF CLASSIFYING SENTIMENTS, BOTH IN THE TRAIN AND VALIDATION SET. THE FIRST ROW, PRE-TRAIN, INDICATES IF THE EXPERIMENT USES TRANSFER LEARNING. EACH EXPERIMENT COLUMN ALSO CONTAINS THE INFORMATION ABOUT THE DATA TYPE USED, EITHER RGB OR RGB PLUS LBP. Flicker and Instagram dataset. We have also evaluated how the different models behave with RBG images (1st and 4th experiment in the table, while the 4th uses pre-trained weights), and RBG and Local Binary Patterns (LBP)<ref type="bibr" target="#b51">[50]</ref>. In the latter, we changed the models to receive 4 inputs and placed the LBP on the fourth channel (3rd experiment in the table). The goal of using LBP is to evidence hidden patterns that assist in detecting the image polarity. All the results show that classifying the sentiment of an image is difficult, mainly due to the subjectivity of the image and due to inter-class similarities, where images that have different classes can be visually similar. Neither the addition of the LBP nor pre-training the models on the Flicker and Instagram dataset improved the results when compared to using only RGB with randomly initialized weights. Furthermore, all models performed similarly, but ResNet34 was the best one, achieving 49.8% accuracy using RGB images, with or without pre-training. Even though ResNet18 had almost the same performance using pre-trained settings, we selected ResNet34 for the proposed method, as it consistently outperformed ResNet18.</figDesc><table><row><cell>Pre-trained</cell><cell>×</cell><cell></cell><cell></cell><cell>×</cell><cell></cell><cell></cell></row><row><cell>Network</cell><cell cols="2">RGB Train</cell><cell>Val</cell><cell cols="2">RGB+LBP Train Val</cell><cell cols="2">RGB Train</cell><cell>Val</cell></row><row><cell>ResNet18</cell><cell cols="3">47.4% 47.7%</cell><cell cols="2">47.4% 47.9%</cell><cell cols="2">46.6% 49.7%</cell></row><row><cell>ResNet34</cell><cell>47.2%</cell><cell cols="2">49.8%</cell><cell>47.3%</cell><cell>48.0%</cell><cell>45.6%</cell><cell>49.8%</cell></row><row><cell>ResNet50</cell><cell cols="3">46.3% 46.4%</cell><cell cols="2">47.2% 47.4%</cell><cell cols="2">48.5% 48.7%</cell></row><row><cell>ResNet101</cell><cell cols="3">44.9% 45.1%</cell><cell cols="2">47.1% 47.1%</cell><cell cols="2">47.6% 47.7%</cell></row><row><cell>ResNet152</cell><cell cols="3">44.5% 44.5%</cell><cell cols="2">45.9% 45.9%</cell><cell cols="2">47.1% 47.5%</cell></row><row><cell cols="4">DenseNet161 46.9% 47.1%</cell><cell cols="2">47.5% 47.5%</cell><cell cols="2">47.2% 47.3%</cell></row><row><cell cols="4">were initially trained on the</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III ACCURACY</head><label>III</label><figDesc>(%) IN THE TEST SET FOR THE PROPOSED METHOD AND A BASELINE USING SVM.</figDesc><table><row><cell>Method</cell><cell>Test Accuracy (%)</cell></row><row><cell>SVM</cell><cell>95.16%</cell></row><row><cell>AutoML-based Fusion (ours)</cell><cell>95.19%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>WITH THE RESULTS OF EXISTING METHODS WITH OUR EXPERIMENTS. THE TM, STANDS FOR SUBSTITUTING THE TEXT CLASSIFIER FROM [14] FOR THE ONE SELECTED IN OUR EXPERIMENTS.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentiment analysis algorithms and applications: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Medhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Korashy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ain Shams engineering journal</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1093" to="1113" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A survey of text classification algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mining text data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="163" to="222" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Twitter power: Tweets as electronic word of mouth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chowdury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2169" to="2188" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Classification of customer reviews based on sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gräbner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zanker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fliedl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fuchs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ENTER</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="460" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Sentiment analysis of big data: Methods, applications, and open challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shayaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">I</forename><surname>Jaafar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sulaiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Z</forename><surname>Piprani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Al-Garadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="37" to="807" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An overview of sentiment analysis in social media and its applications in disaster relief</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Beigi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sentiment analysis and ontology engineering</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="313" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Like it or not: A survey of twitter sentiment analysis methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giachanou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Crestani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey of multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soleymani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="3" to="14" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multimodal sentiment analysis using hierarchical fusion with context modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-based systems</title>
		<imprint>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="124" to="133" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning for sentiment analysis: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1253</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Youtube movie reviews: Sentiment analysis in an audio-visual context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wöllmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weninger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Knaup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sagae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="46" to="53" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Tensor fusion network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07250</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vistanet: Visual aspect attention network for multimodal sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Lauw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence</title>
		<meeting><address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-02-01" />
			<biblScope unit="volume">2019</biblScope>
			<biblScope unit="page" from="305" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A multimodal approach to image sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gaspar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Data Engineering and Automated Learning -IDEAL 2019</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="302" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kotthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanschoren</surname></persName>
		</author>
		<title level="m">Automatic Machine Learning: Methods, Systems, Challenges</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Automl: A survey of the state-of-the-art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">106622</biblScope>
		</imprint>
	</monogr>
	<note>Knowledge-Based Systems</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Random search for hyper-parameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="281" to="305" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Evolutionary neural automl for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meyerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hodjat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mutch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
		<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="401" to="409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Auto-weka 2.0: Automatic model selection and hyperparameter optimization in weka</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kotthoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Thornton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="826" to="830" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Amc: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Automated machine learning techniques in prognostics of railway track defects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kocbek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gabrys</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICDMW. IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Auto-classifier: A robust defect detector based on an automl head</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="137" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An automl application to forecasting bank failures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrapetidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Charonyktakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gogas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Economics Letters</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vader: A parsimonious rule-based model for sentiment analysis of social media text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Hutto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gilbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth international AAAI conference on weblogs and social media</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">textblob documentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Loria</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bag of tricks for efficient text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations</title>
		<meeting><address><addrLine>Scottsdale, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013-05-02" />
		</imprint>
	</monogr>
	<note>Workshop Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Portugal</forename><surname>Lisbon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-09" />
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1412" to="1421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, ser. AAAI&apos;15</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence, ser. AAAI&apos;15</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2267" to="2273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for sentence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barrault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 15th Conference of the European Chapter of the Association for Computational Linguistics<address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-04" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1107" to="1116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Transfer neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wistuba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>1st Workshop on Neural Architecture Search at ICLR 2020</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>H2o</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Automl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
	<note>h2O version 3.30.0.1. [Online</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<ptr target="http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-media learning for image sentiment analysis in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vadicamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Carrara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cimino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cresci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Dell&amp;apos;orletta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tesconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshops (ICCVW)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="308" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A survey of transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Khoshgoftaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Big data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Building a large scale dataset for image emotion recognition: The fine print and the benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, ser. AAAI&apos;16</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence, ser. AAAI&apos;16</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="308" to="314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Y. Bengio and Y. LeCun</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Local binary patterns and its variants for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Meena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Suruliandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Recent Trends in Information Technology (ICRTIT)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="782" to="786" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
