<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Landrieu</surname></persName>
							<email>loic.landrieu@ign.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Université Paris-Est</orgName>
								<orgName type="institution" key="instit2">LASTIG MATIS IGN</orgName>
								<address>
									<country>ENSG</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
							<email>martin.simonovsky@enpc.fr</email>
							<affiliation key="aff1">
								<orgName type="institution">Université Paris-Est</orgName>
								<address>
									<settlement>Ecole des Ponts ParisTech</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel deep learning-based framework to tackle the challenge of semantic segmentation of largescale point clouds of millions of points. We argue that the organization of 3D point clouds can be efficiently captured by a structure called superpoint graph (SPG), derived from a partition of the scanned scene into geometrically homogeneous elements. SPGs offer a compact yet rich representation of contextual relationships between object parts, which is then exploited by a graph convolutional network. Our framework sets a new state of the art for segmenting outdoor LiDAR scans (+11.9 and +8.8 mIoU points for both Semantic3D test sets), as well as indoor scans (+12.4 mIoU points for the S3DIS dataset).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation of large 3D point clouds presents numerous challenges, the most obvious one being the scale of the data. Another hurdle is the lack of clear structure akin to the regular grid arrangement in images. These obstacles have likely prevented Convolutional Neural Networks (CNNs) from achieving on irregular data the impressive performances attained for speech processing or images.</p><p>Previous attempts at using deep learning for large 3D data were trying to replicate successful CNN architectures used for image segmentation. For example, SnapNet <ref type="bibr" target="#b4">[5]</ref> converts a 3D point cloud into a set of virtual 2D RGBD snapshots, the semantic segmentation of which can then be projected on the original data. SegCloud <ref type="bibr" target="#b45">[46]</ref> uses 3D convolutions on a regular voxel grid. However, we argue that such methods do not capture the inherent structure of 3D point clouds, which results in limited discrimination performance. Indeed, converting point clouds to 2D format comes with loss of information and requires to perform surface reconstruction, a problem arguably as hard as semantic segmentation. Volumetric representation of point clouds is Both authors contributed equally to this work. inefficient and tends to discard small details.</p><p>Deep learning architectures specifically designed for 3D point clouds <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b9">10]</ref> display good results, but are limited by the size of inputs they can handle at once.</p><p>We propose a representation of large 3D point clouds as a collection of interconnected simple shapes coined superpoints, in spirit similar to superpixel methods for image segmentation <ref type="bibr" target="#b0">[1]</ref>. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, this structure can be captured by an attributed directed graph called the superpoint graph (SPG). Its nodes represent simple shapes while edges describe their adjacency relationship characterized by rich edge features.</p><p>The SPG representation has several compelling advantages. First, instead of classifying individual points or voxels, it considers entire object parts as whole, which are easier to identify. Second, it is able to describe in detail the relationship between adjacent objects, which is crucial for contextual classification: cars are generally above roads, ceilings are surrounded by walls, etc. Third, the size of the SPG is defined by the number of simple structures in a scene rather than the total number of points, which is typically several order of magnitude smaller. This allows us to model long-range interaction which would be intractable otherwise without strong assumptions on the nature of the pairwise connections. Our contributions are as follows:</p><p>• We introduce superpoint graphs, a novel point cloud representation with rich edge features encoding the contextual relationship between object parts in 3D point clouds.</p><p>• Based on this representation, we are able to apply deep learning on large-scale point clouds without major sacrifice in fine details. Our architecture consists of Point-Nets <ref type="bibr" target="#b38">[39]</ref> for superpoint embedding and graph convolutions for contextual segmentation. For the latter, we introduce a novel, more efficient version of Edge-Conditioned Convolutions <ref type="bibr" target="#b44">[45]</ref> as well as a new form of input gating in Gated Recurrent Units <ref type="bibr" target="#b7">[8]</ref>.</p><p>• We set a new state of the art on two publicly available datasets: Semantic3D <ref type="bibr" target="#b13">[14]</ref> and S3DIS <ref type="bibr" target="#b2">[3]</ref>. In particu- . Based on this preprocessing, a superpoints graph (SPG) is constructed by linking nearby superpoints by superedges with rich attributes (c). Finally, superpoints are transformed into compact embeddings, processed with graph convolutions to make use of contextual information, and classified into semantic labels. lar, we improve mean per-class intersection over union (mIoU) by 11.9 points for the Semantic3D reduced test set, by 8.8 points for the Semantic3D full test set, and by up to 12.4 points for the S3DIS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The classic approach to large-scale point cloud segmentation is to classify each point or voxel independently using handcrafted features derived from their local neighborhood <ref type="bibr" target="#b47">[48]</ref>. The solution is then spatially regularized using graphical models <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49]</ref> or structured optimization <ref type="bibr" target="#b26">[27]</ref>. Clustering as preprocessing <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b12">13]</ref> or postprocessing <ref type="bibr" target="#b46">[47]</ref> have been used by several frameworks to improve the accuracy of the classification.</p><p>Deep Learning on Point Clouds. Several different approaches going beyond naive volumetric processing of point clouds have been proposed recently, notably setbased <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref>, tree-based <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b22">23]</ref>, and graph-based <ref type="bibr" target="#b44">[45]</ref>. However, very few methods with deep learning components have been demonstrated to be able to segment large-scale point clouds. PointNet <ref type="bibr" target="#b38">[39]</ref> can segment large clouds with a sliding window approach, therefore constraining contextual information within a small area only. Engelmann et al. <ref type="bibr" target="#b9">[10]</ref> improves on this by increasing the context scope with multi-scale windows or by considering directly neighboring window positions on a voxel grid. SEGCloud <ref type="bibr" target="#b45">[46]</ref> handles large clouds by voxelizing followed by interpolation back to the original resolution and post-processing with a conditional random field (CRF). None of these approaches is able to consider fine details and long-range contextual information simultaneously. In contrast, our pipeline partitions point clouds in an adaptive way according to their geometric complexity and allows deep learning architecture to use both fine detail and interactions over long distance.</p><p>Graph Convolutions. A key step of our approach is using graph convolutions to spread contextual information. Formulations that are able to deal with graphs of variable sizes can be seen as a form of message passing over graph edges <ref type="bibr" target="#b11">[12]</ref>. Of particular interest are models supporting continuous edge attributes <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b35">36]</ref>, which we use to represent interactions. In image segmentation, convolutions on graphs built over superpixels have been used for postprocessing: Liang et al. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b30">31]</ref> traverses such graphs in a sequential node order based on unary confidences to improve the final labels. We update graph nodes in parallel and exploit edge attributes for informative context modeling. Xu et al. <ref type="bibr" target="#b49">[50]</ref> convolves information over graphs of object detections to infer their contextual relationships. Our work infers relationships implicitly to improve segmentation results. Qi et al. <ref type="bibr" target="#b40">[41]</ref> also relies on graph convolutions on 3D point clouds. However, we process large point clouds instead of small RGBD images with nodes embedded in 3D instead of 2D in a novel, rich-attributed graph. Finally, we note that graph convolutions also bear functional similarity to deep learning formulations of CRFs <ref type="bibr" target="#b50">[51]</ref>, which we discuss more in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The main obstacle that our framework tries to overcome is the size of LiDAR scans. Indeed, they can reach hundreds of millions of points, making direct deep learning approaches intractable. The proposed SPG representation allows us to split the semantic segmentation problem into three distinct problems of different scales, shown in <ref type="figure">Fig</ref>  <ref type="table">table   table   table   chair</ref> chair chair embedding (c) Network architecture <ref type="figure">Figure 2</ref>: Illustration of our framework on a toy scan of a table and a chair. We perform geometric partitioning on the point cloud (a), which allows us to build the superpoint graph (b). Each superpoint is embedded by a PointNet network. The embeddings are then refined in GRUs by message passing along superedges to produce the final labeling (c).</p><p>ing to a geometrically simple primitive, which we assume to be semantically homogeneous. Such primitives can be reliably represented by downsampling small point clouds to at most hundreds of points. This small size allows us to utilize recent point cloud embedding methods such as PointNet <ref type="bibr" target="#b38">[39]</ref>.</p><p>3 Contextual segmentation: The graph of superpoints is by orders of magnitude smaller than any graph built on the original point cloud. Deep learning algorithms based on graph convolutions can then be used to classify its nodes using rich edge features facilitating longrange interactions.</p><p>The SPG representation allows us to perform end-to-end learning of the trainable two last steps. We will describe each step of our pipeline in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Geometric Partition with a Global Energy</head><p>In this subsection, we describe our method for partitioning the input point cloud into parts of simple shape. Our objective is not to retrieve individual objects such as cars or chairs, but rather to break down the objects into simple parts, as seen in <ref type="figure" target="#fig_4">Figure 3</ref>. However, the clusters being geometrically simple, one can expect them to be semantically homogeneous as well, i.e. not to cover objects of different classes. Note that this step of the pipeline is purely unsupervised and makes no use of class labels beyond validation.</p><p>We follow the global energy model described by <ref type="bibr" target="#b12">[13]</ref> for its computational efficiency. Another advantage is that the segmentation is adaptive to the local geometric complexity. In other words, the segments obtained can be large simple shapes such as roads or walls, as well as much smaller components such as parts of a car or a chair.</p><p>Let us consider the input point cloud C as a set of n 3D points. Each point i ∈ C is defined by its 3D position p i , and, if available, other observations o i such as color or intensity. For each point, we compute a set of d g geometric features f i ∈ R dg characterizing the shape of its local neighborhood. In this paper, we use three dimensionality values proposed by <ref type="bibr" target="#b8">[9]</ref>: linearity, planarity and scattering, as well as the verticality feature introduced by <ref type="bibr" target="#b12">[13]</ref>. We also compute the elevation of each point, defined as the z coordinate of p i normalized over the whole input cloud.</p><p>The global energy proposed by <ref type="bibr" target="#b12">[13]</ref> is defined with respect to the 10-nearest neighbor adjacency graph G nn = (C, E nn ) of the point cloud (note that this is not the SPG). The geometrically homogeneous partition is defined as the constant connected components of the solution of the following optimization problem:</p><formula xml:id="formula_0">arg min g∈R dg i∈C g i − f i 2 + µ (i,j)∈Enn w i,j [g i − g j = 0] , (1) where [·]</formula><p>is the Iverson bracket. The edge weight w ∈ R |E| + is chosen to be linearly decreasing with respect to the edge length. The factor µ is the regularization strength and determines the coarseness of the resulting partition.</p><p>The problem defined in Equation 1 is known as generalized minimal partition problem, and can be seen as a continuous-space version of the Potts energy model, or an 0 variant of the graph total variation. The minimized functional being nonconvex and noncontinuous implies that the problem cannot realistically be solved exactly for large point clouds. However, the 0 -cut pursuit algorithm introduced by <ref type="bibr" target="#b25">[26]</ref> is able to quickly find an approximate solution with a few graph-cut iterations. In contrast to other optimization methods such as α-expansion <ref type="bibr" target="#b5">[6]</ref>, the 0 -cut pursuit algorithm does not require selecting the size of the partition in advance. The constant connected components S = {S 1 , · · · , S k } of the solution of Equation 1 define our geometrically simple elements, and are referred as super- points (i.e. set of points) in the rest of this paper.</p><formula xml:id="formula_1">Feature name Size Description mean offset 3 mean m∈δ(S,T ) δ m offset deviation 3 std m∈δ(S,T ) δ m centroid offset 3 mean i∈S p i − mean j∈T p j length ratio 1 log length (S) /length (T ) surface ratio 1 log surface (S) /surface (T ) volume ratio 1 log volume (S) /volume (T ) point count ratio 1 log |S|/|T |</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Superpoint Graph Construction</head><p>In this subsection, we describe how we compute the SPG as well as its key features. The SPG is a structured representation of the point cloud, defined as an oriented attributed graph G = (S, E, F ) whose nodes are the set of superpoints S and edges E (referred to as superedges) represent the adjacency between superpoints. The superedges are annotated by a set of d f features: F ∈ R E×d f characterizing the adjacency relationship between superpoints.</p><p>We define G vor = (C, E vor ) as the symmetric Voronoi adjacency graph of the complete input point cloud as defined by <ref type="bibr" target="#b19">[20]</ref>. Two superpoints S and T are adjacent if there is at least one edge in E vor with one end in S and one end in T :</p><formula xml:id="formula_2">E = (S, T ) ∈ S 2 | ∃ (i, j) ∈ E vor ∩ (S × T ) .<label>(2)</label></formula><p>Important spatial features associated with a superedge (S, T ) are obtained from the set of offsets δ(S, T ) for edges in E vor linking both superpoints:</p><formula xml:id="formula_3">δ (S, T ) = {(p i − p j ) | (i, j) ∈ E vor ∩ (S × T )} . (3)</formula><p>Superedge features can also be derived by comparing the shape and size of the adjacent superpoints. To this end, we compute |S| as the number of points comprised in a superpoint S, as well as shape features length (S) = λ 1 , surface (S) = λ 1 λ 2 , volume (S) = λ 1 λ 2 λ 3 derived from the eigenvalues λ 1 , λ 2 , λ 3 of the covariance of the positions of the points comprised in each superpoint, sorted by decreasing value. In <ref type="table" target="#tab_1">Table 1</ref>, we describe a list of the different superedge features used in this paper. Note that the break of symmetry in the edge features makes the SPG a directed graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Superpoint Embedding</head><p>The goal of this stage is to compute a descriptor for every superpoint S i by embedding it into a vector z i of fixed-size dimensionality d z . Note that each superpoint is embedded in isolation; contextual information required for its reliable classification is provided only in the following stage by the means of graph convolutions.</p><p>Several deep learning-based methods have been proposed for this purpose recently. We choose PointNet <ref type="bibr" target="#b38">[39]</ref> for its remarkable simplicity, efficiency, and robustness. In PointNet, input points are first aligned by a Spatial Transformer Network <ref type="bibr" target="#b18">[19]</ref>, independently processed by multilayer perceptrons (MLPs), and finally max-pooled to summarize the shape.</p><p>In our case, input shapes are geometrically simple objects, which can be reliably represented by a small amount of points and embedded by a rather compact PointNet. This is important to limit the memory needed when evaluating many superpoints on current GPUs. In particular, we subsample superpoints on-the-fly down to n p = 128 points to maintain efficient computation in batches and facilitate data augmentation. Superpoints of less than n p points are sampled with replacement, which in principle does not affect the evaluation of PointNet due to its max-pooling. However, we observed that including very small superpoints of less than n minp = 40 points in training harms the overall performance. Thus, embedding of such superpoints is set to zero so that their classification relies solely on contextual information.</p><p>In order for PointNet to learn spatial distribution of different shapes, each superpoint is rescaled to unit sphere before embedding. Points are represented by their normalized position p i , observations o i , and geometric features f i (since these are already available precomputed from the partitioning step). Furthermore, the original metric diameter of the superpoint is concatenated as an additional feature after PointNet max-pooling in order to stay covariant with shape sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Contextual Segmentation</head><p>The final stage of the pipeline is to classify each superpoint S i based on its embedding z i and its local surroundings within the SPG. Graph convolutions are naturally suited to this task. In this section, we explain the propagation model of our system.</p><p>Our approach builds on the ideas from Gated Graph Neural Networks <ref type="bibr" target="#b29">[30]</ref> and Edge-Conditioned Convolutions (ECC) <ref type="bibr" target="#b44">[45]</ref>. The general idea is that superpoints refine their embedding according to pieces of information passed along superedges. Concretely, each superpoint S i maintains its state hidden in a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b7">[8]</ref>. The hidden state is initialized with embedding z i and is then processed over several iterations (time steps) t = 1 . . . T . At each iteration t, a GRU takes its hidden state h j of neighboring superpoints j. The actual weighting for a superedge (j, i) depends on its attributes F ji,· , listed in Table 1. In particular, it is computed from the attributes by a multi-layer perceptron Θ, so-called Filter Generating Network. Formally:</p><formula xml:id="formula_4">h (t+1) i = (1 − u (t) i ) q (t) i + u (t) i h (t) i q (t) i = tanh(x (t) 1,i + r (t) i h (t) 1,i ) u (t) i = σ(x (t) 2,i + h (t) 2,i ), r (t) i = σ(x (t) 3,i + h (t) 3,i ) (4) (h (t) 1,i , h (t) 2,i , h (t) 3,i ) T = ρ(W h h (t) i + b h ) (x (t) 1,i , x (t) 2,i , x (t) 3,i ) T = ρ(W x x (t) i + b x ) (5) x (t) i = σ(W g h (t) i + b g ) m (t) i (6) m (t) i = mean j|(j,i)∈E Θ(F ji,· ; W e ) h (t) j (7) h (1) i = z i , y i = W o (h (1) i , . . . , h (T +1) i ) T ,<label>(8)</label></formula><p>where is element-wise multiplication, σ(·) sigmoid function, and W · and b · are trainable parameters shared among all GRUs. Equation 4 lists the standard GRU rules <ref type="bibr" target="#b7">[8]</ref> with its update gate u Input Gating. We argue that GRU should possess the ability to down-weight (parts of) an input vector based on its hidden state. For example, GRU might learn to ignore its context if its class state is highly certain or to direct its attention to only specific feature channels. Equation 6 achieves this by gating message m Edge-Conditioned Convolution. ECC plays a crucial role in our model as it can dynamically generate filtering weights for any value of continuous attributes F ji,· by processing them with a multi-layer perceptron Θ. In the original formulation <ref type="bibr" target="#b44">[45]</ref> (ECC-MV), Θ regresses a weight matrix to perform matrix-vector multiplication Θ(F ji,· ; W e )h (t) j for each edge. In this work, we propose a lightweight variant with lower memory requirements and fewer parameters, which is beneficial for datasets with few but large point clouds. Specifically, we regress only an edge-specific weight vector and perform element-wise multiplication as in Equation 7 (ECC-VV). Channel mixing, albeit in an edge-unspecific fashion, is postponed to Equation 5. Finally, let us remark that Θ is shared over time iterations and that self-loops as proposed in <ref type="bibr" target="#b44">[45]</ref> are not necessary due to the existence of hidden states in GRUs.</p><p>State Concatenation. Inspired by DenseNet <ref type="bibr" target="#b16">[17]</ref>, we concatenate hidden states over all time steps and linearly transform them to produce segmentation logits y i in Equation 8. This allows to exploit the dynamics of hidden states due to increasing receptive field for the final classification.</p><p>Relation to CRFs. In image segmentation, postprocessing of convolutional outputs using Conditional Random Fields (CRFs) is widely popular. Several inference algorithms can be formulated as (recurrent) network layers amendable to end-to-end learning <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b42">43]</ref>, possibly with general pairwise potentials <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28]</ref>. While our method of information propagation shares both these characteristics, our GRUs operate on d z -dimensional intermediate feature space, which is richer and less constrained than low-dimensional vectors representing beliefs over classes, as also discussed in <ref type="bibr" target="#b10">[11]</ref>. Such enhanced access to information is motivated by the desire to learn a powerful representation of context, which goes beyond belief compatibilities, as well as the desire to be able to discriminate our often relatively weak unaries (superpixel embeddings). We empirically evaluate these claims in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Further Details</head><p>Adjacency Graphs. In this paper, we use two different adjacency graphs between points of the input clouds: G nn in Section 3.1 and G vor in Section 3.2. Indeed, different definitions of adjacency have different advantages. Voronoi adjacency is more suited to capture long-range relationships between superpoints, which is beneficial for the SPG. Nearest neighbors adjacency tends not to connect objects separated by a small gap. This is desirable for the global energy but tends to produce a SPG with many small connected components, decreasing embedding quality. Fixed radius adjacency should be avoided in general as it handles the variable density of LiDAR scans poorly.</p><p>Training. While the geometric partitioning step is unsupervised, superpoint embedding and contextual segmentation are trained jointly in a supervised way with cross entropy loss. Superpoints are assumed to be semantically homogeneous and, consequently, assigned a hard ground truth label corresponding to the majority label among their contained points. We also considered using soft labels corresponding to normalized histograms of point labels and training with Kullback-Leibler <ref type="bibr" target="#b24">[25]</ref> divergence loss. It performed slightly worse in our initial experiments, though.</p><p>Naive training on large SPGs may approach memory limits of current GPUs. We circumvent this issue by randomly subsampling the sets of superpoints at each iteration and training on induced subgraphs, i.e. graphs composed of subsets of nodes and the original edges connecting them. Specifically, graph neighborhoods of order 3 are sampled to select at most 512 superpoints per SPG with more than n minp points, as smaller superpoints are not embedded. Note that as the induced graph is a union of small neighborhoods, relationships over many hops may still be formed and learned. This strategy also doubles as data augmentation and a strong regularization, together with randomized sampling of point clouds described in Section 3.3. Additional data augmentation is performed by randomly rotating superpoints around the vertical axis and jittering point features by Gaussian noise N (0, 0.01) truncated to [−0.05, 0.05].</p><p>Testing. In modern deep learning frameworks, testing can be made very memory-efficient by discarding layer activations as soon as the follow-up layers have been computed. In practice, we were able to label full SPGs at once. To compensate for randomness due to subsampling of point clouds in PointNets, we average logits obtained over 10 runs with different seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our pipeline on the two currently largest point cloud segmentation benchmarks, Semantic3D <ref type="bibr" target="#b13">[14]</ref> and Stanford Large-Scale 3D Indoor Spaces (S3DIS) <ref type="bibr" target="#b2">[3]</ref>, on both of which we set the new state of the art. Furthermore, we perform an ablation study of our pipeline in Section 4.3.</p><p>Even though the two data sets are quite different in nature (large outdoor scenes for Semantic3D, smaller indoor scanning for S3DIS), we use nearly the same model for both. The deep model is rather compact and 6 GB of GPU memory is enough for both testing and training. We refer to Appendix A for precise details on hyperparameter selection, architecture configuration, and training procedure.</p><p>Performance is evaluated using three metrics: per-class intersection over union (IoU), per-class accuracy (Acc), and overall accuracy (OA), defined as the proportion of correctly classified points. We stress that the metrics are computed on the original point clouds, not on superpoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Semantic3D</head><p>Semantic3D <ref type="bibr" target="#b13">[14]</ref> is the largest available LiDAR dataset with over 3 billion points from a variety of urban and rural scenes. Each point has RGB and intensity values (the latter of which we do not use). The dataset consists of 15 training scans and 15 test scans with withheld labels. We also evaluate on the reduced set of 4 subsampled scans, as common in past work.</p><p>In <ref type="table" target="#tab_3">Table 2</ref>, we provide the results of our algorithm compared to other state of the art recent algorithms and in <ref type="figure" target="#fig_4">Figure 3</ref>, we provide qualitative results of our framework. Our framework improves significantly on the state of the art of semantic segmentation for this data set, i.e. by nearly 12 mIoU points on the reduced set and by nearly 9 mIoU points on the full set. In particular, we observe a steep gain on the "artefact" class. This can be explained by the ability of the partitioning algorithm to detect artifacts due to their singular shape, while they are hard to capture using snapshots, as suggested by <ref type="bibr" target="#b4">[5]</ref>. Furthermore, these small object are often merged with the road when performing spatial regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Stanford Large-Scale 3D Indoor Spaces</head><p>The S3DIS dataset <ref type="bibr" target="#b2">[3]</ref> consists of 3D RGB point clouds of six floors from three different buildings split into individual rooms. We evaluate our framework following two dominant strategies found in previous works. As advocated by <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b9">10]</ref>, we perform 6-fold cross validation with microaveraging, i.e. computing metrics once over the merged predictions of all test folds. Following <ref type="bibr" target="#b45">[46]</ref>, we also report the performance on the fifth fold only (Area 5), corresponding to a building not present in the other folds. Since some classes in this data set cannot be partitioned purely using geometric features (such as boards or paintings on walls), we concatenate the color information o to the geometric features f for the partitioning step.</p><p>The quantitative results are displayed in <ref type="table" target="#tab_5">Table 3</ref>, with qualitative results in <ref type="figure" target="#fig_4">Figure 3</ref> and in Appendix D. S3DIS is a difficult dataset with hard to retrieve classes such as white boards on white walls and columns within walls. From the quantitative results we can see that our framework performs better than other methods on average. Notably, doors are able to be correctly classified at a higher rate than other approaches, as long as they are open, as illustrated in <ref type="figure" target="#fig_4">Figure 3</ref>. Indeed, doors are geometrically similar to walls, but their position with respect to the door frame allows our network to retrieve them correctly. On the other hand, the partition merges white boards with walls, depriving the network from the opportunity to even learn to classify them: the IoU of boards for theoretical perfect classification of superpoints (as in Section 4.3) is only 51.3.</p><p>Computation Time. In <ref type="table" target="#tab_6">Table 4</ref>, we report computation time over the different steps of our pipeline for the inference on Area 5 measured on a 4 GHz CPU and GTX 1080 Ti GPU. While the bulk of time is spent on the CPU for partitioning and SPG computation, we show that voxelization as pre-processing, detailed in Appendix A, leads to a significant speed-up as well as improved accuracy.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>To better understand the influence of various design choices made in our framework, we compare it to several baselines and perform an ablation study. Due to the lack of public ground truth for test sets of Semantic3D, we evaluate on S3DIS with 6-fold cross validation and show comparison of different models to our Best model in <ref type="table">Table 5</ref>.</p><p>Performance Limits. The contribution of contextual segmentation can be bounded both from below and above.</p><p>The lower bound (Unary) is estimated by training PointNet with d z = 13 but otherwise the same architecture, denoted as PointNet13, to directly predict class logits, without SPG and GRUs. The upper bound (Perfect) corresponds to assigning each superpoint its ground truth label, and thus sets the limit of performance due to the geometric partition. We can see that contextual segmentation is able to win roughly 22 mIoU points over unaries, confirming its importance. Nevertheless, the learned model still has room of up to 26   CRFs. We compare the effect of our GRU+ECCbased network to CRF-based regularization. As a baseline (iCRF), we post-process Unary outputs by CRF inference over SPG connectivity with scalar transition matrix, as described by <ref type="bibr" target="#b12">[13]</ref>. Next (CRF − ECC), we adapt CRF-RNN framework of Zheng et al. <ref type="bibr" target="#b50">[51]</ref> to general graphs with edgeconditioned convolutions (see Appendix B for details) and train it with PointNet13 end-to-end. Finally (GRU13), we modify Best to use PointNet13. We observe that iCRF barely improves accuracy (+1 mIoU), which is to be expected, since the partitioning step already encourages spatial regularity. CRF − ECC does better (+15 mIoU) due to end-to-end learning and use of edge attributes, though it is still below GRU13 (+18 mIoU), which performs more complex operations and does not enforce normalization of the embedding. Nevertheless, the 32 channels used in Best instead of the 13 used in GRU13 provide even more freedom for feature representation (+22 mIoU).</p><p>Ablation. We explore the advantages of several design choices by individually removing them from Best in order to compare the framework's performance with and without them. In NoInputGate we remove input gating in GRU; in NoConcat we only consider the last hidden state in GRU for output as  <ref type="table">Table 5</ref>: Ablation study and comparison to various baselines on S3DIS (6-fold cross validation).</p><formula xml:id="formula_5">y i = W o h (T +1) i instead</formula><p>in ECC − VV we use the proposed lightweight formulation of ECC. We can see that each of the first two choices accounts for about 5 mIoU points. Next, without edge features our method falls back even below iCRF to the level of Unary, which validates their design and overall motivation for SPG. ECC − VV decreases the performance on the S3DIS dataset by 3 mIoU points, whereas it has improved the performance on Semantic3D by 2 mIoU. Finally, we invite the reader to Appendix C for further ablations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a deep learning framework for performing semantic segmentation of large point clouds based on a partition into simple shapes. We showed that SPGs allow us to use effective deep learning tools, which would not be able to handle the data volume otherwise. Our method significantly improves on the state of the art on two publicly available datasets. Our experimental analysis suggested that future improvements can be made in both partitioning and learning deep contextual classifiers.</p><p>The source code in PyTorch as well as the trained models are available at https://github.com/loicland/ superpoint_graph. Geometric Partition. We set regularization strength µ = 0.8 for Semantic3D and µ = 0.03 for S3DIS, which strikes a balance between semantic homogeneity of superpoints and the potential for their successful discrimination (S3DIS is composed of smaller semantic parts than Semantic3D). In addition to five geometric features f (linearity, planarity, scattering, verticality, elevation), we use color information o for clustering in S3DIS due to some classes being geometrically indistinguishable, such as boards or doors. PointNet. We use a simplified shallow and narrow Point-Net architecture with just a single Spatial Transformer Network (STN), see <ref type="figure">Figure 4</ref>. We set n p = 128 and n minp = 40. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. CRF-ECC</head><p>In this section, we describe our adaptation of CRF-RNN mean field inference by Zheng et al. <ref type="bibr" target="#b50">[51]</ref> for postprocessing PointNet embeddings in SPG, denoted as unary potentials U i here.</p><p>The original work proposed a dense CRF with pairwise potentials Ψ defined to be a mixture of m Gaussian kernels as Ψ ij = µ m w m K m (F ij ), where µ is label compatibility matrix, w are parameters, and K are fixed Gaussian kernels applied on edge features.</p><p>We replace this definition of the pairwise term with a Filter generating network Θ <ref type="bibr" target="#b44">[45]</ref> parameterized with weights W e , which generalizes the message passing and compatibility transform steps of Zheng et al. . Furthermore, we use superedge connectivity E instead of assuming a complete graph. The pseudo-code is listed in Algorithm 1. Its output are marginal probability distributions Q. In practice we run the inference for T = 10 iterations.</p><formula xml:id="formula_6">Algorithm 1 CRF-ECC Q i ← softmax(U i ) while not converged dô Q i ← j|(j,i)∈E Θ(F ji,· ; W e )Q j Q i ← U i −Q i Q i ← softmax(Q i ) end while</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Extended Ablation Studies</head><p>In this section, we present additional set of experiments to validate our design choices and present their results in <ref type="table" target="#tab_10">Table 6</ref>. a) Spatial Transformer Network. While STN makes superpoint embedding orientation invariant, the relationship with surrounding objects are still captured by superedges, which are orientation variant. In practice, STN helps by 4 mIoU points.</p><p>b) Geometric Features. Geometric features f i are computed in the geometric partition step and can therefore be used in the following learning step for free. While Point-Nets could be expected to learn similar features from the data, this is hampered by superpoint subsampling, and therefore their explicit use helps (+4 mIoU).</p><p>c) Sampling Superpoints. The main effect of subsampling SPG is regularization by data augmentation. Too small a sample size leads to disregarding contextual information (-4 mIoU) while too large a size leads to overfitting (-2 mIoU). Lower memory requirements at training is an extra benefit. There is no subsampling at test time.</p><p>d) Long-range Context. We observe that limiting the range of context information in SPG harms the performance. Specifically, capping distances in G vor to 1 m (as used in PointNet <ref type="bibr" target="#b38">[39]</ref>) or 5 m (as used in SegCloud <ref type="bibr" target="#b45">[46]</ref>) worsens the performance of our method (even more on our Semantic 3D validation set). e) Input Gate. We evaluate the effect of input gating (IG) for GRUs as well as LSTM units. While a LSTM unit achieves higher score than a GRU (-3 mIoU), the proposed IG reverses this situation in favor of GRU (+1 mIoU). Unlike the standard input gate of LSTM, which controls the information flow from the hidden state and input to the cell, our IG controls the input even before it is used to compute all other gates. f) Regularization Strength µ. We investigate the balance between superpoints' discriminative potential and their homogeneity controlled by parameter µ . We observe that the system is able to perform reasonably over a range of SPG sizes. g) Superpoint Sizes. We include a breakdown of superpoint sizes for µ = 0.03 in relation to hyperparameters n minp = 40 and n p = 128, showing that 93% of points are in embedded superpoints, and 79% in superpoints that are subsampled.</p><p>Superedge Features. Finally, in <ref type="table" target="#tab_11">Table 7</ref> we evaluate empirical importance of individual superedge features by removing them from Best. Although no single feature is crucial, the most being offset deviation (+3 mIoU), we remind the reader than without any superedge features the net-Furthermore, SegCloud divides the inference into cubes without overlap, possibly causing inconsistencies across boundaries.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Video Illustration</head><p>We provide a video illustrating our method and qualitative results on S3DIS dataset, which can be viewed at https://youtu.be/Ijr3kGSU_tU.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualization of individual steps in our pipeline. An input point cloud (a) is partitioned into geometrically simple shapes, called superpoints (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(t)i and an incoming message m(t) i as input, and computes its new hidden state h (t+1) i . The incoming message m (t) i to superpoint i is computed as a weighted sum of hidden states h (t)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>improve stability during training, in Equation 5 we apply Layer Normalization [4] defined as ρ(a) := (a−mean(a))/(std(a)+ ) separately to linearly transformed input x (t) i and transformed hidden state h (t) i , with being a small constant. Finally, the model includes three interesting extensions in Equations 6-8, which we detail below.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Example visualizations on both datasets. The colors in (b) are chosen randomly for each element of the partition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 Figure 4 :</head><label>14</label><figDesc>The PointNet embedding n p d p -dimensional samples of a superpoint to a d z -dimensional vector.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Histogram of points contained in superpoints of different size (in log scale) on the full S3DIS dataset. The embedding threshold n minp and subsampling threshold n p are marked in red.work performs distinctly worse (NoEdgeFeat, -22 mIoU).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>List of d f = 13 superedge features characterizing the adjacency between two superpoints S and T .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Intersection over union metric for the different classes of the Semantic3D dataset. OA is the global accuracy, while mIoU refers to the unweighted average of IoU of each class.</figDesc><table><row><cell>(a) RGB point cloud</cell><cell>(b) Geometric partitioning</cell><cell>(c) Prediction</cell><cell>(d) Ground truth</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Method OA mAcc mIoU ceiling floor wall beam column window door chair table bookcase sofa board clutter A5 PointNet [39] -48.98 41.09 88.80 97.33 69.80 0.05 3.92 46.26 10.76 52.61 58.93 40.28 5.85 26.38 33.22 A5 SEGCloud [46] -57.35 48.92 90.06 96.05 69.86 0.00 18.37 38.35 23.12 75.89 70.40 58.42 40.88 12.96 41.60 A5 SPG (Ours) 86.38 66.50 58.04 89.35 96.87 78.12 0.0 42.81 48.93 61.58 84.66 75.41 69.84 52.60 2.10 52.22 Engelmann et al. [10] 81.1 66.4 49.7 90.3 92.1 67.9 44.7</figDesc><table><row><cell>PointNet [39] in [10]</cell><cell>78.5 66.2 47.6 88.0 88.7 69.3 42.4</cell><cell>23.1</cell><cell>47.5</cell><cell>51.6 42.0 54.1</cell><cell>38.2</cell><cell>9.6</cell><cell cols="2">29.4 35.2</cell></row><row><cell></cell><cell></cell><cell>24.2</cell><cell>52.3</cell><cell>51.2 47.4 58.1</cell><cell>39.0</cell><cell>6.9</cell><cell cols="2">30.0 41.9</cell></row><row><cell>SPG (Ours)</cell><cell>85.5 73.0 62.1 89.9 95.1 76.4 62.8</cell><cell>47.1</cell><cell>55.3</cell><cell>68.4 73.5 69.2</cell><cell>63.2</cell><cell>45.9</cell><cell>8.7</cell><cell>52.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Results on the S3DIS dataset on fold "Area 5" (top) and micro-averaged over all 6 folds (bottom). Intersection over union is shown split per class.</figDesc><table><row><cell>Step</cell><cell>Full cloud</cell><cell>2 cm</cell><cell>3 cm</cell><cell>4 cm</cell></row><row><cell>Voxelization</cell><cell>0</cell><cell>40</cell><cell>24</cell><cell>16</cell></row><row><cell>Feature computation</cell><cell>439</cell><cell>194</cell><cell>88</cell><cell>43</cell></row><row><cell>Geometric partition</cell><cell>3428</cell><cell>1013</cell><cell>447</cell><cell>238</cell></row><row><cell>SPG computation</cell><cell>3800</cell><cell>958</cell><cell>436</cell><cell>252</cell></row><row><cell>Inference</cell><cell>10 × 24</cell><cell cols="3">10 × 11 10 × 6 10 × 5</cell></row><row><cell>Total</cell><cell>7907</cell><cell>2315</cell><cell>1055</cell><cell>599</cell></row><row><cell>mIoU 6-fold</cell><cell>54.1</cell><cell>60.2</cell><cell>62.1</cell><cell>57.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>: Computation time in seconds for the inference on</cell></row><row><cell>S3DIS Area 5 (68 rooms, 78 649 682 points) for different</cell></row><row><cell>voxel sizes.</cell></row></table><note>mIoU points for improvement, while about 12 mIoU points are forfeited to the semantic inhomogeneity of superpoints.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Input points are processed by a sequence of MLPs (widths 64, 64, 128, 128, 256) and max pooled to a single vector of 256 features. The scalar metric diameter is appended and the result further processed by a sequence of MLPs (widths 256, 64, d z =32). A residual matrix Φ ∈ R 2×2 is regressed by STN and (I + Φ) is used to transform XY coordinates of input points as the first step.</figDesc><table><row><cell>The archi-</cell></row><row><cell>tecture of STN is a "small PointNet" with 3 MLPs (widths</cell></row><row><cell>64, 64, 128) before max pooling and 3 MLPs after (widths</cell></row><row><cell>128, 64, 4). Batch Normalization [18] and ReLUs are used</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>a) Spatial transf.</figDesc><table><row><cell></cell><cell>no</cell><cell>yes</cell><cell></cell><cell></cell></row><row><cell>mIoU</cell><cell>58.1</cell><cell>62.1</cell><cell></cell><cell></cell></row><row><cell>b) Geometric features</cell><cell>no</cell><cell>yes</cell><cell></cell><cell></cell></row><row><cell>mIoU</cell><cell>58.4</cell><cell>62.1</cell><cell></cell><cell></cell></row><row><cell>c) Max superpoints</cell><cell>256</cell><cell>512</cell><cell>1024</cell><cell></cell></row><row><cell>mIoU</cell><cell>57.9</cell><cell>62.1</cell><cell>60.4</cell><cell></cell></row><row><cell>d) Superedge limit</cell><cell>1 m</cell><cell>5 m</cell><cell>∞</cell><cell></cell></row><row><cell>mIoU</cell><cell>61.0</cell><cell>61.3</cell><cell>62.1</cell><cell></cell></row><row><cell>e) Input gate</cell><cell>LSTM</cell><cell>LSTM+IG</cell><cell>GRU</cell><cell>GRU+IG</cell></row><row><cell>mIoU</cell><cell>61.0</cell><cell>61.0</cell><cell>57.5</cell><cell>62.1</cell></row><row><cell>f) Regularization µ</cell><cell>0.01</cell><cell>0.02</cell><cell>0.03</cell><cell>0.04</cell></row><row><cell># superpoints</cell><cell>785 010</cell><cell>385 091</cell><cell>251 266</cell><cell>186 108</cell></row><row><cell>perfect mIoU</cell><cell>90.6</cell><cell>88.2</cell><cell>86.6</cell><cell>85.2</cell></row><row><cell>mIoU</cell><cell>59.1</cell><cell>59.2</cell><cell>62.1</cell><cell>58.8</cell></row><row><cell>g) Superpoint size</cell><cell>1-40</cell><cell>40-128</cell><cell>128-1000</cell><cell>≥ 1000</cell></row><row><cell>proportion of points</cell><cell>7%</cell><cell>14%</cell><cell>27%</cell><cell>52%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Ablation study of design decisions on S3DIS (6fold cross validation). Our choices in bold.</figDesc><table><row><cell>Model</cell><cell cols="2">mAcc mIoU</cell></row><row><cell>Best</cell><cell>73.0</cell><cell>62.1</cell></row><row><cell>no mean offset</cell><cell>72.5</cell><cell>61.8</cell></row><row><cell>no offset deviation</cell><cell>71.7</cell><cell>59.3</cell></row><row><cell>no centroid offset</cell><cell>74.5</cell><cell>61.2</cell></row><row><cell>no len/surf/vol ratios</cell><cell>71.2</cell><cell>60.7</cell></row><row><cell>no point count ratio</cell><cell>72.7</cell><cell>61.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Ablation study of superedge features on S3DIS (6fold cross validation).</figDesc><table><row><cell></cell><cell>·10 6</cell><cell></cell></row><row><cell></cell><cell>3</cell><cell></cell></row><row><cell>number of points</cell><cell>1 2</cell><cell></cell></row><row><cell></cell><cell>40 128</cell><cell>1,000</cell><cell>10,000</cell></row><row><cell></cell><cell cols="2">size of superpoints</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Geometrically homogeneous partition: The first step of our algorithm is to partition the point cloud into geometrically simple yet meaningful shapes, called superpoints. This unsupervised step takes the whole point cloud as input, and therefore must be computationally very efficient. The SPG can be easily computed from this partition.2 Superpoint embedding: Each node of the SPG corresponds to a small part of the point cloud correspond-</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Model Details</head><p>Voxelization. We pre-process input point clouds with voxelization subsampling by computing per-voxel mean positions and observations over a regular 3D grid (5 cm bins for Semantic3D and 3 cm bins for S3DIS dataset). The resulting semantic segmentation is interpolated back to the original point cloud in a nearest neighbor fashion. Voxelization helps decreasing the computation time and memory requirement, and improves the accuracy of the semantic segmentation by acting as a form of geometric and radiometric denoising as well ( <ref type="table">Table 4</ref> in the main paper). The quality of further steps is practically not affected, as superpoints are usually strongly subsampled for embedding during learning and inference anyway (Section 3.3 in the main paper).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SLIC superpixels compared to state-of-the-art superpixel methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2274" to="2282" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contextually guided semantic labeling and search for threedimensional point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="34" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">3D semantic parsing of largescale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Brilakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Layer normalization. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unstructured point cloud semantic labeling using deep segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Audebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eurographics Workshop on 3D Object Retrieval</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast approximate energy minimization via graph cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Veksler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zabih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1222" to="1239" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast, exact and multi-scale inference for semantic image segmentation with deep gaussian crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ç</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Dimensionality based scale selection in 3D lidar point clouds. International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demantk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vallet</surname></persName>
		</author>
		<idno>XXXVIII-5/W12:97-102</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploring spatial context for 3d semantic segmentation of point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Engelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, 3DRMS Workshop</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Superpixel convolutional networks using bilateral inceptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kappler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Weakly supervised segmentation-aided classification of urban scenes from 3d LiDAR point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guinard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03847</idno>
		<title level="m">Semantic3d. net: A new large-scale point cloud classification benchmark</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast semantic segmentation of 3D point clouds with strongly varying density. ISPRS Annals of Photogrammetry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing &amp; Spatial Information Sciences</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient 3-d scene analysis from streaming data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Relative neighborhood graphs and their relatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Jaromczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">T</forename><surname>Toussaint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1502" to="1517" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3D scene understanding by voxel-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Escape from cells: Deep Kd-networks for the recognition of 3D point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>abs/1704.01222</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semantic labeling of 3D point clouds for indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="244" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On information and sufficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="1951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cut pursuit: Fast algorithms to learn piecewise constant functions on general weighted graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1724" to="1766" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A structured regularization framework for spatially smoothing semantic labelings of 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Raguet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weinmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">132</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="102" to="118" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning arbitrary potentials in CRFs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Kahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<idno>abs/1701.06805</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tosteberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03428</idno>
		<title level="m">Deep projective 3D semantic segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<title level="m">terpretable structure-evolving LSTM. In CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2175" to="2184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semantic object parsing with graph LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="125" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simplified Markov random fields for efficient semantic labeling of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2690" to="2697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3D all the way: Semantic segmentation of urban scenes from start to end in 3D</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5425" to="5434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Contextual classification with functional max-margin Markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vandapel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Contextual classification of lidar data and building object detection in urban areas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Soergel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="152" to="165" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PointNet: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">PointNet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3D graph neural networks for RGBD semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">OctNet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Fully connected deep structured networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>abs/1503.02351</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Spatial inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shapovalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Dynamic edgeconditioned filters in convolutional neural networks on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Segcloud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07563</idno>
		<title level="m">Semantic segmentation of 3D point clouds</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A hybrid semantic point cloud classification-segmentation framework based on geometric features and semantic rules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weinmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weinmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PFG-Journal of Photogrammetry, Remote Sensing and Geoinformation Science</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="183" to="194" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Contextual classification of point cloud data by exploiting individual 3D neighborhoods. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weinmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mallet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rottensteiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jutzi</surname></persName>
		</author>
		<idno>II-3/W4</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Fast semantic segmentation of 3D point clouds using a dense CRF with learned parameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Prankl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3097" to="3106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Input points have d p =11 dimensional features for Semantic3D (position p i , color o i , geometric features f i ), with 3 additional ones for S3DIS (room-normalized spatial coordinates</title>
		<imprint/>
	</monogr>
	<note>as in past work [39])</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">ECC-VV is used for Semantic3D (there are only 15 point clouds even though the amount of points is large), while ECC-MV is used for S3DIS (large number of point clouds). Filter-generating network Θ is a MLP with 4 layers (widths 32, 128, 64, and 32 or 32 2 for ECC-VV or ECC-MV) with ReLUs. Batch Normalization is used only after the third parametric layer. No bias is used in the last layer. Superedges have d f = 13 dimensional features</title>
	</analytic>
	<monogr>
		<title level="m">We use embedding dimensionality d z = 32 and T = 10 iterations</title>
		<imprint/>
	</monogr>
	<note>Segmentation Network. normalized by mean subtraction and scaling to unit variance based on the whole training set</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">and batch size 2, i.e. effectively up to 1024 superpoints per batch. For Semantic3D, we train for 500 epochs with stepwise learning rate decay of 0.7 at epochs 350, 400, and 450. For S3DIS, we train for 250 epochs with steps at 200 and 230. We clip gradients within</title>
		<idno>Training. We train using Adam [22] with initial learning rate 0.01</idno>
		<imprint/>
	</monogr>
	<note>−1, 1</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
