<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2021-03-02">2 Mar 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isti -Cnr</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Italy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amato</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isti -Cnr</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Italy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Esuli</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isti -Cnr</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Italy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Falchi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isti -Cnr</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Italy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gennaro</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isti -Cnr</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Italy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marchand-Maillet</surname></persName>
						</author>
						<title level="a" type="main">Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Switzerland ACM Trans. Multimedia Comput. Commun. Appl</title>
						<imprint>
							<biblScope unit="volume">0</biblScope>
							<biblScope unit="issue">0</biblScope>
							<date type="published" when="2021-03-02">2 Mar 2021</date>
						</imprint>
					</monogr>
					<note type="submission">0. Publication date: 2020.</note>
					<note>ACM Reference Format: 1551-6857/2020/0-ART0 $15.00 0:2 Messina et al.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS Concepts: • Computing methodologies → Semi-supervised learning settings</term>
					<term>Neural networks</term>
					<term>• Information systems → Multimedia and multimodal retrieval Additional Key Words and Phrases: deep learning, cross-modal retrieval, multi-modal matching, computer vision, natural language processing Maillet 2020 Fine-grained Visual Textual Alignment for Cross-Modal Retrieval using Transformer Encoders ACM Trans Multimedia Comput Commun Appl 0, 0, Article 0 ( 2020), 23 pages https://doiorg/XXXXXX/ XXXXXXXXXXXXXX Authors&apos; addresses: Nicola Messina, ISTI -CNR, Pisa, Italy, nicolamessina@isticnrit</term>
					<term>Giuseppe Amato, ISTI -CNR, Pisa, Italy, giuseppeamato@isticnrit</term>
					<term>Andrea Esuli, ISTI -CNR, Pisa, Italy, andreaesuli@isticnrit</term>
					<term>Fabrizio Falchi, ISTI - CNR, Pisa, Italy, fabriziofalchi@isticnrit</term>
					<term>Claudio Gennaro, ISTI -CNR, Pisa, Italy, claudiogennaro@isticnrit</term>
					<term>Stéphane</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite the evolution of deep-learning-based visual-textual processing systems, precise multi-modal matching remains a challenging task. In this work, we tackle the task of cross-modal retrieval through image-sentence matching based on word-region alignments, using supervision only at the global image-sentence level. Specifically, we present a novel approach called Transformer Encoder Reasoning and Alignment Network (TERAN). TERAN enforces a fine-grained match between the underlying components of images and sentences, i.e., image regions and words, respectively, in order to preserve the informative richness of both modalities. TERAN obtains state-of-the-art results on the image retrieval task on both MS-COCO and Flickr30k datasets. Moreover, on MS-COCO, it also outperforms current approaches on the sentence retrieval task.</p><p>Focusing on scalable cross-modal information retrieval, TERAN is designed to keep the visual and textual data pipelines well separated. Cross-attention links invalidate any chance to separately extract visual and textual features needed for the online search and the offline indexing steps in large-scale retrieval systems. In this respect, TERAN merges the information from the two domains only during the final alignment phase, immediately before the loss computation. We argue that the fine-grained alignments produced by TERAN pave the way towards the research for effective and efficient methods for large-scale cross-modal information retrieval. We compare the effectiveness of our approach against relevant state-of-the-art methods. On the MS-COCO 1K test set, we obtain an improvement of 5.7% and 3.5% respectively on the image and the sentence retrieval tasks on the Recall@1 metric. The code used for the experiments is publicly available on GitHub at https://github.com/mesnico/TERAN.</p><p>Since 2012, deep learning has obtained impressive results in several vision and language tasks. Recently, various attempts have been made to merge the two worlds, and state-of-the-art results have been obtained in many of these tasks, including visual question answering <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b51">52]</ref>, image captioning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b64">65]</ref>, and image-text matching <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>. In this work, we deal with the cross-modal retrieval task, with a focus on the visual and textual modalities. The task consists in finding the top-relevant images representing a natural language sentence given as a query (image-retrieval), or, vice versa, in finding a set of sentences that best describe an image given as a query (sentence-retrieval).</p><p>This cross-modal retrieval task is closely related to image-sentence matching, which consists in assigning a score to a pair composed of an image and a sentence. The score is high if the sentence adequately describes the image, and low if the input sentence is unrelated to the corresponding image. The score function learned by solving the matching problem can be then used for deciding which are the top-relevant images and sentences in the two image-and sentence-retrieval scenarios. The matching problem is often very difficult since a deep high-level understanding of images and sentences is needed for succeeding in this task.</p><p>Visuals and texts are used by humans to fully understand the real world. Although they are of equal importance, the information hidden in these two modalities has a very different nature. The text is already a well-structured description developed by humans in hundreds of years, while images are nothing but raw matrices of pixels hiding very high-level concepts and structures. Images and texts do not describe only static entities. In fact, they can easily portray relationships between the objects of interest, e.g.: "The kid kicks the ball". Therefore, it would be helpful to also understand spatial and even abstract relationships linking them together.</p><p>Vision and language matching has been extensively studied <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39]</ref>. Many works employ standard architectures for processing images and texts, such as CNNs-based models for image processing and recurrent networks for language. Usually, in this scenario, the image embeddings are extracted from standard image classification networks, such as ResNet or VGG, by employing the network activations before the classification head. Usually, descriptions extracted from CNN networks trained on classification tasks can only capture global summarized features of the image, ignoring important localized details. For this reason, recent works make extensive use of attention mechanisms, which are able to relate each visual object, extracted from the spatial locations of a feature map or an object detector to the most interesting parts of the sentence, and/or vice-versa.</p><p>Many of these works, such as ViLBERT[39], ImageBERT[45], VL-BERT[51], IMRAM[4], try to learn a complex scoring function = ( , ) that measures the affinity between an image and a caption, where is an image, is the caption and is a normalized score in the range [0, 1]. These are very effective models for tackling the matching task, and they reach state-of-the-art results. However, they remain very inefficient for large-scale image or sentence retrieval: the problem with these approaches is that it is not possible to extract visual and textual descriptions separately, as the pipelines are strongly entangled through cross-attention or memory layers. Thus, if we want to retrieve images related to a given query text, we have to compute all the similarities using the function and then sort the resulting scores in descending order. This is unfeasible if we want to retrieve images or sentences from a large database in a few milliseconds.</p><p>In our previous work, we introduced the Transformer Encoder Reasoning Network (TERN) architecture <ref type="bibr" target="#b42">[43]</ref>, which is a transformer-based model able to independently process images and sentences to match them into the same common space. TERN is a useful architecture for producing compact yet informative features that could be used in cross-modal retrieval setups for efficient indexing using metric-space or text-based approaches. TERN processes visual and textual elements</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>using transformer encoder layers, exploring and reasoning on the relationships among image regions and sentence words. However, its main objective is to match images and sentences as atomic, global entities, by learning a global representation of them inside special tokens (I-CLS and T-CLS) processed by the transformer encoder. This usually leads to performance loss and possibly poor generalization since fine-grained information useful for effective matching is lost during the projection to a fixed-sized common space.</p><p>For this reason, in this work, we propose TERAN (Transformer Encoder Reasoning and Alignment Network) in which we force a fine-grained word-region alignment. Fine-grained matching deals with the accurate understanding of the local correspondences between image regions and words, as opposed to coarse-grained matching, where only a summarized global descriptions of the two modalities is considered. In fact, differently from TERN, the objective function is directly defined on the set of regions and words in output from the architecture, and not on a potentially lossy global representation. Using this objective, TERAN tries to individually align the regions and the words contained in images and sentences respectively, instead of directly matching images and sentences as a whole. The information available to TERAN during training is still coarse-grained, as we do not inject any information about word-region correspondences. The fine-grained alignment is thus obtained in a semi-supervised setup, where no explicit word-region correspondences are given to the network.</p><p>Our TERAN proposal shares most of the previous TERN building blocks and interconnections: the visual and textual pipelines are forwarded separately and they are fused only during the loss computation, in the very last stage of the architecture, making scalable cross-modal information retrieval possible. At the same time, this novel architecture employs state-of-the-art self-attentive modules, based on the transformer encoder architecture <ref type="bibr" target="#b52">[53]</ref>, able to spot out hidden relationships in both modalities for a very effective fine-grained alignment.</p><p>Therefore, TERAN is able to produce independent visual and textual features usable in efficient retrieval scenarios implementing two simple visual and textual pipelines built of modern selfattentive mechanisms. In spite of its overall simplicity, TERAN is able to reach state-of-the-art results in the image and sentence retrieval task, even when compared with complex entangled visual-textual matching models. Experiments show that TERAN can generalize better with respect to the previous TERN approach.</p><p>In the evaluation of the proposed matching procedure, we used a typical information retrieval setup using the Recall@K metrics (with = {1, 5, 10}.) However, in common search engines where the user is searching for related images and not necessarily exact matches, the Recall@K evaluation could be too rigid, especially when = 1. For this reason, as in our previous work <ref type="bibr" target="#b42">[43]</ref>, in addition to the strict Recall@K metric, we propose to measure the retrieval abilities of the system with a normalized discounted cumulative gain metric (NDCG) with relevance computed exploiting caption similarities.</p><p>Summarizing, the contributions of this paper are the following:</p><p>• we introduce the Transformer Encoder Reasoning and Alignment Network (TERAN), able to produce fine-grained region-word alignments for efficient cross-modal information retrieval. • we show that TERAN can reach state-of-the-art results on the cross-modal visual-textual retrieval task, both in terms of Recall@K and NDCG, while producing visually-pleasant regionwords alignments without using supervision at the region-word level. Retrieval results are measured both on MS-COCO and Flickr30k datasets. • we quantitatively compare TERAN with our previous work <ref type="bibr" target="#b42">[43]</ref>, and we perform an extensive study on several variants of our novel model, including weight sharing in the last transformer layers, stop-words removal during training, different pooling protocols for the matching loss function, and the usage of different language models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we review some of the previous works related to image-text joint processing for cross-modal retrieval and alignment, and high-level relational reasoning, on which this work lays its foundations. Also, we briefly summarize the evaluation metrics available in the literature for the cross-modal retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image-Text Processing for Cross-Modal Retrieval</head><p>Image-text matching is often cast to the problem of inferring a similarity score among an image and a sentence. Usually, one of the common approaches for computing this cross-domain similarity is to project images and texts into a common representation space on which some kind of similarity measure can be defined (e.g.: cosine or dot-product similarities). Images and sentences are preprocessed by specialized architectures before being merged at some point in the pipeline. Concerning image processing, the standard approach consists in using Convolutional Neural Networks (CNNs), usually pre-trained on image classification tasks. In particular, <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b53">54]</ref> use VGGs, while <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38]</ref> use ResNets. Concerning sentence processing, many works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref> employ GRU or LSTM recurrent networks to process natural language, often considering the final hidden state as the only feature representing the whole sentence. The problem with these kinds of methodologies is that they usually extract extremely summarized global descriptions of images and sentences. Therefore, a lot of useful fine-grained information needed to reconstruct inter-object relationships for precise image-text alignment is permanently lost.</p><p>For these reasons, many works try to employ region-level information, together with word-level descriptions provided by recurrent networks, to understand fine-grained alignments between words and localized patches in the image. Recent works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b55">56]</ref> exploit the availability of pre-computed region-level features extracted from the Faster-RCNN <ref type="bibr" target="#b46">[47]</ref> object detector. An alternative consists in using the features maps in output from ResNets, without aggregating them, for computing fine-grained attentions over the sentences <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b59">60]</ref>.</p><p>Recently, the transformer architecture <ref type="bibr" target="#b52">[53]</ref> achieved state-of-the-art results in many natural language processing tasks, such as next sentence prediction or sentence classification. The results achieved by the BERT model <ref type="bibr" target="#b6">[7]</ref> are a demonstration of the power of the attention mechanism to produce accurate context-aware word descriptions. For this reason, some works in image-text matching use BERT to extract contextualized word embeddings for representing sentences <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>. Drawing inspiration from the powerful contextualization capabilities of the transformer encoder architecture, some works use BERT-like processing on both visual and textual modalities, such as ViLBERT <ref type="bibr" target="#b38">[39]</ref>, ImageBERT <ref type="bibr" target="#b44">[45]</ref>, Pixel-BERT <ref type="bibr" target="#b18">[19]</ref>, VL-BERT <ref type="bibr" target="#b50">[51]</ref>.</p><p>These latest works achieve state-of-the-art results in sentence and image retrieval, as well as excellent results on the downstream word-region alignment task <ref type="bibr" target="#b4">[5]</ref>. However, they cannot produce separate image and caption descriptions; this is an important requirement in real-world search engines, where usually, at query time, only the query element is forwarded through the network, while all the elements of the database have already been processed by means of an offline feature extraction process.</p><p>Some architectures have been designed so that they are natively able to extract disentangled visual and textual features. In particular, in <ref type="bibr" target="#b8">[9]</ref> the authors introduce the VSE++ architecture. They use VGG and ResNets visual features extractors, together with an LSTM for sentence processing, and they match images and captions exploiting hard-negatives during the loss computation. With their VSRN architecture <ref type="bibr" target="#b29">[30]</ref>, the authors introduce a visual reasoning pipeline built of Graph Convolution Networks (GCNs) and a GRU to sequentially reason on the different image regions. Furthermore, they impose a sentence reconstruction loss to regularize the training process. The authors in <ref type="bibr" target="#b17">[18]</ref> use a similar objective, but employing a pre-trained multi-label CNN to find semantically relevant image patches and their vectorial descriptions. Differently, in <ref type="bibr" target="#b49">[50]</ref> an adversarial learning method is proposed, where a discriminator is used to learn modality-invariant representations. The authors in <ref type="bibr" target="#b10">[11]</ref> use a contextual attention-based LSTM-RNN which can selectively attend to salient regions of an image at each time step, and they employ a recurrent canonical correlation analysis to find hidden semantic relationships between regions and words.</p><p>The works closer to our setup are SAEM <ref type="bibr" target="#b58">[59]</ref> and CAMERA <ref type="bibr" target="#b45">[46]</ref>. In <ref type="bibr" target="#b58">[59]</ref> the authors use triplet and angular loss to project the image and sentence features into the same common space. The visual and textual features are obtained through transformer encoder modules. Differently from our work, they do not enforce fine-grained alignments and they pool the final representations to obtain a single-vector representation. Instead, in <ref type="bibr" target="#b45">[46]</ref> the authors use BERT as language model and an adaptive gating self-attention module to obtain context-enhanced visual features, projecting them into the same common space using cosine similarity. Unlike our work, they specifically focus on multi-view summarization, as multiple sentences can describe the same images in many different but complementary ways.</p><p>The loss used in our work is inspired by the matching loss introduced by the MRNN architecture <ref type="bibr" target="#b23">[24]</ref>, which seems able to produce very good region-word alignments by supervising only the global image-sentence level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High-Level Reasoning</head><p>Another branch of research from which this work draws inspiration is focused on the study of relational reasoning models for high-level understanding. The work in <ref type="bibr" target="#b48">[49]</ref> proposes an architecture that separates perception from reasoning. They tackle the problem of Visual Question Answering by introducing a particular layer called Relation Network (RN), which is specialized in comparing pairs of objects. Object representations are learned using a four-layer CNN, and the question embedding is generated through an LSTM. The authors in <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref> extend the RN for producing compact features for relation-aware image retrieval. However, they do not explore the multi-modal retrieval setup.</p><p>Other solutions try to stick more to a symbolic-like way of reasoning. In particular, <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23]</ref> introduce compositional approaches able to explicitly model the reasoning process by dynamically building a reasoning graph that states which operations must be carried out and in which order to obtain the right answer.</p><p>Recent works employ Graph Convolution Networks (GCNs) to reason about the interconnections between concepts. The authors in <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b62">63]</ref> use GCNs to reason on the image regions for image captioning, while <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b60">61]</ref> use GCN with attention mechanisms to produce the scene graph from plain images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Modal Retrieval Evaluation Metrics</head><p>All the works involved with image-caption matching evaluate their results by measuring how good the system is at retrieving relevant images given a query caption (image-retrieval) and vice-versa (caption-retrieval).</p><p>Usually the Recall@K metric is used <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b44">45]</ref>, where typically = {1, 5, 10}. On the other hand, <ref type="bibr" target="#b2">[3]</ref> introduced a novel metric able to capture non-exact results by weighting the ranked documents using a caption-based similarity measure. We extend the metric introduced in <ref type="bibr" target="#b2">[3]</ref>, giving rise to a powerful evaluation protocol that handles non-exact yet relevant matches. Relaxing the constraints of exact-match similarity search is an important step towards an effective evaluation of real search engines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">REVIEW OF TRANSFORMER ENCODERS</head><p>Our proposed architecture is based on the well established Transformer Encoder (TE) architecture, which heavily relies on the concept of self-attention. The self-attention mechanism tries to weight every vector of the sequence using a scalar value normalized in the range [0, 1] computed as a function of the input vectors themselves. In particular, the attention is computed by using a query vector and a set of key-value ( , ) pairs derived from data using simple feed-forward networks, and processed as shown in Equation 1. More in detail, the attention-aware vector in output from the attention module is computed for every input element as a weighted sum of the values, where the weight assigned to each value is computed as a similarity score (scaled dot-product) between the query with the corresponding key:</p><formula xml:id="formula_0">Att( , , ) = softmax √ .<label>(1)</label></formula><p>, , are the query, the key, and the value respectively, while the factor √ is used to mitigate the vanishing gradient problem of the softmax function in case the inner product assumes too large values. In real implementations, a multi-head attention is used: the input vectors are chunked, and every chunk is processed independently using a different instantiation of the above-described mechanism. This helps in capturing the relationships between the different portions of every input vector.</p><p>Finally, the output from the TE is computed through a simple feed-forward layer applied to the ( , , ) vectors, with a ReLU activation function. This simple feed-forward layer casts in output a set of features having the same dimensionality of the input sequence. Two residual connections followed by layer normalization are also present around the self-attention and the feed-forward sub-modules. An overview of the transformer encoder architecture is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Although the TE was initially developed to work on sequences, there are no architectural constraints that prevent its usage on sets of vectors instead of sequences. In fact, the TE module has not any built-in sequential prior which considers every vector in a precise position in the sequence. This makes the TE suitable for processing visual features coming from an object detector.</p><p>We argue that the transformer encoder self-attention mechanism can drive a simple but powerful reasoning mechanism able to spot hidden relationships between the vector entities, whatever nature they have (visual or textual). Also, the encoder is designed in a way that multiple instances of it could be stacked in sequence. Using multiple levels of attention helps in producing a deeper and more powerful reasoning pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">TRANSFORMER ENCODER REASONING AND ALIGNMENT NETWORK (TERAN)</head><p>Our Transformer Encoder Reasoning and Alignment Network (TERAN) leverages our previous work <ref type="bibr" target="#b42">[43]</ref> that introduced the TERN architecture. TERAN modifies the learning objective of our previous work by forcing a fine-grained alignment between the region and word features in output from the last transformer encoder (TE) layers so that meaningful fine-grained concepts are produced.</p><p>As TERN, our TERAN reasoning engine is built using a stack of TE layers, both for the visual and the textual data pipelines. The TE takes as input sequences or sets of entities, and it can reason upon these entities disregarding their intrinsic nature. In particular, we consider the salient regions in an image as visual entities, and the words present in the caption as textual entities.</p><p>More formally, the input to our reasoning pipeline is a set = { 0 , 1 , . . . , } of image regions (visual entities) representing an image and a sequence = { 0 , 1 , . . . , } of words (textual entities) representing the corresponding caption . Thus, the reasoning module continuously operates on sets and sequences of and objects respectively for images and captions.</p><p>The TERN architecture in <ref type="bibr" target="#b42">[43]</ref> produces summarized representations of both images and words by employing special I-CLS and T-CLS tokens that are forwarded towards the layers of the TEs. In the end, the processed I-CLS and T-CLS tokens gather important global knowledge from both modalities. Contrarily, TERAN does not produce aggregated fixed-sized representations for images and sentences. For this reason, it does not employ the global features constructed inside the I-CLS and T-CLS tokens. Instead, it tries to impose a global matching loss defined on the variable-length sets in output from the last TE layers that is able, as a side effect, to produce also good and interpretable region-word alignments.</p><p>The overall architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We left in the scheme the I-CLS and T-CLS tokens connections for comparison with the TERN architecture presented in <ref type="bibr" target="#b42">[43]</ref>. These tokens are still used for a targeted experiment that exploits the combination of the TERN and TERAN losses (more details in Section 7). However, they are not used in the main TERAN experiments.</p><p>The visual features extracted from Faster-RCNN are conditioned with the information related to the geometry of the bounding-boxes. This is done through a simple fully-connected stack in the early visual pipeline before the reasoning steps. The two linear projection layers within the TE modules are used to project the visual and textual concepts in spaces having the same dimensionality. Then, the latest TE layers perform further processing before outputting the final features that are used to compute the final matching loss.</p><p>Differently from TERN, we initially do not share the weights of the last TE layers. We will discuss the effect of weight sharing in our ablation study, in Section 8.1.</p><p>In our novel TERAN architecture, the features in output from the last TE layers are used to compute a region-word alignment matrix ∈ R | |× | | , where is the set of indexes of the region features from the -th image and is the set of indexes of the words from the -th sentence. We use cosine similarity for measuring affinity between the -th region and the -th word. If { } and { } are the sets of contextualized region and word vectors in output from the network for the k-th image and the l-th sentence respectively, then is constructed as:</p><formula xml:id="formula_1">= ∥ ∥ ∥ ∥ ∈ , ∈<label>(2)</label></formula><p>At this point, the global similarity between the -th image and the -th sentence is computed by pooling this similarity matrix through an appropriate pooling function. Inspired by <ref type="bibr" target="#b23">[24]</ref> and <ref type="bibr" target="#b27">[28]</ref>, we employ the max-sum pooling, which consists in computing the max over the rows of and then summing or, equivalently, max-over-regions sum-over-words ( ) pooling. We explore also the dual version, as in <ref type="bibr" target="#b27">[28]</ref>, by computing the max over the columns and then summing, or max-over-words sum-over-regions ( ) pooling:</p><formula xml:id="formula_2">= ∑︁ ∈ max ∈ = ∑︁ ∈ max ∈<label>(3)</label></formula><p>Since both these similarity functions are not symmetric due to the diverse outcomes we obtain by inverting the order of the sum and max operations, we introduce also the symmetric form, obtained by summing the two: Symm = + (4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning Objective</head><p>Given the global image-sentence similarities computed through alignments pooling, we can proceed as in previous works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30]</ref> using a contrastive learning method: we use a hinge-based triplet ranking loss, focusing the attention on hard negatives, as introduced by <ref type="bibr" target="#b8">[9]</ref>. Therefore, we used the following loss function:</p><formula xml:id="formula_3">= max ′ [ + ′ − ] + + max ′ [ + ′ − ] +<label>(5)</label></formula><p>where [ ] + ≡ (0, ) and is a margin that defines the minimum separation that should hold between the truly matching word-region embeddings and the negative pairs. The hard negatives ′ and ′ are computed as follows:</p><formula xml:id="formula_4">′ = arg max ≠ ( , ) ′ = arg max ≠ ( , )<label>(6)</label></formula><p>where ( , ) is a positive pair. As in <ref type="bibr" target="#b8">[9]</ref>, the hard negatives are sampled from the mini-batch and not globally, for performance reasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Region and Word Features Extraction</head><formula xml:id="formula_5">The = { 0 , 1 , . . . , } and = { 0 , 1 , . . . ,</formula><p>} initial descriptions for images and captions come from state-of-the-art visual and textual pre-trained networks, Faster-RCNN with Bottom-Up attention and BERT respectively.</p><p>Faster-RCNN <ref type="bibr" target="#b46">[47]</ref> is a state-of-the-art object detector. It has been used in many downstream tasks requiring salient object regions extracted from images. Therefore, Faster-RCNN is one of the main architectures implementing human-like visual perception. The work in <ref type="bibr" target="#b1">[2]</ref> introduces bottom-up visual features by training Faster-RCNN with a Resnet-101 backbone on the Visual Genome dataset <ref type="bibr" target="#b26">[27]</ref>. Using these features, they can reach remarkable results on the two downstream tasks of image captioning and visual question answering.</p><p>Concerning text processing, we use BERT <ref type="bibr" target="#b6">[7]</ref> for extracting word embeddings. BERT already uses a multi-layer transformer encoder to process words in sentences and capture their functional relationships through the same powerful self-attention mechanism. BERT embeddings are trained on some general natural language processing tasks such as sentence prediction or sentence classification and demonstrated state-of-the-art results in many downstream natural language tasks. BERT embeddings, unlike word2vec <ref type="bibr" target="#b43">[44]</ref>, capture the context in which each word appears. Therefore, every word embedding carries information about the surrounding context, that could be different from caption to caption.</p><p>Since the transformer encoder architecture does not embed any sequential prior in its architecture, words are given a sequential order by mixing some positional information into the learned input embeddings. For this reason, the authors in <ref type="bibr" target="#b52">[53]</ref> add sine and cosine functions of different frequencies to the input embeddings. This is a simple yet effective way to transform a set into a sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">COMPUTATIONAL EFFICIENCY OF TERAN</head><p>A principled objective of our work is efficient feature extraction for cross-modal retrieval applications. In these scenarios, it is mandatory to have a separable network that can produce visual and textual features by independently forwarding the two disentangled visual and textual pipelines. Furthermore, the similarity function should be simple, so that it is efficient to compute.</p><p>TERAN, as well as TERN <ref type="bibr" target="#b42">[43]</ref> and other works in literature <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b58">59]</ref> adhere to this principle. In fact, if is the number of images and the number of sentences in the database, these methods have a feature space complexity, as well as a feature extraction time complexity, of ( ) + ( ).</p><p>Other works that entangle the visual and textual pipelines, such as <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b59">60]</ref> require a feature space, and a number of network evaluations, scaling with ( ). These methods are impractical to deploy to real-world scalable search engines. Some of these works partially solve this issue by keeping the two representations separated up until a certain point in the network, so that these intermediate representations can be cached, as proposed in <ref type="bibr" target="#b39">[40]</ref>. In all these cases, a new incoming query to the system needs ( ) or ( ) re-evaluations of the whole network (depending on whether we are considering image or sentence retrieval); in the best case, we need to re-evaluate the last attention layers, which could be similarly expensive.</p><p>Regarding the similarity computation, TERN uses simple dot products that enable quick and efficient document rankings in modern search engines. TERAN implements also a very simple similarity function, built of simple dot products and summations without including complex layers of memories or attentions. This possibly enables an implementation that uses metric space approaches to prune the search space and obtain very efficient image or sentence rankings for a given query. However, the implementation of the TERAN similarity function in real-world search engines is left for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">NDCG METRIC FOR CROSS-MODAL RETRIEVAL</head><p>As of now, many works in the computer vision literature treating image-text matching measure the retrieval abilities of the proposed methods by employing the well known Recall@K metric. The Recall@K measures the percentage of queries able to retrieve the correct item among the first K results. This is a metric perfectly suitable for scenarios where the query is very specific and thus we expect to find the elements that match perfectly among the first search results. However, in common search engines, the users are not asked to input a very detailed query, and they are often not searching for an exact match. They expect to find in the first retrieved positions some relevant results, with relevance defined using some pre-defined and often subjective criterion.</p><p>For this reason, inspired by the work in <ref type="bibr" target="#b2">[3]</ref> and following the novel ideas introduced by our previous work on TERN <ref type="bibr" target="#b42">[43]</ref>, we employ a common metric often used in information retrieval applications, the Normalized Discounted Cumulative Gain (NDCG). The NDCG is able to evaluate the quality of the ranking produced by a certain query by looking at the first positions of the ranked elements list. The premise of NDCG is that highly relevant items appearing lower in a search result list should be penalized as the graded relevance value is reduced proportionally to the position of the result.</p><p>The NDCG until position is defined as follows:</p><formula xml:id="formula_6">NDCG = DCG IDCG , where = ∑︁ =1 rel log 2 ( + 1) ;<label>(7)</label></formula><p>rel is a positive number encoding the affinity that the -th element of the retrieved list has with the query element, and IDCG is the DCG of the best possible ranking. Thanks to this normalization, NDCG acquires values in the range [0, 1].</p><p>The rel values can be computed using well-established sentence similarity scores between a sentence and the sentences associated with a certain image. More formally, we could think of computing rel = (¯, ), where¯is the set of all captions associated to the image , and : S × S → [0, 1] is a similarity function defined over a pair of sentences returning their normalized similarity score. With this simple expedient, we could efficiently compute quite large relevance matrices using similarities defined over captions, which are in general computationally much cheaper than similarities computed between images and sentences directly.</p><p>We thus compute the value in the following ways:</p><p>• rel = (¯, ) in case of image retrieval, where is the query caption</p><p>• rel = (¯, ) in case of caption retrieval, where¯is the set of captions associated to the query image . In our work, we use ROUGE-L <ref type="bibr" target="#b32">[33]</ref> and SPICE <ref type="bibr" target="#b0">[1]</ref> as sentence similarity functions for computing caption similarities. These two scoring functions capture different aspects of the sentences. In particular, ROUGE-L operates on the longest common sub-sequences, while SPICE exploits graphs associated with the syntactic parse trees, and has a certain degree of robustness against synonyms. In this way, SPICE is more sensitive to high-level features of the text and semantic dependencies between words and concepts rather than to pure syntactic constructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS</head><p>We trained the TERAN architecture and we measured its performance on the MS-COCO <ref type="bibr" target="#b33">[34]</ref> and Flickr30k datasets <ref type="bibr" target="#b63">[64]</ref>, computing the effectiveness of our approach on the image retrieval and sentence retrieval tasks. We compared our results against state-of-the-art approaches on the same datasets, using the introduced NDCG and the already-in-use Recall@K metrics.</p><p>The MS-COCO dataset comes with a total of 123,287 images. Every image has associated a set of 5 human-written captions describing the image. We follow the splits introduced by <ref type="bibr" target="#b23">[24]</ref> and followed by the subsequent works in this field <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b29">30]</ref>. In particular, 113,287 images are reserved for training, 5,000 for validating, and 5,000 for testing. Differently, Flickr30k consists of 31,000 images and 158,915 English texts. Like MS-COCO, each image is annotated with 5 captions. Following the splits by <ref type="bibr" target="#b23">[24]</ref>, we use 29,000 images for training, 1,000 images for validation, and the remaining 1,000 images for testing. For MS-COCO, at test time the results for both 5k and 1k test-sets are reported. In the case of 1k images, the results are computed by performing 5-fold cross-validation on the 5k test split and averaging the outcomes.</p><p>We computed caption-caption relevance scores for the NDCG metric using ROUGE-L <ref type="bibr" target="#b32">[33]</ref> and SPICE <ref type="bibr" target="#b0">[1]</ref>, as explained in Section 6, and we set the NDCG parameter = 25 as in <ref type="bibr" target="#b2">[3]</ref> in our experiments. We employed the NDCG metrics measured during the validation phase for choosing the best performing model to be used during the test phase.</p><p>For a better comparison with our previous TERN approach, we included three more targeted experiments. In the first two, called TERN Test and TERN Test we used the bestperforming TERN model, trained as explained in <ref type="bibr" target="#b42">[43]</ref>, testing it using the and alignments criteria respectively. TERN is effectively able to output features for every image region or word; however, it is never constrained to produce meaningful descriptions out of these sets of features; hence, this trial is aimed at checking the quality of the alignment of the concepts in output from the previous TERN architecture. In the third experiment, called TERN w. Align, we tried to integrate the objectives of both TERN and TERAN during training, by combining their losses using the uncertainty weighting method proposed in <ref type="bibr" target="#b24">[25]</ref>, and testing the model using the TERN inference protocol. Thus, in this experiment, we effectively reuse the I-CLS and T-CLS tokens as global descriptions for images and sentences, as described in <ref type="bibr" target="#b42">[43]</ref>. This experiment aimed to evaluate if the TERAN alignment objective can help TERN learn better fixed-sized global vectorial descriptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Implementation Details</head><p>We employ the BERT model pre-trained on the masked language task on English sentences, using the PyTorch implementation by HuggingFace 1 . These pre-trained BERT embeddings are 768-D. For the visual pipeline, we extracted the bottom-up features from the work by <ref type="bibr" target="#b1">[2]</ref>, using the code and pre-extracted features provided by the authors 2 . Specifically, for MS-COCO we used the already-extracted bottom-up features, while we extracted from scratch the features for Flickr30k using the available pre-trained model.</p><p>In the experiments, we used the bottom-up features containing the top 36 most confident detections, although our pipeline already handles variable-length sets of regions for each image by appropriately masking the attention weights in the TE layers.</p><p>Concerning the reasoning steps, we used a stack of 4 TE layers for visual reasoning. We found the best results when fine-tuning the BERT pre-trained model, so we did not add further reasoning TE layers for the textual pipeline. The final common space, as in <ref type="bibr" target="#b8">[9]</ref>, is 1024-dimensional. We linearly projected the visual and textual features to a 1024-d space and then we processed the resulting features using 2 final TEs before computing the alignment matrix.</p><p>All the TEs feed-forward layers are 2048-dimensional and the dropout is set to 0.1. We trained for 30 epochs using Adam optimizer with a batch size of 40 and a learning rate of 1 −5 for the first 20 epochs and 1 −6 for the remaining 10 epochs. The parameter of the hinge-based triplet ranking loss is set to 0.2, as in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Results</head><p>We compare our TERAN method against the following baselines: JGCAR <ref type="bibr" target="#b54">[55]</ref>, SAN <ref type="bibr" target="#b20">[21]</ref>, VSE++ <ref type="bibr" target="#b8">[9]</ref>, SMAN <ref type="bibr" target="#b21">[22]</ref>, M3A-Net <ref type="bibr" target="#b19">[20]</ref>, AAMEL <ref type="bibr" target="#b56">[57]</ref>, MRNN <ref type="bibr" target="#b23">[24]</ref>, SCAN <ref type="bibr" target="#b27">[28]</ref>, SAEM <ref type="bibr" target="#b58">[59]</ref>, CASC <ref type="bibr" target="#b59">[60]</ref>, MMCA <ref type="bibr" target="#b57">[58]</ref>, VSRN <ref type="bibr" target="#b29">[30]</ref>, PFAN <ref type="bibr" target="#b55">[56]</ref>, Full-IMRAM <ref type="bibr" target="#b3">[4]</ref>, and CAMERA <ref type="bibr" target="#b45">[46]</ref>. We clustered these methods based on the visual feature extractor they use: VGG, ResNet, or Region CNN (e.g., Faster-RCNN).</p><p>To have a better comparison with our method, we also annotated in the tables whenever they use BERT as the textual model, or if they use disentangled visual-textual pipelines for efficient feature computation, as explained in Section 5. Also note that many of the listed methods report the results using an ensemble of two models having different training initialization parameters, where the final similarity is obtained by averaging the scores in output from each model. Hence, we reported also our ensemble results, for a better comparison with these baselines. In the tables, we indicate ensemble methods postponing "(ens.)" to the method name.</p><p>We used the original implementations from their respective GitHub repositories to compute the NDCG metrics for the baselines, where possible. In the case of missing pre-trained models, we were not able to produce consistent results with the original papers. In this case, we do not report the NDCG metrics ("-").</p><p>On both the 1K and 5K test sets, our novel TERAN approach reaches state-of-the-art results on almost all the metrics. Concerning the results reported in <ref type="table" target="#tab_0">Table 1</ref> regarding 1K test set, the best performing TERAN model is the one implementing the max-over-regions sum-over-words ( ) pooling method, although the model using the symmetric loss reaches comparable results. We chose the same TERAN model to evaluate the ensemble, reaching an improvement of 5.7% and 3.5% on the Recall@1 metric on image and sentence retrieval respectively, with respect to the best baseline using ensemble methods, which is CAMERA <ref type="bibr" target="#b45">[46]</ref>. Notice, however, that even the basic TERAN model without ensemble is able to surpass CAMERA in many metrics. This confirms the power of the TERAN model despite its overall simplicity. <ref type="table" target="#tab_1">Table 2</ref> reports the results for the 5K test set, which confirm the superiority of TERAN over all the baselines also on the full test set. In this scenario, we increase the Recall@1 performance by 11.3% and 7.6% on image and sentence retrieval with respect to the CAMERA approach. On the other hand, the max-over-words sum-over-regions ( ) method loses around 10% on the Recall@1 metrics with respect to the best performing TERAN non-ensemble model. In this case, the Recall@K metric does not improve over top results obtained by the current state-of-the-art approaches. Nevertheless, this model loses only about 1.5% during image-retrieval and about 3.5% during sentence-retrieval as far as the SPICE NDCG metric is concerned, reaching perfectly comparable results with our state-of-the-art method. In light of these results, we deduce that the model is not so effective in retrieving the perfect-matching elements; however, it is still very good at retrieving the relevant ones.</p><p>As far as image retrieval is concerned, in the TERN Test and TERN Test experiments we can see that the TERN architecture trained as in <ref type="bibr" target="#b42">[43]</ref> performs fairly good when the similarity is computed as in the novel TERAN architecture, using the region and words outputs and not the I-CLS and T-CLS global descriptions. In particular, the use of max-over-words sum-over-regions Notice instead that on the sentence retrieval task, the TERN Test experiment obtains a very low performance. This is the consequence of the fact that TERN is trained to produce global-scale image-sentence matchings, while it is never forced to produce meaningful fine-grained aligned concepts. This is further supported by the evidence that if we visualize the region-words alignments as explained in the following Section 8.5 we obtain random word groundings on the image, meaning that the concepts in output from TERN are not sufficiently informative.</p><p>In order to better compare TERAN with our previous TERN approach, in <ref type="figure" target="#fig_2">Figure 3</ref> we report the validation curves for both NDCG and Recall@1 metrics, for both methods. We can notice how the NDCG metric overfits in our previous TERN model, especially when using the SPICE metric, while the Recall@ keeps increasing. On the other hand, TERAN demonstrates better generalization abilities on both metrics. This is a clear indication that TERAN is better able to retrieve relevant items in the first positions, as well as exact matching elements. Instead, TERN is more prone to overfitting to the SPICE metric, meaning that at a certain point in training, the network still searches for the top matching element, but with a tendency to push away possible relevant results compared to the novel TERAN approach.</p><p>However, looking at the results from the TERN w. Align experiment, we can notice that by augmenting the TERN objective with the TERAN alignment loss, we can slightly increase the TERN overall performance. This confirms that a more precise and meaningful region-word alignment has a visible effect also on the quality of the fixed-sized global embeddings produced by TERN.</p><p>In <ref type="table" target="#tab_2">Table 3</ref> we report the results on the Flickr30k dataset. Our single-model TERAN method outperforms the best baseline (CAMERA) on the image retrieval task while approaching the singlemodel CAMERA performance on the sentence retrieval task. Nevertheless, even on Flickr30k our TERAN method with model ensemble obtains state-of-the-art results with respect to all the baselines on all the metrics, gaining 4.6% and 1.5% on the Recall@1 metric on the image and sentence retrieval tasks respectively.</p><p>On the MS-COCO dataset, our system powered by a single GTX 1080Ti can compute a single image-to-sentence query in ∼ 0.12 on 5k sentences of the test split; in the sentence-to-image scenario, it can produce scores and rank the 1K images in ∼ 0.02 . These timings allow the TERAN scores to be effectively used, for example, in a re-ranking phase, where the first 1k images -5k sentences have been previously retrieved using a faster descriptor (e.g., the one from TERN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Qualitative Analysis for Image Retrieval</head><p>The visualization of image retrieval results is a good way to qualitatively appreciate the retrieval abilities of the proposed TERAN model. <ref type="figure" target="#fig_4">Figures 4 and 5</ref> show examples of images retrieved given a textual caption as a query, with scores computed using the max-over-regions sum-over-words method. In particular, <ref type="figure" target="#fig_4">Figure 4</ref> shows image retrieval results for a couple of flexible query captions. The red-marked images represent the exact-matching elements from the ground-truth. We can therefore conclude that the retrieved images in these examples are incorrect results for the Recall@1 metric (and for the first query even for Recall@5). However, in the very first positions, we find non-matching yet relevant images, due to the ambiguity of the query caption. These are common examples where NDCG succeeds over the Recall@K metric since we need a flexible evaluation for not-too-strict query captions. <ref type="figure" target="#fig_5">Figure 5</ref> reports instead image retrieval results for a couple of very specific query captions. For the first two queries, the network succeeds in positioning the only really relevant image in the first position (a dog sitting on a bench on the upper query, and Pennsylvania Avenue, uniquely identifiable by the street sign, on the lower query). In this case, the Recall@1 metric also succeeds, given that the query captions are very selective. The third example, instead, evidences a failure case where the model cannot deal with very subtle details. The (only) correct result is ranked 6th in this case; in the first ranking positions, the model can find images with a vase used as a centerpiece, but the table is not often visible, and when it is visible, it is not in the corner of the room.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">ABLATION STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">The Effect of Weight Sharing</head><p>We tried to apply weight sharing for the last 2 layers of the TERAN architecture, those after the linear projection to the 1024-d space. Weight sharing is used to reduce the size of the network and enforce a structure able to perform common reasoning on the high-level concepts, possibly reducing the overfitting and increasing the stability of the whole network. We experimented with the effects of weight sharing on the MS-COCO dataset with 1K test set, for both the max-over-words sum-over-regions and the max-over-regions sum-over-words scenarios.</p><p>Results are shown in the 2-nd and 6-th rows of <ref type="table">Table 4</ref>. It can be noticed that the values are perfectly comparable with the TERAN results reported in <ref type="table" target="#tab_0">Table 1</ref>, suggesting that at this point in the network the abstraction is high enough that concepts coming from images and sentences can be processed in the exact same way. This result shows that vectors at this stage have been freed from any modality bias and they are fully comparable in the same representation space.</p><p>Also, in the max-over-words sum-over-regions scenario (6-th row), there is a small gain both in terms of Recall@K and NDCG. This confirms the slight regularization effect of the weight sharing approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Averaging Versus Summing</head><p>We tried to do average instead of sum during the last pooling phase of the alignment matrix. We consider only the case in which we average-over-sentences; in fact, since in our experiments the number of visual concepts is always fixed to the 36 more influent ones during the object detection stage, average-over-regions and sum-over-regions do not differ substantially.</p><p>Thus, we considered the case of max-over-regions average-over-words ( Avg ):</p><formula xml:id="formula_7">= ∈ max ∈ | | .<label>(8)</label></formula><p>If we compute the average instead of the sum in the max-over-regions sum-over-words scenario, the final similarity score between the image and the sentence is no more dependent on the number of concepts from the textual pipeline: the similarities are averaged and not accumulated.</p><p>In the 3-rd row of <ref type="table">Table 4</ref> we can notice that by averaging we lose an important amount of information with respect to the max-over-regions sum-over-words scenario (1-st row). This insight suggests that the complexity of the query is beneficial for achieving high-quality matching.</p><p>Another side effect of using average instead of the max is the premature clear overfitting on the NDCG metrics as far as image-retrieval is concerned. The effect is shown in <ref type="figure" target="#fig_6">Figure 6</ref>. The clear overfitting of the NDCG metrics resembles the training curve trajectories of TERN <ref type="figure" target="#fig_2">(Figure 3</ref>). This result demonstrates that although this model can correctly perform exact matching, it is pulling away relevant results from the head of the ranked list of images, during the validation phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">Removing Stop-Words During Alignment</head><p>Some words may carry no substantial meaning by themselves, such as articles or prepositions. These words with a high and diffuse frequency of use are typically called stop-words and are usually removed in classical text analysis processes. In this context, removing stop-words may help the architecture to focus only on the important concepts. Doing so, the training process is simplified as the noise introduced by possibly irrelevant words is removed. Results are reported in the 4-th row of <ref type="table">Table 4</ref>. The overall performance, both in terms of Recall@ and NDCG is comparable, yet with a small decrease, with the one obtained without stop-words removal (1-st row of the table). This suggests that in this context stop-words are linguistic elements that bring some useful information to distinguish ambiguous scenes. Prepositions and adverbs often indicate the spatial arrangement of objects, thus "chair near the table" is not the same as "chair over the table"; distinguishing these fine-grained differences is beneficial for obtaining a precise image-text matching. <ref type="table">Table 4</ref>. Results for the ablation study experiments. We organize the methods in the table clustering them by the pooling method, for an easier comparison (max-over-regions methods in the upper part and max-overwords methods on the lower part). In the first row of both sections we report the TERAN results from </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Using Different Language Models</head><p>Despite the power of BERT <ref type="bibr" target="#b6">[7]</ref> for obtaining contextualized representations for the words in a sentence, many works use recurrent bidirectional networks instead, such as Bi-GRU or Bi-LSTMs. In the 5-th and 6-th row of <ref type="table">Table 4</ref> we report the results for the TERAN model with Bi-GRU and Bi-LSTM in substitution of the BERT model for language processing. We used 300-dimensional word embeddings, and a hidden size of 512 so that the final bi-directional sentence feature is a 1024-dimensional description; we used the same training protocol and hyper-parameters used for the main experiments. The results suggest that BERT is an essential ingredient for reaching top results on the Recall@K metrics, especially when K={1, 5}. In particular, Bi-LSTM and Bi-GRU lose around 14% on image retrieval and 12% on sentence retrieval on the Recall@1 metric compared to the TERAN single-model method. However, we can notice that TERAN with these recurrent language models still maintains a comparable performance with respect to the NDCG metric, especially on the image retrieval task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Visualizing the Visual-Word Alignments</head><p>Inspired by the work in <ref type="bibr" target="#b23">[24]</ref>, we try to visualize the region-word alignments learned by TERAN on some images from the test set of MS-COCO dataset. We recall that no supervision was used at the region-word level during the training phase.</p><p>In <ref type="figure">Figure 7</ref>, we report some figures where every sentence word has been associated with the toprelevant image region. The affinity between visual concepts (region features) and textual concepts (word features) has been measured through cosine similarity, just as during the training phase.</p><p>We can see that the words have overall plausible groundings on the image they describe. Some words are really difficult to ground, such as articles, verbs, or adjectives. However, we can notice that phrases describing a visual entity and composed of nouns with the related articles and adjectives (e.g. "a green tie", or "a wooden table") are often grounded to the same region. This further confirms that the TERAN architecture can produce meaningful concepts, and it is also able to cluster them under the form of complete reasonable phrases. <ref type="figure">Fig. 7</ref>. Visualization of the word-region alignments. Near each word, we report the cosine similarity computed between that word and the top-relevant image region associated with it. We slightly offset the overlapping bounding-boxes for a better visualization.</p><p>We can notice some wrong word groundings in the images, such as the phrase "eyes closed" that is associated with the region depicting the closed mouth. In this case, the error seems to lie on some localized misunderstanding of the scene (in this case the noun "eyes" has probably been misunderstood since the mouth and the eyes are both closed). Overall, however, complex scenes are correctly broken down into their salient elements, and only the key regions are attended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSIONS</head><p>In this work, we introduced the Transformer Encoder Reasoning and Alignment Network (TERAN). TERAN is a relationship-aware architecture based on the Transformer Encoder (TE) architecture, exploiting self-attention mechanisms, able to reason about the spatial and abstract relationships between elements in the image and in the text separately.</p><p>Differently from TERN <ref type="bibr" target="#b42">[43]</ref>, TERAN forces a fine-grained alignment among the region and word features without any supervision at this level. We demonstrated that by enforcing this fine-grained word-region alignment at training time we can obtain state-of-the-art results on the popular MS-COCO and Flickr30K datasets. Besides, thanks to the overall simplicity of the proposed model, we can obtain effective visual and textual features for use in scalable retrieval setups.</p><p>We measured the performance of our TERAN architecture in the context of cross-modal retrieval using both the already-in-use Recall@K metric and the newly introduced NDCG with the ROUGE-L and SPICE textual relevance measures. In spite of its simplicity, TERAN can outperform current state-of-the-art models on these two retrieval metrics, competing with the currently very effective entangled visual-textual matching models, which on the contrary are not able to produce features for scalable retrieval. Furthermore, we showed that TERAN can successfully output visually-pleasant word-region alignments. We also observed that a further reduction of the network complexity can be obtained by sharing the weights of the last TE layers. This has important benefits also on the stability and in the generalization abilities of the whole architecture.</p><p>In the end, we think that this work proposes an interesting path towards efficient and effective cross-modal information retrieval.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>A high-level view of the transformer encoder layer. Every arrow carries fixed-sized vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The proposed TERAN architecture. TEs stands for Transformer Encoders, and it indicates a stack of TE layers whose internals are recalled in Section 3 and explained in detail in [53]. Region and word features are extracted through a bottom-up attention model based on Faster-RCNN and BERT, respectively. The final image-text (I-T) similarity score is obtained by pooling a region-word (R-W) similarity matrix. Note that the special I-CLS and T-CLS are not used in the basic formulation of TERAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Validation metrics on sentence-to-image retrieval, measured during the training phase, for the averageover-sentences scenario. TERN overfits on the NDCG metrics, while Recall@1 still improves. TERAN instead generalizes better on both metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Query:</head><label></label><figDesc>A large jetliner sitting on top of an airport runway.Query: An eating area with a table and a few chairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Example of image retrieval results for a couple of flexible query captions. These are common examples where NDCG succeeds over the Recall@K metric. The ground-truth matching image is not among the very first positions; however, the top-ranked images are also visually very relevant. Query: A large white dog is sitting on a bench beside an elderly man.Query: An old black and white photo of Pennsylvania Avenue.Query:Table situatedin corner of room with a vase for a center piece.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Example of image retrieval results for a couple of very specific query captions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Validation metrics measured during the training phase, for the average-over-sentences scenario. This model overfits on the NDCG metrics on the image-retrieval task, while Recall@1 still improves.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on the MS-COCO dataset, on the 1K test set.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Image Retrieval</cell><cell></cell><cell>Sentence Retrieval</cell></row><row><cell></cell><cell></cell><cell>Recall@K</cell><cell cols="2">NDCG</cell><cell>Recall@K</cell><cell>NDCG</cell></row><row><cell>Model</cell><cell></cell><cell cols="4">K=1 K=5 K=10 ROUGE-L SPICE K=1 K=5 K=10 ROUGE-L SPICE</cell></row><row><cell></cell><cell></cell><cell cols="2">(VGG)</cell><cell></cell></row><row><cell>JGCAR [55]</cell><cell></cell><cell>40.2 74.8 85.7 -</cell><cell></cell><cell>-</cell><cell>52.7 82.6 90.5 -</cell><cell>-</cell></row><row><cell>SAN [21]</cell><cell></cell><cell>60.8 90.3 95.7 -</cell><cell></cell><cell>-</cell><cell>74.9 94.9 98.2 -</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">(ResNet)</cell><cell></cell></row><row><cell>VSE++ [9]  †</cell><cell></cell><cell cols="2">52.0 84.3 92.0 0.712</cell><cell cols="2">0.617 64.6 90.0 95.7 0.705</cell><cell>0.658</cell></row><row><cell>SMAN [22]</cell><cell></cell><cell>58.8 87.4 93.5 -</cell><cell></cell><cell>-</cell><cell>68.4 91.3 96.6 -</cell><cell>-</cell></row><row><cell cols="2">M3A-Net [20]</cell><cell>58.4 87.1 94.0 -</cell><cell></cell><cell>-</cell><cell>70.4 91.7 96.8 -</cell><cell>-</cell></row><row><cell cols="2">AAMEL [57]</cell><cell>59.9 89.0 95.1 -</cell><cell></cell><cell>-</cell><cell>74.3 95.4 98.2 -</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="3">(Region CNN)</cell></row><row><cell>MRNN [24]</cell><cell></cell><cell>27.4 60.2 74.8 -</cell><cell></cell><cell>-</cell><cell>38.4 69.9 80.5 -</cell><cell>-</cell></row><row><cell cols="2">SCAN (ens.) [28]</cell><cell>58.8 88.4 94.8 -</cell><cell></cell><cell>-</cell><cell>72.7 94.8 98.4 -</cell><cell>-</cell></row><row><cell cols="2">SAEM (ens.)[59]  § †</cell><cell>57.8 88.6 94.9 -</cell><cell></cell><cell>-</cell><cell>71.2 94.1 97.7 -</cell><cell>-</cell></row><row><cell>CASC [60]</cell><cell></cell><cell>58.9 89.8 96.0 -</cell><cell></cell><cell>-</cell><cell>72.3 96.0 99.0 -</cell><cell>-</cell></row><row><cell cols="2">MMCA [58]  §</cell><cell>61.6 89.8 95.2 -</cell><cell></cell><cell>-</cell><cell>74.8 95.6 97.7 -</cell><cell>-</cell></row><row><cell cols="2">VSRN [30]  †</cell><cell cols="2">60.8 88.4 94.1 0.723</cell><cell cols="2">0.621 74.0 94.3 97.8 0.737</cell><cell>0.690</cell></row><row><cell cols="2">VSRN (ens.) [30]  †</cell><cell cols="2">62.8 89.7 95.1 0.732</cell><cell cols="2">0.637 76.2 94.8 98.2 0.748</cell><cell>0.704</cell></row><row><cell cols="2">PFAN (ens.) [56]</cell><cell>61.6 89.6 95.2 -</cell><cell></cell><cell>-</cell><cell>76.5 96.3 99.0 -</cell><cell>-</cell></row><row><cell cols="2">Full-IMRAM [4]</cell><cell>61.7 89.1 95.0 -</cell><cell></cell><cell>-</cell><cell>76.7 95.6 98.5 -</cell><cell>-</cell></row><row><cell cols="2">CAMERA [46]  § †</cell><cell>62.3 90.1 95.2 -</cell><cell></cell><cell>-</cell><cell>75.9 95.5 98.6 -</cell><cell>-</cell></row><row><cell cols="3">CAMERA (ens.) [46]  § † 63.4 90.9 95.8 -</cell><cell></cell><cell>-</cell><cell>77.5 96.3 98.8 -</cell><cell>-</cell></row><row><cell>TERN [43]</cell><cell></cell><cell cols="2">51.9 85.6 93.6 0.725</cell><cell cols="2">0.653 63.7 90.5 96.2 0.716</cell><cell>0.674</cell></row><row><cell>TERN</cell><cell>Test</cell><cell cols="2">51.5 84.9 93.1 0.722</cell><cell cols="2">0.642 26.6 70.3 86.3 0.568</cell><cell>0.530</cell></row><row><cell>TERN</cell><cell>Test</cell><cell cols="2">51.2 84.6 92.9 0.722</cell><cell cols="2">0.643 61.9 88.9 95.7 0.713</cell><cell>0.666</cell></row><row><cell cols="2">TERN w. Align</cell><cell cols="2">54.5 86.9 94.2 0.724</cell><cell cols="2">0.643 65.5 91.0 96.5 0.720</cell><cell>0.675</cell></row><row><cell cols="2">TERAN Symm.</cell><cell cols="2">63.5 91.1 96.3 0.739</cell><cell cols="2">0.666 76.3 95.3 98.4 0.741</cell><cell>0.701</cell></row><row><cell>TERAN</cell><cell></cell><cell cols="2">57.5 88.4 94.9 0.730</cell><cell cols="2">0.658 70.8 93.5 97.3 0.725</cell><cell>0.681</cell></row><row><cell>TERAN</cell><cell></cell><cell cols="2">65.0 91.2 96.4 0.741</cell><cell cols="2">0.668 77.7 95.9 98.6 0.746</cell><cell>0.707</cell></row><row><cell>TERAN</cell><cell>(ens.)</cell><cell cols="2">67.0 92.2 96.9 0.747</cell><cell cols="2">0.680 80.2 96.6 99.0 0.756</cell><cell>0.720</cell></row><row><cell cols="3">§ Uses BERT as language model</cell><cell></cell><cell></cell></row><row><cell cols="3">† Uses disentangled visual-textual pipelines</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on the MS-COCO dataset, on the 5K test set. quite well compared to the similarity computed through I-CLS and T-CLS global visual and textual features as it is in TERN.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Image Retrieval</cell><cell></cell><cell>Sentence Retrieval</cell></row><row><cell></cell><cell></cell><cell>Recall@K</cell><cell cols="2">NDCG</cell><cell>Recall@K</cell><cell>NDCG</cell></row><row><cell>Model</cell><cell></cell><cell cols="4">K=1 K=5 K=10 ROUGE-L SPICE K=1 K=5 K=10 ROUGE-L SPICE</cell></row><row><cell></cell><cell></cell><cell cols="2">(ResNet)</cell><cell></cell></row><row><cell>VSE++ [9]  †</cell><cell></cell><cell cols="2">30.3 59.4 72.4 0.656</cell><cell cols="2">0.577 41.3 71.1 81.2 0.597</cell><cell>0.551</cell></row><row><cell cols="2">M3A-Net [20]</cell><cell>38.3 65.7 76.9 -</cell><cell></cell><cell>-</cell><cell>48.9 75.2 84.4 -</cell><cell>-</cell></row><row><cell cols="2">AAMEL [57]</cell><cell>39.9 71.3 81.7 -</cell><cell></cell><cell>-</cell><cell>51.9 84.2 91.2 -</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="3">(Region CNN)</cell></row><row><cell>MRNN [24]</cell><cell></cell><cell>10.7 29.6 42.2 -</cell><cell></cell><cell>-</cell><cell>16.5 39.2 52.0 -</cell><cell>-</cell></row><row><cell cols="2">SCAN (ens.) [28]</cell><cell>38.6 69.3 80.4 -</cell><cell></cell><cell>-</cell><cell>50.4 82.2 90.0 -</cell><cell>-</cell></row><row><cell cols="2">VSRN [30]  †</cell><cell cols="2">37.9 68.5 79.4 0.676</cell><cell cols="2">0.596 50.3 79.6 87.9 0.639</cell><cell>0.598</cell></row><row><cell cols="2">VSRN (ens.) [30]  †</cell><cell cols="2">40.5 70.6 81.1 0.684</cell><cell cols="2">0.609 53.0 81.1 89.4 0.652</cell><cell>0.612</cell></row><row><cell cols="2">Full-IMRAM [4]</cell><cell>39.7 69.1 79.8 -</cell><cell></cell><cell>-</cell><cell>53.7 83.2 91.0 -</cell><cell>-</cell></row><row><cell cols="2">MMCA [58]  §</cell><cell>38.7 69.7 80.8 -</cell><cell></cell><cell>-</cell><cell>54.0 82.5 90.7 -</cell><cell>-</cell></row><row><cell cols="2">CAMERA [46]  § †</cell><cell>39.0 70.5 81.5 -</cell><cell></cell><cell>-</cell><cell>53.1 81.3 89.8 -</cell><cell>-</cell></row><row><cell cols="3">CAMERA (ens.) [46]  § † 40.5 71.7 82.5 -</cell><cell></cell><cell>-</cell><cell>55.1 82.9 91.2 -</cell><cell>-</cell></row><row><cell>TERN [43]</cell><cell></cell><cell cols="2">28.7 59.7 72.7 0.665</cell><cell cols="2">0.599 38.4 69.5 81.3 0.601</cell><cell>0.556</cell></row><row><cell>TERN</cell><cell>Test</cell><cell cols="2">28.3 59.1 72.2 0.663</cell><cell cols="2">0.592 6.8 28.4 46.7 0.406</cell><cell>0.372</cell></row><row><cell>TERN</cell><cell>Test</cell><cell cols="2">28.1 58.6 71.8 0.663</cell><cell cols="2">0.592 35.5 67.5 78.9 0.600</cell><cell>0.551</cell></row><row><cell cols="2">TERN w. Align</cell><cell cols="2">31.4 62.5 75.3 0.667</cell><cell cols="2">0.597 40.2 71.1 81.9 0.606</cell><cell>0.561</cell></row><row><cell cols="2">TERAN Symm.</cell><cell cols="2">41.0 71.6 82.3 0.680</cell><cell cols="2">0.607 54.8 82.7 90.9 0.641</cell><cell>0.601</cell></row><row><cell>TERAN</cell><cell></cell><cell cols="2">34.1 65.7 77.8 0.669</cell><cell cols="2">0.596 45.3 76.3 86.2 0.611</cell><cell>0.564</cell></row><row><cell>TERAN</cell><cell></cell><cell cols="2">42.6 72.5 82.9 0.682</cell><cell cols="2">0.610 55.6 83.9 91.6 0.643</cell><cell>0.606</cell></row><row><cell>TERAN</cell><cell>(ens.)</cell><cell cols="2">45.1 74.6 84.4 0.689</cell><cell cols="2">0.622 59.3 85.8 92.4 0.658</cell><cell>0.624</cell></row><row><cell cols="3">§ Uses BERT as language model</cell><cell></cell><cell></cell></row><row><cell cols="3">† Uses disentangled visual-textual pipelines</cell><cell></cell><cell></cell></row><row><cell cols="2">similarity still works</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results on the Flickr30k dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Image Retrieval</cell><cell></cell><cell>Sentence Retrieval</cell></row><row><cell></cell><cell></cell><cell>Recall@K</cell><cell cols="2">NDCG</cell><cell>Recall@K</cell><cell>NDCG</cell></row><row><cell>Model</cell><cell></cell><cell cols="4">K=1 K=5 K=10 ROUGE-L SPICE K=1 K=5 K=10 ROUGE-L SPICE</cell></row><row><cell></cell><cell></cell><cell cols="2">(VGG)</cell><cell></cell></row><row><cell>JGCAR [55]</cell><cell></cell><cell>35.2 62.0 72.4 -</cell><cell></cell><cell>-</cell><cell>44.9 75.3 82.7 -</cell><cell>-</cell></row><row><cell>SAN [21]</cell><cell></cell><cell>51.4 77.2 85.2 -</cell><cell></cell><cell>-</cell><cell>67.0 88.0 94.6 -</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">(ResNet)</cell><cell></cell></row><row><cell>VSE++ [9]  †</cell><cell></cell><cell cols="2">39.6 70.1 79.5 0.631</cell><cell cols="2">0.494 52.9 80.5 87.2 0.601</cell><cell>0.514</cell></row><row><cell cols="2">TIMAM [50]  § †</cell><cell>42.6 71.6 81.9 -</cell><cell></cell><cell>-</cell><cell>53.1 78.8 87.6 -</cell><cell>-</cell></row><row><cell>SMAN [22]</cell><cell></cell><cell>43.4 73.7 83.4 -</cell><cell></cell><cell>-</cell><cell>57.3 85.3 92.2 -</cell><cell>-</cell></row><row><cell>M3A-Net [20]</cell><cell></cell><cell>44.7 72.4 81.1 -</cell><cell></cell><cell>-</cell><cell>58.1 82.8 90.1 -</cell><cell>-</cell></row><row><cell>AAMEL [57]</cell><cell></cell><cell>49.7 79.2 86.4 -</cell><cell></cell><cell>-</cell><cell>68.5 91.2 95.9 -</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="3">(Region CNN)</cell></row><row><cell>MRNN [24]</cell><cell></cell><cell>15.2 37.7 50.5 -</cell><cell></cell><cell>-</cell><cell>22.2 48.2 61.4 -</cell><cell>-</cell></row><row><cell cols="2">SCAN (ens.) [28]</cell><cell>48.6 77.7 85.2 -</cell><cell></cell><cell>-</cell><cell>67.4 90.3 95.8 -</cell><cell>-</cell></row><row><cell cols="2">PFAN (ens.) [56]</cell><cell>50.4 78.7 86.1 -</cell><cell></cell><cell>-</cell><cell>70.0 91.8 95.0 -</cell><cell>-</cell></row><row><cell cols="2">SAEM (ens.)[59]  § †</cell><cell>52.4 81.1 88.1 -</cell><cell></cell><cell>-</cell><cell>69.1 91.0 95.1 -</cell><cell>-</cell></row><row><cell>VSRN [30]  †</cell><cell></cell><cell cols="2">53.0 77.9 85.7 0.673</cell><cell cols="2">0.545 70.4 89.2 93.7 0.676</cell><cell>0.592</cell></row><row><cell cols="2">VSRN (ens.) [30]  †</cell><cell cols="2">54.7 81.8 88.2 0.680</cell><cell cols="2">0.556 71.3 90.6 96.0 0.688</cell><cell>0.606</cell></row><row><cell cols="2">Full-IMRAM [4]</cell><cell>53.9 79.4 87.2 -</cell><cell></cell><cell>-</cell><cell>74.1 93.0 96.6 -</cell><cell>-</cell></row><row><cell>MMCA [58]  §</cell><cell></cell><cell>54.8 81.4 87.8 -</cell><cell></cell><cell>-</cell><cell>74.2 92.8 96.4 -</cell><cell>-</cell></row><row><cell>CASC [60]</cell><cell></cell><cell>60.2 78.3 86.3 -</cell><cell></cell><cell>-</cell><cell>68.5 90.6 95.9 -</cell><cell>-</cell></row><row><cell cols="2">CAMERA [46]  § †</cell><cell>58.9 84.7 90.2 -</cell><cell></cell><cell>-</cell><cell>76.5 95.1 97.2 -</cell><cell>-</cell></row><row><cell cols="3">CAMERA (ens.) [46]  § † 60.3 85.9 91.7 -</cell><cell></cell><cell>-</cell><cell>78.0 95.1 97.9 -</cell><cell>-</cell></row><row><cell>TERN [43]</cell><cell></cell><cell cols="2">41.1 71.9 81.2 0.647</cell><cell cols="2">0.512 53.2 79.4 86.0 0.624</cell><cell>0.529</cell></row><row><cell cols="2">TERAN Symm.</cell><cell cols="2">55.7 83.1 89.3 0.678</cell><cell cols="2">0.555 71.8 90.5 94.7 0.676</cell><cell>0.603</cell></row><row><cell>TERAN</cell><cell></cell><cell cols="2">49.4 78.3 85.9 0.664</cell><cell cols="2">0.536 60.5 85.1 92.2 0.651</cell><cell>0.558</cell></row><row><cell>TERAN</cell><cell></cell><cell cols="2">59.5 84.9 90.6 0.686</cell><cell cols="2">0.564 75.8 93.2 96.7 0.687</cell><cell>0.614</cell></row><row><cell>TERAN</cell><cell>(ens.)</cell><cell cols="2">63.1 87.3 92.6 0.695</cell><cell cols="2">0.577 79.2 94.4 96.8 0.707</cell><cell>0.636</cell></row><row><cell cols="3">§ Uses BERT as language model</cell><cell></cell><cell></cell></row><row><cell cols="3">† Uses disentangled visual-textual pipelines</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Experiments are computed on the MS-COCO dataset, 1K test set.</figDesc><table><row><cell></cell><cell cols="2">Image Retrieval</cell><cell></cell><cell>Sentence Retrieval</cell></row><row><cell></cell><cell>Recall@K</cell><cell cols="2">NDCG</cell><cell>Recall@K</cell><cell>NDCG</cell></row><row><cell>Model</cell><cell cols="4">K=1 K=5 K=10 ROUGE-L SPICE K=1 K=5 K=10 ROUGE-L SPICE</cell></row><row><cell>(from Table 1)</cell><cell cols="2">65.0 91.2 96.4 0.741</cell><cell cols="2">0.668 77.7 95.9 98.6 0.746</cell><cell>0.707</cell></row><row><cell>Shared-W</cell><cell cols="2">64.5 91.3 96.3 0.740</cell><cell cols="2">0.667 77.3 95.9 98.4 0.746</cell><cell>0.706</cell></row><row><cell>Avg</cell><cell cols="2">57.2 87.6 93.6 0.705</cell><cell cols="2">0.587 68.6 92.4 96.7 0.721</cell><cell>0.671</cell></row><row><cell cols="3">StopWordsFilter 64.2 91.1 96.3 0.737</cell><cell cols="2">0.658 76.8 95.9 98.6 0.745</cell><cell>0.705</cell></row><row><cell>Bi-LSTM</cell><cell cols="2">55.6 86.9 93.9 0.734</cell><cell cols="2">0.666 67.4 92.5 96.9 0.717</cell><cell>0.677</cell></row><row><cell>Bi-GRU</cell><cell cols="2">56.3 87.1 94.0 0.735</cell><cell cols="2">0.666 69.1 93.4 97.1 0.720</cell><cell>0.678</cell></row><row><cell>(from Table 1)</cell><cell cols="2">57.5 88.4 94.9 0.730</cell><cell cols="2">0.658 70.8 93.5 97.3 0.725</cell><cell>0.681</cell></row><row><cell>Shared-W</cell><cell cols="2">58.1 88.4 95.0 0.730</cell><cell cols="2">0.657 71.1 93.1 97.7 0.728</cell><cell>0.683</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/huggingface/transformers 2 https://github.com/peteanderson80/bottom-up-attention</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by "Intelligenza Artificiale per il Monitoraggio Visuale dei Siti Culturali" (AI4CHSites) CNR4C program, CUP B15J19001040004, by the AI4EU project, funded by the EC (H2020 -Contract n. 825619), and AI4Media under GA 951911.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Picture it in your mind: Generating high level visual representations from textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Carrara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiziano</forename><surname>Fagni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Moreo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval J</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="208" to="229" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">IMRAM: Iterative Matching with Recurrent Attention Memory for Cross-Modal Image-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12655" to="12663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Show, control and tell: A framework for generating controllable and grounded captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcella</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8307" to="8316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT 2019</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Linking image and text with 2-way nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4601" to="4611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">VSE++: Improving Visual-Semantic Embeddings with Hard Negatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fartash</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2018</title>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Look, imagine and match: Improving textual-visual cross-modal retrieval with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiuxiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shafiq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7181" to="7189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Associating Images with Sentences Using Recurrent Canonical Correlation Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">5516</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Computer Vision</title>
		<meeting>of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Bi-directional spatial-semantic attention networks for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghua</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhoujun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2008" to="2020" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yong</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Computer Vision</title>
		<meeting>of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4634" to="4643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Acmm: Aligned cross-modal memory for few-shot image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Computer Vision</title>
		<meeting>of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5774" to="5783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Instance-aware image and sentence matching with selective multimodal lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2310" to="2318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning semantic concepts and order for image and sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6163" to="6171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image and sentence matching via semantic concepts and order learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.00849</idno>
		<title level="m">Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-Modal Memory Enhancement Attention Network for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="38438" to="38447" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Saliency-guided attention network for image-sentence matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Computer Vision</title>
		<meeting>of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5754" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">SMAN: Stacked Multimodal Attention Network for Cross-Modal Image-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Computer Vision</title>
		<meeting>of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2989" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7482" to="7491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Associating neural word embeddings with deep image representations using fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gil</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4437" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV</title>
		<meeting>of the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
	<note>Gang Hua, Houdong Hu, and Xiaodong He</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning visual relation priors for image-text matching and image captioning with neural scene graph generators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09953</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual semantic reasoning for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Computer Vision</title>
		<meeting>of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4654" to="4662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Know more say less: Image captioning based on scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqiang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="2117" to="2130" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Factorizable net: an efficient subgraph-based framework for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="335" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Leveraging visual question answering for image-caption ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="261" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Focus your attention: A bidirectional focal attention network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An-An</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 27th ACM International Conference on Multimedia</title>
		<meeting>of the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Graph Structured Network for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10921" to="10930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning a recurrent residual fusion network for multimodal matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanming</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Erwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Bakker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Computer Vision</title>
		<meeting>of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4107" to="4116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13" to="23" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Macavaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria</forename><surname>Franco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raffaele</forename><surname>Nardini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Perego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nazli</forename><surname>Tonellotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ophir</forename><surname>Goharian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frieder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.14255</idno>
		<title level="m">Efficient Document Re-Ranking for Transformers by Precomputing Term Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning relationshipaware visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Carrara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Gennaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV</title>
		<meeting>of the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning visual features for relational CBIR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Amato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Carrara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Gennaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Multimedia Information Retrieval</title>
		<imprint>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Transformer Reasoning Network for Image-Text Matching and Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Messina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Falchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Esuli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giuseppe</forename><surname>Amato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Accepted</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Imagebert: Cross-modal pre-training with large-scale weak-supervised image-text data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Sacheti</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07966</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Context-Aware Multi-View Summarization Network for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leigang</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 28th ACM International Conference on Multimedia</title>
		<meeting>of the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1047" to="1055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youssef</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerret</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaibhava</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Adversarial representation learning for text-to-image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Computer Vision</title>
		<meeting>of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5814" to="5824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">VL-BERT: Pre-training of Generic Visual-Linguistic Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>of the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Order-Embeddings of Images and Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vendrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations, ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Joint global and co-attentive representation learning for image-sentence retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbao</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 26th ACM international conference on Multimedia</title>
		<meeting>of the 26th ACM international conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1398" to="1406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Position focused attention network for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaxiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueming</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.09748</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adversarial Attentive Multi-modal Embedding Learning for Image-Text Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaimin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multi-Modality Cross Attention Network for Image and Sentence Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10941" to="10950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning fragment self-attention embeddings for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiling</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoli</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the 27th ACM International Conference on Multimedia</title>
		<meeting>of the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2088" to="2096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Cross-modal attention with semantic consistence for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European conference on computer vision (ECCV</title>
		<meeting>of the European conference on computer vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="670" to="685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European conference on computer vision (ECCV)</title>
		<meeting>of the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alice</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micah</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unified Vision-Language Pre-Training for Image Captioning and VQA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13041" to="13049" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
