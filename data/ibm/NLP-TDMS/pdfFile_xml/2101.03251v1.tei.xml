<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unobtrusive Pain Monitoring in Older Adults with Dementia using Pairwise and Contrastive Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siavash</forename><surname>Rezaei</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Moturu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">M</forename><surname>Prkachin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hadjistavropoulos</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Taati</surname></persName>
						</author>
						<title level="a" type="main">Unobtrusive Pain Monitoring in Older Adults with Dementia using Pairwise and Contrastive Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms Computer Vision</term>
					<term>Dementia</term>
					<term>Facial Expression</term>
					<term>Older adults</term>
					<term>Pain</term>
					<term>Deep Learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although pain is frequent in old age, older adults are often undertreated for pain. This is especially the case for longterm care residents with moderate to severe dementia who cannot report their pain because of cognitive impairments that accompany dementia. Nursing staff acknowledge the challenges of effectively recognizing and managing pain in long-term care facilities due to lack of human resources and, sometimes, expertise to use validated pain assessment approaches on a regular basis. Vision-based ambient monitoring will allow for frequent automated assessments so care staff could be automatically notified when signs of pain are displayed. However, existing computer vision techniques for pain detection are not validated on faces of older adults or people with dementia, and this population is not represented in existing facial expression datasets of pain. We present the first fully automated vision-based technique validated on a dementia cohort. Our contributions are threefold. First, we develop a deep learning-based computer vision system for detecting painful facial expressions on a video dataset that is collected unobtrusively from older adult participants with and without dementia. Second, we introduce a pairwise comparative inference method that calibrates to each person and is sensitive to changes in facial expression while using training data more efficiently than sequence models. Third, we introduce a fast contrastive training method that improves cross-dataset performance. Our pain estimation model outperforms baselines by a wide margin, especially when evaluated on faces of people with dementia. Pre-trained model and demo code available at https://github.com/TaatiTeam/pain_detection_demo</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION A. Motivation</head><p>Pain is common and frequent in old age <ref type="bibr" target="#b0">[1]</ref>, but older adults are often underdiagnosed and undertreated for pain <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. This problem is especially serious for people with dementia who are often unable to verbally express or otherwise communicate their experience due to cognitive impairment <ref type="bibr" target="#b2">[3]</ref>. Effective and validated assessment approaches, based on observation of non-verbal pain behaviours -e.g. vocalizations, facial and body movements -are available for this population <ref type="bibr" target="#b3">[4]</ref>; but these approaches are not implemented in long-term care (LTC), because significant staff shortages make the ongoing monitoring of expressed pain infeasible <ref type="bibr" target="#b4">[5]</ref>. Untreated pain can have serious physical (e.g. underlying conditions getting worse) and psychological (e.g. agitation and aggression) consequences and addressing pain in a timely manner can have a meaningful positive impact on the quality of life of people residing in LTC. The motivation for this work is to develop an ambient monitoring technology to reliably, automatically, and consistently assess pain in order to improve pain management in LTC and to ultimately provide a better quality of care to older adults.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Clinically Valid Assessments of Pain in Dementia</head><p>Two clinically validated metrics for assessing pain in dementia exist: 1) the Prkachin and Solomon Pain Index (PSPI) <ref type="bibr" target="#b5">[6]</ref> and 2) the Pain Assessment Checklist for Seniors with Limited Ability to Communicate-II (PACSLAC-II) <ref type="bibr" target="#b6">[7]</ref>.</p><p>PSPI is a 16-point metric, and is based on the Facial Action Coding System (FACS) <ref type="bibr" target="#b7">[8]</ref>. FACS is a detailed taxonomy of different actions units (AU) expressed through facial muscle movements and, as such, provides a good basis for evaluating painful facial expressions. PSPI is computed as follows: P SP I = AU 43 + max(AU 6 , AU 7 ) + max(AU 9 , AU 10 ) + AU <ref type="bibr" target="#b3">4</ref> (1) (orange), AU 6 (yellow), AU 7 (blue), AU 9 (red), AU 10 (green), and AU 43 (purple).</p><p>where AU 43 corresponds to eyes closed, AU 6 and AU 7 correspond to activation of cheek raiser and eyelid tightener muscles, AU 9 and AU 10 correspond to levator muscles, and AU 4 corresponds to brow lowering muscles ( <ref type="figure" target="#fig_0">Figure 1</ref>). Here, AU 43 is a binary indicator and the others are scored in a [0, 5] range. The PACSLAC-II is a 31-item checklist-based metric designed to have a faster learning curve for annotators. It considers movements of the body and vocalizations in addition to a set of facial expressions. To employ either of these metrics at a care facility, caregivers need to complete the required training.</p><p>Both PSPI and PACSLAC-II are well validated measures for assessing pain in people with dementia <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>. In this paper, we focus on the automatic estimation of PSPI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Related Work</head><p>The advances in computer vision in the last two decades have prompted significant volume of work on automatic pain intensity estimation <ref type="bibr" target="#b10">[11]</ref>. While other pain datasets exist (e.g. the X-ITE database <ref type="bibr" target="#b11">[12]</ref> and the BioVid Emo DB <ref type="bibr" target="#b12">[13]</ref>), the vast majority of this work has been developed and validated using two openly available datasets dedicated to pain estimation, the UNBC-McMaster Shoulder Pain Expression Archive Database <ref type="bibr" target="#b13">[14]</ref> and the BioVid Heat Pain Database <ref type="bibr" target="#b14">[15]</ref>. The range of work is diverse in terms of the learning algorithms, learning tasks, and modalities used.</p><p>Ashraf et al. <ref type="bibr" target="#b16">[16]</ref> used features derived from an active appearance model (AAM) to train a Support Vector Machine (SVM) classifier. They performed pain detection (binary classification) both per-frame and for a sequence of frames. For sequence-level prediction, they aggregated frame level predictions by averaging. On the UNBC-McMaster dataset they achieve an F1 score of 0.56 and 0.48 for per-frame and sequence level prediction, respectively. Interestingly, the performance of their model degrades for sequence level prediction. Also on the UNBC-McMaster dataset, Hammal and Cohn <ref type="bibr" target="#b17">[17]</ref> applied Log-Normal filter based features and SVMs to classify pain into for four levels of intensity.</p><p>Kaltwang et al. <ref type="bibr" target="#b18">[18]</ref> first used each of facial landmarks, Discrete Cosine Transform (DCT), and Local Binary Patterns (LBP) features to train three Relevance Vector Regression (RVR) models separately. They showed that best performance was achieved by training a fourth RVR model to combine the predictions of their three separately trained RVR models. Their method also achieved better performance when they directly predicted PSPI as opposed to predicting the action units and then computing PSPI from them.</p><p>More recently, deep learning methods have also been applied to pain estimation directly from images or a sequence of frames <ref type="bibr" target="#b19">[19]</ref>- <ref type="bibr" target="#b22">[22]</ref>. Zhou et. al <ref type="bibr" target="#b23">[23]</ref> used the UNBC-McMaster dataset to train a recurrent convolutional model end-to-end. They used frame sequences of length 30 frames (1 second) with the goal of estimating the pain level in the last frame. They achieved a Pearson correlation of 0.65 with manually annotated pain levels. Egede et. al <ref type="bibr" target="#b24">[24]</ref> fused prediction from pre-trained deeplearned features, Histogram of Oriented Gradients (HOG) features, and facial landmarks using RVR models. They achieved a Pearson correlation of 0.67 on the UNBC-McMaster dataset. Tavakolian &amp; Hadid <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref> used 3D convolutions to model temporal aspects of videos in their two recent works. In <ref type="bibr" target="#b25">[25]</ref> and <ref type="bibr" target="#b26">[26]</ref>, they report a Pearson Correlation Coefficient (PCC) of 0.83 and 0.92 on the UNBC-McMaster dataset, respectively.</p><p>The UNBC-McMaster dataset is a unimodal dataset and is used for developing and evaluating vision-based pain assessment techniques. The BioVid, BioVid Emo DB, and X-ITE datasets, by contrast, are multimodal and their availability has sparked extensive interest in multimodal pain detection <ref type="bibr" target="#b27">[27]</ref>, <ref type="bibr" target="#b28">[28]</ref>.</p><p>It is also worth noting the rich body of work focused on pain detection in neonates. In one of the earliest works, Brahnam et al. <ref type="bibr" target="#b29">[29]</ref> developed the first pain classification dataset, the Infant Classification of Pain Expressions (COPE) database. They used Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and SVM to discriminate between induced pain and other stressors. More recently, Celona &amp; Manoni <ref type="bibr" target="#b30">[30]</ref> used a mixture of hand crafted features such as LBP and HOG and features extracted from a pre-trained convolutional neural network. They then applied PCA and trained a linear SVM to perform pain assessment in the COPE database. More recently, Salekin et al. <ref type="bibr" target="#b31">[31]</ref> proposed a fully deep learning-based model which takes into account both facial expressions of neonates as well as their body movements. They also model the temporal aspect of the task using a recurrent neural network. They evaluate their algorithm on video data collected from neonates at Neonatal Intensive Care Units (NICU) using the NIPS <ref type="bibr" target="#b32">[32]</ref> pain scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Contributions</head><p>Our work has three main contributions, addressing gaps in existing work, as detailed below. In experimental analysis, we show that our method outperforms baselines by a wide margin specially on participants with dementia.</p><p>Fully Automated Pain Detection in Dementia: Although there has been extensive work on vision-based pain detection <ref type="bibr" target="#b33">[33]</ref>, work focusing on detecting pain in older adults with dementia, who stand to benefit the most from this technology, is scarce <ref type="bibr" target="#b34">[34]</ref>. Atee et al. <ref type="bibr" target="#b35">[35]</ref> developed a system for pain assessment specifically for people with dementia, but their system requires manual entry of observations with an operator through a mobile app. Previous work has shown facial analysis models that work well on faces of healthy young adults do not necessarily yield similar performance when evaluated on faces of older adults with dementia <ref type="bibr" target="#b36">[36]</ref>. Our first contribution is the development and evaluation of a computer vision model to fully automatically and unobtrusively estimate pain from facial expressions of older adults with dementia. To our knowledge, our work is the first of its kind to employ these techniques on a relatively large dataset that is collected unobtrusively from the target population.</p><p>Pairwise Pain Detection: Our model was developed by drawing on insights from how trained human annotators perform the same task. It has long been recognized that incorporating temporal information leads to significant gains in performance in automatic facial expression recognition. As a result, many models have been developed to take advantage of temporal information in a sequence of frames in videos <ref type="bibr" target="#b19">[19]</ref>- <ref type="bibr" target="#b22">[22]</ref>. However, given the small size of the datasets available for facial expression recognition, the number of distinct sequences (i.e. training samples) is very small compared to the situation where each frame is treated as a training sample. Our second contribution is the use of pairwise pain detection to address this issue. We propose an architecture that takes two frames as input: a reference frame and a target frame. With this method, the number of samples is not only uncompromised, but expanded, because there are more possible pairings than there are individual frames. At the same time, our method captures what is good about using sequences, namely analysing changes in expression from frame to frame.</p><p>Contrastive Training: Contrastive training which was originally developed to train probabilistic and energy-based models <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b38">[38]</ref>, has had a resurgence in deep learning and been shown to improve classification metrics <ref type="bibr" target="#b39">[39]</ref>- <ref type="bibr" target="#b41">[41]</ref>. Our third contribution is the introduction of a contrastive training method in a regression setting, and a fast method for generating out-of-distribution samples for image data. We show that this approach improves within-dataset and cross-dataset performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Organization</head><p>The remainder of this paper is organized as follows. Data used to develop and validate this model is reviewed in Section II. The details of the method and comparison baselines are presented in Section III. The results of evaluations on both cognitively healthy control and participants with dementia are reported and discussed in Section IV. Ethical considerations and further work required to deploy this model in a clinical setting are presented in Section V and Section VI, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. DATA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets Used</head><p>We used two datasets to conduct our experiments, the University of Regina (UofR) Pain in Severe Dementia dataset <ref type="bibr" target="#b9">[10]</ref> and the UNBC-McMaster Shoulder Pain Expression Archive Database <ref type="bibr" target="#b13">[14]</ref>. The publicly available portion of the UNBC-McMaster dataset contains video data from 25 participants (13 females) with a shoulder injury during painful and non-painful movements, recorded at 30 frames per second (fps) and in Quarter VGA (240×320) resolution. The videos are manually annotated with FACS codes so that the PSPI score can be calculated for each video frame. The dataset contains 48,391 image frames in total (1936 ± 837 per participant).</p><p>The UofR dataset contains video data from 102 older adult participants with and without dementia, recorded at 15 fps. In our experiments, UofR videos were transcoded to VGA (640×480) resolution. Each session was filmed with three synchronized cameras recording top, left, and right views of the face. The videos were first recorded during a baseline state when the participant was lying on a bed and then during an examination state in which a licensed physiotherapist assisted the participant to execute a sequence of movements to identify painful areas. Videos of 95 people from the dataset (74 females) were annotated manually by trained annotators according to PSPI and PACSLAC-II pain rating scales. Of these 95 older adults, 47 were community dwelling and cognitively healthy (age: 75.5 ± 6.1), whereas the remaining 48 (age: 82.5 ± 9.2) were individuals with severe dementia residing in LTC. <ref type="figure" target="#fig_1">Figure 2</ref> depicts the distribution of pain levels (PSPI) in each dataset. As the figure illustrates, the distribution is highly skewed, and frames with zero or low PSPI occur significantly more frequently than those with a high PSPI. For instance, the ratio of frames with PSPI &gt; 5 to those with PSPI &lt; 2 is 1.8% for the UNBC-McMaster dataset, and 0.8% and 0.7% for the dementia and healthy control portions of the UofR dataset, respectively. Both datasets used in this paper involved human participants and were collected in accordance with the ethical standards and approval of the respective institutional review boards and with the Helsinki declaration. All participants, or substitute decision makers in cases of severe dementia, provided informed consent. The UofR protocol was approved by the institutional review board at the University of Regina (REB# 2014-132) and at the University Health Network (REB# 15-9342-DE)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Preprocessing</head><p>The same preprocessing steps were applied to the video data from both the UofR and the UNBC datasets. The main aim with preprocessing was to remove variability in the data that was known not to be related to facial actions, thereby limiting the opportunity for models to overfit. As a first step, frames were cropped around the face, using the single shot scale-invariant face detection (S3FD) model <ref type="bibr" target="#b42">[42]</ref>. Frames in which S3FD did not detect the face (e.g. because of partial occlusion, poor lighting, extreme angles) were not further processed.</p><p>A challenge in preprocessing was the high variability in head pose and camera angle across different participants and over the length of each video. This was particularly the case for the UofR dataset, where the recording protocol involved movements such as transitioning from lying down to sitting up. In order to remove this variability, 68 facial landmarks were extracted using the Face Alignment model (FAN) <ref type="bibr" target="#b43">[43]</ref>. The landmarks were then used to create a frontal and upright rendering of the face. This approach also normalized variance due to differences in face geometry. Two methods were experimented to create the frontalized images. The first method was a piecewise affine transform and the second method involved fitting a face-shaped 3D mesh <ref type="bibr" target="#b44">[44]</ref> onto the face and then projecting the image onto this mesh. The second method avoided some of the distortions introduced by the piecewise affine transformation, especially at extreme angles. However, while being computationally more expensive, it did not result in performance improvements. As such, all experimental results reported in this paper are based on using the piecewise affine transform.</p><p>It was observed that the accuracy of the FAN landmarks rapidly declined as the participants' head pose deviated from facing the camera. The misplacement of landmarks resulted in heavily distorted frontal renderings. To address this, a small subset of the frames (1740) were randomly selected and manually annotated with a binary front view vs. non-front view label. This dataset was subsequently used to train a model to assign a frontal score to each frame. This model was then used to discard profile view faces from the UofR dataset. After discarding the frames in which S3FD did not detect the face and the frames which had a low frontal score, the total number of frames used from the UofR dataset was 162,629 (65% of total). Videos were recorded at vastly different lighting conditions and camera settings. In order to remove this source of variability, Contrast Limited Adaptive Histogram Equalization <ref type="bibr" target="#b45">[45]</ref> was applied to each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Proposed Model</head><p>Architecture: Our primary aim was to train a model to predict pain level at each image frame such that estimated PSPI scores correlated strongly with trained human annotations. The model proposed is based on the observation that human annotators: 1) adjust to each person's resting face, and 2) are sensitive to movements <ref type="bibr" target="#b46">[46]</ref>. To integrate this insight into the model without adding too much complexity, a convolutional model was used to take two images from each participant as input: a reference image and a target image. The goal of the training process is for the network to estimate the difference in FACS codes and PSPI intensities between the target and the reference image. To get the predicted output for the target frame, we add the model's predicted difference to the reference frame's ground truth. <ref type="figure" target="#fig_2">Figure 3</ref> depicts the network architecture. The first layer of the network is applied to both reference and target frames independently and the resulting feature maps from each frame are subtracted. The rest of the network follows the typical motif of convolutional network architectures (resembling LeNet <ref type="bibr" target="#b47">[47]</ref>) where a convolution operation is followed by an affine batch-normalization layer and ReLU activation. The output of each convolutional layer goes through a max-pooling layer and a dropout layer. The last convolution layer is followed by two fully connected layers.</p><p>Multi-task Learning: The model was trained in a multi-task manner. That is, all action units and PSPI scores from each dataset were included as separate target variables for the model to estimate. <ref type="table" target="#tab_0">Table I</ref> indicates which annotations were available and used for each dataset. For the UofR dataset PACSLAC-II annotations were also available, and therefore, PACSLAC-II action units corresponding to facial actions were also included as targets. As the model was trained on pairs of images (reference and target images), multi-task learning meant estimating the difference between FACS AUs, PSPI, and PACSLAC-II annotations. Each training sample was created by randomly selecting two frames from the same person. At each epoch a fresh set of pairs was created as a data augmentation strategy. When the model is deployed in a clinical setting, only a rest state reference image of the person can be obtained, e.g. an image capture from each LTC resident at the time of admission. Since the resting image will be in a no pain (PSPI=0) condition, the delta PSPI will always be non-negative. In light of this, the loss function was designed to generate a learning signal only when the target delta PSPI was non-negative. This means for all outputs the target is always bigger than reference during training. At test time, only frames with a PSPI score of zero are chosen as reference frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dementia</head><p>Older Adult Control UNBC-McMaster  <ref type="bibr" target="#b41">[41]</ref> note that there is an additional (unused) degree of freedom in the softmax function of neural network based classifiers. They train that degree of freedom contrastively to distinguish between in-distribution (ID) and out-of-distribution (OOD) samples. Their experiments showed that when a classifier is trained in this manner, it develops desirable properties such as better calibration, robustness to adversarial perturbations, and the ability to detect OOD samples. Inspired by their findings, here, a contrastive training method is developed for regression. We note that the output of a regression neural network model is the dot product between the flattened output (features) of the penultimate layer and the weights of the last fully connected linear layer. The dot product depends on both the magnitudes and cosine of the angle between the two vectors. Here, an additional contrastive loss term is added as follows, constraining the direction (but not the magnitude) of the feature vectors:</p><formula xml:id="formula_0">PSPI FACS AU43 FACS AU4 FACS max(AU9, AU10) FACS max(AU6, AU7) PACSLAC-II P1 PACSLAC-II P2 PACSLAC-II P3 PACSLAC-II P4 PACSLAC-II P5 PACSLAC-II P6 PACSLAC-II P7 PACSLAC-II P8 PACSLAC-II P9 PACSLAC-II P10 PACSLAC-II P11 FACS AU9 FACS AU10 FACS AU6 FACS AU7</formula><formula xml:id="formula_1">Loss contrastive = 1 col col i f (x) · W f c [i] f (x) W f c [i] − f (x) · W f c [i] f (x) W f c [i]<label>(2)</label></formula><p>wherex represents an OOD sample, x represents an ID sample, f represent the network up to (but excluding) the last linear layer, W f c represents the weights of the last fully connected linear layer, and col is the number of columns in W f c . The sum is over the columns of W f c . Minimizing this loss term pushes the cosine of the angle between the feature vectors and the last layer's weights towards zero for OOD samples, and pushes it away from zero for ID samples. Total loss is then the regression loss -mean squared error in this case -plus the scaled contrastive loss:</p><formula xml:id="formula_2">Loss total = Loss regression + c · Loss contrastive<label>(3)</label></formula><p>In <ref type="bibr" target="#b41">[41]</ref>, OOD samples are generated using Stochastic Gradient Langevin Dynamics (SGLD) <ref type="bibr" target="#b48">[48]</ref>. This method of generation requires an inner loop within the training loop for the OOD samples to converge. This slows down training significantly. To avoid this computational cost, we used random distortions instead to generate OOD samples. Images were distorted aggressively such that they no longer resembled an upright face, but did preserve characteristics of natural images such as local structures. The distortions that were used here were flipping and elastic transformation <ref type="bibr" target="#b49">[49]</ref>. This method of OOD sample generation does not fully follow the logic of contrastive divergence employed in energy-based model training. However, we intuit that by starting from ID images and distorting them, we obtain OOD samples that are similar enough to ID samples to provide a useful learning signal. The results are reported with and without contrastive training.</p><p>Augmentation and Hyper-parameters: Experiments were run using several data augmentation techniques. Best results were obtained by combining random crops and horizontal flipping. The model was trained with a constant learning rate and weight decay of 1e − 4 with the Adam optimizer for 70 epochs. Batch sizes were 32 and dropout probability was 0.25 for all dropout layers. The coefficient c for contrastive training was set to 0.05.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline Models</head><p>To establish a baseline, we used three methods. The first baseline was the method proposed by Kaltwang et al. <ref type="bibr" target="#b18">[18]</ref>, in which a Relevance Vector Regression (RVR) model is trained on the discrete cosine transform (DCT) of each frame to estimate the PSPI. Here Support Vector Regression (SVR) was used instead of RVR.</p><p>The second baseline was the openly available OpenFace library; a facial behavior analysis toolkit which provides FACS action unit detection functionality from videos. OpenFace takes a non-deep learning approach proposed by Baltrusaitis et al. <ref type="bibr" target="#b50">[50]</ref>. It uses HOG features and facial landmarks as predictors and trains an SVR model to infer FACS codes. To adapt to individual differences across people, they calibrate to the face of each person by computing a neutral face as the mean of the features across a sequence of frames in the video. The predicted FACS codes from OpenFace were then used to compute the PSPI pain score. OpenFace does not predict AU 43 (eyes closed), but it does predict AU 45 (blink). We used AU 45 instead of AU 43; so there is a small deviation from the true definition of PSPI.</p><p>The third baseline was the model proposed by Pau et al. <ref type="bibr" target="#b19">[19]</ref>. Here, we replicated their method by first fine-tuning a pretrained VGG-Face [51] model with our data, and then training an LSTM with features extracted from FC6 layer of the said VGG model. The reasoning behind this approach is that the LSTM is able to model temporal relationships between video frames, and thereby improve performance over models that estimate pain from a single frame. It is worth pointing out that due to considerable size and computational requirements of VGG16, using this model for real-time analysis of video data is not currently feasible.</p><p>The fourth baseline was a convolutional neural network (CNN) model developed by Ertugrul et al. <ref type="bibr" target="#b52">[52]</ref> as part of an automated facial affect recognition (AFAR) tool. The AFAR model was shown to be effective in predicting FACS probabilities.</p><p>Here the AFAR convolutional model was trained with a minor modification to convert it to a regression model rather than a classification model. The FACS predictions were then used to compute PSPI. We also evaluated a commercially available software (FaceReader, by Noldus Information Technology, Netherlands), but the results are not reported due to poor performance and thus not constituting an appropriate baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Cross Validation</head><p>To perform validation and compare performance, the data was divided into 5 folds with some additional constraints. First, the folds were constructed such that all samples (i.e. video frames) from each participant were either in the training or in the validation set; i.e. leave-k-subjects-out cross validation. Second, the proportion of the dementia and healthy control people were kept the same between training and validation sets. The number of samples across folds varied as different people had different numbers of samples. The same folds were used across all experiments to ensure that the results were comparable.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Binary Classification</head><p>An important use case of this model in a clinical environment is to notify the caregiving staff when painful expressions are detected. The decision to notify the staff can be framed as a classification problem. To derive class predictions, the output of the best performing regression model is thresholded. The decision threshold for pain was selected to be 2 and above. The rationale for this threshold is discussed in Appendix I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluation Metrics</head><p>To evaluate the quality of predictions on the test data, two types of metrics were used. In regression, the performance was quantified via the Pearson correlation between the predictions and the human annotated ground-truth. In binary classification, to distinguish pain vs. no-pain frames, F1 score, average precision, and Area Under the ROC Curve (AUC) were used to quantify the performance. <ref type="table" target="#tab_0">Table III</ref> reports the results of six experiments. The first row reports the per-frame Pearson correlation coefficients between our model's predictions and ground truth annotations. One benefit of the proposed model is that multiple predictions can be made for a target frame by using different reference frames. Those predictions can then be averaged to obtain a more accurate prediction. Note that in all the results reported throughout the paper, the reference frame is always selected to have a PSPI score of 0. The second row in <ref type="table" target="#tab_0">Table III</ref> reports the results when the prediction for each target frame is obtained by averaging 5 predictions from pairing that target frame with 5 different reference frames (of the same person). The third row of <ref type="table" target="#tab_0">Table III</ref> reports the results for the case where the reference and target frame pairs were not constrained to be from the same person during training. Comparing the second and third rows of the table, we see that the model performs significantly better when it is trained with reference and target frame pairs from the same person, as opposed to when the reference frames are from randomly picked people. Note that both rows are reporting test results where the reference and target frames are from the same person. This provides evidence that by making sure that the reference and target frames are from the same person during training, the model learns to use the reference frame to calibrate itself to each individual. Note that all other results reported in the paper are from models that were trained with reference frames from the same person. Moreover, all reported test results are calculated by averaging the predictions obtained using five different reference frames. Finally, <ref type="table" target="#tab_0">Table III</ref>   The gap in performance of the models between the UofR and UNBC-McMaster datasets, highlights the difficulty of the task in unobtrusive and less controlled environments; as the UofR dataset was collected in less controlled conditions, both in terms of lighting and camera positioning. This is in contrast to UNBC-McMaster dataset where people face the camera and are well lit. This gap in performance could also, at least partially, be attributed to the difficulty in distinguishing expressions of pain in older faces in the UofR dataset. The large gap in performance between the dementia and healthy control cohorts of the UofR dataset could be primarily attributed to the difficulty in distinguishing expressions of pain in faces of people with dementia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. RESULTS AND DISCUSSION</head><p>Our proposed model has three PSPI outputs, one for each of UofR dementia, UofR healthy control, and UNBC-McMaster. Each output is only trained on samples from its corresponding dataset. To support the assertion about the difficulty of the UofR dataset, cross-dataset performance was measured. That is, each of the three PSPI outputs were used to get predictions for samples from all three datasets. This is similar to the analysis performed by Othman et al. <ref type="bibr" target="#b53">[53]</ref>, who performed cross-database evaluation of vision-based pain recognition models trained on the BioVid and X-ITE datasets. The difference is that, in our case, a single model is concurrently trained on all three datasets, but each output is only trained with samples from the corresponding dataset. For instance, if a training sample is from the UNBC dataset, only outputs corresponding to UNBC contribute to the loss. These results can be seen in <ref type="table" target="#tab_0">Table IV</ref>. One can see that both the dementia and control outputs perform significantly better on the UNBC-McMaster dataset than their own datasets, while the UNBC-McMaster output performs worse on both the dementia and control datasets. Furthermore, it can be seen that contrastive training not only generally boosts performance <ref type="table" target="#tab_0">(Table III)</ref>, but also helps in reducing the performance variation on each dataset across outputs <ref type="table" target="#tab_0">(Table IV)</ref>  Tables V and VI compare regression and classification results from our proposed model to those of the baseline models. The results are reported for both per-frame predictions and rolling windows. The rolling window results are reported for three different lengths of rolling windows. The rolling windows have more ecological validity than individual frames in that in a clinical setting judgements about pain are made by observing the person for a period of time. For each window, predictions were aggregated by taking the maximum. As shown, in both regression and classification tasks, further improvement can be achieved on the UofR dataset by this aggregation.</p><p>In order to make the comparison to the AFAR model more fair, another experiment was performed where the AFAR model was trained and tested on images where the mean face of each person was subtracted from every image of that person. This is a way of amplifying deviations from a persons resting face. The results of this experiment are reported on the fourth rows of Tables V and VI. We can see an improvement in performance, specifically for the healthy control participants. However, the performance still lags behind our proposed model, especially on participants with dementia.</p><p>The VGG16+LSTM model proposed in <ref type="bibr" target="#b19">[19]</ref> is similar to AFAR (mean-subtracted) and also our model in that it uses more information than a single frame to estimate pain. Particularly, this model uses the previous 9 frames plus the current frame in order to estimate pain for the current frame. However, it performs rather poorly, except for the classification task on the UNBC dataset, as can be seen in Tables V and VI. We attribute this poor performance to over-parameterization which resulted in severe overfitting. We alleviate the overfitting problem by using regularization and early-stopping, however, this did not fully eliminate the problem. It is worth noting that this model has 30× more parameters than our proposed model. Moreover, its computational requirements make it infeasible for real-time pain detection. We were only able to obtain a PCC of 0.48 when we replicated <ref type="bibr" target="#b19">[19]</ref>, which is significantly lower than their reported PCC of 0.78. We believe that this discrepancy is due to the fact that the validation set in this work is balanced following recommendation from <ref type="bibr" target="#b54">[54]</ref>. However, here we are computing PCC from unbalanced test data. We believe balancing the validation set defeats the purpose of cross-validation, because the purpose of cross-validation for us is to estimate the performance in a test scenario, i.e. actual deployment of the algorithm. By balancing the validation set, we would change the data distribution even further away from what is expected in a test scenario. Therefore, performance statistics on a balanced validation set do not provide a good estimate of test performance in real deployment.</p><p>Comparing the two last rows (our models) with the rest (baselines) in Tables V and VI shows that pairwise prediction is superior to single-frame prediction methods. We emphasize that this gain cannot solely have originated from the models themselves, but rather due to pairwise training and inference. We base this inference on the fact that AFAR and our proposed model have about the same number of parameters and a very similar architecture.</p><p>For the Regina dataset, videos from a random subset of 25 participants were selected and annotated by a second rater in an identical procedure to the first raters. This inter-rater agreement (quantified by the PCC between the first and second raters) is reported in the last row to serve as an upper bound for achievable performance. The UNBC-McMaster dataset similarly reports the Pearson correlation between the first and second raters. However, this correlation was not computed for PSPI, which is the pain scale of interest in this work. <ref type="figure" target="#fig_3">Figures 4 and 5</ref> depict classification precision-recall and ROC curves for per-frame and 20-second rolling window predictions. It is evident from these figures that rolling window aggregation results in a significant improvement in average precision. AUC for the dementia cohort also improves with aggregation. There are, however, no significant gains in AUC from window aggregation for the healthy control cohort. Also notable is the boost in average precision for the dementia group from contrastive training.     In order to assess the effect of multi-task learning, the model was trained to only estimate PSPI and not FACS and PACSLAC-II action units. <ref type="table" target="#tab_0">Table VII</ref> reports the results. We notice performance degrades when multi-task training is eliminated. This effect is more prominent for cross-dataset performance. Interestingly, without multi-task training, the model achieves better performance on the training set, which we attribute to overfitting. This suggests multitask training prevents the model from over-fitting the training data, and explains the improved cross-dataset performance.  To get a qualitative feel for how contrastive training influences the network, we plot saliency maps following Selvaraju et al. <ref type="bibr" target="#b55">[55]</ref>. <ref type="figure" target="#fig_5">Figure 6</ref> depicts the saliency maps of the same sample (from the UNBC dataset) with high pain score (PSPI = 12). The high pain frame is shown in the middle column and a reference frame (PSPI = 0) is shown on the right column. Upper and lower left images show the saliency map generated from the model without and with contrastive training, respectively. We can see the area around the mouth is more salient for the model with contrastive training. This comports with expert field knowledge that movements in the mouth region are indicative of pain. <ref type="figure" target="#fig_6">Figure 7</ref> is generated by averaging the saliency maps of the top 500 samples according to their PSPI scores. Again, we can see the mouth area is more salient to the model with contrastive training (right). A Perspective on Contrastive Training: The proposed model has 39 outputs in total. The outputs correspond to individual FACS and PACSLAC-II action units and PSPI for UofR control, UofR dementia, and UNBC. The weights of the last linear layer (W f c ) map a 200-dimensional feature vector to these 39 outputs. These 200-dimensional weight vectors (i.e. columns of W f c ) can be thought of as a basis set of (at most) rank 39 in a 200 dimensional space. The proposed contrastive loss jointly trains both f and W f c (the basis vectors) such that feature vectors computed for OOD samples will lie outside of the column space of W f c , and feature vectors computed for ID samples lie in that column space. In contrast to Grathwohl et al. <ref type="bibr" target="#b41">[41]</ref>, who rely on the formulation of the softmax function to formalize their interpretation, here we show softmax is not necessary to do contrastive training. This disentanglement from a non-linear activation function, allows us to do contrastive training even for the for the middle layers of any neural network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ETHICAL CONSIDERATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Equity and Algorithmic Bias</head><p>Buolamwini and Gebru famously demonstrated that the performance of commercially available facial analysis software was significantly dependent on skin tone and gender <ref type="bibr" target="#b56">[56]</ref>. Specifically, performance was as much as 34 percentage points worse on faces of darker skinned females than it was on faces of lighter skinned males. Our recent work has shown a similar bias in performance when facial analysis models are evaluated on faces of older adults with a physical or cognitive disability. We showed, for instance, that pre-trained facial landmark detection models and pre-trained facial action unit detection models performed significantly worse when evaluated on faces of older adults with dementia vs. cognitively healthy older adults <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b57">[57]</ref>. A similar bias was observed when models were evaluated on faces of individuals with facial palsy <ref type="bibr" target="#b58">[58]</ref>, <ref type="bibr" target="#b59">[59]</ref>, amyotrophic lateral sclerosis (ALS), and post-stroke <ref type="bibr" target="#b60">[60]</ref>. This is not surprising, as the dataset used to train these models consists primarily of younger adults and virtually entirely of healthy individuals. However, the existence of this bias places a major limitation on the application of existing computer vision models in healthcare settings. In the specific case of ambient pain monitoring in LTC homes, equitable access to quality care and pain management demands the same level of model performance regardless of race, gender, and underlying medical condition, especially the particular use case of this research, dementia.</p><p>In this paper we presented the first fully automated computer vision pain detection model to be trained and evaluated on a large corpus of video data from older adults with dementia. In the analysis, we presented results separately for the dementia cohort and older adult controls so differences in performance could be studied. When evaluating per-frame regression results, the gap in performance between the two groups was large, but aggregating over a length of time closed the gap <ref type="table" target="#tab_8">(Table V)</ref>.</p><p>We do not have ethnicity/race breakdown for participants in either of the two datasets used, so sensitivity to race cannot be evaluated and remains the topic of future work. Participants' gender, however, is known and we can evaluate performance separately for male and female participants. The model (with contrastive training, evaluated per-frame, and using 5 reference frames from the same person) obtained a Pearson correlation of 0.48 in the dementia cohort of the UofR dataset <ref type="table" target="#tab_0">(Table III)</ref>. Evaluating performance separately for male and female participants, the model obtained a Pearson correlation of 0.46 on male faces and 0.50 on female faces. While the difference is relatively small, future work should focus on reducing this performance gap. We note that the number of female face image frames in the UofR dataset was more than twice that of male faces; so collecting more data from male participants could be explored as a potential solution for reducing the gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Privacy</head><p>The use of ambient systems to monitor behaviour in LTC brings up immediate concerns about privacy. The overall vision is that the final automated system will be deployed onboard, e.g. using a Google Coral or an NVIDIA Jetson Nano, so image capture and processing is performed onboard and in real-time. This will remove the need for storage and will alleviate privacy concerns related to the viewing of videos by individuals who lack the necessary permissions to view private information. Previous studies have shown that monitoring technologies can be viewed as acceptable by older adults when the benefits outweigh privacy concerns <ref type="bibr" target="#b61">[61]</ref>. In this case, potential loss of privacy could be outweighed by the benefits gained through prompt and effective management of pain among LTC residents.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>In this paper, we (1) developed the first fully automated computer vision model capable of estimating the level of pain based on facial expressions of older adults with dementia. We (2) proposed a new neural network architecture which compares two images from the same person and estimates the difference in the pain level in them. This is based on the observation that human annotators implicitly adjust their judgements to a person's resting facial expression. We also (3) introduced a contrastive training method where the model is trained to discriminate between in-distribution and out-of-distribution samples in addition to its default objective without changes to architecture. This contrastive training can be used with any neural network regardless of its architecture, and our results suggest that it may help with generalization.</p><p>In the experimental evaluation with a large corpus of video data the resulting model outperformed existing baselines by a wide margin. Nevertheless, it is evident that there is still a large gap between human and machine performance on this task. This is specifically true for the UofR dataset, where the video recording setting was less controlled and more in line with what one can expect in an unobtrusive monitoring setting. The development of strong computer vision models in the past decade has been spearheaded by supervised deep learning which relies on the availability of a massive amount of labeled data. Collecting a large amount of clinical data, e.g. from a dementia population, is challenging, time consuming, and expensive due to difficulties in participant recruitment, obtaining informed consent from a vulnerable population with a cognitive disability, and the need for trained clinical staff to assist with data collection. For a task such as FACS-based pain detection, the barrier is even higher due to the great effort and resources required for manual FACS coding.</p><p>Even though the currently available FACS coded datasets (i.e. UNBC-McMaster) come close to classic computer vision datasets (e.g. CIFAR10) in terms of the number of frames, they still lack the variability of object classification datasets. This is because the images are frames from videos, and videos have low frame-to-frame variability and are collected from a small number of participants, e.g. N=25 in the case of the publicly available portion of the UNBC-McMaster dataset. This makes the effective size of these datasets much smaller. This, added to the fact that the datasets are very imbalanced, means there are only a handful of frames with high pain for the models to learn from. It also makes large models prone to overfitting to the specific faces in the dataset. All of this, we believe, makes automatic pain detection a suitable venue for leveraging unsupervised training methods. This is a line that we will pursue in our future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX I RATIONALE FOR SELECTION OF PAIN CRITERION</head><p>The task is to identify a PSPI score that is likely to be able to meaningfully identify pain in the behavioural stream. The UNBC-McMaster database has data that could help identify a criterion empirically. In the complete UNBC-McMaster, approximately 130 people went through range of motion tests to activate both shoulders-one affected by pain, the other unaffected. There were four tests performed twice actively and twice passively, yielding 16 separate experiments in effect. During each test, participants rated the maximum pain using a 10-cm visual analogue scale (VAS), providing a self-report measure of how painful it was. An independent observer also viewed the video of each test, rating how painful it was on a 0-5 scale of pain intensity. Each test was also scored for pain expression intensity using PSPI.</p><p>An empirical criterion for identifying pain on the PSPI scale can be suggested by validating it against one or the other of the self-report or observer-rating measures, so long as there is reason to believe that the PSPI scale is related to them. <ref type="table" target="#tab_0">Table VIII</ref> presents the Pearson correlations between self-report VAS ratings and PSPI scores and also between the observer ratings and PSPI scores on the 16 pain experiments. In every case but one (passive internal rotation retest -VAS correlation), the PSPI scores and the validating measures are significantly correlated. Correlations against the observer ratings are strong and substantially higher than correlations against VAS scores. That raises the problem of what parameter to use to identify pain on those measures. Implicitly, a value of 0 on either measure should, in principle, mean no pain, but natural variability and error need to be taken into account. A conservative approach would be to take that variation into account and specify a value that one could assert with reasonable certainty would imply no pain.</p><p>In the original study, the tests were conducted on the affected and the unaffected side. Challenging the affected side should produce pain at some level at least some of the time and this should be evident in both the self-reports and the observer ratings. Challenging the unaffected side should, in general, not be painful, but there will be variability associated with the test and the individual. In line with this, a conservative approach to establishing what is no pain would be to take the average self-report or observer rating of the tests that should not be painful (unaffected side tests) and establish the 99% confidence interval around it. Ratings outside that interval can be taken to mean that it is likely that sometime is truly in pain and this can establish a criterion by which the ability of the PSPI score to detect pain can be evaluated.</p><p>ROC analyses was run separately for each of the 16 experiments, specifying a VAS of 5 or higher or an observer rating score of 3 or higher to define pain. The results are in <ref type="table" target="#tab_0">Table VIII</ref>. In 11 of 16 cases (active abduction test, active flexion test and retest, active internal rotation retest, active external rotation retest, passive abduction test and retest, passive flexion test and retest, passive internal rotation test and passive external rotation test), the analysis based on VAS scores yielded AUC values greater than 0.5. Of those, four were statistically significantly different from 0.5 and achieved a model quality index greater than 0.5. Suggested PSPI scores across active and passive tests for defining pain (in the 'Crit' column) varied between 1 and 7, with no clear consensual value. In the analyses based on the observer ratings, all AUC values substantially exceeded 0.5 and were significant and all the model quality indices were good. On 7 of 8 active tests, the recommended PSPI criterion for defining pain was 2. For passive tests, there was more variability in the recommended criterion. They average to 2.5. Given the much higher correlations between PSPI scores and observer rating scores and the clearly superior metric properties when using observer ratings as a criterion, the recommendations based on those analyses are preferable. The results suggest a criterion of 2 or 3 for using PSPI scores in developing the algorithm. <ref type="table" target="#tab_0">Table IX</ref> outlines results for a training run where histogram equalization was not used. <ref type="table" target="#tab_16">Table X</ref> shows results from using 3D-mesh for frontalization instead of piecewise affine. In both experiments contrastive training was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX II RESULTS FROM ADDITIONAL EXPERIMENTS</head><p>Many other works (e.g. <ref type="bibr" target="#b17">[17]</ref>, <ref type="bibr" target="#b19">[19]</ref>, <ref type="bibr" target="#b62">[62]</ref>) also report intraclass Correlation Coefficient <ref type="bibr" target="#b63">[63]</ref> as a metric to judge the performance of their algorithm. Here we also report ICC <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b0">1)</ref> in <ref type="table" target="#tab_0">Table XI</ref>    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The areas of the face where the action units pertaining to PSPI are located: AU 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Number of frames at each pain level (PSPI score) in the UofR dataset (separated by cognitive status) and in the UNBC-McMaster dataset. Note that the y-axis is in log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>The proposed model architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Precision-Recall curves. Blue: per-frame prediction w/ contrastive training, Green: 20 second window prediction w/ contrastive training, Orange: 20 second window prediction w/o contrastive training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>ROC curves. Blue: per-frame prediction w/ contrastive training, Green: 20 second window prediction w/ contrastive training, Orange: 20 second window prediction w/o contrastive training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Top: saliency map generated from a model w/o contrastive training. Bottom: saliency map generated for the same sample from a model w/ contrastive training. Sample PSPI is 12.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Mean of the saliency maps of the top 500 highest pain samples. Left: generated from a model w/o contrastive training. Right: generated from a model w/ contrastive training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Check marks indicate that the annotation was available for that dataset and was used as a target during training. There are 39 check marks corresponding to the 39 outputs of the network</figDesc><table><row><cell>Contrastive</cell></row></table><note>Training: Grathwohl et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Number of test / train frames and subjects in each fold broken down by dataset. Note: number of frames is rounded to thousands.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>compares the performance of our model with vs. without contrastive training.</figDesc><table><row><cell>Experiment</cell><cell>Dementia</cell><cell>Older Adult Control</cell><cell>UNBC-McMaster</cell></row><row><cell>Trained with reference frames from the same person</cell><cell>0.38 / 0.45</cell><cell>0.54 / 0.56</cell><cell>0.69 / 0.68</cell></row><row><cell>Trained with reference frames from the same person (mean of 5)</cell><cell>0.42 / 0.48</cell><cell>0.58 / 0.58</cell><cell>0.71 / 0.69</cell></row><row><cell>Trained with reference frames from random persons (mean of 5)</cell><cell>0.29 / 0.32</cell><cell>0.35 / 0.36</cell><cell>0.58 / 0.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III :</head><label>III</label><figDesc>Pearson correlations for per-frame predictions of different experiments; without / with contrastive training. Note: In all of the rows, the test results are obtained by using reference frames from the same person. For rows 2 and 3, the mean of 5 predictions was used, where the 5 predictions were obtained by using 5 different reference frames.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>. This suggests that contrastive training may help the model to generalize better to unseen data.</figDesc><table><row><cell>Output</cell><cell></cell><cell></cell><cell></cell></row><row><cell>head</cell><cell>Dementia</cell><cell>Older Adult Control</cell><cell>UNBC-McMaster</cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dementia</cell><cell>0.42 / 0.48</cell><cell>0.43 / 0.49</cell><cell>0.39 / 0.48</cell></row><row><cell>Older adult control</cell><cell>0.48 / 0.54</cell><cell>0.58 / 0.58</cell><cell>0.51 / 0.57</cell></row><row><cell>UNBC-McMaster</cell><cell>0.68 / 0.69</cell><cell>0.69 / 0.70</cell><cell>0.71 / 0.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Cross-dataset Pearson correlations of per-frame PSPI predictions; without/with contrastive training.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V :</head><label>V</label><figDesc>Regression Pearson correlations of the models and the second rater annotations.</figDesc><table><row><cell>Model</cell><cell cols="2">Dementia</cell><cell></cell><cell></cell><cell cols="3">Older Adult Control</cell><cell></cell><cell cols="3">UNBC-McMaster</cell><cell></cell></row><row><cell></cell><cell cols="4">frame 1sec 5sec 20sec</cell><cell cols="4">frame 1sec 5sec 20sec</cell><cell cols="4">frame 1sec 5sec 20sec</cell></row><row><cell>SVR with DCT features</cell><cell>0.14</cell><cell>0.22</cell><cell>0.32</cell><cell>0.47</cell><cell>0.15</cell><cell>0.23</cell><cell>0.39</cell><cell>0.62</cell><cell>0.48</cell><cell>0.57</cell><cell>0.79</cell><cell>N/A</cell></row><row><cell>OpenFace</cell><cell>0.18</cell><cell>0.21</cell><cell>0.36</cell><cell>0.64</cell><cell>0.23</cell><cell>0.26</cell><cell>0.34</cell><cell>0.59</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell>VGG16+LSTM</cell><cell>0.19</cell><cell>0.24</cell><cell>0.23</cell><cell>0.43</cell><cell>0.23</cell><cell>0.27</cell><cell>0.26</cell><cell>0.60</cell><cell>0.59</cell><cell>0.65</cell><cell>0.85</cell><cell>N/A</cell></row><row><cell>AFAR</cell><cell>0.20</cell><cell>0.28</cell><cell>0.45</cell><cell>0.58</cell><cell>0.25</cell><cell>0.34</cell><cell>0.48</cell><cell>0.67</cell><cell>0.57</cell><cell>0.62</cell><cell>0.80</cell><cell>N/A</cell></row><row><cell>AFAR (mean subtracted)</cell><cell>0.23</cell><cell>0.27</cell><cell>0.35</cell><cell>0.52</cell><cell>0.43</cell><cell>0.45</cell><cell>0.56</cell><cell>0.70</cell><cell>0.59</cell><cell>0.63</cell><cell>0.85</cell><cell>N/A</cell></row><row><cell>Pairwise (ours)</cell><cell>0.30</cell><cell>0.35</cell><cell>0.36</cell><cell>0.64</cell><cell>0.47</cell><cell>0.47</cell><cell>0.54</cell><cell>0.75</cell><cell>0.56</cell><cell>0.62</cell><cell>0.79</cell><cell>N/A</cell></row><row><cell cols="2">Pairwise w/ contrastive training (ours) 0.30</cell><cell>0.38</cell><cell>0.42</cell><cell>0.63</cell><cell>0.48</cell><cell>0.49</cell><cell>0.61</cell><cell>0.75</cell><cell>0.54</cell><cell>0.63</cell><cell>0.82</cell><cell>N/A</cell></row><row><cell>Second rater</cell><cell>0.74</cell><cell>0.77</cell><cell>0.82</cell><cell>0.87</cell><cell>0.90</cell><cell>0.90</cell><cell>0.90</cell><cell>0.90</cell><cell></cell><cell cols="2">Data not available</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE VI :</head><label>VI</label><figDesc>Classification F1 scores of the models and the second rater annotations.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE VII :</head><label>VII</label><figDesc>Pearson correlations for per-frame PSPI predictions; without/with multi-task training. Off-diagonal entries reflect crossdataset performance.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>p&lt;.001, † p&lt;.01, ‡ p&lt;.05, ? indicates a statistically insignificant model from which no criterion can be specified Boldface identifies parameters meeting criteria for statistical significance. VAS = visual analogue scale; AUC = area under the curve; mq = model quality; Crit = ROC suggested criterion. ROC analysis for relationship between pain as defined by VAS criteria and observer ratings and PSPI scores. Optimal PSPI score for establishing cutoffs are identified in the Crit column.</figDesc><table><row><cell>Test</cell><cell cols="2">VAS and PSPI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Observer ratings and PSPI</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Active tests</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Passive tests</cell><cell></cell><cell></cell><cell>Active tests</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Passive tests</cell><cell></cell><cell></cell></row><row><cell></cell><cell>r</cell><cell>AUC</cell><cell>mq</cell><cell>Crit</cell><cell>r</cell><cell>AUC</cell><cell>mq</cell><cell>Crit</cell><cell>r</cell><cell>AUC</cell><cell>mq</cell><cell>Crit</cell><cell>r</cell><cell>AUC</cell><cell>mq</cell><cell>Crit</cell></row><row><cell>Abduction1</cell><cell>.39  *</cell><cell>.65</cell><cell>.31</cell><cell>3</cell><cell>.46  *</cell><cell>.59</cell><cell>.46</cell><cell>1</cell><cell>.70  *</cell><cell>.83  *</cell><cell>.75</cell><cell>2</cell><cell>.71  *</cell><cell>.86  *</cell><cell>.80</cell><cell>3</cell></row><row><cell>Flexion1</cell><cell>.36  *</cell><cell>.86  *</cell><cell>.75</cell><cell>4</cell><cell>.20  ‡</cell><cell>.69</cell><cell>.29</cell><cell>5</cell><cell>.67  *</cell><cell>.91  *</cell><cell>.84</cell><cell>2</cell><cell>.70  *</cell><cell>.87  *</cell><cell>.79</cell><cell>3</cell></row><row><cell>Internal rotation1</cell><cell>.38  *</cell><cell>.23</cell><cell>-.04</cell><cell>?</cell><cell>.37  *</cell><cell>.86  *</cell><cell>.80</cell><cell>5</cell><cell>.67  *</cell><cell>.75  *</cell><cell>.57</cell><cell>3</cell><cell>.70  *</cell><cell>.92  *</cell><cell>.84</cell><cell>2</cell></row><row><cell>External rotation1</cell><cell>.28  †</cell><cell>.18</cell><cell>-.03</cell><cell>?</cell><cell>.32  *</cell><cell>.51</cell><cell>.40</cell><cell>1</cell><cell>.53  *</cell><cell>.77  *</cell><cell>.63</cell><cell>2</cell><cell>.76  *</cell><cell>.91  *</cell><cell>.85</cell><cell>4</cell></row><row><cell>Abduction2</cell><cell>.38  *</cell><cell>.32</cell><cell>.12</cell><cell>?</cell><cell>.35  †</cell><cell>.71  *</cell><cell>.60</cell><cell>2</cell><cell>.67  *</cell><cell>.83  *</cell><cell>.75</cell><cell>2</cell><cell>.68  *</cell><cell>.73  †</cell><cell>.57</cell><cell>2</cell></row><row><cell>Flexion2</cell><cell>.37  *</cell><cell>.68</cell><cell>.14</cell><cell>4</cell><cell>.32  †</cell><cell>.59</cell><cell>.23</cell><cell>2</cell><cell>.65  *</cell><cell>.88  *</cell><cell>.79</cell><cell>2</cell><cell>.65  *</cell><cell>.77  †</cell><cell>.61</cell><cell>1</cell></row><row><cell>Internal rotation2</cell><cell>.28  †</cell><cell>.61</cell><cell>.11</cell><cell>7</cell><cell>.13</cell><cell>.46</cell><cell>.38</cell><cell>5</cell><cell>.75  *</cell><cell>.88  *</cell><cell>.77</cell><cell>2</cell><cell>.58  *</cell><cell>.80  *</cell><cell>.69</cell><cell>3</cell></row><row><cell>External rotation2</cell><cell>.27  †</cell><cell>.92  *</cell><cell>.87</cell><cell>5</cell><cell>.33  *</cell><cell>.49</cell><cell>.30</cell><cell>4</cell><cell>.58  *</cell><cell>.81  *</cell><cell>.62</cell><cell>2</cell><cell>.68  *</cell><cell>.83  *</cell><cell>.70</cell><cell>2</cell></row><row><cell>Average</cell><cell>.34</cell><cell>.55</cell><cell>.28</cell><cell></cell><cell>.31</cell><cell>.67</cell><cell>.51</cell><cell></cell><cell>.65</cell><cell>.83</cell><cell>.72</cell><cell></cell><cell>.68</cell><cell>.84</cell><cell>.73</cell><cell></cell></row><row><cell>*</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE VIII :</head><label>VIII</label><figDesc>Metrics for the self-report VAS ratings and PSPI scores on the left; and observer ratings and PSPI scores on the right.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>.</figDesc><table><row><cell>Output</cell><cell></cell><cell></cell><cell></cell></row><row><cell>head</cell><cell>Dementia</cell><cell>Older Adult Control</cell><cell>UNBC-McMaster</cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dementia</cell><cell>0.48 / 0.48</cell><cell>0.46 / 0.49</cell><cell>0.46 / 0.48</cell></row><row><cell>Older adult control</cell><cell>0.51 / 0.54</cell><cell>0.56 / 0.58</cell><cell>0.55 / 0.57</cell></row><row><cell>UNBC-McMaster</cell><cell>0.66 / 0.69</cell><cell>0.70 / 0.70</cell><cell>0.68 / 0.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE IX :</head><label>IX</label><figDesc>Pearson correlations for per-frame PSPI predictions; without/with histogram equalization. Off-diagonal entries reflect cross-dataset performance.</figDesc><table><row><cell>Output</cell><cell></cell><cell></cell><cell></cell></row><row><cell>head</cell><cell>Dementia</cell><cell>Older Adult Control</cell><cell>UNBC-McMaster</cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dementia</cell><cell>0.36 / 0.48</cell><cell>0.38 / 0.49</cell><cell>0.36 / 0.48</cell></row><row><cell>Older adult control</cell><cell>0.53 / 0.54</cell><cell>0.54 / 0.58</cell><cell>0.53 / 0.57</cell></row><row><cell>UNBC-McMaster</cell><cell>0.67 / 0.69</cell><cell>0.67 / 0.70</cell><cell>0.67 / 0.69</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>TABLE X :</head><label>X</label><figDesc>Pearson correlations for per-frame PSPI predictions; 3D-mesh/piecewise-affine frontalization method. Off-diagonal entries reflect cross-dataset performance.</figDesc><table><row><cell>Output</cell><cell></cell><cell></cell><cell></cell></row><row><cell>head</cell><cell>Dementia</cell><cell>Older Adult Control</cell><cell>UNBC-McMaster</cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dementia</cell><cell>0.34</cell><cell>0.35</cell><cell>0.31</cell></row><row><cell>Older adult control</cell><cell>0.41</cell><cell>0.47</cell><cell>0.40</cell></row><row><cell>UNBC-McMaster</cell><cell>0.53</cell><cell>0.56</cell><cell>0.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>TABLE XI :</head><label>XI</label><figDesc>Intraclass Correlation Coefficient (specifically ICC(3, 1)) for per-frame PSPI predictions of the contrastively trained model . Off-diagonal entries reflect cross-dataset performance.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Core curriculum for professional education in pain: a report of the Task Force on Professional Education of the International Association for the Study of Pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Edmond</forename><surname>Charlton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IASP Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The quality of medical care provided to vulnerable older patients with chronic pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Chodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">H</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carol</forename><forename type="middle">P</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">T</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><forename type="middle">H</forename><surname>Maclean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruce</forename><forename type="middle">A</forename><surname>Ferrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Shekelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">S</forename><surname>Wenger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Geriatrics Society</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="756" to="761" />
			<date type="published" when="2004-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A comparison of pain and its treatment in advanced dementia and cognitively intact patients with hip fracture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Morrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert L</forename><surname>Siu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Pain and Symptom Management</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="240" to="248" />
			<date type="published" when="2000-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pain assessment in elderly adults with dementia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hadjistavropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keela</forename><surname>Herr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prkachin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Craig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Lukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet Neurology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1216" to="1227" />
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Transforming long-term care pain management in north america: The policy-clinical interface</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hadjistavropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">P</forename><surname>Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perry</forename><forename type="middle">G</forename><surname>Fine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keela</forename><surname>Herr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><forename type="middle">A</forename><surname>Palley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharon</forename><surname>Kaasalainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Béland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pain Medicine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="506" to="520" />
			<date type="published" when="2009-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The structure, reliability and validity of pain expression: Evidence from patients with shoulder pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><forename type="middle">E</forename><surname>Prkachin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAIN</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="267" to="274" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Evidence-based development and initial validation of the pain assessment checklist for seniors with limited ability to communicate-II (PACSLAC-II)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hadjistavropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Lints-Martindale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Clinical Journal of Pain</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="816" to="824" />
			<date type="published" when="2014-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Manual for the Facial Action Coding System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ekman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">V</forename><surname>Friesen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Manual for the Facial Action Coding System</title>
		<imprint>
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The facial expression of pain in patients with dementia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegfried</forename><surname>Scharmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uli</forename><surname>Hemmeter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Schepelmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lautenbacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pain</title>
		<imprint>
			<biblScope unit="volume">133</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="228" />
			<date type="published" when="2007-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pain in severe dementia: A comparison of a fine-grained assessment approach to an observational checklist designed for clinical settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hadjistavropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Prkachin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihailidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Pain</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="915" to="925" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatic recognition methods supporting pain assessment: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Al-Hamadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gruss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Picard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-modal signals for analyzing pain responses to thermal and electrical stimuli</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Gruss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mattis</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Harald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayoub</forename><surname>Traue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Al-Hamadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JoVE (Journal of Visualized Experiments)</title>
		<imprint>
			<biblScope unit="page">59057</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">biovid emo db&quot;: A multimodal database for emotion analyses validated by subjective ratings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueyao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayoub</forename><surname>Al-Hamadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><forename type="middle">C</forename><surname>Traue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Gruss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Symposium Series on Computational Intelligence (SSCI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">PAINFUL DATA: The UNBC-McMaster Shoulder Pain Expression Archive Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">M</forename><surname>Prkachin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><forename type="middle">E</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PAINFUL DATA: The UNBC-McMaster Shoulder Pain Expression Archive Database</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Gruss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>Ehleiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwen</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><forename type="middle">C</forename><surname>Traue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Crawcour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayoub</forename><surname>Al-Hamadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriano</forename><forename type="middle">O</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The BioVid heat pain database data for the advancement and systematic validation of an automated pain recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Cybernetics (CYBCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The painful facepain expression recognition using active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed Bilal</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zara</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">M</forename><surname>Prkachin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><forename type="middle">E</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1788" to="1796" />
			<date type="published" when="2009-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Automatic detection of pain intensity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zakia</forename><surname>Hammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on Multimodal interaction -ICMI &apos;12</title>
		<meeting>the 14th ACM international conference on Multimodal interaction -ICMI &apos;12</meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Continuous pain intensity estimation from facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ognjen</forename><surname>Maja Pantic Sebastian Kaltwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rudovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Visual Computing</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="368" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep pain: Exploiting long short-term memory networks for facial expression classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzàlez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nasrollahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Moeslund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Roca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Action unit detection with region adaptation, multi-labeling learning and optimal temporal fusing. In Action Unit Detection with Region Adaptation, Multi-labeling Learning and Optimal Temporal Fusing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farnaz</forename><surname>Abitahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">AU r-CNN: Encoding expert prior knowledge into r-CNN for action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhai</forename><surname>Yong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">355</biblScope>
			<biblScope unit="page" from="35" to="47" />
			<date type="published" when="2019-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cct: A cross-concat and temporal neural network for multi-label action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network regression for continuous pain intensity estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1535" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fusing deep learned and hand-crafted features of appearance, shape, and dynamics for automatic pain estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><surname>Egede</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brais</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fusing Deep Learned and Hand-Crafted Features of Appearance, Shape, and Dynamics for Automatic Pain Estimation</title>
		<imprint>
			<date type="published" when="2017-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep spatiotemporal representation of the face for automatic pain intensity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tavakolian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="350" to="354" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A spatiotemporal convolutional neural network for automatic pain intensity estimation from facial dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Tavakolian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdenour</forename><surname>Hadid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1413" to="1425" />
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multimodal data fusion for person-independent, continuous estimation of pain intensity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Kächele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Thiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Amirian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedhelm</forename><surname>Schwenker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Günther</forename><surname>Palm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Engineering Applications of Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="275" to="285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Twofold-multimodal pain recognition with the x-ite pain database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayoub</forename><surname>Al-Hamadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sascha</forename><surname>Gruss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 8th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="290" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Machine recognition and representation of neonatal facial displays of acute pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheryl</forename><surname>Brahnam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chao-Fa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><forename type="middle">Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melinda</forename><forename type="middle">R</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Slack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="222" />
			<date type="published" when="2006-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neonatal facial pain assessment combining hand-crafted and deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luigi</forename><surname>Celona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Manoni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">New Trends in Image Analysis and Processing -ICIAP 2017</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="197" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-channel neural network for assessing neonatal pain from videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Salekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zamzmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldgof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kasturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Systems, Man and Cybernetics (SMC)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1551" to="1556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Validation of the pain assessment in neonates (PAIN) scale with the neonatal infant pain scale (NIPS)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Hudson-Barr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beverly</forename><surname>Capper-Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sally</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tonya</forename><forename type="middle">Mizell</forename><surname>Palermo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Morbeto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Lombardo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neonatal Network</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="15" to="21" />
			<date type="published" when="2002-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic detection of pain from facial expressions: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teena</forename><surname>Hassan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominik</forename><surname>Seus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Wollenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>Weitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Kunz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lautenbacher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens-Uwe</forename><surname>Garbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ute</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automated pain detection from facial expressions using FACS: A review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanli</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashid</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Wilkie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Automated Pain Detection from Facial Expressions using FACS: A Review</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pain assessment in dementia: Evaluation of a point-of-care technological solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Atee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kreshnik</forename><surname>Hoti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Parsons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffery</forename><forename type="middle">D</forename><surname>Hughes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Alzheimer&apos;s Disease</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2017-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Algorithmic bias in clinical populations-evaluating and improving facial analysis technology in older adults with dementia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asgarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Prkachin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mihailidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hadjistavropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="25527" to="25534" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Products of experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conf. on Artificial Neural Networks</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Products of experts</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1771" to="1800" />
			<date type="published" when="2002-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Supervised contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Supervised Contrastive Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">S 3 fd: Single shot scale-invariant face detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">How far are we from solving the 2d &amp; 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A multiresolution 3d morphable face model and fitting framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Tena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pouria</forename><surname>Mortazavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><forename type="middle">P</forename><surname>Koppen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. SCITEPRESS -Science and and Technology Publications</title>
		<meeting>the 11th Joint Conference on Computer Vision, Imaging and Computer Graphics Theory and Applications. SCITEPRESS -Science and and Technology Publications</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Zuiderveld</surname></persName>
		</author>
		<title level="m">Graphics gems IV. AP Professional</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Emotion recognition: The role of facial movement and the relative importance of upper and lower areas of the face</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">N</forename><surname>Bassili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Personality and Social Psychology</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2049" to="2058" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient langevin dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Best Practices for Convolutional Neural Networks Applied to Visual Document Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="958" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cross-dataset learning and person-specific normalisation for automatic action unit detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tadas</forename><surname>Baltrusaitis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marwa</forename><surname>Mahmoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Robinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">AFAR: A deep learning based tool for automated facial affect recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laszlo</forename><forename type="middle">A</forename><surname>Itir Onal Ertugrul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqiao</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">F</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cross-database evaluation of pain recognition from facial video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Othman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Werner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frerk</forename><surname>Saxen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayoub</forename><surname>Al-Hamadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 11th International Symposium on Image and Signal Processing and Analysis (ISPA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="181" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Facing imbalanced data-recommendations for the use of performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 Humaine Association Conference on Affective Computing and Intelligent Interaction</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="245" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Grad-CAM: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="336" to="359" />
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Gender shades: Intersectional accuracy disparities in commercial gender classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joy</forename><surname>Buolamwini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timnit</forename><surname>Gebru</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Conference on Fairness, Accountability and Transparency</title>
		<editor>Sorelle A. Friedler and Christo Wilson</editor>
		<meeting>the 1st Conference on Fairness, Accountability and Transparency<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-02" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="23" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Limitations and biases in facial landmark detection d an empirical study on older adults with dementia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azin</forename><surname>Asgarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">B</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Erin</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><forename type="middle">M</forename><surname>Prkachin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mihailidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hadjistavropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Taati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Toward an automatic system for computer-aided assessment in facial palsy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><forename type="middle">L</forename><surname>Guarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yana</forename><surname>Yunusova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Taati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">R</forename><surname>Dusseldorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suresh</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joana</forename><surname>Tavares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martinus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Van Veen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tessa</forename><forename type="middle">A</forename><surname>Fortier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Hadlock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jowett</surname></persName>
		</author>
		<idno type="PMID">32053425</idno>
	</analytic>
	<monogr>
		<title level="j">Facial Plastic Surgery &amp; Aesthetic Medicine</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="49" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Automatic facial landmark localization in clinical populations -improving model performance with a small dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Guarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Taati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tessa</forename><surname>Hadlock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yana</forename><surname>Yunusova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Facial Landmark Localization in Clinical Populations -Improving Model Performance with a Small Dataset</title>
		<imprint>
			<biblScope unit="volume">03</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A new dataset for facial motion analysis in individuals with neurological disorders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bandini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rezaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Guarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Boulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zinman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yunusova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The acceptability of home monitoring technology among community-dwelling older adults and baby boomers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mihailidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Cockburn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Longley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Boger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Assistive Technology</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2008-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A framework for automated measurement of the intensity of non-posed facial action units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cadavid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Messinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="74" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Intraclass correlations: uses in assessing rater reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Shrout</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fleiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="420" to="428" />
			<date type="published" when="1979-03" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
