<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Published as a conference paper at ICLR 2020 DEFORMABLE KERNELS: ADAPTING EFFECTIVE RE- CEPTIVE FIELDS FOR OBJECT DEFORMATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Gao</surname></persName>
							<email>hangg@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">UC Berkeley</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Lin</surname></persName>
							<email>stevelin@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
							<email>jifdai@microsoft.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Published as a conference paper at ICLR 2020 DEFORMABLE KERNELS: ADAPTING EFFECTIVE RE- CEPTIVE FIELDS FOR OBJECT DEFORMATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional networks are not aware of an object's geometric variations, which leads to inefficient utilization of model and data capacity. To overcome this issue, recent works on deformation modeling seek to spatially reconfigure the data towards a common arrangement such that semantic recognition suffers less from deformation. This is typically done by augmenting static operators with learned free-form sampling grids in the image space, dynamically tuned to the data and task for adapting the receptive field. Yet adapting the receptive field does not quite reach the actual goal -what really matters to the network is the effective receptive field (ERF), which reflects how much each pixel contributes. It is thus natural to design other approaches to adapt the ERF directly during runtime. In this work, we instantiate one possible solution as Deformable Kernels (DKs), a family of novel and generic convolutional operators for handling object deformations by directly adapting the ERF while leaving the receptive field untouched. At the heart of our method is the ability to resample the original kernel space towards recovering the deformation of objects. This approach is justified with theoretical insights that the ERF is strictly determined by data sampling locations and kernel values. We implement DKs as generic drop-in replacements of rigid kernels and conduct a series of empirical studies whose results conform with our theories. Over several tasks and standard base models, our approach compares favorably against prior works that adapt during runtime. In addition, further experiments suggest a working mechanism orthogonal and complementary to previous works.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>The rich diversity of object appearance in images arises from variations in object semantics and deformation. Semantics describe the high-level abstraction of what we perceive, and deformation defines the geometric transformation tied to specific data <ref type="bibr" target="#b5">(Gibson, 1950)</ref>. Humans are remarkably adept at making abstractions of the world <ref type="bibr" target="#b10">(Hudson &amp; Manning, 2019)</ref>; we see in raw visual signals, abstract semantics away from deformation, and form concepts.</p><p>Interestingly, modern convolutional networks follow an analogous process by making abstractions through local connectivity and weight sharing <ref type="bibr" target="#b35">(Zhang, 2019)</ref>. However, such a mechanism is an inefficient one, as the emergent representations encode semantics and deformation together, instead of as disjoint notions. Though a convolution responds accordingly to each input, how it responds is primarily programmed by its rigid kernels, as in <ref type="figure">Figure 1(a, b)</ref>. In effect, this consumes large model capacity and data modes .</p><p>We argue that the awareness of deformations emerges from adaptivity -the ability to adapt at runtime <ref type="bibr" target="#b13">(Kanazawa et al., 2016;</ref><ref type="bibr" target="#b12">Jia et al., 2016;</ref><ref type="bibr" target="#b15">Li et al., 2019)</ref>. Modeling of geometric transformations has been a constant pursuit for vision researchers over decades <ref type="bibr" target="#b19">(Lowe et al., 1999;</ref><ref type="bibr" target="#b14">Lazebnik et al., 2006;</ref><ref type="bibr" target="#b11">Jaderberg et al., 2015;</ref><ref type="bibr" target="#b2">Dai et al., 2017)</ref>. A basic idea is to spatially recompose data towards a common mode such that semantic recognition suffers less from deformation. A recent work that <ref type="figure">Figure 1</ref>: Adaptation for deformation. We show how different 3 × 3 convolutions interact with deformations of two images. Kernel spaces are visualized as flat 2D Gaussians. Each "+" indicates a computation between a pixel and a kernel value sampled from the data and kernel space. Their colors represent corresponding kernel values. (a, b) Rigid kernels cannot adapt to specific deformations, thus consuming large model and data capacity. (c) Deformable Convolutions <ref type="bibr" target="#b2">(Dai et al., 2017)</ref> reconfigure data towards common arrangement to counter the effects of geometric deformation. (d) Our Deformable Kernels (DKs) instead resample kernels and, in effect, adapt kernel spaces while leaving the data untouched. Note that (b) and (c) share kernel values but sample different data locations, while (b) and (d) share data locations but sample different kernel values.</p><p>is representative of this direction is Deformable Convolution <ref type="bibr" target="#b2">(Dai et al., 2017;</ref><ref type="bibr" target="#b36">Zhu et al., 2019)</ref>. As shown in <ref type="figure">Figure 1(c)</ref>, it augments the convolutions with free-form sampling grids in the data space. It is previously justified as adapting receptive field, or what we phrase as the "theoretical receptive field", that defines which input pixels can contribute to the final output. However, theoretical receptive field does not measure how much impact an input pixel actually has. On the other hand, <ref type="bibr" target="#b20">Luo et al. (2016)</ref> propose to measure the effective receptive field (ERF), i.e. the partial derivative of the output with respect to the input data, to quantify the exact contribution of each raw pixel to the convolution. Since adapting the theoretical receptive field is not the goal but a means to adapt the ERF, why not directly tune the ERF to specific data and tasks at runtime? Toward this end, we introduce Deformable Kernels (DKs), a family of novel and generic convolutional operators for deformation modeling. We aim to augment rigid kernels with the expressiveness to directly interact with the ERF of the computation during inference. Illustrated in <ref type="figure">Figure 1(d)</ref>, DKs learn free-form offsets on kernel coordinates to deform the original kernel space towards specific data modality, rather than recomposing data. This can directly adapt ERF while leaving receptive field untouched. The design of DKs that is agnostic to data coordinates naturally leads to two variants -the global DK and the local DK, which behave differently in practice as we later investigate. We justify our approach with theoretical results which show that ERF is strictly determined by data sampling locations and kernel values. Used as a generic drop-in replacement of rigid kernels, DKs achieve empirical results coherent with our developed theory. Concretely, we evaluate our operator with standard base models on image classification and object detection. DKs perform favorably against prior works that adapt during runtime. With both quantitative and qualitative analysis, we further show that DKs can work orthogonally and complementarily with previous techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>We distinguish our work within the context of deformation modeling as our goal, and dynamic inference as our means.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deformation Modeling:</head><p>We refer to deformation modeling as learning geometric transformations in 2D image space without regard to 3D. One angle to attack deformation modeling is to craft certain geometric invariances into networks. However, this usually requires designs specific to certain kinds of deformation, such as shift, rotation, reflection and scaling <ref type="bibr" target="#b26">(Sifre &amp; Mallat, 2013;</ref><ref type="bibr" target="#b0">Bruna &amp; Mallat, 2013;</ref><ref type="bibr" target="#b13">Kanazawa et al., 2016;</ref><ref type="bibr" target="#b1">Cohen &amp; Welling, 2016;</ref><ref type="bibr" target="#b31">Worrall et al., 2017;</ref><ref type="bibr" target="#b4">Esteves et al., 2018)</ref>. Another line of work on this topic learns to recompose data by either semi-parameterized or completely free-form sampling in image space: Spatial Transformers <ref type="bibr" target="#b11">(Jaderberg et al., 2015)</ref> learns 2D affine transformations, Deep Geometric Matchers <ref type="bibr" target="#b23">(Rocco et al., 2017)</ref> learns thin-plate spline transformations, Deformable Convolutions <ref type="bibr" target="#b2">(Dai et al., 2017;</ref><ref type="bibr" target="#b36">Zhu et al., 2019)</ref> learns free-form transformations.</p><p>We interpret sampling data space as an effective approach to adapt effective receptive fields (ERF) by directly changing receptive field. At a high-level, our Deformable Kernels (DKs) share intuitions with this line of works for learning geometric transformations, yet are instantiated by learning to sample in kernel space which directly adapt ERF while leaving theoretical receptive fields untouched. While kernel space sampling is also studied in Deformable Filter <ref type="bibr" target="#b33">(Xiong et al., 2019)</ref> and KPConv <ref type="bibr" target="#b27">(Thomas et al., 2019)</ref>, but in their contexts, sampling grids are computed from input point clouds rather than learned from data corpora.</p><p>Dynamic Inference: Dynamic inference adapts the model or individual operators to the observed data. The computation of our approach differs from self-attention <ref type="bibr" target="#b28">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b30">Wang et al., 2018)</ref> in which linear or convolution modules are augmented with subsequent queries that extract from the same input. We consider our closest related works in terms of implementation as those approaches that adapt convolutional kernels at run time. It includes but is not limited to Dynamic Filters <ref type="bibr" target="#b12">(Jia et al., 2016)</ref>, Selective Kernels <ref type="bibr" target="#b15">(Li et al., 2019)</ref> and Conditional Convolutions . All of these approaches can learn and infer customized kernel spaces with respect to the data, but are either less inefficient or are loosely formulated. Dynamic Filters generate new filters from scratch, while Conditional Convolutions extend this idea to linear combinations of a set of synthesized filters. Selective Kernels are, on the other hand, comparably lightweight, but aggregating activations from kernels of different size is not as compact as directly sampling the original kernel space. Another line of works contemporary to ours  is to compose free-form filters with structured Gaussian filters, which essentially transforms kernel spaces by data. Our DKs also differ from these works with the emphasize of direct adaptation the ERF rather than the theoretical receptive field. As mentioned previously, the true goal should be to adapt the ERF, and to our knowledge, our work is the first to study dynamic inference of ERFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">APPROACH</head><p>We start by covering preliminaries on convolutions, including the definition of effective receptive field (ERF). We then formulate a theoretical framework for analyzing ERFs, and thus motivate our Deformable Kernels (DKs). We finally elaborate different DK variants within such a framework. Our analysis suggests compatibility between DKs and the prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">A DIVE INTO CONVOLUTIONS</head><p>2D Convolution: Let us first consider an input image I ∈ R D×D . By convolving it with a kernel W ∈ R K×K of stride 1, we have an output image O whose pixel values at each coordinate j ∈ R 2 can be expressed as</p><formula xml:id="formula_0">O j = k∈K I j+k W k ,<label>(1)</label></formula><p>by enumerating discrete kernel positions k within the support K = [−K/2, K/2] 2 ∩ Z. This defines a rigid grid for sampling data and kernels.</p><p>Theoretical Receptive Field: The same kernel W can be stacked repeatedly to form a linear convolutional network with n layers. The theoretical receptive field can then be imagined as the "accumulative coverage" of kernels at each given output unit on the input image by deconvolving back through the network. This property characterizes a set of input fields that could fire percepts onto corresponding output pixels. The size of a theoretical receptive field scales linearly with respect to the network depth n and kernel size K <ref type="bibr" target="#b7">(He et al., 2016)</ref>.</p><p>Effective Receptive Field: Intuitively, not all pixels within a theoretical receptive field contribute equally. The influence of different fields varies from region to region thanks to the central emphasis of stacked convolutions and also to the non-linearity induced by activations. The notion of effective receptive field (ERF) <ref type="bibr" target="#b20">(Luo et al., 2016)</ref> is thus introduced to measure the impact of each input pixel on the output at given locations. It is defined as a partial derivative field of the output with respect to the input data. With the numerical approximations in linear convolution networks, the ERF was previously identified as a Gaussian-like soft attention map over input images whose size grows fractionally with respect to the network depth n and linearly to the kernel size K. Empirical results validate this idea under more complex and realistic cases when networks exploit non-linearities, striding, padding, skip connections, and subsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">ANALYSIS ON EFFECTIVE RECEPTIVE FIELDS</head><p>We aim to revisit and complement the previous analysis on ERFs by <ref type="bibr" target="#b20">Luo et al. (2016)</ref>. While the previous analysis concentrates on studying the expectation of an ERF, i.e., when network depth n approaches infinity or all kernels are randomly distributed without learning in general, our analysis focuses on how we can perturb the computation such that the change in ERF is predictable, given an input and a set of kernel spaces.</p><p>We start our analysis by considering a linear convolutional network, without any unit activations, as defined in Section 3.1. For consistency, superscripts are introduced to image I, kernel W , and subscripts to kernel positions k to denote the index s ∈ [1, n] of each layer. Formally, given an input image I (0) and a set of K × K kernels {W (s) } n s=1 of stride 1, we can roll out the final output O ≡ I (n) by unfolding Equation 1 as</p><formula xml:id="formula_1">I (n) j = kn∈K I (n−1) j+kn W (n) kn = (k n−1 ,kn)∈K 2 I (n−2) j+kn+k n−1 W (n) kn W (n−1) k n−1 = · · · = (k1,k2,...,kn)∈K n I (0) j+ n s=1 ks · n s=1 W (s) ks .</formula><p>(2) By definition 1 , the effective receptive field value R (n) (i; j) ≡ ∂I (n) j /∂I (0) i of output coordinate j that takes input coordinate i can be computed by</p><formula xml:id="formula_2">R (n) (i; j) = (k1,k2,...,kn)∈K n 1 j + n s=1 ks = i · n s=1 W (s) ks ,<label>(3)</label></formula><p>where 1[·] denotes the indicator function. This result indicates that ERF is related only to the data sampling location j, kernel sampling location k, and kernel matrices {W (s) }.</p><p>If we replace the m th kernel W (m) with a 1 × 1 kernel of a single parameter W (m) km sampled from it, the value of ERF becomes to R (n) (i; j,km) = (k1,...,km−1,km+1,...,kn)∈K n−1</p><formula xml:id="formula_3">1 j + s∈S ks = i · s∈S W (s) ks · W (m) km ,<label>(4)</label></formula><p>where S = [1, n] \ {m}. Since a K × K kernel can be deemed as a composition of K 2 1 × 1 kernels distributed on a square grid, Equation 3 can thus be reformulated as</p><formula xml:id="formula_4">R (n) (i; j) = km∈K R (n) (i; j + km, km).<label>(5)</label></formula><p>For the case of complex non-linearities, where we here consider post ReLU 2 activations in Equation 1,</p><formula xml:id="formula_5">O j = max( k∈K I j+k W k , 0).<label>(6)</label></formula><p>1 The original definition of ERF in <ref type="bibr" target="#b20">Luo et al. (2016)</ref> focuses on the central coordinate of the output, i.e. j = (0, 0), to partially avoid the effects of zero padding. In this work, we will keep j in favor of generality while explicitly assuming input size D → ∞.</p><p>2 Our analysis currently only considers the ReLU network for its nice properties and prevalent popularity.</p><p>We can follow a similar analysis and derive corresponding ERF as</p><formula xml:id="formula_6">R (n) (i; j,km) = (k 1 ,··· ,k m−1 ,k m+1 ,··· ,kn)∈K n−1 C (n) (i; j, k1, · · · , kn,km) · s∈S W (s) ks · W m km where C (n) (i; j, k1, · · · , kn,km) = 1 j + s∈S ks = i s∈S 1 I (s−1) j W (s) ks &gt; 0 1 I (m−1) j W (m) km &gt; 0 .</formula><p>Here we can see that the ERF becomes data-dependent due to the coefficient C, which is tied to input coordinates, kernel sampling locations, and input data I (0) . The more detailed analysis of this coefficient is beyond the scope of this paper. However, it should be noted that this coefficient only "gates" the contribution of the input pixels to the output. So in practice, ERF is "porous" -there are inactive (or gated) pixel units irregularly distributed around the ones that fire. This phenomenon also appeared in previous studies (such as in <ref type="bibr" target="#b20">Luo et al. (2016)</ref>, <ref type="figure">Figure 1</ref>). The maximal size of an ERF is still controlled by the data sampling location and kernel values as in the linear cases in Equation 5.</p><p>A nice property of Equation 4 and Equation 5 is that all computations are linear, making it compatible with any linear sampling operators for querying kernel values of fractional coordinates. In other words, sampling kernels in effect samples the ERF on the data in the linear case, but also roughly generalizes to non-linear cases as well. This finding motivates our design of Deformable Kernels (DKs) in Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">DEFORMABLE KERNELS</head><p>In the context of Equation 1, we resample the kernel W with a group of learned kernel offsets denoted as {∆k} that correspond to each discrete kernel position k. This defines our DK as</p><formula xml:id="formula_7">O j = k∈K I j+k W k+∆k ,<label>(7)</label></formula><p>and the value of ERF as</p><formula xml:id="formula_8">R (n) DK (i; j) = km∈K R (n) (i; j + km, km + ∆km).<label>(8)</label></formula><p>Note that this operation leads to sub-pixel sampling in the kernel space. In practice, we use bilinear sampling to interpolate within the discrete kernel grid.</p><p>Intuitively, the size (resolution) of the original kernel space can affect sampling performance. Concretely, suppose we want to sample a 3 × 3 kernel. DKs do not have any constraint on the size of the original kernel space, which we call the "scope size" of DKs. That said, we can use a W of any size K even though the number of sampling locations is fixed as K 2 . We can thus exploit large kernels -the largest ones can reach 9×9 in our experiments with nearly no overhead in computation since bilinear interpolations are extremely lightweight compared to the cost of convolutions. This can also increase the number of learning parameters, which in practice might become intractable if not handled properly. In our implementation, we will exploit depthwise convolutions <ref type="bibr" target="#b8">(Howard et al., 2017)</ref> such that increasing scope size induces a negligible amount of extra parameters.</p><p>As previously discussed, sampling the kernel space in effect transforms into sampling the ERF. On the design of locality and spatial granularity of our learned offsets, DK naturally delivers two variants -the global DK and the local DKs, as illustrated in <ref type="figure" target="#fig_3">Figure 2</ref>. In both operators, we learn a kernel offset generator G that maps an input patch into a set of kernel offsets that are later applied to rigid kernels.</p><p>In practice, we implement G global as a stack of one global average pooling layer, which reduces feature maps into a vector, and another fully-connected layer without non-linearities, which projects the reduced vector into an offset vector of 2K 2 dimensions. Then, we apply these offsets to all convolutions for the input image following Equation 7. For local DKs, we implement G local as an extra convolution that has the same configuration as the target kernel, except that it only has 2K 2 output channels. This produces kernel sampling offsets {∆k} that are additionally indexed by output locations j. It should be noted that similar designs were also discussed in <ref type="bibr" target="#b12">Jia et al. (2016)</ref>, in which filters are generated given either an image or individual patches from scratch rather than by resampling.    Intuitively, we expect the global DK to adapt kernel space between different images but not within a single input. The local DK can further adpat to specific image patches: for smaller objects, it is better to have shaper kernels and thus denser ERF; for larger objects, flatter kernels can be more beneficial for accumulating a wider ERF. On a high level, local DKs can preserve better locality and have larger freedom to adapt kernel spaces comparing to its global counterpart. We later compare these operators in our experiments.</p><formula xml:id="formula_9">= " &gt; A A A B + H i c b V D L S g N B E O y N r x h f U Y 9 e F o M g H s J u F P Q Y 9 O I x A f O A Z A m z k 9 5 k y M z s M j M r x J A v 8 K p 3 b + L V v / H q l z h J 9 q C J B Q 1 F V T f V V J h w p o 3 n f T m 5 t f W N z a 3 8 d m F n d 2 / / o H h 4 1 N R x q i g 2 a M x j 1 Q 6 J R s 4 k N g w z H N u J Q i J C j q 1 w d D f z W 4 + o N I v l g x k n G A g y k C x i l B g r 1 S 9 6 x Z J X 9 u Z w V 4 m f k R J k q P W K 3 9 1 + T F O B 0 l B O t O 7 4 X m K C C V G G U Y 7 T Q j f V m B A 6 I g P s W C q J Q B 1 M 5 o 9 O 3 T O r 9 N 0 o V n a k c e f q 7 4 s J E V q P R W g 3 B T F D v e z N x H + 9 U C w l m + g m m D C Z p A Y l X Q R H K X d N 7 M 5 a c P t M I T V 8 b A m h i t n f X T o k i l B j u y r Y U v z l C l Z J s 1 L 2 L 8 u V + l W p e p v V k 4 c T O I V z 8 O E a q n A P N W g A B Y R</formula><formula xml:id="formula_10">N Z M v h p v S V k b D o V Q C s / C Y 5 4 = " &gt; A A A B + H i c d V D L S g M x F M 3 U V 6 2 v q k s 3 w S K I i 2 E 6 a l t 3 R T c u W 7 A P a I e S S T N t a J I Z k o x Q h 3 6 B W 9 2 7 E 7 f + j V u / x E w 7 g h Y 9 c O F w z r 3 c e 4 8 f M a q 0 4 3 x Y u Z X V t f W N / G Z h a 3 t n d 6 + 4 f 9 B W Y S w x a e G Q h b L r I 0 U Y F a S l q W a k G 0 m C u M 9 I x 5 / c p H 7 n n k h F Q 3 G n p x H x O B o J G l C M t J G a Z 4 N i y b G v K p V a t Q o d 2 5 k j J W 7 F v X R h O V N K I E N j U P z s D 0 M c c y I 0 Z k i p X t m J t J c g q S l m Z F b o x 4 p E C E / Q i P Q M F Y g T 5 S X z Q 2 f w x C h D G I T S l N B w r v 6 c S B B X</formula><formula xml:id="formula_11">v Z h T j l M = " &gt; A A A C E 3 i c b V D L S s N A F J 3 U V 6 2 v q D v d B I v g q i R V 0 G X R h S 4 r 2 A c 0 I U y m 0 3 b o J B N m b s Q S A v 6 E v + B W 9 + 7 E r R / g 1 i 9 x 0 m a h r Q c G z j 3 n X u 6 d E 8 S c K b D t L 6 O 0 t L y y u l Z e r 2 x s b m 3 v m L t 7 b S U S S W i L C C 5 k N 8 C K c h b R F j D g t B t L i s O A 0 0 4 w v s r 9 z j 2 V i o n o D i Y x 9 U I 8 j N i A E Q x a 8 s 0 D N 8 Q w I p i n 1 3 7 q A n 2 A l A t d Z l n m m 1 W 7 Z k 9 h L R K n I F V U o O m b 3 2 5 f k C S k E R C O l e o 5 d g x e i i U w w m l W c R N F Y 0 z G e E h 7 m k Y 4 p M p L p 3 / I r G O t 9 K 2 B k P p F Y E 3 V 3 x M p D p W a h I H u z C 9 W 8 1 4 u / u s F 4 d x m G F x 4 K Y v i B G h E Z o s H C b d A W H l A V p 9 J S o B P N M F E M n 2 7 R U Z Y Y g I 6 x o o O x Z m P Y J G 0 6 z X n t F a / P</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">LINK WITH DEFORMABLE CONVOLUTIONS</head><p>The core idea of DKs is to learn adaptive offsets to sample the kernel space for modeling deformation, which makes them similar to Deformable Convolutions <ref type="bibr" target="#b2">(Dai et al., 2017;</ref><ref type="bibr" target="#b36">Zhu et al., 2019)</ref>, at both the conceptual and implementation levels. Here, we distinguish DKs from Deformable Convolutions and show how they can be unified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deformable Convolutions can be reformulated in a general form as</head><formula xml:id="formula_12">O j = k∈K I j+k+∆j W k ,<label>(9)</label></formula><p>where they aim to learn a group of data offsets {∆j} with respect to discrete data positions j. For consistency for analysis, the value of effective receptive field becomes</p><formula xml:id="formula_13">R (n) DC (i; j) = km∈K R (n) (i; j + km + ∆jm, km).<label>(10)</label></formula><p>This approach essentially recomposes the input image towards common modes such that semantic recognition suffers less from deformation. Moreover, according to our previous analysis in Equation 5, sampling data is another way of sampling the ERF. This, to a certain extent, also explains why Deformable Convolutions are well suited for learning deformation-agnostic representations. </p><p>We also investigate this operator in our experiments. Although the two techniques may be viewed as serving a similar purpose, we find the collaboration between Deformable Kernels and Deformable Convolutions to be powerful in practice, suggesting strong compatibility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We evaluate our Deformable Kernels (DKs) on image classification using ILSVRC and object detection using the COCO benchmark. Necessary details are provided to reproduce our results, together with descriptions on base models and strong baselines for all experiments and ablations. For taskspecific considerations, we refer to each corresponding section.</p><p>Implementation Details: We implement our operators in PyTorch and CUDA. We exploit depthwise convolutions when designing our operator for better computational efficiency 3 . We initialize kernel grids to be uniformly distributed within the scope size. For the kernel offset generator, we set its learning rate to be a fraction of that of the main network, which we cross-validate for each base model. We also find it important to clip sampling locations inside the original kernel space, such that k + ∆k ∈ K in Equation 7.</p><p>Base Models: We choose our base models to be ResNet-50 <ref type="bibr" target="#b7">(He et al., 2016)</ref> and MobileNet-V2 <ref type="bibr" target="#b24">(Sandler et al., 2018)</ref>, following the standard practice for most vision applications. As mentioned, we exploit depthwise convolution and thus make changes to the ResNet model. Concretely, we define our ResNet-50-DW base model by replacing all 3 × 3 convolutions by its depthwise counterpart while doubling the dimension of intermediate channels in all residual blocks. We find it to be a reasonable base model compared to the original ResNet-50, with comparable performance on both tasks. During training, we set the weight decay to be 4 × 10 −5 rather than the common 10 −4 for both models since depthwise models usually underfit rather than overfit <ref type="bibr" target="#b32">(Xie et al., 2017;</ref><ref type="bibr" target="#b8">Howard et al., 2017;</ref><ref type="bibr" target="#b9">Hu et al., 2018)</ref>. We set the learning rate multiplier of DK operators as 10 −2 for ResNet-50-DW and 10 −1 for MobileNet-V2 in all of our experiments.</p><p>Strong Baselines: We develop our comparison with two previous works: Conditional Convolutions  for dynamics inference, and Deformable Convolutions <ref type="bibr" target="#b2">(Dai et al., 2017;</ref><ref type="bibr" target="#b36">Zhu et al., 2019)</ref> for deformation modeling. We choose Conditional Convolutions due to similar computation forms -sampling can be deemed as an elementewise "expert voting" mechanism. For fair comparisons, We reimplement and reproduce their results. We also combine our operator with these previous approach to show both quantitative evidence and qualitative insight that our working mechanisms are compatible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">IMAGE CLASSIFICATION</head><p>We first train our networks on the ImageNet 2012 training set <ref type="bibr" target="#b3">(Deng et al., 2009</ref>). We adopt a common experiment protocol for fair comparisons as in <ref type="bibr" target="#b6">Goyal et al. (2017)</ref>; <ref type="bibr" target="#b18">Loshchilov &amp; Hutter (2017)</ref>. For more details, please refer to our supplement.</p><p>We first ablate the scope size of kernels for our DKs and study how it can affect model performance using ResNet-50-DW. As shown in <ref type="table" target="#tab_2">Table 1</ref>, our DKs are sensitive to the choice of the scope size. We shown that when only applied to 3 × 3 convolutions inside residual bottlenecks, local DKs induce a +0.7 performance gain within the original scope. By further enlarging the scope size, performance increases yet quickly plateaus at scope 4 × 4, yielding largest +1.4 gain for top-1 accuracy. Our speculation is that, although increasing scope size theoretically means better interpolation, it also makes the optimization space exponentially larger for each convolutional layer. And since number of entries for updating is fixed, this also leads to relatively sparse gradient flows. In principle, we set default scope size at 4 × 4 for our DKs.</p><p>We next move on and ablate our designs by comparing the global DK with the local DK, shown in the table. Both operators helps while the local variants consistently performs better than their global counterparts, bringing a +0.5 gap on both base models. We also study the effect of using more DKs in the models -the 1 × 1 convolutions are replaced by global DKs 4 with scope 2 × 2. Note that all 1 × 1 convolutions are not depthwise, and therefore this operation induces nearly 4 times of parameters. We refer their results only for ablation and show that adding more DKs still helpsespecially for MobileNet-V2 since it is under-parameterized. This finding also holds for previous models  as well.    <ref type="table" target="#tab_3">Table 2</ref>. We can see that DKs perform comparably on ResNet-V2 and compare favorably on MobileNet-V2 -improve +0.9 from Deformable Convolutions and achieve comparable results with less than a quarter number of parameters compared to Conditional Convolutions. Remarkably, we also show that if combined together, even larger performance gains are in reach. We see consistent boost in top-1 accuracy compared to strong baselines: +1.3/+1.0 on ResNet-50-DW, and +1.2/+1.2 on MobileNet-V2. These gaps are bigger than those from our own ablation, suggesting the working mechanisms across the operators to be orthogonal and compatible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">OBJECT DETECTION</head><p>We examine DKs on the COCO benchmark <ref type="bibr" target="#b16">(Lin et al., 2014)</ref>. For all experiments, we use Faster R- <ref type="bibr">CNN (Ren et al., 2015)</ref> with FPN <ref type="bibr" target="#b17">(Lin et al., 2017)</ref> as the base detector, plugging in the backbones we previously trained on ImageNet. For MobileNet-V2, we last feature maps of the each resolution for FPN post aggregation. Following the standard protocol, training and evaluation are performed on the 120k images in the train-val split and the 20k images in the test-dev split, respectively. For evaluation, we measure the standard mean average precision (mAP) and shattered scores for small, medium and large objects.   For the comparisons with strong baselines, an interesting phenomenon worth noting is that though DKs perform better than Deformable Convolutions on image classification, they fall noticeably short for object detection measured by mAP. We speculate that even though both techniques can adapt ERF in theory (as justified in Section 3.2), directly shifting sampling locations on data is easier to optimize. Yet after combining DKs with previous approaches, we can consistently boost performance for all the methods -+0.7/+1.2 for Deformable Convolutions on each base models, and +1.7/+1.1 for Conditional Convolutions. These findings align with the results from image classification. We next investigate what DKs learn and why they are compatible with previous methods in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">WHAT DO DEFORMABLE KERNELS LEARN?</head><p>Awareness of Object Scale: Since deformation is hard to quantify, we use object scale as a rough proxy to understand what DKs learn. In <ref type="figure">Figure 3,</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we introduced Deformable Kernels (DKs) to adapt effective receptive fields (ERFs) of convolutional networks for object deformation. We proposed to sample kernel values from the original kernel space. This in effect samples the ERF in linear networks and also roughly generalizes to non-linear cases. We instantiated two variants of DKs and validate our designs, showing connections to previous works. Consistent improvements over them and compatibility with them were found, as illustrated in visualizations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A COMPUTATION FLOW OF DEFORMABLE KERNELS</head><p>We now cover more details on implementing DKs by elaborating the computation flow of their forward and backward passes. We will focus on the local DK given its superior performance in practice. The extension to global DK implementation is straight-forward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 FORWARD PASS</head><p>In Section 3.3, we introduce a kernel offset generator G and a bilinear sampler B. <ref type="figure" target="#fig_4">Figure 5</ref> illustrates an example of the forward pass.</p><p>Concretely, given a kernel W and a learned group of kernel offsets {∆k} on top of a regular 2D grid {k}, we can resample a new kernel W by a bilinear operator B as</p><formula xml:id="formula_15">W ≡ W k+∆k = k ∈K B(k + ∆k, k )W k ,<label>(12)</label></formula><p>where B(k + ∆k, k ) = max(0, 1 − |k x + ∆k x − k x |) · max(0, 1 − |k y + ∆k y − k y |).</p><p>Given this resampled kernel, DK convolves it with the input image just as in normal convolutions using rigid kernels, characterized by Equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 BACKWARD PASS</head><p>The backward pass of local DK consists of three types of gradients: (1) the gradient to the data of the previous layer, (2) the gradient to the full scope kernel of the current layer and (3) the additional gradient to the kernel offset generator of the current layer. The first two types of gradients share same forms of the computation comparing to the normal convolutions. We now cover the computation for the third flow of gradient that directs where to sample kernel values.</p><p>In the context of Equation 7, the partial derivative of a output item O j w.r.t. x component of a given kernel offset ∆k x (similar for its y component ∆k y ) can be computed as</p><formula xml:id="formula_16">∂O j ∂∆k x = k I j+k k W k ∂B(k + ∆k, k ) ∂∆k x ,<label>(13)</label></formula><p>where ∂B(k + ∆k, k ) ∂∆k x = max(0, 1 − |k y + ∆k y − k y |) ·    0 |k x + ∆k x − k x | ≥ 1 1 k x + ∆k x &lt; k x −1 k x + ∆k x ≥ k x . #P (M) 25.6 23.7 GFLOPs 3.86 3.82 <ref type="table">Table 5</ref>: Network architecture of our ResNet-50-DW comparing to the original ResNet-50 Inside the brackets are the general shape of a residual block, including filter sizes and feature dimensionalities. The number of stacked blocks on each stage is presented outside the brackets. "G = 128" suggests the depthwise convolution with 128 input channels. Two models have similar numbers of parameters and FLOPs. At the same time, depthwise convolutions facilitate the computation efficiency of our Deformable Kernels.</p><p>B NETWORK ARCHITECTURES <ref type="table">Table 5</ref> shows the comparison between the original ResNet-50 <ref type="bibr" target="#b7">(He et al., 2016)</ref> and our modified ResNet-50-DW. The motivation of introducing depthwise convolutions to ResNet is to accelerate the computation of local DKs based on our current implementations. The ResNet-50-DW model has similar model capacity/complexity and performance (see <ref type="table" target="#tab_2">Table 1</ref>) compared to its non-depthwise counterpart, making it an ideal base architecture for our experiments.</p><p>On the other hand, in all of our experiments, MobileNet-V2 <ref type="bibr" target="#b24">(Sandler et al., 2018)</ref> base model is left untouched.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C ADDITIONAL COMPARISON OF EFFECTIVE RECEPTIVE FIELDS</head><p>We here show additional comparison of ERFs when objects have different kinds of deformations in <ref type="figure" target="#fig_5">Figure 6</ref>. Comparing to baseline, our method can adapt ERFs to be more persistent to objects semantic rather than its geometric configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D ADDITIONAL EXPERIMENT DETAILS</head><p>Image Classification: Similar to <ref type="bibr" target="#b6">Goyal et al. (2017)</ref>; <ref type="bibr" target="#b18">Loshchilov &amp; Hutter (2017)</ref>, training is performed by SGD for 90 epochs with momentum 0.9 and batch size 256. We set our learning rate of 10 −1 so that it linearly warms up from zero within first 5 epochs. A cosine training schedule is applied over the training epochs. We use scale and aspect ratio augmentation with color perturbation as standard data augmentations. We evaluate the performance of trained models on the ImageNet 2012 validation set. The images are resized so that the shorter side is of 256 pixels. We then centrally crop 224 × 224 windows from the images as input to measure recognition accuracy. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>local deformable kern. ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m I 6 F y X r F i + p j e 9 I m h 0 D x 7 h H 0 6 h 0 = " &gt; A A A B + H i c b V D L S g N B E O y N r x h f U Y 9 e F o M g H s J u F P Q Y 9 O I x A f O A Z A m z k 9 5 k y M z s M j M r x J A v 8 K p 3 b + L V v / H q l z h J 9 q C J B Q 1 F V T f V V J h w p o 3 n f T m 5 t f W N z a 3 8 d m F n d 2 / / o H h 4 1 N R x q i g 2 a M x j 1 Q 6 J R s 4 k N g w z H N u J Q i J C j q 1 w d D f z W 4 + o N I v l g x k n G A g y k C x i l B g r 1 S 9 6 x Z J X 9 u Z w V 4 m f k R J k q P W K 3 9 1 + T F O B 0 l B O t O 7 4 X m K C C V G G U Y 7 T Q j f V m B A 6 I g P s W C q J Q B 1 M 5 o 9 O 3 T O r 9 N 0 o V n a k c e f q 7 4 s J E V q P R W g 3 B T F D v e z N x H + 9 U C w l m + g m m D C Z p A Y l X Q R H K X d N 7 M 5 a c P t M I T V 8 b A m h i t n f X T o k i l B j u y r Y U v z l C l Z J s 1 L 2 L 8 u V + l W p e p v V k 4 c T O I V z 8 O E a q n A P N W g A B Y R n e I F X 5 8 l 5 c 9 6 d j 8 V q z s l u j u E P n M 8 f p J e T W A = = &lt; / l a t e x i t &gt; ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m I 6 F y X r F i + p j e 9 I m h 0 D x 7 h H 0 6 h 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>n e I F X 5 8 l 5 c 9 6 d j 8 V q z s l u j u E P n M 8 f p J e T W A = = &lt; / l a t e x i t &gt; G global &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 3 9 Q q E D G 1 u o t 6 k D i L d 4 U 0 h V e x Y E = " &gt; A A A C F H i c b V B N S 8 N A E N 3 U r 1 q / o h 5 7 C R b B U 0 m q o M e i B z 1 W s B / Q h r D Z b t q l m 2 z Y n Y g l 5 O C f 8 C 9 4 1 b s 3 8 e r d q 7 / E T Z u D t j 4 Y e L w 3 w 8 w 8 P + Z M g W 1 / G a W V 1 b X 1 j f J m Z W t 7 Z 3 f P 3 D / o K J F I Q t t E c C F 7 P l a U s 4 i 2 g Q G n v V h S H P q c d v 3 J V e 5 3 7 6 l U T E R 3 M I 2 p G + J R x A J G M G j J M 6 u D E M O Y Y J 5 e e + k A 6 A O k I y 5 8 z L M s 8 8 y a X b d n s J a J U 5 A a K t D y z O / B U J A k p B E Q j p X q O 3 Y M b o o l M M J p V h k k i s a Y T P C I 9 j W N c E i V m 8 6 e y K x j r Q y t Q E h d E V g z 9 f d E i k O l p q G v O / O T 1 a K X i / 9 6 f r i w G Y I L N 2 V R n A C N y H x x k H A L h J U n Z A 2 Z p A T 4 V B N M J N O 3 W 2 S M J S a g c 6 z o U J z F C J Z J p 1 F 3 T u u N 2 7 N a 8 7 K I p 4 y q 6 A i d I A e d o y a 6 Q S 3 U R g Q 9 o m f 0 g l 6 N J + P N e D c + 5 q 0 l o 5 g 5 R H 9 g f P 4 A F Q + f u Q = = &lt; / l a t e x i t &gt; ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " + d s 0 H</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>a s p 9 0 8 m R H q t l L x X / 9 H y + t F k H N S + h I o o 1 E X i x O I g Z 1 C F M U 4 B D K g n W b G o I w p K a 2 y E e I 4 m w N l k V T C j f n 8 P / S d u 1 y + e 2 2 7 w o 1 a + z e P L g C B y D U 1 A G V V A H t 6 A B W g A D A h 7 B E 3 i 2 H q w X 6 9 V 6 W 7 T m r G z m E P y C 9 f 4 F A C G T l A = = &lt; / l a t e x i t &gt; ⇤ &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 Y m k T O G s F j r + H d / X u F c x n d h m W a c = " &gt; A A A B + H i c d V B N T w I x E J 3 F L 8 Q v 1 K O X R m J i P J B d N A o 3 o h e P k A i Y w I Z 0 S x c a 2 u 6 m 7 Z o g 4 R d 4 1 b s 3 4 9 V / 4 9 V f Y o E 1 Q a I v m e T l v Z n M z A t i z r R x 3 U 8 n s 7 K 6 t r 6 R 3 c x t b e / s 7 u X 3 D 5 o 6 S h S h D R L x S N 0 H W F P O J G 0 Y Z j i 9 j x X F I u C 0 F Q x v p n 7 r g S r N I n l n R j H 1 B e 5 L F j K C j Z X q Z 9 1 8 w S 2 6 M 6 A F U q m U v c o l 8 l K l A C l q 3 f x X p x e R R F B p C M d a t z 0 3 N v 4 Y K 8 M I p 5 N c J 9 E 0 x m S I + 7 R t q c S C a n 8 8 O 3 S C T q z S Q 2 G k b E m D Z u r i x B g L r U c i s J 0 C m 4 F e 9 q b i n 1 4 g l j a b s O y P m Y w T Q y W Z L w 4 T j k y E p i m g H l O U G D 6 y B B P F 7 O 2 I D L D C x N i s c j a U n 8 / R / 6 R Z K n r n x V L 9 o l C 9 T u P J w h E c w y l 4 c A V V u I U a N I A A h S d 4 h h f n 0 X l 1 3 p z 3 e W v G S W c O 4 R e c j 2 / i k Z O C &lt; / l a t e x i t &gt; G local &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o m 7 L t m w a S y N n U m h s 4 j r 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>a s 2 L o t 4 y u g Q H a E T 5 K B z 1 E A 3 q I l a i K B H 9 I x e 0 K v x Z L w Z 7 8 b H r L V k F D P 7 6 A + M z x 9 E I p 9 J &lt; / l a t e x i t &gt; G local &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o m 7 L t m w a S y N n U m h s 4 j r 7 v Z h T j l M = " &gt; A A A C E 3 i c b V D L S s N A F J 3 U V 6 2 v q D v d B I v g q i R V 0 G X R h S 4 r 2 A c 0 I U y m 0 3 b o J B N m b s Q S A v 6 E v + B W 9 + 7 E r R / g 1 i 9 x 0 m a h r Q c G z j 3 n X u 6 d E 8 S c K b D t L 6 O 0 t L y y u l Z e r 2 x s b m 3 v m L t 7 b S U S S W i L C C 5 k N 8 C K c h b R F j D g t B t L i s O A 0 0 4 w v s r 9 z j 2 V i o n o D i Y x 9 U I 8 j N i A E Q x a 8 s 0 D N 8 Q w I p i n 1 3 7 q A n 2 A l A t d Z l n m m 1 W 7 Z k 9 h L R K n I F V U o O m b 3 2 5 f k C S k E R C O l e o 5 d g x e i i U w w m l W c R N F Y 0 z G e E h 7 m k Y 4 p M p L p 3 / I r G O t 9 K 2 B k P p F Y E 3 V 3 x M p D p W a h I H u z C 9 W 8 1 4 u / u s F 4 d x m G F x 4 K Y v i B G h E Z o s H C b d A W H l A V p 9 J S o B P N M F E M n 2 7 R U Z Y Y g I 6 x o o O x Z m P Y J G 0 6 z X n t F a / P a s 2 L o t 4 y u g Q H a E T 5 K B z 1 E A 3 q I l a i K B H 9 I x e 0 K v x Z L w Z 7 8 b H r L V k F D P 7 6 A + M z x 9 E I p 9 J &lt; / l a t e x i t &gt; shared = &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 6 + A F I D N r p c 5 o z Z 0 G t t V / 2 v h A B g = " &gt; A A A B + H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q S 9 C 0 I v H B M w D k i X M T n q T I T O 7 y 8 y sE E O + w K v e v Y l X / 8 a r X + I k 2 Y M m F j Q U V d 1 U U 0 E i u D a u + + X k 1 t Y 3 N r f y 2 4 W d 3 b 3 9 g + L h U V P H q W L Y Y L G I V T u g G g W P s G G 4 E d h O F F I Z C G wF o 7 u Z 3 3 p E p X k c P Z h x g r 6 k g 4 i H n F F j p f p N r 1 h y y + 4 c Z J V 4 G S l B h l q v + N 3 t x y y V G B k m q N Y d z 0 2 M P 6 H K c C Z w W u i m G h P K R n S A H U s j K l H 7 k / m j U 3 J m l T 4 J Y 2 U n M m S u / r 6 Y U K n 1 W A Z 2 U 1 I z 1 M v e T P z X C + R S s g m v / Q m P k t R g x B b B Y S q I i c m s B d L n C p k R Y 0 s o U 9 z + T t i Q K s q M 7 a p g S / G W K 1 g l z U r Z u y h X 6 p e l 6 m 1 W T x 5 O 4 B T O w Y M r q M I 9 1 K A B D B C e 4 Q V e n S f n z X l 3 P h a r O S e 7 O Y Y / c D 5 / A M K T k 2 s = &lt; / l a t e x i t &gt; Conv &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " M A T U G o i x b W s K p L U j y 5 m S Q U 0 + 4 L I = " &gt; A A A C A n i c b V C 7 S g N B F J 3 1 G e M r a m k z G A S r s B s F L Y N p L C O Y B y R L m J 1 M k i H z W G b u B s O S z l + w 1 d 5 O b P 0 R W 7 / E S b K F J h 6 4 c D j n X s 7 l R L H g F n z / y 1 t b 3 9 j c 2 s 7 t 5 H f 3 9 g 8 O C 0 f H D a s T Q 1 m d a q F N K y K W C a 5 Y H T g I 1 o o N I z I S r B m N q j O / O W b G c q 0 e Y B K z U J K B 4 n 1 O CT i p 3 Q H 2 C G l V q / G 0 W y j 6 J X 8 O v E q C j B R R h l q 3 8 N 3 p a Z p I p o A K Y m 0 7 8 G M I U 2 K A U 8 G m + U 5 i W U z o i A x Y 2 1 F F J L N h O n 9 5 i s + d 0 s N 9 b d w o w H P 1 9 0 V K p L U T G b l N S W B o l 7 2 Z + K 8 X y a V k 6 N + E K V d x A k z R R X A / E R g 0 n v W B e 9 w w C m L i C K G G u 9 8 x H R J D K L j W 8 q 6 U Y L m C V d I o l 4 L L U v n + q l i 5 z e r J o V N 0 h i 5 Q g K 5 R B d 2 h G q o j i j R 6 R i / o 1 X v y 3 r x 3 7 2 O x u u Z l N y f o D 7 z P H 0 / K m E E = &lt; / l a t e x i t &gt; G local&lt; l a t e x i t s h a 1 _ b a s e 6 4 = " o m 7 L t m w a S y N n U m h s 4 j r 7v Z h T j l M = " &gt; A A A C E 3 i c b V D L S s N A F J 3 U V 6 2 v q D v d B I v g q i R V 0 G X R h S 4 r 2 A c 0 I U y m 0 3 b o J B N m b s Q S A v 6 E v + B W 9 + 7 E r R / g 1 i 9 x 0 m a h r Q c G z j 3 n X u 6 d E 8 S c K b D t L 6 O 0 t L y y u l Z e r 2 x s b m 3 v m L t 7 b S U S S W i L C C 5 k N 8 C K c h b R F j D g t B t L i s O A 0 0 4 w v s r 9 z j 2 V i o n o D i Y x 9 U I 8 j N i A E Q x a 8 s 0 D N 8 Q w I p i n 1 3 7 q A n 2 A l A t d Z l n m m 1 W 7 Z k 9 h L R K n I F V U o O m b 3 2 5 f k C S k E R C O l e o 5 d g x e i i U w w m l W c R N F Y 0 z G e E h 7 m k Y 4 p M p L p 3 / I r G O t 9 K 2 B k P p F Y E 3 V 3 x M p D p W a h I H u z C 9 W 8 1 4 u / u s F 4 d x m G F x 4 K Y v i B G h E Z o s H C b d A W H l A V p 9 J S o B P N M F E M n 2 7 R U Z Y Y g I 6 x o o O x Z m P Y J G 0 6 z X n t F a / Pa s 2 L o t 4 y u g Q H a E T 5 K B z 1 E A 3 q I l a i K B H 9 I x e 0 K v x Z L w Z 7 8 b H r L V k F D P 7 6 A + M z x 9 E I p 9 J &lt; / l a t e x i t &gt; G global &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " S 3 9 Q q E D G 1 u o t 6 k D i L d 4 U 0 h V e x Y E = " &gt; A A A C F H i c b V B N S 8 N A E N 3 U r 1 q / o h 5 7 C R b B U 0 m q o M e i B z 1 W s B / Q h r D Z b t q l m 2 z Y n Y g l 5 O C f 8 C 9 4 1 b s 3 8 e r d q 7 / E T Z u D t j 4 Y e L w 3 w 8 w 8 P + Z M g W 1 / G a W V 1 b X 1 j f J m Z W t 7 Z 3 f P 3 D / o K J F I Q t t E c C F 7 P l a U s 4 i 2 g Q G n v V h S H P q c d v 3 J V e 5 3 7 6 l U T E R 3 M I 2 p G + J R x A J G M G j J M 6 u D E M O Y Y J 5 e e + k A 6 A O k I y 5 8 z L M s 8 8 y a X b d n s J a J U 5 A a K t D y z O / B U J A k p B E Q j p X q O 3 Y M b o o l M M J p V h k k i s a Y T P C I 9 j W N c E i V m 8 6 e y K x j r Q y t Q E h d E V g z 9 f d E i k O l p q G v O / O T 1 a K X i / 9 6 f r i w G Y I L N 2 V R n A C N y H x x k H A L h J U n Z A 2 Z p A T 4 V B N M J N O 3 W 2 S M J S a g c 6 z o U J z F C J Z J p 1 F 3 T u u N 2 7 N a 8 7 K I p 4 y q 6 A i d I A e d o y a 6 Q S 3 U R g Q 9 o m f 0 g l 6 N J + P N e D c + 5 q 0 l o 5 g 5 R H 9 g f P 4 A F Q + f u Q = = &lt; / l a t e x i t &gt; = &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 6 + A F I D N r p c 5 o z Z 0 G t t V / 2 v h A B g = " &gt; A A A B + H i c b V D L S g N B E O y N r x h f U Y 9 e B o P g K e x G Q S 9 C 0 I v H B M w D k i X M T n q T I T O 7 y 8 y s E E O + w K v e v Y l X / 8 a r X + I k 2 Y M m F j Q U V d 1 U U 0 E i u D a u + + X k 1 t Y 3 N r f y 2 4 W d 3 b 3 9 g + L h U V P H q W L Y Y L G I V T u g G g W P s G G 4 E d h O F F I Z C G w F o 7 u Z 3 3 p E p X k c P Z h x g r 6 k g 4 i H n F F j p f p N r 1 h y y + 4 c Z J V 4 G S l B h l q v + N 3 t x y y V G B k m q N Y d z 0 2 M P 6 H K c C Z w W u i m G h P K R n S A H U s j K l H 7 k / m j U 3 J m l T 4 J Y 2 U n M m S u / r 6 Y U K n 1 W A Z 2 U 1 I z 1 M v e T P z X C + R S s g m v / Q m P k t R g x B b B Y S q I i c m s B d L n C p k R Y 0 s o U 9 z + T t i Q K s q M 7 a p g S / G W K 1 g l z U r Z u y h X 6 p e l 6 m 1 W T x 5 O 4 B T O w Y M r q M I 9 1 K A B D B C e 4 Q V e n S f n z X l 3 P h a r O S e 7 O Y Y / c D 5 / A M K T k 2 s = &lt; / l a t e x i t &gt; Global &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " B 2 D Y f q K / f 8 Q B Q Z e 5 2 o l 9 w J p A E n k = " &gt; A A A C B H i c b V C 7 S g N B F J 3 1 G e M r a m k z G A S r s B s F L Y M W W k Y w D 0 i W M D u Z J E P m s c 7 c F c O S 1 l + w 1 d 5 O b P 0 P W 7 / E S b K F J h 6 4 c D j n X s 7 l R L H g F n z / y 1 t a X l l d W 8 9 t 5 D e 3 t n d 2 C 3 v 7 d a s T Q 1 m N a q F N M y K W C a 5 Y D T g I 1 o w N I z I S r B E N r y Z + 4 4 E Z y 7 W 6 g 1 H M Q k n 6 i v c 4 J e C k s A 3 s E d J r o S M i x p 1 C 0 S / 5 U + B F E m S k i D J U O 4 X v d l f T R D I F V B B r W 4 E f Q 5 g S A 5 w K N s 6 3 E 8 t i Q o e k z 1 q O K i K Z D d P p 0 2 N 8 7 J Q u 7 m n j R g G e q r 8 v U i K t H c n I b U o C A z v v T c R / v U j O J U P v I k y 5 i h N g i s 6 C e 4 n A o P G k E d z l h l E Q I 0 c I N d z 9 j u m A G E L B 9 Z Z 3 p Q T z F S y S e r k U n J b K t 2 f F y m V W T w 4 d o i N 0 g g J 0 j i r o B l V R D V F 0 j 5 7 R C 3 r 1 n r w 3 7 9 3 7 m K 0 u e d n N A f o D 7 / M H y m q Z E A = = &lt; / l a t e x i t &gt; AvgPool &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " w Y l 2 v N 9 C G L G f q z 9 E f a 9 D Y 7 c s q S M = " &gt; A A A C B X i c b V C 7 S g N B F J 2 N r x h f U U u b w S B Y h d 0 o a B m 1 s Y x g H p C s Y X Y y m w y Z n V l m 7 g b D k t p f s N X e T m z 9 D l u / x E m y h S Y e u H A 4 5 1 7 O 5 Q S x 4 A Z c 9 8 v J r a y u r W / k N w t b 2 z u 7 e 8 X 9 g 4 Z R i a a s T p V Q u h U Q w w S X r A 4 c B G v F m p E o E K w Z D G + m f n P E t O F K 3 s M 4 Z n 5 E + p K H n B K w 0 k M H 2 C O k V 6 N + T S k x 6 R Z L b t m d A S 8 T L y M l l K H W L X 5 3 e o o m E Z N A B T G m 7 b k x + C n R w K l g k 0 I n M S w m d E j 6 r G 2 p J B E z f j r 7 e o J P r N L D o d J 2 J O C Z + v s i J Z E x 4 y i w m x G B g V n 0 p u K / X h A t J E N 4 6 a d c x g k w S e f B Y S I w K D y t B P e 4 Z h T E 2 B J C N b e / Y z o g m l C w x R V s K d 5 i B c u k U S l 7 Z + X K 3 X m p e p 3 V k 0 d H 6 B i d I g 9 d o C q 6 R T V U R x R p 9 I x e 0 K v z 5 L w 5 7 8 7 H f D X n Z D e H 6 A + c z x + X P J m B &lt; / l a t e x i t &gt; + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 K n x d A U b b P 9 f F O k K 5 C 3 u P U W 4 I h o = " &gt; A A A B + H i c b V D L S g N B E O y N r x h f U Y 9 e F o M g C G E 3 C n o M e v G Y g H l A s o T Z S W 8 y Z G Z 2 m Z k V Y s g X e N W 7 N / H q 3 3 j 1 S 5 w k e 9 D E g o a i q p t q K k w 4 0 8 b z v p z c 2 v r G 5 l Z + u 7 C z u 7 d / U D w 8 a u o 4 V R Q b N O a x a o d E I 2 c S G 4 Y Z j u 1 E I R E h x 1 Y 4 u p v 5 r U d U m s X y w Y w T D A Q Z S B Y x S o y V 6 h e 9 Y s k r e 3 O 4 q 8 T P S A k y 1 H r F 7 2 4 / p q l A a S g n W n d 8 L z H B h C j D K M d p o Z t q T A g d k Q F 2 L J V E o A 4 m 8 0 e n 7 p l V + m 4 U K z v S u H P 1 9 8 W E C K 3 H I r S b g p i h X v Z m 4 r 9 e K J a S T X Q T T J h M U o O S L o K j l L s m d m c t u H 2 m k B o + t o R Q x e z v L h 0 S R a i x X R V s K f 5 y B a u k W S n 7 l + V K / a p U v c 3 q y c M J n M I 5 + H A N V b i H G j S A A s I z v M C r 8 + S 8 O e / O x 2 I 1 5 2 Q 3 x / A H z u c P p i u T W Q = = &lt; / l a t e x i t &gt; FC &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " b h 8 + 7 2 0 z t M c + p p E X 4 8 M u S s t x z 9 4 = " &gt; A A A C A H i c b V C 7 S g N B F J 2 N r x h f U U u b w S B Y h d 0 o a B k M i G U E 8 5 B k C b O T 2 W T I z O w y c 1 c M S x p / w V Z 7 O 7 H 1 T 2 z 9 E i f J F p p 4 4 M L h n H s 5 l x P E g h t w 3 S 8 n t 7 K 6 t r 6 R 3 y x s b e / s 7 h X 3 D 5 o m S j R l D R q J S L c D Y p j g i j W A g 2 D t W D M i A 8 F a w a g 2 9 V s P T B s e q T s Y x 8 y X Z K B 4 y C k B K 9 1 3 g T 1 C e l 2 b 9 I o l t + z O g J e J l 5 E S y l D v F b + 7 / Y g m k i m g g h j T 8 d w Y / J R o 4 F S w S a G b G B Y T O i I D 1 r F U E c m M n 8 4 e n u A T q / R x G G k 7 C v B M / X 2 R E m n M W A Z 2 U x I Y m k V v K v 7 r B X I h G c J L P + U q T o A p O g 8 O E 4 E h w t M 2 c J 9 r R k G M L S F U c / s 7 p k O i C Q X b W c G W 4 i 1 W s E y a l b J 3 V q 7 c n p e q V 1 k 9 e X S E j t E p 8 t A F q q I b V E c N R J F E z + g F v T p P z p v z 7 n z M V 3 N O d n O I / s D 5 / A F V C p c g &lt; / l a t e x i t &gt; Instantiations. We show how DK variants works with an example image that contains a large and a small object. (a) The global DK learns one set of kernel sampling grid given an input image and apply it to all data positions. (b) The local DK adapts kernels for each input patches, and induces better locality for deformation modeling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Illustration of feed-forwarding through a 3×3 local Deformable Kernel from a 4×4 scope. For each input patch, local DK first generates a group of kernel offsets {∆k} from input feature patch using the light-weight generator G (a 3×3 convolution of rigid kernel). Given the original kernel weights W and the offset group {∆k}, DK samples a new set of kernel W using a bilinear sampler B. Finally, DK convolves the input feature map and the sampled kernels to complete the whole computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Effective Receptive Field Comparison between rigid kernels and DKs under different kinds of Object Deformation. At each row and from left to right, we show the original image (1300×800), the image rotated by -90 degrees and the image scaled by 1.5 times. Images are cropped and resized for the typesetting purpose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Moreover, we can learn both data and kernel offsets in one convolutional operator. Conceptually, this can be done by merging Equation 7 with Equation 9, which leads to</figDesc><table><row><cell></cell><cell>O j =</cell><cell>I j+k+∆j W k+∆k ,</cell></row><row><cell></cell><cell>k∈K</cell></row><row><cell>R</cell><cell>(n) DC+DK (i; j) =</cell></row></table><note>km∈K R (n) (i; j + km + ∆jm, km + ∆km).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Ablations of scope size and different instantiations of DK for image classification.Using proper scope size, and more DK layers boosts performance. Modeling individual offset kernel grid for each data entries is also beneficial.</figDesc><table><row><cell>Backbone</cell><cell cols="5">1×1 Deformable Kernels 3×3 Deformable Kernels top1 (%) #P (M) GFLOPs</cell></row><row><cell>ResNet-50-DW</cell><cell>w/o</cell><cell>local, scope size 4×4</cell><cell>78.1</cell><cell>25.0</cell><cell>4.32</cell></row><row><cell>ResNet-50-DW with SCC</cell><cell>w/o</cell><cell>w/o local, scope size 4×4</cell><cell>77.6 78.9</cell><cell>42.5 43.7</cell><cell>7.13 7.61</cell></row><row><cell>ResNet-50-DW with DCN</cell><cell>w/o</cell><cell>w/o local, scope size 4×4</cell><cell>78.0 79.0</cell><cell>24.8 26.1</cell><cell>4.10 4.60</cell></row><row><cell>MobileNet-V2</cell><cell>w/o</cell><cell>local, scope size 4×4</cell><cell>74.1</cell><cell>4.7</cell><cell>0.73</cell></row><row><cell>MobileNet-V2 with SCC</cell><cell>w/o</cell><cell>w/o local, scope size 4×4</cell><cell>74.3 75.5</cell><cell>19.0 19.7</cell><cell>2.19 2.48</cell></row><row><cell>MobileNet-V2 with DCN</cell><cell>w/o</cell><cell>w/o local, scope size 4×4</cell><cell>73.2 74.4</cell><cell>4.6 5.8</cell><cell>0.52 0.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparisons to strong baselines for image classification DKs perform comparably or superiorly to previous methods. Further combinations yield consistent gain, suggesting orthogonal and compatible working mechanisms.We further compare and combine DKs with Conditional Convolutions and Deformable Convolutions. Results are recorded in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 and</head><label>3</label><figDesc>Table 4follow the same style of analysis as in image classification.</figDesc><table><row><cell>While the baseline</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablations for object detection. Consistent results with image classification.</figDesc><table><row><cell>Backbone</cell><cell cols="6">1×1 Deformable Kernels 3×3 Deformable Kernels mAP mAPS mAPM mAPL</cell></row><row><cell>ResNet-50-DW</cell><cell>global, scope size 2×2</cell><cell>local, scope size 4×4</cell><cell>38.4</cell><cell>23.4</cell><cell>42.0</cell><cell>49.4</cell></row><row><cell>ResNet-50-DW with SCC</cell><cell>w/o</cell><cell>w/o local, scope size 4×4</cell><cell>36.3 38.0</cell><cell>22.1 23.4</cell><cell>39.3 41.9</cell><cell>47.0 48.4</cell></row><row><cell>ResNet-50-DW with DCN</cell><cell>w/o</cell><cell>w/o local, scope size 4×4</cell><cell>39.9 40.6</cell><cell>24.0 24.6</cell><cell>43.4 43.9</cell><cell>52.6 53.3</cell></row><row><cell>MobileNet-V2</cell><cell>global, scope size 2×2</cell><cell>local, scope size 4×4</cell><cell>33.7</cell><cell>20.2</cell><cell>36.7</cell><cell>44.0</cell></row><row><cell>MobileNet-V2 with SCC</cell><cell>w/o</cell><cell>w/o local, scope size 4×4</cell><cell>33.2 34.3</cell><cell>20.5 20.2</cell><cell>35.6 37.3</cell><cell>43.3 44.7</cell></row><row><cell>MobileNet-V2 with DCN</cell><cell>w/o</cell><cell>w/o local, scope size 4×4</cell><cell>34.4 35.6</cell><cell>20.5 20.6</cell><cell>37.0 38.5</cell><cell>44.7 47.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparisons to strong baselines for object detection DKs perform fall short to Deformable Convolution, but combination still improves performance. ment when replacing both 1x1 and 3x3 rigid kernels. This trend magnifies on MobileNet-v2 models, where we see an improvement of +1.6 mAP and +2.4 mAP, respectively. Results also confirm the effectiveness of local DKs against global DKs, which is again in line with our expectation that local DKs can model locality better.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>we show the t-SNE<ref type="bibr" target="#b21">(Maaten &amp; Hinton, 2008)</ref> of learned model dynamics by the last convolutional layers in MobileNet-V2 using Conditional Convolution and our DKs. We validate the finding as claimed by that the experts of Conditional Convolutions have better correlation with object semantics than their scales (in reference toFigure 6from their paper). Instead, our DKs learn kernel sampling offsets that strongly correlate to scales rather than semantics. This sheds light on why the two operators are complementary in our previous experiments.Adaptation of Effective Receptive Fields:To verify our claim that DK indeed adapts ERFs in practice, we show ERF visualizations on a set of images in which they display different degrees of deformations. We compare the results of rigid kernels, Deformable Convolutions, our DKs, and the combination of the two operators. For all examples, note that the theoretical receptive field covers every pixel in the image but ERFs contain only a central portion of it. Deformable Convolutions Semantics vs. Scales. We show t-SNE results of learned model dynamics using 10 random classes of objects from the COCO test-dev split. Each point represents an object extracted by ground-truth bounding box, whose color either denotes its class label or bounding box scale. The color of an object scale is its normalized area rank discretized by every 10th percentile among all data. Numbers inside parentheses indicate the dimension of learned dynamics before t-SNE. (a) The dynamics of Conditional Convolutions are closer to semantics than to object scales. (b) On the contrary, our DKs learn dynamics that are significantly related to scales rather than semantics. Learned Effective Receptive Fields. We show learned ERFs on three images with large, medium, and small objects from the COCO test-dev split. Given each ground-truth bounding box, we visualize the non-zero ERF values of its central point. Theoretical RFs cover the whole image for all three examples and we thus ignore them in our plots.(a) Rigid kernels have strong central effects and a Gaussian-like ERF that cannot deal with object deformation alone. (b) Deformable Convolutions and (c) Deformable Kernels both tune ERFs to data. (d) Combining both operators together enables better modeling of 2D geometric transformation of objects.and DKs perform similarly in terms of adapting ERFs, but Deformable Convolutions tend to spread out and have sparse responses while DKs tend to concentrate and densely activate within an object region. Combining both operators yields more consistent ERFs that exploit both of their merits.</figDesc><table><row><cell>semantics</cell><cell>scales</cell><cell>semantics</cell><cell>scales</cell></row><row><cell cols="2">(a) conditional conv. : expert scores (8)</cell><cell cols="2">(b) deformable kern. : kernel offsets (18=2x3x3)</cell></row><row><cell cols="4">skateboard bird Figure 3: person skateboard skateboard bird bird bird bird bird bird skateboard skateboard skateboard person person person</cell></row><row><cell>(a) rigid kern.</cell><cell>(b) deformable conv.</cell><cell>(c) deformable kern.</cell><cell>(d) deformable conv. + kern.</cell></row><row><cell>Figure 4:</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">This makes enlarging the kernel scope size tractable and prevents extensive resource competition in CUDA kernels when applying local DKs.4  The implementation of local DKs right now cannot support large number of output channels.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Invariant scattering convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Polar transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Allen-Blanchette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The perception of the visual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1950" />
			<pubPlace>Houghton Mifflin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning by abstraction: The neural state machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03950</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Locally scale-invariant convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Selective kernel networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional neural network architecture for geometric matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ignacio</forename><surname>Rocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mo-bilenetv2: Inverted residuals and linear bottlenecks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Blurring the line between structure and learning to optimize and adapt receptive fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11487</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rotation, scaling and deformation invariant scattering for texture discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Dynamic scale inference by entropy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.03182</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Harmonic networks: Deep translation and rotation equivariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">J</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniyar</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><forename type="middle">J</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deformable filter convolution for point cloud reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13079</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brandon</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ngiam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.04971</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Soft conditional computation</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
