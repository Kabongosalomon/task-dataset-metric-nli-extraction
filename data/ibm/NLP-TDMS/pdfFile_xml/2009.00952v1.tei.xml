<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mutual Teaching for Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhan</surname></persName>
							<email>kzhan@lzu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science &amp; Engineering</orgName>
								<orgName type="institution">Lanzhou University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Niu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science &amp; Engineering</orgName>
								<orgName type="institution">Lanzhou University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mutual Teaching for Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph convolutional networks produce good predictions of unlabeled samples due to its transductive label propagation. Since samples have different predicted confidences, we take high-confidence predictions as pseudo labels to expand the label set so that more samples are selected for updating models. We propose a new training method named as mutual teaching, i.e., we train dual models and let them teach each other during each batch. First, each network feeds forward all samples and selects samples with high-confidence predictions. Second, each model is updated by samples selected by its peer network. We view the high-confidence predictions as useful knowledge, and the useful knowledge of one network teaches the peer network with model updating in each batch. In mutual teaching, the pseudo-label set of a network is from its peer network. Since we use the new strategy of network training, performance improves significantly. Extensive experimental results demonstrate that our method achieves superior performance over stateof-the-art methods under very low label rates.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Although graph convolutional network (GCN) <ref type="bibr" target="#b10">[Kipf and Welling, 2017]</ref> recently made great achievement in semisupervised learning (SSL) and many GCN-based SSL algorithms were developed, GCN-based SSL methods currently still have some issues:</p><p>How to expand the label set. To expand the label set, we utilize predictions of GCN to expand the label set to select more samples involved in updating models. Since GCN has the property of transductive label propagation, i.e., Laplacian smoothing <ref type="bibr" target="#b10">[Li et al., 2018]</ref>, each connected component in a graph tends to have the same label. Labeled nodes propagate its label to unlabeled nodes in the graph. We select samples with high-confidence prediction probabilities produced by the softmax layer of GCN as pseudo labels.</p><p>How to improve the training strategy. To further improve the network performance, we use dual models with a new training strategy called as mutual teaching. In each batch, each network regards its high-confidence predictions as the useful knowledge and teaches the knowledge to its peer network. Both soft and hard predicted targets are exploited to improve the performance. One network generates useful knowledge and the other learns from its peer network.</p><p>Most of them require many labeled data. We consider SSL when the labeled samples are very limited, i.e., even two or three labeled samples per class are available for training a model, which is a very challenge problem. Most of the existing GCN-based SSL methods used 20 labeled samples per class to train only one GCN model, and they did not learn the models when very few labeled samples are available. Compared with these algorithms, SSL from very few labeled data is vitally important. Due to the expensive labeling cost, it is hard to obtain many labeled data.</p><p>To address these issues, we propose mutual teaching for GCN (MT-GCN). We present a new MT-GCN SSL algorithm to overcome the limits mentioned above. <ref type="figure" target="#fig_0">Fig. 1</ref> shows a simple example of mutual teaching algorithm in dual models. This method trains dual GCN models and then learns from each other with the most confidence pseudo labels. What is more, each model is updated with three loss terms, a supervised loss with labeled samples, a pseudo-label loss, and a consistency loss. Except for the supervised loss, the two other loss terms use the mutual teaching strategy. The pseudo-label loss uses hard pseudo targets while the consistency loss uses soft targets produced by the softmax layer of GCN. Overall, the contributions of MT-GCN are summarized below: 1) We use dual GCN models for improving the prediction performance. With very small number of labeled data (e.g., even two or three samples are available per class) and the exploited pseudo labels, the performance of the proposed MT-GCN is better than other GCN-based algorithms in SSL.</p><p>2) Different from only using the loss function with labeled samples, we use the pseudo labels to calculate two new loss terms. The new loss terms are designed to encourage implicitly cross-model prediction alignment for each class from both labeled samples and selected pseudo labels.</p><p>3) We obtain high quantitative metrics, especially when only two or three labeled samples per class are available. Extensive experiments demonstrate that our method is better than state-of-the-art approaches in all considered datasets with a small number of labeled data.</p><p>The rest of the paper is organized as follows: Section 2 introduces some related work. GCN are introduced in Section 3. In Section 4, we propose our MT-GCN method to solve SSL with very few labeled data. In Section 5, we conduct experiments to demonstrate the effectiveness of our proposed method. In Section 6, we present conclusion of the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Neural Networks</head><p>Graph neural networks are widely exploited for machine learning tasks recently <ref type="bibr" target="#b17">[Wu et al., 2020b;</ref><ref type="bibr" target="#b9">Ji et al., 2020;</ref>. <ref type="bibr">GCN [Kipf and Welling, 2017]</ref> was applied to SSL because the graph convolution of GCN is a special form of Laplacian smoothing over the graph <ref type="bibr" target="#b10">[Li et al., 2018]</ref>. GraphSAGE <ref type="bibr" target="#b6">[Hamilton et al., 2017]</ref> proposed an inductive framework that generates embedding by sampling and aggregating features from a node's local neighborhood. <ref type="bibr">GAT [Veličković et al., 2018]</ref> applied the multi-head selfattention mechanism to parameterize the edge weight. <ref type="bibr" target="#b18">Xu et al., [Xu et al., 2019]</ref> discussed the representational capacity by analyzing different structures and proposed a graph isomorphism network. <ref type="bibr" target="#b17">Wu et al. [Wu et al., 2020a]</ref> used GCN for unsupervised domain adaption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Self-Supervised Learning</head><p>Self-supervised learning utilizes auxiliary tasks to produce pseudo labels <ref type="bibr" target="#b4">[Doersch et al., 2015]</ref>. Most self-supervised learning algorithms used the same network architecture in both the pretext task and the fine-tuning task <ref type="bibr" target="#b5">[Goyal et al., 2019]</ref>. For transferring knowledge between two models, knowledge distillation <ref type="bibr" target="#b1">[Bucilua et al., 2006;</ref><ref type="bibr" target="#b8">Hinton et al., 2015]</ref> can use to transfer the representation in a trained model with the pretext task to the other one employed on the target task. DeepCluster <ref type="bibr" target="#b2">[Caron et al., 2018]</ref> took a set of embedding features and grouped them into different clusters to generate pseudo labels. In <ref type="bibr">GCN-based SSL, Li et al., [Li et al., 2018]</ref> proposed two strategies to train GCN with very few labeled data and showed GCN might result in features within connected component converging to the same value. Sun et al., <ref type="bibr" target="#b14">[Sun et al., 2020]</ref> also used very few labeled data and utilized pseudo labels to expand the label set. Two methods, MultiStage and M3S, were proposed. In MultiStage, it added most confidence vertices with predicted pseudo labels to expand the label set in each stage. M3S additionally utilized algorithm self-checking mechanism to choose nodes with precise pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning with Dual Models</head><p>Knowledge distillation <ref type="bibr" target="#b8">[Hinton et al., 2015]</ref> is mainly used to transfer the good performance of a large model to a small model. Inspired by knowledge distillation, Zhang et al.,  employed two convolutional neural networks with Kullback-Leibler divergence (KLD) for alignment the feature maps. For exploiting complementary of two models, Wu et al.,  designed a subnet to capture complementary information for image classification. Mean Teacher <ref type="bibr" target="#b15">[Tarvainen and Valpola, 2017]</ref> improved performance of Student by a knowledge distillation loss between Student and Teacher while the input of Student is degraded by noise, and Teacher copied the averaged weights of Students. Different from Teacher-Student strategy, we use a mutual teaching strategy. In knowledge distillation <ref type="bibr" target="#b8">[Hinton et al., 2015]</ref> and label smoothing <ref type="bibr" target="#b10">[Müller et al., 2019]</ref>, both of them exploit hard and soft targets. Knowledge distillation directly changes the temperature of softmax layer, while label smoothing directly changes the ground-truth labels. In MT-GCN, both hard and soft targets are used for different purposes, hard targets are used to expand the label set while soft targets are used to each network matches its peer network.</p><p>3 Graph convolutional networks G = (V, E) denotes a graph, its vertex set is denoted by V , and its edge set is E. There are |V | = n vertices in G. Its edge is described by an affinity matrix A = [a ij ] ∈ R n×n and a ij denotes the pairwise connection weight between two vertices. Each vertex corresponds to a data vector x and the data matrix X = [x 1 , x 2 , . . . , x n ] ∈ R n×c has n data points and c input channels.</p><p>In the graph Fourier domain <ref type="bibr" target="#b13">[Shuman et al., 2013]</ref>, Fourier coefficientsx is transformed by a spatial domain signal x, i.e.,x = U x , where U is a Fourier basis. The inverse transform is x = Ux . the Fourier basis of GCN is a matrix of eigenvectors of the normalized Laplacian L =</p><formula xml:id="formula_0">I n − D − 1 2 AD − 1 2 = U ΛU , where Λ is a diagonal ma- trix of eigenvalues of L , D is the degree matrix of A, i.e.,</formula><p>d ii = j a ij , and I n denotes an identity matrix.</p><p>According to convolution theorem, x convolutes a filter h is given by,</p><formula xml:id="formula_1">x ⊗ h = U (ĥ x) = U diag(ĥ)U x<label>(1)</label></formula><p>whereĥ = U h is a vector of Fourier coefficients of a filter h, ⊗ denotes the graph convolution operator, and is the element-wise Hadamard product.</p><p>K-th order Chebyshev approximation of diag(ĥ) <ref type="bibr" target="#b7">[Hammond et al., 2011]</ref> is given by,</p><formula xml:id="formula_2">diag(ĥ) ≈ K i=0 θ i 2Λ λ max − I n i (2)</formula><p>where θ i is the polynomial coefficient and λ max is the largest eigenvalue of L.</p><formula xml:id="formula_3">Since (U ΛU ) i = U Λ i U , we substitute Eq. (2) into Eq. (1), x ⊗ h ≈ K i=0 θ i 2L λ max − I n i x .<label>(3)</label></formula><p>Employing a localized first-order truncated Chebyshev polynomial approximation <ref type="bibr" target="#b3">[Defferrard et al., 2016;</ref><ref type="bibr" target="#b10">Kipf and Welling, 2017]</ref>, Eq. (3) simplifies to,</p><formula xml:id="formula_4">x ⊗ h ≈ θ(I n + D − 1 2 AD − 1 2 )x .<label>(4)</label></formula><p>Kipf and Welling <ref type="bibr" target="#b10">[Kipf and Welling, 2017]</ref> renormalized Thus, given the graphÂ and the matrix X, a prorogation layer of GCN is defined by,</p><formula xml:id="formula_5">I n + D − 1 2 AD − 1 2 toÂ =D − 1 2ÃD − 1 2 withÃ = A + I n and d ii = jã ij .</formula><formula xml:id="formula_6">Z = ReLU(ÂXΘ)<label>(5)</label></formula><p>where Θ ∈ R c×f has f number of filters, Z is the convolved feature matrix, and ReLU(·) = max(0, ·) is the nonlinear activation function.</p><p>In this paper, we use a two-layer GCN model according to <ref type="bibr" target="#b10">[Kipf and Welling, 2017]</ref>,</p><formula xml:id="formula_7">Z =Â ReLU(ÂXΘ 0 )Θ 1 (6) where the output feature map Z = [z ij ] ∈ R n×k is the logit.</formula><p>The prediction probability of a sample x i given by the GCN model is computed as</p><formula xml:id="formula_8">p i = softmax(z i ) (7) = exp(z i ) j exp(z ij )<label>(8)</label></formula><p>where p i is a row vector and the softmax output of the GCN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Mutual teaching GCN</head><p>Suppose that there are k classes in the data matrix X. The labeled data is denoted by</p><formula xml:id="formula_9">D L = {(x i , y ij ), ∀ i ∈ V L , j ∈ [1, k]} and the unlabeled data is D U = {x i , ∀ i ∈ V U }, where V L is the labeled vertices set, V U is the unlabeled set, and V = V L ∪ V U .</formula><p>The goal of SSL is to exploit labeled and unlabeled data to predict the label of the data in V U when the number of labeled data is very few.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Top t High-Confidence Predictions</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, pseudo labelsŷ ij of i ∈ D U easily obtains from p ij by using a one-hot operation,</p><formula xml:id="formula_10">j = arg max([p i1 , p i2 , . . . , p ik ]), y ij = 1<label>(9)</label></formula><p>where the column index j of the maximum of a row i is the pseudo label of the data point x i and then the (i, j)-th element ofŶ is set to 1.</p><p>The prediction confidence c i of a data point x i is assigned to the maximum of the i-th row p i of the matrix P ,</p><formula xml:id="formula_11">c i = max([p i1 , p i2 , . . . , p ik ]) .<label>(10)</label></formula><p>Then, we sort elements in the column vector c = [c 1 ; c 2 ; . . . ; c n ] in descending order, and it returns the index set,</p><formula xml:id="formula_12">q = arg sort(c) = arg sort         c 1 c 2 . . . c n        <label>(11)</label></formula><p>where q returns an index vector of the ordered confidence.</p><p>With the index vector of the ordered confidence, it is easy to obtain top t pseudo labels for each class. Specifically, for each single GCN, i.e., the g-th GCN ∀ g ∈ {1, 2}, we use q to obtain its index set V (g) of the top t pseudo labels for each class as shown in Algorithm 1.</p><p>Algorithm 1 Top t high-confidence predictions for each class.</p><p>1: Input: n, k, t, D U ,ŷ ij and q . 2: Output: </p><formula xml:id="formula_13">V (g) = {idx i , idx 2 , . . . , idx t×k }. 3: Initialize: cnt j ← 0, ∀ j ∈ [1, k] and m ← 1 . 4: for i ∈ {q 1 , q 2 , . . . , q n } do 5: for j ∈ [1, k] do 6: if cnt j &lt;= t &amp;ŷ ij = 1 &amp; i ∈ D U<label>then</label></formula><formula xml:id="formula_14">L sup = − i∈V L k j=1 y ij ln p ij<label>(12)</label></formula><p>where p ij is the (i, j)-th element of P .</p><p>In this paper, we call Eq. (12) as the supervised loss function since it only uses the labeled samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Pseudo-label Loss</head><p>Besides the supervised loss, Eq. (12), we exploit two other loss functions, a pseudo-label loss and a consistency loss. Both the two loss functions use pseudo labels for mutual teaching.</p><p>For two GCN models, we obtain two index sets V (1) and V (2) by using Algorithm 1, respectively. After we obtain the top t high-confidence predictions for each class, we can expand them to the label set for updating model. Besides adding a loss function with the pseudo labels directly, we use information entropy as a measure of uncertainty <ref type="bibr">[Iscen et al., 2019]</ref> to assign a weighted value for each sample x i . Given a probability p i of a sample x i , its certainty w i can be defined by,</p><formula xml:id="formula_15">w i = 1 − H(p i ) log k<label>(13)</label></formula><p>where H(·) is the information entropy.</p><p>Eq. <ref type="formula" target="#formula_1">(13)</ref> shows it tends to zero if all of elements in p i are 1 k and it is assigned to a high value if p i is one-hot. Then, the pseudo-label loss of the first model is defined by,</p><formula xml:id="formula_16">L (1) pl = − 1 |V (2) | i∈V (2) w i k j=1ŷ (2) ij log p (1) ij .<label>(14)</label></formula><p>Similarly, the pseudo-label loss of the second model is given by,</p><formula xml:id="formula_17">L (2) pl = − 1 |V (1) | i∈V (1) w i k j=1ŷ (1) ij log p (2) ij .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Consistency Loss</head><p>The consistency loss function encourages consistency under different network embedding of the same data in each batch. To quantify the prediction consistency of the dual GCN models, we use KLD as the consistency loss. KLD from p</p><p>(1) i and p</p><p>(2) i is given by,</p><formula xml:id="formula_18">L (1) cl = i∈V (2) k j=1 p (2) ij log p (2) ij p (1) ij .<label>(16)</label></formula><p>KLD from p</p><p>(2) i and p</p><p>(1) i is given by,</p><formula xml:id="formula_19">L (2) cl = i∈V (1) k j=1 p (1) ij log p (1) ij p (2) ij .<label>(17)</label></formula><p>While each one learns to match the probability of its peer with consistency loss functions, each network learns to correctly predict true labels with the supervised loss Eq. (12),</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Mutual Teaching</head><p>Mutual teaching approach is formulated by a cohort of dual GCN models.</p><p>In section 4.1, we mainly attain two index sets V (1) and V (2) of the top t high-confidence predictions to expand the label set. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, it is straightforward to check that pseudo-label set can V (1) and V (2) easily obtain.</p><p>In the two loss functions, one network uses an index set from its peer network. The high-confidence predictions of one network teach its peer network to update its model. In mutual teaching, the pseudo-label loss uses hard targetŝ y ij while the consistency loss uses soft targets p ij . Crossupdating dual networks, one network learns knowledge from the peer network. Hard targets mainly expand the label set while the soft targets improve model calibration, which can significantly improve performance.</p><p>The overall loss function of each model is given by,</p><formula xml:id="formula_20">L (g) o = L (g) sup + L (g) pl + L (g) cl , ∀ g ∈ {1, 2} .<label>(18)</label></formula><p>The detailed algorithm of the proposed method is summarized in Algorithm 2. In mutual teaching, we exchange indices of pseudo-label sets. With the pseudo-label loss and the consistency loss, each network teaches knowledge to its peer network. The highconfidence predictions are useful knowledge, so it teaches its peer network with such knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2 Mutual teaching GCNs</head><formula xml:id="formula_21">1: Input: D L , D U , A, N . 2: Output: Z (1) and Z (2) . 3: Initialize:Â =D − 1 2ÃD − 1 2 withÃ = A+I n andD ii = jÃ ij . Θ (1) 0 , Θ (1) 1 , Θ<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we conduct experiments on three popular benchmarks with different label rates to demonstrate the effectiveness of our proposed MT-GCN method. We evaluate the performance by the metric of classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets</head><p>Three widely used citation datasets are used in this paper:</p><p>• Cora: Cora consists of seven classes with 2708 scientific publications and contains 5429 citation links. Each publication is described by a bag-of-words feature, i.e., a 0/1 value vector indicates the absence/presence of a certain word. The feature dimension of a publication of Cora is 1433. We evaluate MT-GCN under different label rates, 0.5%, 1%, 2%, and 3%, i.e., 2, 4, 8, and 12 per class.</p><p>• Citeseer: Citeseer contains 3327 scientific publications which are classified into six classes and has 4732 citation links. The feature dimension of a publication of Citeseer is 3703. We evaluate MT-GCN under different label rates over Citeseer: 0.5%, 1%, 2%, and 3%, i.e., 3, 6, 12, and 18 per class.</p><p>• PubMed: PubMed consists of three classes with 19717 scientific publications and contains 44338 citation links. The feature dimension of a publication of PubMed is 500. We evaluate MT-GCN under different label rates: 0.03%, 0.05%, and 0.1%, i.e., 2, 3, and 7 per class.</p><p>The statistics of these three datasets are summarized in Table 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We compare MT-GCN to following state-of-the-art methods:</p><p>• LP: Label propagation algorithm used ParWalks <ref type="bibr" target="#b16">[Wu et al., 2012]</ref>. Partially absorbing random walk is a secondorder Markov chain with partial absorption at each state.</p><p>• Chebyshev: Chebyshev approach <ref type="bibr" target="#b3">[Defferrard et al., 2016]</ref> used K-th order Chebyshev filter to perform convolutions. The parameter K is set to 2 [Kipf and Welling, 2017].</p><p>• GCN: GCN [Kipf and Welling, 2017] follows a recursive average neighborhood aggregation scheme by stacking two graph convolutional layers. Results of GCN with validation (GCN+V) and GCN without validation (GCN-V) are considered for comparing.</p><p>• Co-training: By employing a pretext random walk model to explore the global structure of the graph, GCN finds high-confidence vertices of a random walk model and adds them to the label set to train a GCN <ref type="bibr" target="#b10">[Li et al., 2018</ref>].</p><p>• Self-training: Different form Co-training, Self-training trains GCN at first and secondly selects its highconfidence predictions to expand the label set. Selftraining continues to train GCN with the expanded label set.</p><p>• Union: Union <ref type="bibr" target="#b10">[Li et al., 2018]</ref> expands the label set by integrating high-confidence predictions found by Cotraining and Self-training, and continues to train the network pre-trained by Self-training with the expanded label set.</p><p>• Intersection: The difference between Union and Intersection <ref type="bibr" target="#b10">[Li et al., 2018]</ref> is that Intersection expands the label set by adding high-confidence predictions found by both co-training and self-training.</p><p>• MultiStage: This method <ref type="bibr" target="#b14">[Sun et al., 2020]</ref> performs multi-stage training. At each stage, it adds highconfidence vertices with predicted pseudo labels to expand the labeled set.</p><p>• M3S: Compared to MultiStage, M3S <ref type="bibr" target="#b14">[Sun et al., 2020]</ref> additionally utilizes DeepCluster <ref type="bibr" target="#b2">[Caron et al., 2018]</ref> to choose nodes with precise pseudo labels.</p><p>Since <ref type="bibr" target="#b10">[Li et al., 2018]</ref> and <ref type="bibr" target="#b14">[Sun et al., 2020]</ref> used very few labeled samples, we mainly compare to their algorithms and their accuracy metrics in our Tables are from their papers, respectively.</p><p>The main difference between these comparisons and MT-GCN is that we use dual models to improve the confidence of the pseudo labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Setting</head><p>For experimental setting of MT-GCN, we use a learning rate 0.01, a dropout rate of 0.5, 2 -norm weight decay 5 × 10 −4 , 16 hidden units without a validation set for fair comparison, and the number of train epochs is N = 400. After the top 200 epoches, we set t to 72, 216, and 975 for Cora, Citeseer, and PubMed as in <ref type="bibr" target="#b10">[Li et al., 2018]</ref>, respectively. Following <ref type="bibr" target="#b10">[Li et al., 2018]</ref>, we report the mean classification accuracy of 30 times on Cora and Citeseer and we average over 10 times for PubMed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results</head><p>With a low labeled rate, MT-GCN propagates label information to the entire graph efficiently. By adding pseudo labels and using the mutual teaching strategy, MT-GCN is verified by comparing mainly with <ref type="bibr" target="#b10">[Li et al., 2018]</ref> and <ref type="bibr" target="#b14">[Sun et al., 2020]</ref>. Compared to theirs, our MT-GCN method makes full use of mutual knowledge during the training process and obtains better results than them.</p><p>Cora: <ref type="table" target="#tab_3">Table 2</ref> reports the mean classification accuracy. As we can see, MT-GCN performs very well and outperforms most other methods by a large margin, especially with a lower label rate. As shown in <ref type="table" target="#tab_3">Table 2</ref>, we can see that our method achieves the best performance and outperform other baselines by a large margin. For instance, with label rate 0.5%, 1%, and 2% MT-GCN improves M3S by 5.4%, 5.9%, and 1.2%, respectively.</p><p>Citeseer: Results of Citeseer are shown in <ref type="table" target="#tab_4">Table 3</ref>. It can be seen from <ref type="table" target="#tab_4">Table 3</ref> that Union obtains the best results among these baselines. For example, with label rate 0.5%, 1%, and 2%, our method improves Union by 21.4%, 9.8%, and 3.1%, respectively, demonstrating the superiority of our method.   PubMed: We report the result on PubMed in <ref type="table" target="#tab_5">Table 4</ref>. We can see that our method achieves the best performance with different label rates. Again, our methods are far better than others with lower label rates. With the label rate 0.03% and 0.05%, the proposed method improve M3S by 6.3% and 3.5%, respectively.  <ref type="bibr" target="#b16">[Weston et al., 2012]</ref>, iterative classification algorithm (ICA) <ref type="bibr" target="#b12">[Sen et al., 2008]</ref>, Planetoid <ref type="bibr" target="#b19">[Yang et al., 2016]</ref> are also included. It can be seen from <ref type="table" target="#tab_6">Table 5</ref> that MT-GCN achieves state-of-the-art performance. With the number of labeled data increases, the performance gap between the proposed MT-GCN method and other variants of GCN becomes small. It implies that the given labeled data is becoming sufficient for training a good GCN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>We propose a new strategy to train SSL GCN models with very few labeled samples, and it can enhance classification accuracy for most SSL algorithms. We train dual models with labeled samples at the beginning then pseudo labels are used for mutual teaching. Besides a supervised loss, two other loss functions are designed to update networks. A network produces pseudo labels and the other network uses the pseudo labels produced by its peer network. With the two loss functions, one network is updated with the expanded label set from its peer network.</p><p>With very few labeled samples, we obtain higher metrics than other state-of-the-art methods. Different from MT-GCN, most GCN-based methods only train network with labeled samples, which may result in the network fits unlabeled data and classification performance degrades. Different from them, we present a simple but effective graph-based SSL method, MT-GCN, which trains GCNs under extreme a low label rate, i.e., very low labeled samples per class. The idea behind MT-GCN is to maintain two GCNs simultaneously and exploits mutual knowledge between them. The mutual teaching process is accomplished by selecting the top t pseudo labels for each class and adding them to enlarge the labeled data set. Experimental results on three popular datasets demonstrate the effectiveness of our method when given very few labeled data.</p><p>In the future, we will extend the strategy of mutual teaching to other domains such as image classification, sentence classification, few-shot learning, and so on. Contrastive learning can be combined with the mutual teaching strategy since contrastive learning supervised by the consistency loss in different inputs. In ML-GCN, Since the different initialization of the two layers and the dropout, predictions of the two networks are different.</p><p>7 Acknowledgment</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A simple example of mutual teaching algorithm in dual models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>For semi-supervised multi-class classification,<ref type="bibr" target="#b10">Kipf and Welling [Kipf and Welling, 2017]</ref> evaluated the cross-entropy loss over the labeled data set D L ,</figDesc><table><row><cell>4.2 Supervised Loss</cell><cell></cell></row><row><cell>7:</cell><cell>idx m ← i .</cell></row><row><cell>8:</cell><cell>m ← m + 1 .</cell></row><row><cell>9:</cell><cell>cnt j ← cnt j + 1 .</cell></row><row><cell>10:</cell><cell>end if</cell></row><row><cell>11:</cell><cell>end for</cell></row><row><cell cols="2">12: end for</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Datasets statistics.</figDesc><table><row><cell>Dataset</cell><cell cols="4">Nodes Edges Classes Dimensions</cell></row><row><cell>Cora</cell><cell>2708</cell><cell>5429</cell><cell>7</cell><cell>1433</cell></row><row><cell>Citeseer</cell><cell>3327</cell><cell>4732</cell><cell>6</cell><cell>3703</cell></row><row><cell cols="3">PubMed 19717 44338</cell><cell>3</cell><cell>500</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Accuracy comparisons between the proposed MT-GCN and other state-of-the-art algorithms on Cora.</figDesc><table><row><cell>Labeled per class</cell><cell>2</cell><cell>4</cell><cell>8</cell><cell>12</cell></row><row><cell>Label rate</cell><cell cols="2">0.5% 1%</cell><cell>2%</cell><cell>3%</cell></row><row><cell>LP</cell><cell cols="4">56.4 62.3 65.4 67.5</cell></row><row><cell>Chebyshev</cell><cell cols="4">38.0 52.0 62.4 70.8</cell></row><row><cell>GCN-V</cell><cell cols="4">42.6 56.9 67.8 74.9</cell></row><row><cell>GCN+V</cell><cell cols="4">50.9 62.3 72.2 76.5</cell></row><row><cell>Co-training</cell><cell cols="4">56.6 66.4 73.5 75.9</cell></row><row><cell>Self-training</cell><cell cols="4">53.7 66.1 73.8 77.2</cell></row><row><cell>Union</cell><cell cols="4">58.5 69.9 75.9 78.5</cell></row><row><cell>Intersection</cell><cell cols="4">49.7 65.0 72.9 77.1</cell></row><row><cell>MultiStage</cell><cell cols="4">61.1 63.7 74.4 76.1</cell></row><row><cell>M3S</cell><cell cols="4">61.5 67.2 75.6 77.8</cell></row><row><cell>MT-GCN</cell><cell cols="4">66.9 73.1 76.8 78.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="5">: Accuracy comparisons between the proposed MT-GCN and</cell></row><row><cell cols="3">other state-of-the-art algorithms on Citeseer.</cell><cell></cell><cell></cell></row><row><cell>Labeled per class</cell><cell>3</cell><cell>6</cell><cell>12</cell><cell>18</cell></row><row><cell>Label rate</cell><cell cols="2">0.5% 1%</cell><cell>2%</cell><cell>3%</cell></row><row><cell>LP</cell><cell cols="4">34.8 40.2 43.6 45.3</cell></row><row><cell>Chebyshev</cell><cell cols="4">31.7 42.8 59.9 66.2</cell></row><row><cell>GCN-V</cell><cell cols="4">33.4 46.5 62.6 66.9</cell></row><row><cell>GCN+V</cell><cell cols="4">43.6 55.3 64.9 67.5</cell></row><row><cell>Co-training</cell><cell cols="4">47.3 55.7 62.1 62.5</cell></row><row><cell>Self-training</cell><cell cols="4">43.3 58.1 68.2 69.8</cell></row><row><cell>Union</cell><cell cols="4">46.3 59.1 66.7 66.7</cell></row><row><cell>Intersection</cell><cell cols="4">42.9 59.1 68.6 70.1</cell></row><row><cell>MultiStage</cell><cell cols="4">53.0 57.8 63.8 68.0</cell></row><row><cell>M3S</cell><cell cols="4">56.1 62.1 66.4 70.3</cell></row><row><cell>MT-GCN</cell><cell cols="4">67.7 68.9 69.1 69.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Accuracy comparisons between the proposed MT-GCN and other state-of-the-art algorithms on PubMed.</figDesc><table><row><cell>Labeled per class</cell><cell>2</cell><cell>3</cell><cell>7</cell></row><row><cell>Label rate</cell><cell cols="3">0.03% 0.05% 0.1%</cell></row><row><cell>LP</cell><cell>61.4</cell><cell>66.4</cell><cell>65.4</cell></row><row><cell>Chebyshev</cell><cell>40.4</cell><cell>47.3</cell><cell>51.2</cell></row><row><cell>GCN-V</cell><cell>46.4</cell><cell>49.7</cell><cell>56.3</cell></row><row><cell>GCN+V</cell><cell>60.5</cell><cell>57.5</cell><cell>65.9</cell></row><row><cell>Co-training</cell><cell>62.2</cell><cell>68.3</cell><cell>72.7</cell></row><row><cell>Self-training</cell><cell>51.9</cell><cell>58.7</cell><cell>66.8</cell></row><row><cell>Union</cell><cell>58.4</cell><cell>64.0</cell><cell>70.7</cell></row><row><cell>Intersection</cell><cell>52.0</cell><cell>59.3</cell><cell>69.4</cell></row><row><cell>MultiStage</cell><cell>57.4</cell><cell>64.3</cell><cell>70.2</cell></row><row><cell>M3S</cell><cell>59.2</cell><cell>64.4</cell><cell>70.6</cell></row><row><cell>MT-GCN</cell><cell>65.5</cell><cell>69.5</cell><cell>73.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Accuracy comparisons between the proposed MT-GCN and other state-of-the-art algorithms under 20 labels per Class.</figDesc><table><row><cell>Dataset</cell><cell cols="3">Cora Citeseer PubMed</cell></row><row><cell>ManiReg</cell><cell>59.5</cell><cell>60.1</cell><cell>70.7</cell></row><row><cell>SemiEmb</cell><cell>59.0</cell><cell>59.6</cell><cell>71.7</cell></row><row><cell>LP</cell><cell>68.0</cell><cell>45.3</cell><cell>63.0</cell></row><row><cell>DeepWalk</cell><cell>67.2</cell><cell>43.2</cell><cell>65.3</cell></row><row><cell>ICA</cell><cell>75.1</cell><cell>69.1</cell><cell>73.9</cell></row><row><cell>Planetoid</cell><cell>75.7</cell><cell>64.7</cell><cell>77.2</cell></row><row><cell>GCN-V</cell><cell>80.0</cell><cell>68.1</cell><cell>78.2</cell></row><row><cell>GCN+V</cell><cell>80.3</cell><cell>68.9</cell><cell>79.1</cell></row><row><cell>Co-training</cell><cell>79.6</cell><cell>64.0</cell><cell>77.1</cell></row><row><cell cols="2">Self-training 80.2</cell><cell>67.8</cell><cell>76.9</cell></row><row><cell>Union</cell><cell>80.5</cell><cell>65.7</cell><cell>78.3</cell></row><row><cell>Intersection</cell><cell>79.8</cell><cell>69.9</cell><cell>77.0</cell></row><row><cell>MT-GCN</cell><cell>80.9</cell><cell>69.8</cell><cell>79.5</cell></row><row><cell cols="4">5.5 Comparison of 20 labeled samples per class</cell></row><row><cell cols="4">Since most of the GCN-based SSL algorithms use 20 la-</cell></row><row><cell cols="4">beled samples per class, we compare MT-GCN with the</cell></row><row><cell cols="4">other state-of-the-art methods in Table 5. The experimental</cell></row><row><cell cols="4">setup is that we sample 20 labels for each class in all three</cell></row><row><cell cols="4">datasets. The results of these baselines copied from [Kipf and</cell></row><row><cell cols="4">Welling, 2017] and [Li et al., 2018]. Besides the above meth-</cell></row><row><cell cols="4">ods, DeepWalk [Perozzi et al., 2014], manifold regularization</cell></row><row><cell cols="4">(ManiReg) [Belkin et al., 2006], semi-supervised embedding</cell></row><row><cell>(SemiEmb)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This work has been supported by the National Science Foundation of China under the Grant No. 61201422.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cristian Bucilua, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bucilua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Caron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2002-05-02" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Defferrard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
	<note>Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Doersch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1422" to="1430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Scaling and benchmarking selfsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.01235</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hammond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Label propagation for deep semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Giorgos Tolias, Yannis Avrithis, and Ondrej Chum</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="5070" to="5079" />
		</imprint>
	</monogr>
	<note>Proc. CVPR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00388,2020.2.1</idno>
		<title level="m">A survey on knowledge graphs: Representation, acquisition and applications</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeper insights into graph convolutional networks for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02629</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2475" to="2487" />
		</imprint>
	</monogr>
	<note type="report_type">When does label smoothing help? arXiv preprint</note>
	<note>Learning graph embedding with adversarial training methods</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. KDD</title>
		<meeting>KDD</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Shuman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-stage self-supervised learning for graph convolutional networks on graphs with few labeled nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI, 2020</title>
		<meeting>AAAI, 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>. 2.2, 5.2, 5.4</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Veličković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1195" to="1204" />
		</imprint>
	</monogr>
	<note>Proc. ICLR</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Mutual learning of complementary networks via residual correction for improving semisupervised classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="6500" to="6509" />
		</imprint>
	</monogr>
	<note>Proc. CVPR</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptive graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020</title>
		<meeting>The Web Conference 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
	<note>A comprehensive survey on graph neural networks</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="4320" to="4328" />
		</imprint>
	</monogr>
	<note>Proc. CVPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
