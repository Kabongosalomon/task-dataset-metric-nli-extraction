<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Viewpoints and Keypoints</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shubham</forename><surname>Tulsiani</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley -Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
							<email>malik@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<postCode>94720</postCode>
									<settlement>Berkeley -Berkeley</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Viewpoints and Keypoints</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We characterize the problem of pose estimation for rigid objects in terms of determining viewpoint to explain coarse pose and keypoint prediction to capture the finer details. We address both these tasks in two different settings -the constrained setting with known bounding boxes and the more challenging detection setting where the aim is to simultaneously detect and correctly estimate pose of objects. We present Convolutional Neural Network based architectures for these and demonstrate that leveraging viewpoint estimates can substantially improve local appearance based keypoint predictions. In addition to achieving significant improvements over state-of-the-art in the above tasks, we analyze the error modes and effect of object characteristics on performance to guide future efforts towards this goal.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There are two ways in which one can describe the pose of the car in <ref type="figure" target="#fig_0">Figure 1</ref> -either via its viewpoint or via specifying the locations of a fixed set of keypoints. The former characterization provides a global perspective about the object whereas the latter provides a more local one. In this work, we aim to reliably predict both these representations of pose for objects.</p><p>Our overall approach is motivated by the theory of global precedence -that humans perceive the global structure before the fine level local details <ref type="bibr" target="#b26">[27]</ref>. It was also noted by Koenderink and van Doorn <ref type="bibr" target="#b21">[22]</ref> that viewpoint determines appearance and several works have shown that larger wholes improve the discrimination performance of parts <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref>. Inspired by this philosophy, we propose an algorithm which first estimates viewpoint for the target object and leverages the predicted viewpoint to improve the local appearance based keypoint predictions.</p><p>Viewpoint is manifested in a 2D image by the spatial relationships among the different features of the object. Convolutional Neural Network (CNN) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref> based methods which can implicitly capture and hierarchically build on such relations are therefore suitable candidates for view- A robot which merely knows that a cup exists but cannot find its handle will not be able to grasp it. Towards the goal of developing a finer understanding of objects, we tackle the task of predicting keypoints by modeling appearances at multiple scales -a fine scale appearance model, while prone to false positives can localize accurately and a coarser scale appearance model is more robust to mis-localizations. Note that merely reasoning over local appearance is not sufficient to solve the task of keypoint prediction. For example, the notion of the 'front wheel' assumes its meaning in context of the whole bicycle. The local appearance of the patch might also correspond to the 'back wheel' -it is because we know the bicycle is front facing that we are able to disambiguate. Motivated by this, we use the viewpoint predicted by our system to improve the local appearance based keypoint predictions.</p><p>Our proposed algorithm, as illustrated in <ref type="figure">Figure 2</ref> has the following components -Viewpoint Prediction : We formulate the problem of viewpoint prediction as predicting three euler angles ( azimuth, elevation and cyclorotation) corresponding to the instance. We train a CNN based architecture which can implicitly capture and aggregate local evidences for predicting the euler angles to obtain a viewpoint estimate.</p><p>Local Appearance based Keypoint Activation : We propose a fully convolutional CNN based architecture to model local part appearance. We capture the appearance at multiple scales and combine the CNN responses across scales to obtain a resulting heatmap which corresponds to a spatial log-likelihood distribution for each keypoint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 2:</head><p>Overview of our approach. To recover an estimate of the global pose, we use a CNN based architecture to predict viewpoint. For each keypoint, a spatial likelihood map is obtained via combining multiscale convolutional response maps and it is then combined with a likelihood conditioned on predicted viewpoint to obtain our final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Viewpoint Conditioned Keypoint Likelihood :</head><p>We propose a viewpoint conditioned keypoint likelihood, implemented as a non-parametric mixture of gaussians, to model the probability distribution of keypoints given the viewpoint prediction. We combine it with the appearance based likelihood computed above to obtain our keypoint predictions.</p><p>Keypoint prediction methods have traditionally been evaluated assuming ground-truth boxes as input <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref>. This means that the evaluation setting is quite different from the conditions under which these methods would be usedin conjunction with imprecisely localized object detections. Yang and Ramanan <ref type="bibr" target="#b37">[38]</ref> argued for the importance of this task for human pose estimation and introduced an evaluation criterion which we adapt to generic object categories. To the best of our knowledge, we are the first to empirically evaluate the applicability of a keypoint prediction algorithm not restricted to a specific object category in this challenging setting.</p><p>Furthermore, inspired by the analysis of the detection methods presented by Hoeim et al. <ref type="bibr" target="#b17">[18]</ref>, we present an analysis of our algorithm's failure modes as well as the impact of object characteristics on the algorithm's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work Viewpoint Prediction:</head><p>Recently, CNNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24]</ref> have been shown to outperform Deformable Part Model (DPM) <ref type="bibr" target="#b7">[8]</ref> based methods for recognition tasks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23]</ref>. Whereas DPMs explicitly model part appearances and their deformations, the CNN architecture allows such relations to be captured implicitly using a hierarchical convolutional structure. Girshick et al. <ref type="bibr" target="#b11">[12]</ref> argued that DPMs could also be thought as a specific instantiation of CNNs and therefore training an end-to-end CNN for the corresponding task should outperform a method which instead explicitly models part appearances and relations.</p><p>This result is particularly applicable to viewpoint estimation where the prominent approaches, from the initial instance based methods <ref type="bibr" target="#b18">[19]</ref> to current state-of-the-art <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b29">30]</ref> explicitly model local appearances and aggregate evidence to infer viewpoint. Pepik et al. <ref type="bibr" target="#b29">[30]</ref> extend DPMs to 3D to model part appearances and rely on these to infer pose and Xiang et al. <ref type="bibr" target="#b36">[37]</ref> introduce a separate DPM component corresponding to each viewpoint. Ghodrati et al. <ref type="bibr" target="#b9">[10]</ref> differ from the explicit part-based methodology, using a fixed global descriptor to estimate viewpoint. We build on both these approaches by using a method which, while using a global descriptor, can implicitly capture part appearances.</p><p>Keypoint Prediction: Keypoint Prediction can be classified into two settings -a) 'Keypoint Localization' where the task is to find keypoints for objects with known bounding boxes and b) 'Keypoint Detection' where bounding box is unknown. This problem has been particularly well studied for humans -tracing back from classic model-based approaches for video <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b16">17]</ref> to more recent pictorial structure based approaches <ref type="bibr" target="#b37">[38]</ref> on challenging single image based real world datasets like LSP <ref type="bibr" target="#b20">[21]</ref> or MPII Human Pose <ref type="bibr" target="#b0">[1]</ref>. Recently Toshev et al. <ref type="bibr" target="#b35">[36]</ref> demonstrated that CNN based models can successfully be used for keypoint prediction for humans and Tompson et al. <ref type="bibr" target="#b34">[35]</ref> significantly improved upon these results using a purely convolutional approach. These evaluations, however, are restricted to keypoint localizations. A more general task of keypoint detection without assuming ground truth box annotations was also recently introduced for humans by Yang and Ramanan <ref type="bibr" target="#b37">[38]</ref> and Gkioxari et al. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref> evaluated their keypoint prediction algorithm in this setting.</p><p>For generic object categories, annotations for keypoints on the challenging PASCAL VOC dataset <ref type="bibr" target="#b6">[7]</ref> were introduced by Bourdev et al. <ref type="bibr" target="#b3">[4]</ref>. Though similar annotations or fitted CAD models have been successfully used to train better object detection systems <ref type="bibr" target="#b2">[3]</ref> as well as for simultaneous object detection and viewpoint estimation <ref type="bibr" target="#b29">[30]</ref>, the task of keypoint prediction has largely been unaddressed for generic object categories. Long et al. <ref type="bibr" target="#b24">[25]</ref> recently evaluated keypoint localization results across all PASCAL categories but, to the best of our knowledge, the more general setting of keypoint detection for generic object categories has not yet been explored.</p><p>Previous works <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref> have also jointly tackled the problem of keypoint detection and pose estimation. While these are perhaps the closest to ours in terms of goals, they differ markedly in methodology -they explicitly aggregate local evidence for pose estimation and have either been restricted to a specific object category <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> or use instance model based matching <ref type="bibr" target="#b31">[32]</ref>. Long et al. <ref type="bibr" target="#b24">[25]</ref>, on the other hand share many commonalities with our methodology for the task of keypoint prediction -convolutional keypoint detections augmented with global priors to predict keypoints. However, we show that we can significantly improve their results by combining multiscale convolutional predictions from a trained CNN with a more principled, viewpoint estimation based global model. Both <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> only evaluate keypoint localization performance whereas we also evaluate our method in the setting of keypoint detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Viewpoint Estimation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formulation</head><p>We formulate the global pose estimation for rigid categories as predicting the viewpoint wrt to a canonical pose. This is equivalent to determining the three euler angles corresponding to azimuth (φ), elevation(ϕ) and cyclorotation(ψ). We frame the task of predicting the euler angles as a classification problem where the classes {1, . . . N θ } correspond to N θ disjoint angular bins. We note that the euler angles, and therefore every viewpoint, can be equivalently described by a rotation matrix. We will use the notion of viewpoints, euler angles and rotation matrices interchangeably.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture and Training</head><p>Let N c be the number of object classes, N a be number of angles to be predicted per instance. The number of output units per class is N a * N θ resulting in a total of N c * N a * N θ outputs. We adopt an approach similar to Girshick et al. <ref type="bibr" target="#b10">[11]</ref> and finetune a CNN model whose weights are initialized from a model pretrained on the Imagenet [5] classification task. We experimented with the architectures from Krizhevsky et al. <ref type="bibr" target="#b22">[23]</ref> (denoted as TNet) and Simonyan et al. <ref type="bibr" target="#b32">[33]</ref> (denoted as ONet). The architecture of our network is the same as the corresponding pre-trained network with an additional fully-connected layer having N c * N a * N θ output units. We provide an alternate detailed visualization of the network architecture in the supplementary material.</p><p>Instead of training a separate CNN for each class, we implement a loss layer that selectively considers the N a * N θ outputs corresponding the class of the training instance and computes a logistic loss for each of the angle predictions. This allows us to train a CNN which can jointly predict viewpoint for all classes, thus enabling learning a shared feature representation across all categories. We use the Caffe framework <ref type="bibr" target="#b19">[20]</ref> to train and extract features from the CNN described above. We augment the training data with jittered ground-truth bounding boxes that overlap with the annotated bounding box with IoU &gt; 0.7. Xiang et al. <ref type="bibr" target="#b36">[37]</ref> provide annotations for (φ, ϕ, ψ) corresponding to all the instances in the PASCAL VOC 2012 detection train, validation set as well as for ImageNet images. We use the PASCAL train set and the ImageNet annotations to train the network described above and use the PASCAL VOC 2012 validation set annotations to evaluate our performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Viewpoint Informed Keypoint Prediction</head><p>As we noted earlier, parts assume their meaning in context of the whole. Thus, in addition to local appearance, we should take into account the global context. To operationalize this observation, we propose a two-component approach to keypoint prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multiscale Convolutional Response Maps</head><p>We use CNN based architectures to learn the appearance of keypoints across an object class. Using a fully convolutional architecture allows us to capture local appearance in a more hierarchical and robust way than HOG feature based models while still allowing for efficient inference by sharing computations across evaluations at different spatial locations in the same image.</p><p>Let C denote the set of classes, K c denote the set of keypoints for class c and N c = |K c |. The total number of keypoints N kp is therefore c∈C N c . We train a fully convolutional network with an input size (384 × 384) such that the channels in its last layer correspond to the keypoints i.e. we use a loss which forces the channels in the last layer to only fire at positions which correspond to the locations of the respective keypoint. The CNN architecture we use has the convolutional layers from ONet followed by an additional convolution layer with the output size 12 × 12 × N kp such that each channel of the output corresponds to a specific keypoint of a particular class.</p><p>The architecture enforces that the receptive field of an output unit in the location (i, j) has a centre corresponding to (32 * i, 32 * j) in the input image. For each training instance with annotated keypoints with locations</p><formula xml:id="formula_0">{(x k , y k )|k ∈ K c }, we construct a target response map T with T (k i , k j , k) = 1 and zero otherwise (where (k i , k j )</formula><p>is the index of the unit with its receptive field's centre closest to the annotated keypoint). For each keypoint, this is similar to training with multiple classification examples per image centered at the repective fields of output units, akin to the formulation used for object detection by Szegedy et al. <ref type="bibr" target="#b33">[34]</ref>. Similar to the details described in section 3.2, we use a loss layer that only selects the channels corresponding to the instance class and implements a euclidean loss between the output and the target map, thus enabling us to jointly train a single network to predict keypoints for all classes. We train using the annotations from Bourdev et al. <ref type="bibr" target="#b3">[4]</ref> and use ground truth and jittered boxes as training examples.</p><p>The above network captures the appearance of the keypoints at a particular scale. A coarser scale would be more robust to false positives as it captures more context but would not be able to localize well. In order to benefit from the predictions at a coarser level, without compromising localization, we propose using a multiscale ensemble of networks. We therefore train another network with exactly the same architecture with a smaller input size (192 × 192) and a smaller output size 6 × 6 × N kp . We upsample the outputs of the smaller network and linearly combine them with the outputs of the larger network to get a spatial log-likelihood response map L(·, ·, k) for each keypoint k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Viewpoint Conditioned Keypoint Likelihood</head><p>If we know that a particular car is left-facing, we'd expect its left wheels to be visible but not the right wheels. In addition to the ability to predict visibility, we'd also have a strong intuition about the approximate locations of the keypoints. If the problem setting was restricted to a particular instance, the exact locations of the keypoints could be inferred geometrically from the exact global pose. However, the two assumptions that would allow this approach do not hold true -we have to deal with different instances of the object category and our inferred global pose would only be approximate. To counter this, we propose a nonparametric solution -we would expect the keypoints of a given instance to lie at positions similar to other training instances whose global pose is close to the predicted global pose for the given instance.</p><p>Let the training instances for class c be denoted by</p><formula xml:id="formula_1">{R i , {(x i k , y i k )|k ∈ K c }}</formula><p>where R i is the rotation matrix and {(x i k , y i k )|k ∈ K c } the annotated keypoints corresponding to the i th instance. Let R be the predicted rotation matrix corresponding to which we want a prior for keypoint locations denoted by P st P (i, j, k) indicates the likelihood of keypoint k being present at loca-</p><formula xml:id="formula_2">tion (i, j). Let ∆(R 1 , R 2 ) = log(R T 1 R2) F √ 2</formula><p>denote the geodesic distance between rotation matrices R 1 , R 2 and N (R) = {i|∆(R, R i ) &lt; π 6 } represent the the training instances whose viewpoint is close to the predicted viewpoint. Our non-parametric global pose conditional likelihood (P ) is defined as a mixture of gaussians and we combine it with the local appearance likelihood (L) to get keypoint locations as follows -</p><formula xml:id="formula_3">P (·, ·, k) = 1 |N (R)| i∈N (R) N ((x i k , y i k ), σI) (1) (x k , y k ) = argmax y,x log(P (x, y, k)) + L(x, y, k)<label>(2)</label></formula><p>Note that all the coordinates above are normalized by warping the instance bounding box to a fixed size (12 × 12) and we choose σ = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments : Viewpoint Prediction</head><p>In this section, we use the the PASCAL3D+ <ref type="bibr" target="#b36">[37]</ref> annotations to evaluate the viewpoint estimation performance of our approach in the two different settings described below -</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Viewpoint Estimation with Ground Truth box</head><p>To analyze the performance of our viewpoint estimation method independent of factors like mis-localization, we first tackle the task of estimating the viewpoint of an object with known bounds. Let ∆(R 1 , R 2 ) = log(R T 1 R2) F √ 2 denote the geodesic distance function over the manifold of rotation matrices. ∆(R gt , R pred ) captures the difference between ground truth viewpoint R gt and predicted viewpoint R pred . We use two complementary metrics for evaluation -• Median Error : The common confusions for the task of viewpoint estimation often are predictions which are far apart (eg. left facing vs right facing car) and the median error (M edErr) is a widely use metric that is robust to these if a significant fraction of the estimates are accurate.</p><p>• Accuracy at θ : A small median error does not necessarily imply accurate estimates for all instances, a complementary performance measure is the fraction of instances whose predicted viewpoint is within a fixed threshold of the target viewpoint. We denote this metric by Acc θ where θ is the threshold. We use θ = π <ref type="figure">Figure 3</ref>: Viewpoint predictions for unoccluded groundtruth instances using our algorithm. The columns show 15th, 30th, 45th, 60th, 75th and 90th percentile instances respectively in terms of the error. We visualize the predictions by rendering a 3D model using our predicted viewpoint.  Recently, Ghodrati et al. <ref type="bibr" target="#b9">[10]</ref> achieved results comparable to state-of-the art by using a linear classifier over layer 5 features of TNet. We denote this method as 'Pool5-TNet' and implement it as a baseline. To study the effect of endto-end training of the CNN architecture, we use a linear classifier on top of the fc7 layer of TNet as another baseline (denoted as 'fc7-TNet' ). With the aim of analyzing viewpoint estimation independently, the evaluations were restricted only to objects marked as non-occluded and nontruncated and we defer the study of the effects of occlusion/truncation in this setting to section 7.1. The performance of our method and comparisons to the baseline are shown in <ref type="table">Table 2</ref>. The results clearly demonstrate that endto-end training improves results and that our method with the TNet architecture performs significantly better than the 'Pool5-TNet' method used in <ref type="bibr" target="#b9">[10]</ref>. We also observe a significant improvement by using the ONet architecture and only use this architecture for further experiments/analysis. In <ref type="figure">figure 3</ref>, we show our predictions sorted in terms of the error and it can be seen that the predictions for most categories are reliable even at the 90th percentile.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Detection and Viewpoint Estimation</head><p>Xiang et al. <ref type="bibr" target="#b36">[37]</ref> introduced the AV P metric to measure advances in the task of viewpoint estimation in the setting where localizations are not known a priori. The metric is similar to the AP criterion used for PASCAL VOC detection except that each detection candidate has an associated viewpoint and the detection is labeled correct if it has a correct predicted viewpoint bin as well as a correct localization (bounding box IoU &gt; 0.5). Xiang et al. <ref type="bibr" target="#b36">[37]</ref> also compared to Pepik et al. <ref type="bibr" target="#b29">[30]</ref> on the AVP metric using various viewpoint bin sizes and Ghodrati et al. <ref type="bibr" target="#b9">[10]</ref> also showed comparable results on the metric. To evaluate our method, we obtain detections from RCNN <ref type="bibr" target="#b10">[11]</ref> using MCG <ref type="bibr" target="#b1">[2]</ref> object proposals and augment them with a pose predicted using the corresponding detection's bounding box. We note that there are two issues with the AV P metric -it only evaluates the prediction for the azimuth (φ) angle and discretizes viewpoint instead of treating it continuously. Therefore, we also introduce two additional evaluation metrics which follow the IoU &gt; 0.5 criteria for localization but modify the criteria for assigning a viewpoint prediction to be correct as follows -</p><formula xml:id="formula_4">• AV P θ : δ(φ gt , φ pred ) &lt; θ • ARP θ : ∆(R gt , R pred ) &lt; θ</formula><p>Note that ARP θ requires the prediction of all euler angles instead of just φ and therefore, is a stricter metric.</p><p>The performance of our CNN based approach for viewpoint prediction is shown in <ref type="table">Table 2</ref> and it can be seen that we significantly outperform the state-of-the-art methods across all categories. While it is not possible to compare our pose estimation performance independent of detection with DPM based methods like <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b29">30]</ref>, an indirect comparison results from the analysis using ground truth boxes where we demonstrate that our pose estimation approach is an improvement over <ref type="bibr" target="#b9">[10]</ref> which in turn performs similar to <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b29">30]</ref>   <ref type="table">Table 2</ref>: Mean performance of our approach for various metrics. We report the performance for individual classes with the supplementary material</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments : Keypoint Prediction</head><p>The task of keypoint prediction is commonly studied in the setting with known location of the object but some methods, restricted to specific categories like 'people' recently evaluated their performance in the more general detection setting. We extend these metrics to generic categories and evaluate our predictions in both the settings using the following metrics proposed by Yang and Ramanan [38] -• PCK (Keypoint Localization) : For each annotated instance, the algorithm predicts a location for each keypoint and a groundtruth keypoint is said to have been found correctly if the corresponding prediction lies within α * max(h, w) of the annotated keypoint with the corresponding object's dimension being (h, w).</p><p>For each keypoint, we measure the fraction of objects where it was found correctly.</p><p>• APK (Keypoint Detection) : A keypoint candidate is deemed correct if it lies within α * max(h, w) of a groundtruth keypoint. Each keypoint hypothesis has an associated score and the area under the precisionrecall curve is used as the evaluation criterion.</p><p>We use the keypoint annotations from <ref type="bibr" target="#b3">[4]</ref> and use the PAS-CAL VOC train set for training and the validation set images for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Keypoint Localization</head><p>The performance of our system and comparison to <ref type="bibr" target="#b24">[25]</ref> are shown in <ref type="table" target="#tab_4">Table 3</ref>. We denote by 'conv6' ('conv12') the predictions using only the 6 × 6 (12 × 12) output size network, by 'conv6+conv12' the predictions using the multiscale convolutional response and by 'conv6+conv12+pLikelihood' the predictions using our full system. Our baseline system ( 'conv6+conv12') performs much better than <ref type="bibr" target="#b24">[25]</ref>, indicating the importance of end-toend training and multiscale response maps. We also see that incorporating the viewpoint conditioned likelihood induces a significant performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Keypoint Detection</head><p>Given an image, we use RCNN <ref type="bibr" target="#b10">[11]</ref> combined with MCG <ref type="bibr" target="#b1">[2]</ref> object proposals to obtain detection candidates, each comprising of a class label and location. We then predict keypoints on each candidate using our system and score each keypoint hypothesis by linearly combining the keypoint log-likelihood score and the object detection system score. Our results for the task of keypoint detection are summarized in <ref type="table" target="#tab_8">Table 4</ref>. The pose conditioned likelihood consistently improves the local appearance based predictions. Though the task of keypoint detection on PASCAL VOC has not yet been analyzed for categories other than person, we believe our results of 33.2% mean APK with a reasonably strict threshold indicate a promising start.</p><p>The above results support our three main assertions -a global prior obtained in the form of a viewpoint conditioned likelihood improves the local appearance based predictions, that end-to-end trained CNNs can effectively model part appearances and combining responses from multiple scales significantly improves performance.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Generalization to Articulated Pose</head><p>While the focus of our work is pose prediction for rigid objects, we note that our multiscale convolutional response based approach is also applicable for articulated pose estimation. To demonstrate this, we trained our convolutional response map system to detect keypoints for the category 'person' in PASCAL VOC 2012 and achieved an APK = 0.22 which is a significant improvement compared to the state-of-the-art method <ref type="bibr" target="#b12">[13]</ref> which achieves APK = 0.15. We refer the reader to <ref type="bibr" target="#b12">[13]</ref> for further details on the evaluation metrics for the task of articulated pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Analysis</head><p>An understanding of failure cases and effect of object characteristics on performance can often suggest insights for future directions. Hoeim et al. <ref type="bibr" target="#b17">[18]</ref> suggested some excellent diagnostics for object detection systems and we adapt those for the task of pose estimation. We evaluate our system's output for both the task of viewpoint prediction as well as keypoint prediction but restrict our analysis to the setting with known bounding boxes -this enables     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Viewpoint Prediction</head><p>Object Characteristics : <ref type="table" target="#tab_6">Table 5</ref> shows the effect of object characteristics by reporting the mean across the classes of the median viewpoint error and accuracy. We can see that the method performs worse for occluded objects. There is also a significant difference between the performance for small and large objects -while such error trends are acceptable in the robotic setting where ambiguity for the farther objects is tolerable, one may need to capture more context to perform well without higher resolution input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Error Modes:</head><p>Since it is difficult to characterize error modes for generic rotations, we restrict the analysis to only the predicted azimuth. Assuming the image plane to be XY, we denote by Z − ref the pose for the instance reflected along the XY plane and by π −f lip a rotation of π along the Z axis. <ref type="table" target="#tab_7">Table 6</ref> reports the percentage of instances whose predicted pose corresponds to various modes. We observe that these error modes are equally common and that only about 3% of the errors are not explained by these.</p><p>Note that we exclude 'diningtable' and 'bottle' categories from the above analysis due to small number of unoccluded instances and insignificant variations respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Keypoint Prediction</head><p>We use the PCK metric (section 6.2) to characterize our algorithm's performance for various settings. Our results using the full method (local appearance combined with viewpoint conditioned likelihood) are reported in <ref type="table" target="#tab_9">Table 7</ref>. We report the analysis using various components (single scale prediction, purely local appearance etc.) in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Characteristics :</head><p>The effect of object characteristics is similar to the viewpoint prediction setting -occluded objects are not handled well and there is a significant performance gap between small and large objects.</p><p>Error Modes : In the 'left/right' setting, we label a prediction to be correct if it was in the vicinity of the corresponding or the laterally inverted keypoint. Surprisingly, the performance is similar to the base performance -indicating that laterally symmetric keypoints are not a significant error mode. The difference between the base performance and P CK[α = 0.2] analyzes the inaccurate localizations which we find to be the main source of error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We have presented an algorithm which leverages CNN architectures to predict viewpoint, and combines multiscale appearance with a viewpoint conditioned likelihood to predict keypoints. We demonstrated that our approach significantly improve state-of-the-art in settings with and without annotated bounding boxes for both viewpoint and keypoint prediction tasks. We also present evaluations for the keypoint detection setting alongwith a detailed ablation study of our performance on various tasks and hope that these will contribute towards progress on the task of pose estimation for generic objects. We will make our code and trained models publicly available.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Alternate characterizations of pose in terms of viewpoint and keypoint locations point prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of keypoints predicted in the detection setting. We visualize every 15 th detection, sorted by score, for 'Nosetip' of aeroplanes, 'Crankcentre' of bicycles, 'Left Headlight' of cars and 'Right Base' of buses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Viewpoint Estimation with Ground Truth box</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>while using similar detectors.</figDesc><table><row><cell></cell><cell></cell><cell>AV P</cell><cell></cell><cell></cell><cell>AV P π 6</cell><cell>ARP π 6</cell></row><row><cell>Number of bins</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>24</cell><cell>-</cell><cell>-</cell></row><row><cell>Xiang et al. [37]</cell><cell cols="4">19.5 18.7 15.6 12.1</cell><cell>-</cell><cell>-</cell></row><row><cell>Pepik et al. [30]</cell><cell cols="4">23.8 21.5 17.3 13.6</cell><cell>-</cell><cell>-</cell></row><row><cell cols="5">Ghodrati et al. [10] 24.1 22.3 17.3 13.7</cell><cell>-</cell><cell>-</cell></row><row><cell>ours</cell><cell cols="4">49.1 44.5 36.0 31.1</cell><cell>50.7</cell><cell>46.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Keypoint Localization</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Object characteristics vs viewpoint prediction error</figDesc><table><row><cell>Setting</cell><cell>Accuracy</cell></row><row><cell>Error&lt; π 9</cell><cell>83.7</cell></row><row><cell>π 9 &lt;Error &lt; 2π 9</cell><cell>5.7</cell></row><row><cell>Error&gt; π 9 &amp; Error(π − f lip)&lt; π 9</cell><cell>5.8</cell></row><row><cell>Error&gt; π 9 &amp; Error(z − ref )&lt; π 9</cell><cell>6.5</cell></row><row><cell>Other</cell><cell>2.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Analysis of error modes for viewpoint prediction us to analyze our pose estimation method independent of the detection system. We denote as 'large objects' the top third of instances and by 'small objects' the bottom third</figDesc><table><row><cell>APK[α = 0.1]</cell><cell cols="4">aero bike boat bottle</cell><cell>bus</cell><cell>car</cell><cell cols="3">chair table mbike</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>mean</cell></row><row><cell>conv6+conv12</cell><cell>41.9</cell><cell>47.1</cell><cell>15.4</cell><cell>29.0</cell><cell cols="2">58.2 37.1</cell><cell>11.2</cell><cell>8.1</cell><cell>40.7</cell><cell>25.0</cell><cell>36.9</cell><cell>25.5</cell><cell>31.3</cell></row><row><cell>conv6+conv12+pLikelihood</cell><cell>44.9</cell><cell>48.3</cell><cell>17.0</cell><cell>30.0</cell><cell cols="2">60.8 40.7</cell><cell>14.6</cell><cell>8.6</cell><cell>42.8</cell><cell>25.7</cell><cell>38.3</cell><cell>26.2</cell><cell>33.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Keypoint Detection</figDesc><table><row><cell>PCK[α = 0.1]</cell><cell cols="4">aero bike boat bottle</cell><cell>bus</cell><cell>car</cell><cell cols="3">chair table mbike</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>mean</cell></row><row><cell>Default</cell><cell>66.0</cell><cell>77.8</cell><cell>52.1</cell><cell>83.8</cell><cell cols="2">88.7 81.3</cell><cell>65.0</cell><cell>47.3</cell><cell>68.3</cell><cell>58.8</cell><cell>72.0</cell><cell>65.1</cell><cell>68.8</cell></row><row><cell>Occluded Objects</cell><cell>55.2</cell><cell>56.6</cell><cell>38.7</cell><cell>68.8</cell><cell cols="2">64.4 62.8</cell><cell>48.1</cell><cell>40.5</cell><cell>53.1</cell><cell>59.6</cell><cell>68.6</cell><cell>47.3</cell><cell>55.3</cell></row><row><cell>Small Objects</cell><cell>51.6</cell><cell>66.4</cell><cell>48.1</cell><cell>81.2</cell><cell>85</cell><cell>67.4</cell><cell>57.4</cell><cell>48.2</cell><cell>57.9</cell><cell>53.8</cell><cell>57.4</cell><cell>56.8</cell><cell>60.9</cell></row><row><cell>Large Objects</cell><cell>74.6</cell><cell>87.4</cell><cell>57.2</cell><cell>86.3</cell><cell cols="2">90.9 90.6</cell><cell>65.1</cell><cell>37.7</cell><cell>76.1</cell><cell>68.5</cell><cell>74.1</cell><cell>65.3</cell><cell>72.8</cell></row><row><cell>left/right</cell><cell>71.1</cell><cell>80.2</cell><cell>53.4</cell><cell>84.4</cell><cell cols="2">90.9 84.1</cell><cell>74.7</cell><cell>49.2</cell><cell>69.8</cell><cell>63.4</cell><cell>75.0</cell><cell>68.2</cell><cell>72.0</cell></row><row><cell>PCK[α = 0.2]</cell><cell>79.9</cell><cell>88.7</cell><cell>69.1</cell><cell>95.2</cell><cell>92</cell><cell>88.3</cell><cell>79.6</cell><cell>67.5</cell><cell>87.3</cell><cell>72.2</cell><cell>82.2</cell><cell>78.1</cell><cell>81.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Analysis of Keypoint Prediction of instances. The label 'occluded' describes all the objects marked as truncated or occluded according to the PASCAL VOC annotations. We summarize our observations below.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Abhishek Kar, João Carreira and Saurabh Gupta for their valuable comments. This work was supported in part by NSF Award IIS-1212798 and ONR MURI -N00014-10-1-0933 and the Berkeley Graduate Fellowship. We gratefully acknowledge NVIDIA corporation for the donation of Tesla GPUs for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object detection using stronglysupervised deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th European Conference on Computer Vision -Volume Part I, ECCV&apos;12</title>
		<meeting>the 12th European Conference on Computer Vision -Volume Part I, ECCV&apos;12<address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="836" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detecting people using mutually consistent poselet activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1310.1531</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.3" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object detection with discriminatively trained partbased models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukushima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological Cybernetics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="202" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Is 2d information enough for viewpoint estimation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">N</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/1409.5403</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Rcnns for pose estimation and action detection. CoRR, abs/1406</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5212</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Using k-poselets for detecting people and localizing their keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Viewpoint-aware object detection and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alpert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1275" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Analyzing 3d objects in cluttered images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>P. Bartlett, F. Pereira, C. Burges, L. Bottou, and K. Weinberger</editor>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="602" to="610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Model-based vision: a program to see a walking person</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hogg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="20" />
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<meeting><address><addrLine>Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Recognizing solid objects by alignment with an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="195" to="212" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.24.12.2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The internal representation of solid shape with respect to vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Koenderink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Van Doorn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="211" to="216" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989-12-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structural factors in figure perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Perception &amp; Psychophysics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="229" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Forest before trees: The precedence of global features in visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Navon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Model-based image analysis of human motion using constraint propagation. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>O&amp;apos;rourke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Badler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="522" to="536" />
			<date type="published" when="1980-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Configural effects in perceived pointing of ambiguous triangles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Bucher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology: Human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">88</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Teaching 3d geometry to deformable part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pepik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Perception of wholes and of their component parts: Some configural superiority effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Pomerantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Sager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Stoever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Psychology-human Perception and Performance</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="422" to="435" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d generic object categorization, localization and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2553" to="2561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<idno>abs/1406.2984</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beyond pascal: A benchmark for 3d object detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11</title>
		<meeting>the 2011 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;11<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
