<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Parameter Sharing Exploration and Hetero-center Triplet Loss for Visible-Thermal Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijun</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoheng</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xichuan</forename><surname>Zhou</surname></persName>
						</author>
						<title level="a" type="main">Parameter Sharing Exploration and Hetero-center Triplet Loss for Visible-Thermal Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Visible-thermal person re-identification</term>
					<term>cross- modality discrepancy</term>
					<term>parameters sharing</term>
					<term>hetero-center triplet loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper focuses on the visible-thermal crossmodality person re-identification (VT Re-ID) task, whose goal is to match person images between the daytime visible modality and the nighttime thermal modality. The two-stream network is usually adopted to address the cross-modality discrepancy, the most challenging problem for VT Re-ID, by learning the multi-modality person features. In this paper, we explore how many parameters a two-stream network should share, which is still not well investigated in the existing literature. By splitting the ResNet50 model to construct the modality-specific feature extraction network and modality-sharing feature embedding network, we experimentally demonstrate the effect of parameter sharing of two-stream network for VT Re-ID. Moreover, in the framework of part-level person feature learning, we propose the hetero-center triplet loss to relax the strict constraint of traditional triplet loss by replacing the comparison of the anchor to all the other samples by the anchor center to all the other centers. With extremely simple means, the proposed method can significantly improve the VT Re-ID performance. The experimental results on two datasets show that our proposed method distinctly outperforms the state-of-the-art methods by large margins, especially on the RegDB dataset achieving superior performance, rank1/mAP/mINP 91.05%/83.28%/68.84%. It can be a new baseline for VT Re-ID, with a simple but effective strategy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>P ERSON re-identification (Re-ID) can be regarded as a retrieval task, which aims at searching a person of interest from multi-disjoint cameras deployed at different locations. It has received increasing interest in the computer vision community due to its importance in intelligent video surveillance and criminal investigation applications. Visible-visible Re-ID (VV Re-ID), the most common single-modality Re-ID task, has progressed and achieved high performance in recent years <ref type="bibr" target="#b32">[33]</ref>.</p><p>However, in practical scenarios, a 24-hour intelligent surveillance system, the visible-thermal cross-modality person re-identification (VT Re-ID) problem, is frequently encountered, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. For example, criminals always collect information during the day and execute crimes at night, in which case, the query image may be obtained from the thermal camera (or the infrared camera) during the nighttime, while the gallery images may be captured by the visible cameras during the daytime.</p><p>In recent years, an increasing number of researchers have focused on the VT Re-ID task, achieving substantial progresses with novel and effective ideas. However, many works evaluated the effectiveness of their methods with a poor baseline, which seriously impedes the development of the VT Re-ID community. In the present study, our proposed method can be set as a strong and effective baseline for VT Re-ID with some extremely simple means.</p><p>The VT Re-ID task suffers from two major problems, the large cross-modality discrepancy arisen from the different reflective visible spectra and sensed emissivities of visible and thermal cameras, and the large intra-modality variations, similar to the VV Re-ID task, caused by viewpoint changing and different human poses, etc. To alleviate the extra crossmodality discrepancy in VT Re-ID, an intuitive and apparent method is to map the cross-modality persons into a common feature space to realize the similarity measure. Therefore, a two-stream framework is always adopted, including two modality-specific networks with independent parameters for feature extraction, and a parameter-sharing network for feature embedding to project the modality-specific features into a common feature space. Generally, the two modality-specific networks are not required to have the same architecture. The only criterion is that their outputs should have the same dimension shapes to be the input of the parameter-sharing network for feature embedding. In the literature, the ResNet50 <ref type="bibr" target="#b6">[7]</ref> model is preferentially adopted as the backbone to construct the two-stream network, all the res-convolution blocks for feature extraction and some parameter-sharing fully connected layers for feature embedding. However, is this setting the best choice to construct the two-stream network? Those parametersharing fully-connected layers can only process the 1D-shaped vector, ignoring the spatial structure information of persons. To take advantage of the convolutional layers for processing the 3D-shaped tensor with spatial structure information, we can share some parameters of res-convolution blocks for feature embedding. In this situation, how many parameters of the two-stream network to share is the point of this study to investigate.</p><p>In addition, the network is always trained with identification loss and triplet loss to simultaneously enlarge the inter-class distance and minimize the intra-class distance. The triplet loss is performed on each anchor sample to all the other samples from both the same modality and cross modality. This may be a strong constraint for constraining the pairwise distance of those samples, especially when there exist some outliers (bad examples), which would form the adverse triplet to destroy other well-learned pairwise distances. It also leads to high complexity with a large number of triplets. The cross-modality and intra-modality training strategy is separately employed to enhance feature learning <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b34">[35]</ref>. In the authors' opinion, the separate cross-modality and intra-modality training strategy may be unnecessary if those learned person features by the two-stream network are good enough in the common feature space, where the features can hardly be distinguished from which modality. Therefore, we propose the hetero-center triplet loss that directly performs in the unified common feature space. The hetero-center triplet loss is performed on each anchor center to all the other centers, which can also reduce the computational complexity.</p><p>The main contributions can be summarized as follows.</p><p>• We achieve state-of-the-art performance on two datasets by large margins, which can be a strong VT Re-ID baseline to boost future research with high quality. • We explore the parameter-sharing problem in a twostream network. To the best of our knowledge, it is the first attempt to analyze the impact of the number of parameter sharing for cross-modality feature learning. • We propose the hetero-center triplet loss to constrain the distance of different class centers from both the same modality and cross modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>This section briefly reviews those existing VT Re-ID approaches. Compared to the traditional VV Re-ID, except for the intra-modality variations, VT Re-ID should handle the extra cross-modality discrepancy. To alleviate this problem, researchers focus on projecting (or translating) the heterogeneous cross-modality person images into a common space for similarity measure, mainly including the following aspects: network designing, metric learning and image translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network designing</head><p>Feature learning is the fundamental step of Re-Identification before similarity measure. Most studies focus on the visible and thermal person feature learning through designing deep neural networks (DNN). Ye et.al <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b34">[35]</ref> proposed adopting a two-stream network to separately extract the modality-specific features, and then performed the feature embedding to project those features into the common feature space with parameters sharing fully connected layers. Based on the two-stream network, Liu et al. <ref type="bibr" target="#b11">[12]</ref> introduced mid-level features incorporation to enhance the modality-shared person features with more discriminability. To learn good modalityshared person features, Dai et al. <ref type="bibr" target="#b1">[2]</ref> proposed the crossmodality generative adversarial network (cmGAN) under the adversarial learning framework, including a discriminator to distinguish whether the input features are from the visible modality or thermal modality. Zhang et al. <ref type="bibr" target="#b36">[37]</ref> proposed a dual-path cross-modality feature learning framework, including a dual-path spatial-structure-preserving common space network and a contrastive correlation network, which preserves intrinsic spatial structures and attends to the difference of input cross-modality image pairs. To explore the potential of both the modality-shared information and the modality-specific characteristics to boost the re-identification performance, Lu et al. <ref type="bibr" target="#b12">[13]</ref> proposed modeling the affinities of different modality samples according to the shared features and then transferring both shared and specific features among and across modalities.</p><p>Moreover, for handling the cross-modality discrepancy, some works concentrate on the input design of a singlestream network to simultaneously utilize visible and thermal information. Wu et al. <ref type="bibr" target="#b26">[27]</ref> first proposed to study the VT Re-ID problem, built the SYSU-MM01 dataset, and developed the zero-padding method to extract the modality-shared person features with a single-stream network. Kang et al. <ref type="bibr" target="#b8">[9]</ref> proposed combining the visible and thermal images as a single input with different image channels. Additionally, Wang et al. <ref type="bibr" target="#b23">[24]</ref> also adopted a multispectral image as the input for feature learning, where the multispectral image consists of the visible image and corresponding generated thermal image, or the generated visible image and corresponding thermal image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Metric learning</head><p>Metric learning is the key step of Re-ID for similarity measure. In the deep learning framework, due to the advantage of DNN on feature learning, Re-ID could achieve good performance with only the Euclidean distance metric. Therefore, metric learning is inherent in the training loss function of DNN, guiding the training process to make the extracted features more discriminative and robust. Ye et al. <ref type="bibr" target="#b30">[31]</ref> proposed a hierarchical cross-modality matching model by jointly optimizing the modality-specific and modality-shared metrics in a sequential manner. Then, they presented a bidirectional dual-constrained top-ranking loss to learn discriminative feature representations based on a two-stream network <ref type="bibr" target="#b34">[35]</ref>, based on which, the center-constraint was also introduced to improve performance <ref type="bibr" target="#b31">[32]</ref>. Zhu et al. <ref type="bibr" target="#b38">[39]</ref> proposed the hetero-center loss to reduce the intra-class cross-modality variations. Liu et al. <ref type="bibr" target="#b11">[12]</ref> also proposed the dual-modality triplet loss to guide the training procedures by simultaneously considering the cross-modality discrepancy and intra-modality variations. Hao et al. <ref type="bibr" target="#b5">[6]</ref> proposed an end-to-end two-stream hypersphere manifold embedding network with both classification and identification loss, constraining the intra-modality variations and cross-modality variations on this hypersphere. Zhao et al. <ref type="bibr" target="#b37">[38]</ref> introduced the hard pentaplet loss to improve the performance of the cross-modality re-identification. Wu et al. <ref type="bibr" target="#b25">[26]</ref> cast the learning shared knowledge for crossmodality matching as the problem of cross-modality similarity preservation, and proposed a focal modality-aware similaritypreserving loss to leverage the intra-modality similarity to guide the inter-modality similarity learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Image translation</head><p>The aforementioned works handle the cross-modality discrepancy and intra-modality variations from the feature extraction level. Recently, image generation methods based on generative adversarial network (GAN) have drawn much attention in VT Re-ID, reducing the domain gap between visible and thermal modalities from image level. Kniaz et al. <ref type="bibr" target="#b9">[10]</ref> first introduced GAN to translate a single visible image to a multimodal thermal image set, and then performed the Re-ID in the thermal domain. Wang et al. <ref type="bibr" target="#b19">[20]</ref> proposed an end-to-end alignment generative adversarial network (Align-GAN) for VT Re-ID, to jointly bridge the cross-modality gap with feature alignment and pixel alignment. Wang et al. <ref type="bibr" target="#b23">[24]</ref> proposed a dual-level discrepancy reduction learning framework based on a bi-directional cycleGAN to reduce the domain gap, from both the image and feature levels. Choi et al. <ref type="bibr" target="#b0">[1]</ref> proposed a hierarchical cross-modality disentanglement (Hi-CMD) method, which automatically disentangles IDdiscriminative factors and ID-excluded factors from visiblethermal images. Hi-CMD includes an ID-preserving person image generation network and a hierarchical feature learning module.</p><p>However, a person in the thermal modality can have different colors of clothes in the visible modality, leading to one thermal person image corresponding to multiple reasonable visible person images by image generation. It is hard to know which one is the correct target to be generated for Re-ID since when generating images, the model cannot access the gallery images that only appear in the inference phase. Image generation-based methods always have performance uncertainty, high complexity and high training trick demands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR PROPOSED METHOD</head><p>In this section, we introduce the framework of our proposed feature learning model for VT Re-ID, as depicted in <ref type="figure">Fig. 2</ref>. The model mainly consists of three components: (1) the twostream backbone network, exploring the parameter sharing, (2) the part-level feature extraction block and (3) the loss, our proposed hetero-center triplet loss and identity softmax loss. </p><formula xml:id="formula_0">s0 - stage{0 − 4} s1 stage{0} stage{1 − 4} s2 stage{0 − 1} stage{2 − 4} s3 stage{0 − 2} stage{3 − 4} s4 stage{0 − 3} stage{4} s5 stage{0 − 4} -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Two-stream backbone network</head><p>The two-stream network is a conventional way to extract features in visible-thermal cross-modality person reidentification, first introduced in <ref type="bibr" target="#b34">[35]</ref>. It consists of two parts: feature extractor and feature embedding. The feature extractor aims at learning the modality-specific information from two heterogeneous modalities, while the feature embedding focuses on learning the multi-modality shared features for cross-modality re-identification by projecting those modalityspecific features into a modality-shared common feature space.</p><p>In the existing literature, feature embedding is always computed by some shared fully connected layers, and the feature extractor is always a well-designed convolution neural network, such as ResNet50. In this situation, there may be two problems we should pay attention.</p><p>1) The feature extractor consists of two branches with independent parameters. If each branch consists of the whole well-designed CNN architecture, the number of network parameters (model size) would increase by double. 2) Feature embedding consists of some shared fully connected layers, which can only process the 1D-shaped feature vector without any person spatial structure information. However, the person spatial structure information is crucial to describe a person. To simultaneously address the aforementioned two problems, we propose to split the well-designed CNN model into two parts. The former part can be set as a two-stream feature extractor with independent parameters, while the latter part can be set as the feature embedding model. In this way, the whole model size will be reduced (corresponding to problem 1). The input of the feature embedding block is the output of the feature extractor, only the middle 3D feature maps of the well-designed CNN model, which is full of the person spatial structure information (corresponding to problem 2).</p><p>Therefore, the key point is how to split the well-designed CNN model. Namely, how many parameters of the two-stream network should be independent to learn the modality-specific information?</p><p>For simplicity in presentation, we denote the visible-stream feature extraction network as function φ v , the thermal-stream feature extraction network as φ t to learn the modality-specific The pipeline of our proposed framework for VT Re-ID, which mainly contains three components: two-stream backbone network, part-level feature learning block and loss. The two-stream backbone network includes two modality-specific branches with independent parameters and follows one modalityshared branch with shared parameters. For example, we take the ResNet50 model as the backbone, the first two stages (stage0 and stage1) form the modality-specific branches and the following three stages (stage2, stage3 and stage4) form the modality-shared branch. Then, the feature map outputted from the backbone is horizontally split into p 3D tensors, here p = 4, which are pooled into vectors by generalized-mean (GeM ) pooling operation. For each part vector, a 1 × 1 Conv block reduces the dimension of features. Afterward, the reduced part features are respectively input to compute the identification loss L id and our proposed hetero-center triplet loss L hc tri . Finally, all the part features are concatenated (cat) to form the final person features, which is supervised by L hc tri .</p><p>information, and the feature embedding network as φ vt to project modality-specific person features into the shared common feature space. Given a visible image I v and a thermal image I t , the learned 3D person features v and t in common space can be represented as,</p><formula xml:id="formula_1">v = φ vt φ v (I v ) , t = φ vt φ t (I t ) .<label>(1)</label></formula><p>We adopt the ResNet50 model as the backbone, with the consideration of its competitive performance in some Re-ID systems as well as its relatively concise architecture. The ResNet50 model mainly consists of one shallow convolution block stage0 and four res-convolution blocks, stage1, stage2, stage3 and stage4. To split the ResNet50 model into our modality-specific feature extractor and modality-shared feature embedding network, we can sequentially obtain the split scheme as shown in <ref type="table" target="#tab_0">Table I</ref>, where si, i = {0, 1, 2, 3, 4, 5} means φ vt starts from the i th stage. s0 and s5 are two extreme cases. s0 means that the two-stream backbone network shares all the ResNet50 model without the modality-specific feature extractor, while s5 means that all parameters of the two streams for the visible and thermal modalities are totally independent as in <ref type="bibr" target="#b34">[35]</ref>. Which is the best choice for a two-stream backbone network for cross-modality Re-ID? In the authors' opinion, these two extreme cases s0 and s5 are not good, since they ignore some important information in the cross-modality Re-ID task. Experimental results in Sec. IV-B1 show that the modality-shared feature embedding network comprising some res-convolution blocks is a good choice, since the input of modality-shared feature embedding network φ vt is 3D shape feature maps, with the spatial structure information of persons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Part-level feature extraction block</head><p>In VV Re-ID, state-of-the-art results are always achieved with part-level deep features <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b27">[28]</ref>. A typical and sim-ple approach is partitioning persons into horizontal strips to coarsely extract the part-level features, which can then be concatenated to describe the person's body structure. Body structure is the inherent characteristic of a person, which is invariant information of the person's body whatever modality the image is captured from. Namely, the body structure information is modality-invariant, which can be adopted as modality-shared information to represent a person. Therefore, according to the part-level feature extraction method in <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b21">[22]</ref>, we also adopt the uniform partition strategy to obtain coarse body part features.</p><p>Given a person (visible or thermal) images, it will become the 3D feature map after undergoing all the layers inherited from the two-stream backbone network. Based on the 3D feature maps, as shown in <ref type="figure">Fig. 2</ref>, there are 3 steps to extract the part-level person features as follows.</p><p>1) The 3D feature maps are uniformly partitioned into p strips in the horizontal orientation to generate the coarse body part feature maps, as shown in <ref type="figure">Fig. 2</ref>, where p = 4. 2) Instead of utilizing the widely used max-pooling or average pooling, we adopt a generalized-mean (GeM) <ref type="bibr" target="#b15">[16]</ref> pooling layer to translate the 3D part feature maps into the 1D part feature vectors. Given a 3D feature patch X ∈ R C×H×W , the GeM can be formulated as,</p><formula xml:id="formula_2">x = 1 |X| xi∈X x p i 1 p ,<label>(2)</label></formula><p>wherex ∈ R C×1×1 is the pooled result, | · | denotes the element number, and p is the pooling hyperparameter, which can be preset or learned by back-propagation. 3) Afterwards, a 1 × 1 convolutional (1 × 1 Conv) block is employed to reduce the dimension of part-level feature vectors. The block includes a 1 × 1 convolutional layer . Illustration of the hetero-center triplet loss, which aims pulling close those centers with the same identity label from different modalities, while pushing far away those centers with different identity labels regardless of which modality it is from. We compare the center to center similarity rather than sample to sample similarity or sample to center similarity. The stars denote the centers. Different colors denote different identities. whose output channel number is d, following a batch normalization layer and a ReLU layer. Moreover, each part-level feature vector is first adopted to perform metric learning with triplet loss L tri (or our proposed hetero-center triplet loss L hc tri ). Then, a fully connected layer with desired dimensions (corresponding to the number of identities in our model) is adopted to perform the identification with softmax L id . There are p part-level features that need p different classifiers without sharing parameters.</p><p>Finally, all the p part-level features are concatenated (cat) to form the final person features for the similarity measure during testing. Additionally, the final person features can also be supervised by the L tri (or L hc tri ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. The hetero-center triplet loss</head><p>In this subsection, we introduce the designed hetero-center triplet loss to guide the network training for part-level feature learning. The learning objective is directly conducted in the common feature space to simultaneously deal with both crossmodality discrepancy and intra-modality variations. First, we revisit the general triplet loss.</p><p>1) Triplet loss revisit: Triplet loss was first proposed in FaceNet <ref type="bibr" target="#b16">[17]</ref>, and then improved by mining the hard triplets <ref type="bibr" target="#b7">[8]</ref>. The core idea is to form batches by randomly sampling P identities, and then randomly sampling K images of each identity, resulting in a mini-batch with P K images. For each sample x a in the mini-batch, we can select the hardest positive and hardest negative samples within the mini-batch to form the triplets for computing the batch hard triplet loss, </p><formula xml:id="formula_3">L bh tri (X) = all anchors P i=1 K a=1 ρ + hardest positive max p=1...K x i a − x i p 2 (3) − min j=1...P n=1...K j =i x i a − x j n 2 hardest negative + ,</formula><formula xml:id="formula_4">L bh tri 2P K × (2K − 1) 2P K × 2(P − 1)K L hc tri 2P 2P × 2(P − 1)</formula><p>which is defined for a mini-batch X, where a data point x i a denotes the a th image of the i th person in the batch, [x] + = max(x, 0) denotes the standard hinge loss, x a −x p 2 denotes the Euclidean distance of data point x a and x p , ρ is the margin.</p><p>2) Batch sampling method: Due to our two-stream structure respectively extracting features for visible and thermal images, we introduce the following online batch sampling strategy. Specifically, P person identities are first randomly selected at each iteration, and then we randomly select K visible images and K thermal images of the selected identity to form the mini-batch, in which a total of 2 * P K images are obtained. This sampling strategy can fully utilize the relationship of all the samples within a mini-batch. In this manner, the sample size of each class is the same, which is important to avoid the perturbations caused by class imbalance. Moreover, due to the random sampling mechanism, the local constraint in the mini-batch can achieve the same effect as the global constraint in the entire set.</p><p>3) Hetero-center triplet loss: Eq. <ref type="formula">(3)</ref> shows that triplet loss computes the loss by comparison of the anchor to all the other samples. It is a strong constraint, perhaps too strict to constrain the pairwise distance if there exist some outliers (bad examples), which would form the adverse triplet to destroy other pairwise distances. Therefore, we consider adopting the center of each person as the identity agent. In this manner, we can relax the strict constraint by replacing the comparison of the anchor to all the other samples by the anchor center to all the other centers.</p><p>First, in a mini-batch, the center for the features of every identity from each modality is computed,</p><formula xml:id="formula_5">c i v = 1 K K j=1 v i j ,<label>(4)</label></formula><formula xml:id="formula_6">c i t = 1 K K j=1 t i j ,</formula><p>which is defined for a mini-batch, where v i j denotes the j th visible image feature of the i th person in the mini-batch, while t i j corresponds to the thermal image feature. Therefore, based on our P K sampling method, in each mini-batch, there are P visible image centers {c i v |i = 1, · · · , P } and P thermal centers {c i t |i = 1, · · · , P }, as shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. In the following, all the computations are only performed on the centers.</p><p>The goal of metric learning is to make those features from the same class close to each other (intra-class compactness), while those features from different classes are far away from each other (inter-class separation). Therefore, in our VT crossdomain Re-ID, based on the P K sampling strategy and calculated centers, we can define the hetero-center triplet loss as,</p><formula xml:id="formula_7">L hc tri (C) = P i=1 ρ + c i v − c i t 2 − min n∈{v,t} j =i c i v − c j n 2 +<label>(5)</label></formula><formula xml:id="formula_8">+ P i=1 ρ + c i t − c i v 2 − min n∈{v,t} j =i c i t − c j n 2 + ,</formula><p>which is defined on mini-batch centers C including both visible centers {c i v |i = 1, · · · , P } and thermal centers {c i t |i = 1, · · · , P }. For each identity, L hc tri concentrates on only one cross-modality positive pair and the mined hardest negative pair in both the intra-and inter-modality.</p><p>Comparing the general triplet loss L bh tri (Eq. <ref type="formula">(3)</ref>) to our proposed center-based triplet loss L hc tri (Eq. (5)), we replace the comparison of the anchor to all the other samples by the anchor center to all the other centers. This modification has two major advantages: a) It reduces the computational cost, as shown in <ref type="table" target="#tab_0">Table II</ref>. For a mini-batch with 2P K images, L bh tri requires computing pairwise distance 2P K × (2K − 1) for hardest positive sample mining and 2P K × 2(P − 1)K for hardest negative sample mining. In comparison, L hc tri only needs to compute the pairwise distance 2P for positive sample pairs (there are only P cross-modality positive center pairs), and 2P × 2(P − 1) for hardest negative center sample mining. The computational cost is largely reduced. b) It relaxes the sample-based triplet constraint to the centerbased triplet constraint, which also preserves the property of handling both the intra-class and inter-class variations simultaneously on visible and thermal modalities in the common feature space. For each identity, minimizing the only cross-modality positive center pairwise distance can ensure intra-class feature compactness. The hardest negative center mining can ensure the inter-class feature distinguishable property both in visible and thermal modalities. 4) Comparison to other center-based losses: There are two kinds of center-based losses: the learned centers <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b31">[32]</ref> and the computed centers <ref type="bibr" target="#b38">[39]</ref>. The main difference lies in the method of obtaining the centers. One learns them by pre-setting a center parameter for each class, while the other computes the centers directly based on the learned deep features.</p><p>The learned centers. The learned center loss <ref type="bibr" target="#b24">[25]</ref> is first introduced in face verification to learn a center for the features of each class and penalizes the distances between the deep features and their corresponding centers. For our crossmodality VT Re-ID task with the P K sampling strategy, the learned center loss can be extended in a bi-directional manner <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b34">[35]</ref> as follows, where v i j denotes the j th visible image feature of the i th person in the mini-batch, while t i j corresponds to the thermal image feature. c i is the i th class center for both visible and thermal modalities.</p><formula xml:id="formula_9">L lc = 1 2 P i=1 K j=1 v i j − c i 2 + t i j − c i 2 ,<label>(6)</label></formula><p>Comparing the learned center loss L lc (Eq. (6)) to our proposed hetero-center triplet loss L hc tri (Eq. (5)), there are the following differences. 1) L hc tri is in a comparison of the anchor center to centers rather than L lc 's anchor sample to centers. 2) L hc tri computes the centers for visible and thermal modalities, while L lc unifies the i th class center for both visible and thermal modalities into one learned vector. 3) L hc tri is formulated by triplet mining the properties of both the inter-class separability and intra-class compactness, while L lc only focuses on the intra-class compactness, ignoring the inter-class separability.</p><p>As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, our proposed hetero-center triplet loss L hc tri concentrates on both of the inter-class separability and intra-class compactness, while the learned center loss L lc ignores the inter-class separability for both the intra-and inter modality. L lc only performs well on intra-modality intraclass compactness, but poorly on cross-modality intra-class compactness. This may be due to the hard training of learned center loss combined with identification loss, which leads to the unsatisfactory performance.</p><p>The computed centers. The other method for obtaining the center of each class is to calculate it directly based on the learned deep features <ref type="bibr" target="#b38">[39]</ref>. We also adopt this approach. Instead of pre-setting a center parameter to be learned as Eq. (6), the centers are directly calculated as Eq. (4). In <ref type="bibr" target="#b38">[39]</ref>, the hetero-center loss L hc was proposed to improve the intraclass cross-modality similarity, penalizing the center distance between two modality distributions, which can be formulated as follows,</p><formula xml:id="formula_10">L hc = P i=1 c i v − c i t 2 .<label>(7)</label></formula><p>Comparing the hetero-center loss L hc (Eq. <ref type="formula" target="#formula_10">(7)</ref>) to our proposed hetero-center triplet loss L hc tri (Eq. (5)), the main difference is that L hc only focuses on the intra-class crossmodality compactness (the red arrows in <ref type="figure" target="#fig_2">Fig. 3</ref>), while our L hc tri additionally focuses on the inter-class separability for both the intra-and inter-modality (the grey arrows in <ref type="figure" target="#fig_2">Fig. 3</ref>) with triplet mining. In summary, L hc is only a part of our proposed L hc tri .</p><p>5) The overall loss: Moreover, similar to some state-ofthe-art VT Re-ID methods <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b38">[39]</ref>, for the sake of feasibility and effectiveness for classification, identification loss is also utilized to integrate the identityspecific information by treating each person as a class. The identification loss with label smooth operation is adopted to prevent overfitting the Re-ID model training. Given an image, we denote y as the truth ID label and p i as the ID prediction logits of the i th class. The identification loss is calculated as follows,</p><formula xml:id="formula_11">L id = N i=1 −q i log(p i ) (8) s.t. q i = 1 − N −1 N ξ, y = i, ξ N , y = i,</formula><p>where N is the number of identities in the total training set, and ξ is a constant to encourage the model to be less confident on the training set. In this work, ξ is set to 0.1. We adopt both the identification loss and hetero-center triplet loss for each part-level feature, while only the heterocenter triplet loss L g hc tri is for the final concatenated global features. Therefore, the final loss is,</p><formula xml:id="formula_12">L all = L g hc tri + p i=1 L i id + λL i hc tri ,<label>(9)</label></formula><p>where λ is a predefined tradeoff parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>In this section, we evaluate the effectiveness of our proposed methods for extracting the person features for VT Re-ID tasks on two public datasets, RegDB <ref type="bibr" target="#b14">[15]</ref> and SYSU-MM01 <ref type="bibr" target="#b26">[27]</ref>. The example images are shown in <ref type="figure">Fig. 5</ref>.</p><p>A. Experimental settings 1) Datasets and settings: SYSU-MM01 <ref type="bibr" target="#b26">[27]</ref> is a largescale dataset collected by 6 cameras, including 4 visible and 2 infrared cameras, captured in the SYSU campus. Some cameras are deployed in indoor environments and others are deployed in outdoor environments. The training set contains 395 persons, including 22,258 visible images and 11,909 infrared images. The testing set contains another 96 persons, including 3,803 infrared images for the query and 301 randomly selected visible images as the gallery set. In the allsearch mode, the gallery set contains all the visible images captured from all four visible cameras. In indoor-search mode, the gallery set only contains the visible images captured by two indoor visible cameras. Generally, the all-search mode is more challenging than the indoor-search mode. We follow existing methods to perform 10 trials of gallery set selection in the single-shot setting <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b34">[35]</ref>, and then report the average visible images thermal images SYSU-MM01 RegDB <ref type="figure">Fig. 5</ref>. Illustration of the visible-thermal images from two datasets, SYSU-MM01 <ref type="bibr" target="#b26">[27]</ref> and RegDB <ref type="bibr" target="#b14">[15]</ref>, for cross-modality person re-identification. The first row is the visible images, while the second is the thermal images. Each column contains images of the same person.</p><p>retrieval performance. A detailed description of the evaluation protocol can be found in <ref type="bibr" target="#b26">[27]</ref>. RegDB <ref type="bibr" target="#b14">[15]</ref> was constructed by dual-camera (one visible and one thermal camera) systems, and includes 412 persons. For each person, 10 visible images were captured by a visible camera, and 10 thermal images are obtained by a thermal camera. We follow the evaluation protocol in <ref type="bibr" target="#b30">[31]</ref> and <ref type="bibr" target="#b34">[35]</ref>, where the dataset is randomly split into two halves, one for training and the other for testing. For testing, the images from one modality (default is thermal) were used as the gallery set while those from the other modality (default is visible) were used as the probe set. The procedure is repeated for 10 trials to achieve statistically stable results, recording the mean values.</p><p>2) Evaluation Metrics: Following existing works, cumulative matching characteristics (CMC), mean average precision (mAP) and the mean inverse negative penalty (mINP) are adopted as the evaluation metrics. CMC (rank-r accuracy) measures the probability of a correct cross-modality person image occurring in the top-r retrieved results. mAP measures the retrieval performance when multiple matching images occur in the gallery set. Moreover, mINP considers the hardest correct match that determines the workload of inspectors <ref type="bibr" target="#b32">[33]</ref>. Note that all the person features are first L2 normalized for testing.</p><p>3) Implementation details: The implementation 1 of our method is with the Pytorch framework. Following the existing person Re-ID works, the ResNet50 model is adopted as the backbone network for a fair comparison, and the pretrained ImageNet parameters are adopted for the network initialization. Specifically, the stride of the last convolutional block is changed from 2 to 1 to obtain fine-grained feature maps with large body size. In the training phase, the input images are resized to 288 × 144 and padded with 10, then randomly left-right flipped and cropped to 288 × 144 for data augmentation. We adopt the stochastic gradient descent (SGD) optimizer for optimization, and the momentum parameter is set to 0.9. We set the initial learning rate as 0.1 for both datasets. The warmup learning rate strategy is applied to bootstrap the network to enhance performance. The learning rate (lr) at </p><formula xml:id="formula_13">lr(t) =        0.1 × t+1 10 , 0 ≤ t &lt; 10 0.1, 10 ≤ t &lt; 20 0.01, 20 ≤ t &lt; 50 0.001, 50 ≤ t .<label>(10)</label></formula><p>We set the predefined margin ρ = 0.3 for all triplet losses. For the P K sampling strategy, we set P = 8, K = 4 for the RegDB dataset, and P = 6, K = 8 for the SYSU-MM01 dataset. For the tradeoff parameter, we set λ = 2.0 for the RegDB dataset, and λ = 1.0 for the SYSU-MM01 dataset. The dimension of part-level feature d is set to 256, and the number of part-level stripes p is set to 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation experiments</head><p>We evaluate the effectiveness of our proposed method, including three components, two-stream backbone network, part-level feature learning and hetero-center triplet loss. <ref type="bibr" target="#b1">2</ref> 1) Two-stream backbone network setting: As analyzed in Sec. III-A, the key point of the two-stream backbone network setting is how to split the well-designed CNN model to construct a modality-specific feature extractor with independent parameters and modality-shared feature embedding with shared parameters. Based on the AGW baseline <ref type="bibr" target="#b32">[33]</ref> which is designed on top of BagTricks <ref type="bibr" target="#b13">[14]</ref>, we optionally build the following baseline network with the ResNet50 model. As shown in <ref type="figure">Fig. 6</ref>, the 3D feature maps outputted from the twostream backbone network are pooled by the generalized-mean pooling (GeM) layer to obtain the 2D feature vector. Then the batch normalization (BN) neck is adopted to train the network, where triplet loss <ref type="figure" target="#fig_2">(Eq. (3)</ref>) is first utilized on the 2D feature vector, and then the identification loss is sequentially utilized on the batch normalized feature vector.  <ref type="figure">Fig. 6</ref>. Illustration of the baseline network, mainly illustrating how to split the ResNet50 model to set the two-stream backbone network. si, i = {0, 1, 2, 3, 4, 5} denotes that the modality-shared feature embedding block with parameter sharing, starting from i th stage. Then, the batch normalization neck with triplet loss and identification loss is adopted to train the network.</p><p>The results of different backbone splits on RegDB and SYSU-MM01 datasets are listed in <ref type="table" target="#tab_0">Table III</ref>, from which we can observe that. a) s5, without sharing any res-convolutional layers, obtains the worst performance on both the RegDB and SYSU-MM01 datasets, with large margins compared to other splits. s5 only shares the last fully connected layer to process the 1D feature vector without any person spatial structure information. It demonstrates the effectiveness of the 3D feature maps with person spatial structure information to describe a person. b) s0, sharing all the backbone networks without the modality-specific feature extractor, obtains good performances on both the RegDB and SYSU-MM01 datasets. s0 equally treats both visible and thermal person images, without focusing additionally on the color information of visible images, focusing on the spatial structure information of a person existing on both visible and thermal images.</p><p>The results may demonstrate that the person spatial structure information is more important compared to the color information in VT cross-modality Re-ID. c) On the RegDB dataset, s0, s1, s2 and s3 achieve comparable performances. On the SYSU-MM01 dataset, s2 obtains much better Rank1, mAP and mINP results compared to s0 and s1. The different performances may be from the different settings of the two datasets. RegDB is collected by a dual-camera system, where the visible image and corresponding thermal image are well aligned. SYSU-MM01 is collected by 6 disjoint cameras deployed at different locations, where the visible image and corresponding infrared image have arbitrary poses and views. Therefore, SYSU-MM01 needs more modality-specific layers to extract the person spatial structure compared to RegDB. d) Overall, s2 can achieve the best performance, which only sets stage0 and stage1 as the modality-specific feature  extractor with acceptable independent parameters.</p><p>2) Part-level feature learning: To evaluate the effectiveness of part-level feature learning compared to global feature learning, we add the uniform partitioning strategy between the two-stream backbone network and the loss layer, as shown in <ref type="figure">Fig. 2</ref>. Based on the above experimental results, we adopt the s2 split as the two-stream backbone network, still with the supervision of identification loss and triplet loss.</p><p>Three points are considered: 1) the number of partition strips p, 2) the generalized-mean pooling (GeM) layer instead of the traditional average pooling layer or max-pooling layer, and 3) the dimension of each part-level feature d corresponding to the output channel number of 1 × 1 Conv.</p><p>The effect of partition strips. The number of partition strips determines the granularity of a person's local feature. <ref type="figure" target="#fig_5">Fig. 7</ref> shows the results of different partition strips p on the RegDB and SYSU-MM01 datasets. We observe that p = 6 is the best setting for partition strips to extract the local person feature.</p><p>The effect of GeM. This subsection verifies the effectiveness of the generalized-mean pooling (GeM) method compared to the traditional average pooling (Mean) and maxpooling (Max) methods. <ref type="table" target="#tab_0">Table IV</ref> lists the results of different pooling methods on the RegDB and SYSU-MM01 datasets.</p><p>We observe that max-pooling performs better than average pooling, while the generalized-mean pooling method performs the best. The effect of part-level feature dimension. This subsection shows the effect of part-level feature dimension d, corresponding to the output channel number of 1 × 1 Conv in <ref type="figure">Fig. 2</ref>. The final dimension of the person feature is the product of the part-level feature dimension d and the number of partition strips p. <ref type="table">Table V</ref> lists the results of different dimensions of each part-level feature d on the RegDB and SYSU-MM01 datasets. We find that on SYSU-MM01 dataset, d = 256 performs the best, while on the RegDB dataset d = 512 performs the best under the rank1 and mAP criteria, and d = 256 achieves the best performance under the mINP criterion. considering both the performance and final person feature dimension, we set d = 256 for both the RegDB and SYSU-MM01 datasets.</p><p>3) Hetero-center based triplet loss: In this subsection, we verify the effectiveness of our proposed hetero-center triplet loss L hc tri from two aspects. On the one hand, L hc tri is compared to traditional triplet loss L bh tri to demonstrate the effectiveness of anchor center to all the other centers compared to anchor to all the other samples. On the other hand, L hc tri is compared to the learned center loss L lc and hetero-center loss L hc to demonstrate the effectiveness of constraining both the inter-class separability and intra-class compactness.</p><p>L hc tri vs. L bh tri . We conducted experiments under the framework shown in <ref type="figure">Fig. 2</ref> with different triplet losses, L hc tri and L bh tri , fine-tuning the tradeoff parameter λ in Eq. <ref type="bibr" target="#b8">(9)</ref>. The results are listed in <ref type="table" target="#tab_0">Table VI</ref>. From the table, we can observe that 1) L hc tri outperforms L bh tri on both RegDB and SYSU-MM01 datasets, demonstrating the effectiveness of anchor center to all the other centers compared to anchor to all the other samples. 2) With the final loss Eq. (9), those nonconvergent cases on the SYSU-MM01 dataset may show that the anchor to all the other samples of L bh tri is truly a strict constraint, demonstrating the effectiveness of the anchor center to all the other centers relaxation operation. L hc tri vs. L lc and L hc . We conducted experiments with different center-based losses, including the learned center loss L lc , hetero-center loss L hc , and our proposed hetero-center triplet loss L hc tri . The network is in two frameworks: the baseline network that extracts the global person features (Sec. IV-B1) and the part-level local feature learning network (Sec. IV-B2). The results are listed in <ref type="table" target="#tab_0">Table VII</ref>. We can observe that in both networks, L hc tri outperforms L lc and L hc with large margins. It demonstrates the effectiveness of our proposed L hc tri concentrating on both the inter-class separability and intra-class compactness, compared to L lc and L hc which only focus on intra-class cross-modality compactness, ignoring the inter-class separability for both the intra-and inter-modality. It is also illustrated in <ref type="figure" target="#fig_3">Fig. 4</ref> through visualizing the features extracted by the baseline model with different center-based losses.</p><p>4) Ablation summarization: Moreover, to show the effect of every component, we also summarize the corresponding ablation study in <ref type="table" target="#tab_0">Table VIII, whose results are copied from  Tables III, VI</ref> and VII. It shows that when the component works alone, the performance on the two datasets is different and is not always improved. However, the combination of three components can greatly boost the performance on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Comparison to the state-of-the-art</head><p>This section compares the state-of-the-art VT Re-ID methods. The results on the RegDB and SYSU-MM01 datasets are listed in Tables IX and X, respectively. <ref type="bibr" target="#b2">3</ref> The experiments on the RegDB dataset <ref type="table" target="#tab_0">(Table IX)</ref> demonstrate that our proposed method obtains the best performance in both query settings, always by large margins. We set a new baseline for this dataset, achieving superior performance rank1/mAP/mINP 91.05%/83.28%/68.84% for visible → thermal query setting. The experiments suggest that our proposed method can learn better cross-modality sharing features by well designing the two-stream parameter-sharing network, learning the part-level local person features, and computing the triplet loss on heterogeneous centers from different modalities. The experiments on the SYSU-MM01 dataset <ref type="table" target="#tab_7">(Table X)</ref> show that our proposed method can achieve comparable performance compared to the current state-of-the-art results obtained by HAT <ref type="bibr" target="#b33">[34]</ref>, and outperforms all the other comparison methods. However, in the more challenging mode all-search, our method performs much better than HAT <ref type="bibr" target="#b33">[34]</ref> in the two key criteria rank1/mAP, 61.68%/57.51% vs. 55.29%/53.89%.</p><p>Compared to AliGAN <ref type="bibr" target="#b19">[20]</ref>, D 2 RL <ref type="bibr" target="#b23">[24]</ref>and Hi-CMD <ref type="bibr" target="#b0">[1]</ref>, our method achieves much better performance on both datasets, and does not need the sophisticated cross-modality image translation operation. Our method also does not require complicated adversarial learning with many tricks, which is always difficult for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>This paper aims to enhance the discriminative person feature learning through simple means for VT Re-ID. On the one hand, we explore the parameter-sharing settings in the two-stream network. The experimental results show that the modality-sharing feature embedding network with some convolution blocks is an effective strategy, that could process the 3D shape feature maps with the spatial structure of a person. On the other hand, we also propose the hetero-center triplet loss to improve the traditional triplet loss for VT Re-ID by replacing the comparison of the anchor to all the other samples with the anchor center to all the other centers. With part-level person feature learning, hetero-center triplet loss performs much better than traditional triplet loss. The experimental results with remarkable improvements on two VT Re-ID datasets demonstrate the effectiveness of our proposed method compared to the current state-of-the-art methods. Our method with a simple but effective strategy can be a strong VT Re-ID baseline to boost future research with high quality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>H. Liu, X. Tan and X. Zhou are with the School of Microelectronics and Communication Engineering, Chongqing University, Chongqing, 400044, China. (Corresponding author: haijun liu@126.com) Visible image (day) Infrared image (night) Cross-modality Re-ID Illustration of VT Re-ID. For example, searching a person captured by a visible camera in the daytime among multiple persons captured by infrared (or thermal) cameras at night, and vice versa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig. 2. The pipeline of our proposed framework for VT Re-ID, which mainly contains three components: two-stream backbone network, part-level feature learning block and loss. The two-stream backbone network includes two modality-specific branches with independent parameters and follows one modalityshared branch with shared parameters. For example, we take the ResNet50 model as the backbone, the first two stages (stage0 and stage1) form the modality-specific branches and the following three stages (stage2, stage3 and stage4) form the modality-shared branch. Then, the feature map outputted from the backbone is horizontally split into p 3D tensors, here p = 4, which are pooled into vectors by generalized-mean (GeM ) pooling operation. For each part vector, a 1 × 1 Conv block reduces the dimension of features. Afterward, the reduced part features are respectively input to compute the identification loss L id and our proposed hetero-center triplet loss L hc tri . Finally, all the part features are concatenated (cat) to form the final person features, which is supervised by L hc tri .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3. Illustration of the hetero-center triplet loss, which aims pulling close those centers with the same identity label from different modalities, while pushing far away those centers with different identity labels regardless of which modality it is from. We compare the center to center similarity rather than sample to sample similarity or sample to center similarity. The stars denote the centers. Different colors denote different identities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>The visualization of features extracted by the baseline model with (a) the learned center loss and (b) our proposed hetero-center triplet loss. The features are from 8 randomly chosen identities in the RegDB testing dataset, whose dimension of features is reduced to 2 by tSNE. Points with different colors denote features belonging to different identities. Points of different shapes denote different modalities. The red points with different shapes denote the feature centers of each identity from different modalities. Blue arrows in the blue circles link the two centers of one identity from two modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>The effect of partition strips p on (a) RegDB and (b) SYSU-MM01 datasets. Re-identification rates of rank1, mAP and mINP (%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I DIFFERENT</head><label>I</label><figDesc>SPLITS OF RESNET50 MODEL TO FORM THE TWO-STREAM BACKBONE NETWORK. φv AND φt RESPECTIVELY DENOTE THE VISIBLE-STREAM AND THERMAL-STREAM FEATURE EXTRACTION NETWORK. φvt DENOTES THE MODALITY-SHARED FEATURE EMBEDDING NETWORK. stage{0 − 2} DENOTES stage0, stage1 AND stage2. IT IS DETAILEDLY ILLUSTRATED IN FIG. 6.</figDesc><table><row><cell>Modality-specific</cell><cell>Modality-shared</cell></row><row><cell>feature extractor</cell><cell>feature embedding</cell></row><row><cell>(φv and φt)</cell><cell>(φvt)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II THE</head><label>II</label><figDesc>COMPARISON OF COMPUTATIONAL COST BETWEEN GENERAL TRIPLET LOSS L bh tri AND OUR PROPOSED HETERO-CENTER TRIPLET LOSS L hc tri . DUE TO THE SYMMETRICAL PROPERTY OF DISTANCE MEASURE, THE COMPUTATIONAL COST COULD DIVIDE 2.</figDesc><table /><note>positive negative</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III THE</head><label>III</label><figDesc>RESULTS OF DIFFERENT SPLITS OF THE BACKBONE TO FORM THE TWO-STREAM NETWORK. si, i = {0, 1, 2, 3, 4, 5} DENOTES THAT THE MODALITY-SHARED FEATURE EMBEDDING BLOCK WITH PARAMETERS SHARING STARTS FROM i th stage. RE-IDENTIFICATION RATES AT RANK R, MAP AND MINP (%).</figDesc><table><row><cell>splits</cell><cell>r = 1</cell><cell>r = 5</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell><cell>mINP</cell></row><row><cell></cell><cell></cell><cell cols="2">RegDB</cell><cell></cell><cell></cell><cell></cell></row><row><cell>s0</cell><cell>77.52</cell><cell>86.50</cell><cell>90.49</cell><cell>93.50</cell><cell>69.79</cell><cell>54.58</cell></row><row><cell>s1</cell><cell>76.94</cell><cell>85.68</cell><cell>89.71</cell><cell>93.88</cell><cell>69.36</cell><cell>54.82</cell></row><row><cell>s2</cell><cell>77.14</cell><cell>87.33</cell><cell>91.94</cell><cell>95.19</cell><cell>69.82</cell><cell>54.62</cell></row><row><cell>s3</cell><cell>76.99</cell><cell>87.23</cell><cell>91.21</cell><cell>94.51</cell><cell>69.51</cell><cell>53.74</cell></row><row><cell>s4</cell><cell>64.95</cell><cell>78.30</cell><cell>85.00</cell><cell>90.49</cell><cell>60.98</cell><cell>48.62</cell></row><row><cell>s5</cell><cell>48.93</cell><cell>61.99</cell><cell>71.50</cell><cell>80.44</cell><cell>48.30</cell><cell>37.66</cell></row><row><cell></cell><cell></cell><cell cols="2">SYSU-MM01</cell><cell></cell><cell></cell><cell></cell></row><row><cell>s0</cell><cell>54.38</cell><cell>80.78</cell><cell>88.96</cell><cell>95.06</cell><cell>52.18</cell><cell>38.57</cell></row><row><cell>s1</cell><cell>54.48</cell><cell>80.38</cell><cell>88.61</cell><cell>94.61</cell><cell>52.67</cell><cell>39.19</cell></row><row><cell>s2</cell><cell>57.09</cell><cell>81.78</cell><cell>88.80</cell><cell>94.61</cell><cell>54.99</cell><cell>41.26</cell></row><row><cell>s3</cell><cell>52.20</cell><cell>78.23</cell><cell>86.83</cell><cell>93.06</cell><cell>51.43</cell><cell>39.49</cell></row><row><cell>s4</cell><cell>45.23</cell><cell>71.50</cell><cell>79.65</cell><cell>88.51</cell><cell>45.43</cell><cell>33.25</cell></row><row><cell>s5</cell><cell>37.81</cell><cell>69.18</cell><cell>79.88</cell><cell>87.96</cell><cell>39.40</cell><cell>27.68</cell></row><row><cell cols="4">epoch t is computed as follows,</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV THE</head><label>IV</label><figDesc>RESULTS OF DIFFERENT POOLING METHODS ON REGDB AND SYSU-MM01 DATASETS, INCLUDING GENERALIZED-MEAN POOLING (GEM), AVERAGE POOLING (MEAN) AND MAX POOLING (MAX). RE-IDENTIFICATION RATES OF RANK1, MAP AND MINP (%).</figDesc><table><row><cell></cell><cell></cell><cell>RegDB</cell><cell></cell><cell cols="3">SYSU-MM01</cell></row><row><cell>Methods</cell><cell>rank1</cell><cell>mAP</cell><cell>mINP</cell><cell>rank1</cell><cell>mAP</cell><cell>mINP</cell></row><row><cell>GeM</cell><cell>85.10</cell><cell>81.40</cell><cell>72.13</cell><cell>57.90</cell><cell>55.10</cell><cell>40.29</cell></row><row><cell>Mean</cell><cell>76.75</cell><cell>76.08</cell><cell>68.36</cell><cell>52.09</cell><cell>49.92</cell><cell>35.88</cell></row><row><cell>Max</cell><cell>84.22</cell><cell>79.75</cell><cell>69.04</cell><cell>56.74</cell><cell>54.88</cell><cell>40.72</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE V</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">THE RESULTS OF DIFFERENT DIMENSIONS OF EACH PART-LEVEL FEATURE</cell></row><row><cell cols="7">d ON THE REGDB AND SYSU-MM01 DATASETS. RE-IDENTIFICATION</cell></row><row><cell></cell><cell cols="5">RATES OF RANK1, MAP AND MINP (%).</cell><cell></cell></row><row><cell></cell><cell></cell><cell>RegDB</cell><cell></cell><cell cols="3">SYSU-MM01</cell></row><row><cell>d</cell><cell>rank1</cell><cell>mAP</cell><cell>mINP</cell><cell>rank1</cell><cell>mAP</cell><cell>mINP</cell></row><row><cell>128</cell><cell>82.72</cell><cell>79.66</cell><cell>69.96</cell><cell>55.25</cell><cell>53.49</cell><cell>39.36</cell></row><row><cell>256</cell><cell>85.10</cell><cell>81.40</cell><cell>72.13</cell><cell>57.90</cell><cell>55.10</cell><cell>40.29</cell></row><row><cell>512</cell><cell>86.99</cell><cell>82.02</cell><cell>71.66</cell><cell>57.17</cell><cell>53.89</cell><cell>38.80</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE VI THE</head><label>VI</label><figDesc>EXPERIMENTAL RESULTS OF L hc tri IS COMPARED TO TRADITIONAL TRIPLET LOSS L bh tri ON THE REGDB AND SYSU-MM01 DATASETS. RE-IDENTIFICATION RATES OF RANK1, MAP AND MINP (%). NOTE THAT L bh tri LOSS IS NOT CONVERGED ON SYSU-MM01 DATASET WHEN λ ≥ 0.5. BASED LOSSES ON THE REGDB AND SYSU-MM01 DATASETS, INCLUDING THE LEARNED CENTER LOSS L lc , HETERO-CENTER LOSS L hc , AND OUR PROPOSED HETERO-CENTER TRIPLET LOSS L hc tri . RE-IDENTIFICATION RATES OF RANK1, MAP AND MINP (%).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">RegDB</cell><cell></cell><cell></cell><cell cols="2">SYSU-MM01</cell></row><row><cell>Loss</cell><cell>λ</cell><cell>rank1</cell><cell>mAP</cell><cell cols="2">mINP</cell><cell cols="2">rank1</cell><cell>mAP</cell><cell>mINP</cell></row><row><cell></cell><cell>0.1</cell><cell>80.68</cell><cell>75.35</cell><cell cols="2">64.50</cell><cell cols="2">55.30</cell><cell>54.21</cell><cell>41.09</cell></row><row><cell>L bh tri</cell><cell>0.5 1.0</cell><cell>82.43 85.10</cell><cell>78.93 81.40</cell><cell cols="2">69.22 72.13</cell><cell>--</cell><cell></cell><cell>--</cell><cell>--</cell></row><row><cell></cell><cell>1.5</cell><cell>72.33</cell><cell>69.67</cell><cell cols="2">60.55</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>0.1</cell><cell>80.73</cell><cell>75.08</cell><cell cols="2">64.14</cell><cell cols="2">57.53</cell><cell>54.68</cell><cell>40.22</cell></row><row><cell></cell><cell>0.5</cell><cell>87.96</cell><cell>81.97</cell><cell cols="2">71.87</cell><cell cols="2">60.43</cell><cell>56.41</cell><cell>40.56</cell></row><row><cell>L hc tri</cell><cell>1.0 1.5</cell><cell>85.24 90.63</cell><cell>81.76 83.64</cell><cell cols="2">72.33 71.21</cell><cell cols="2">61.95 57.45</cell><cell>57.25 53.01</cell><cell>40.44 36.93</cell></row><row><cell></cell><cell>2.0</cell><cell>92.48</cell><cell>84.41</cell><cell cols="2">71.53</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>3.0</cell><cell>88.93</cell><cell>78.64</cell><cell cols="2">62.22</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE VII</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">THE RESULTS OF DIFFERENT CENTER-RegDB</cell><cell></cell><cell></cell><cell></cell><cell>SYSU-MM01</cell></row><row><cell>Network</cell><cell></cell><cell>Loss</cell><cell>rank1</cell><cell>mAP</cell><cell cols="2">mINP</cell><cell>rank1</cell><cell>mAP</cell><cell>mINP</cell></row><row><cell></cell><cell></cell><cell>L lc</cell><cell>44.76</cell><cell>40.60</cell><cell cols="2">27.52</cell><cell>52.67</cell><cell>50.48</cell><cell>36.84</cell></row><row><cell>Global-level</cell><cell></cell><cell>L hc</cell><cell>52.96</cell><cell>44.52</cell><cell cols="2">27.05</cell><cell>51.30</cell><cell>48.12</cell><cell>33.73</cell></row><row><cell></cell><cell cols="2">L hc tri</cell><cell>79.22</cell><cell>68.35</cell><cell cols="2">48.87</cell><cell>57.61</cell><cell>53.03</cell><cell>37.22</cell></row><row><cell></cell><cell></cell><cell>L lc</cell><cell>67.38</cell><cell>64.30</cell><cell cols="2">54.83</cell><cell>46.02</cell><cell>47.55</cell><cell>36.70</cell></row><row><cell>Part-level</cell><cell></cell><cell>L hc</cell><cell>85.34</cell><cell>80.83</cell><cell cols="2">70.46</cell><cell>47.83</cell><cell>46.22</cell><cell>32.48</cell></row><row><cell>(ours)</cell><cell cols="2">L hc tri</cell><cell>92.48</cell><cell>84.41</cell><cell cols="2">71.53</cell><cell>61.95</cell><cell>57.25</cell><cell>40.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VIII THE</head><label>VIII</label><figDesc>ABLATION STUDY OF DIFFERENT COMPONENTS: WEIGHT SHARING (ws), HETERO-CENTER TRIPLET LOSS (L hc tri ) AND PART-LEVEL FEATURE LEARNING (plf ). RE-IDENTIFICATION RATES OF RANK1, MAP AND MINP (%). OF-THE-ART METHODS ON THE REGDB DATASETS IN VISIBLE → THERMAL AND THERMAL → VISIBLE QUERY SETTINGS. RE-IDENTIFICATION RATES AT RANK R, MAP AND MINP (%).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RegDB</cell><cell></cell><cell>SYSU-MM01</cell><cell></cell></row><row><cell>ws</cell><cell>L hc tri</cell><cell>plf</cell><cell>rank1</cell><cell>mAP</cell><cell>mINP</cell><cell>rank1</cell><cell>mAP</cell><cell>mINP</cell></row><row><cell>×</cell><cell>×</cell><cell>×</cell><cell>77.52</cell><cell>69.79</cell><cell>54.58</cell><cell>54.38</cell><cell>52.18</cell><cell>38.57</cell></row><row><cell></cell><cell>×</cell><cell>×</cell><cell>77.14</cell><cell>69.82</cell><cell>54.62</cell><cell>57.09</cell><cell>54.99</cell><cell>41.26</cell></row><row><cell></cell><cell></cell><cell>×</cell><cell>79.22</cell><cell>68.35</cell><cell>48.87</cell><cell>57.61</cell><cell>53.03</cell><cell>37.22</cell></row><row><cell></cell><cell>×</cell><cell></cell><cell>85.10</cell><cell>81.40</cell><cell>72.13</cell><cell>55.30</cell><cell>54.21</cell><cell>41.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell>92.48</cell><cell>84.41</cell><cell>71.53</cell><cell>61.95</cell><cell>57.25</cell><cell>40.44</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TABLE IX</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">COMPARISON TO THE STATE-Methods Venue r = 1</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell><cell>mINP</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Visible → Thermal</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Zero-Pad [27]</cell><cell cols="2">ICCV17</cell><cell>17.75</cell><cell>34.21</cell><cell>44.35</cell><cell>18.90</cell><cell>-</cell></row><row><cell cols="2">HCML [31]</cell><cell cols="2">AAAI18</cell><cell>24.44</cell><cell>47.53</cell><cell>56.78</cell><cell>20.80</cell><cell>-</cell></row><row><cell cols="2">HSME [6]</cell><cell cols="2">AAAI19</cell><cell>50.85</cell><cell>73.36</cell><cell>81.66</cell><cell>47.00</cell><cell>-</cell></row><row><cell cols="2">D 2 RL [24]</cell><cell cols="2">CVPR19</cell><cell>43.40</cell><cell>66.10</cell><cell>76.30</cell><cell>44.10</cell><cell>-</cell></row><row><cell cols="2">MAC [30]</cell><cell cols="2">MM19</cell><cell>36.43</cell><cell>62.36</cell><cell>71.63</cell><cell>37.03</cell><cell>-</cell></row><row><cell cols="2">AliGAN [20]</cell><cell cols="2">ICCV19</cell><cell>57.90</cell><cell>-</cell><cell>-</cell><cell>53.60</cell><cell>-</cell></row><row><cell cols="2">DFE [5]</cell><cell cols="2">MM19</cell><cell>70.13</cell><cell>86.32</cell><cell>91.96</cell><cell>69.14</cell><cell>-</cell></row><row><cell cols="2">eBDTR [33]</cell><cell cols="2">TIFS20</cell><cell>34.62</cell><cell>58.96</cell><cell>68.72</cell><cell>33.46</cell><cell>-</cell></row><row><cell cols="2">MSR [4]</cell><cell>TIP20</cell><cell></cell><cell>48.43</cell><cell>70.32</cell><cell>79.95</cell><cell>48.67</cell><cell>-</cell></row><row><cell cols="2">JSIA [21]</cell><cell cols="2">AAAI20</cell><cell>48.50</cell><cell>-</cell><cell>-</cell><cell>48.90</cell><cell>-</cell></row><row><cell cols="2">EDFL [12]</cell><cell cols="2">Neuro20</cell><cell>52.58</cell><cell>72.10</cell><cell>81.47</cell><cell>52.98</cell><cell>-</cell></row><row><cell cols="2">XIV [11]</cell><cell cols="2">AAAI20</cell><cell>62.21</cell><cell>83.13</cell><cell>91.72</cell><cell>60.18</cell><cell>-</cell></row><row><cell cols="2">CDP [3]</cell><cell cols="2">Arxiv20</cell><cell>65.00</cell><cell>83.50</cell><cell>89.60</cell><cell>62.70</cell><cell>-</cell></row><row><cell cols="2">expAT [29]</cell><cell cols="2">Arxiv20</cell><cell>66.48</cell><cell>-</cell><cell>-</cell><cell>67.31</cell><cell>-</cell></row><row><cell cols="2">CMSP [26]</cell><cell cols="2">IJCV20</cell><cell>65.07</cell><cell>83.71</cell><cell>-</cell><cell>64.50</cell><cell>-</cell></row><row><cell cols="2">Hi-CMD [1]</cell><cell cols="2">CVPR20</cell><cell>70.93</cell><cell>86.39</cell><cell>-</cell><cell>66.04</cell><cell>-</cell></row><row><cell cols="2">HAT [34]</cell><cell cols="2">TIFS20</cell><cell>71.83</cell><cell>87.16</cell><cell>92.16</cell><cell>67.56</cell><cell>-</cell></row><row><cell cols="2">cmSSFT [13]</cell><cell cols="2">CVPR20</cell><cell>72.30</cell><cell>-</cell><cell>-</cell><cell>72.90</cell><cell>-</cell></row><row><cell cols="2">MPMN [23]</cell><cell cols="2">TMM20</cell><cell>86.56</cell><cell>96.68</cell><cell>98.28</cell><cell>82.91</cell><cell>-</cell></row><row><cell cols="2">AGW [33]</cell><cell cols="2">Arxiv20</cell><cell>70.05</cell><cell>-</cell><cell>-</cell><cell>66.37</cell><cell>50.19</cell></row><row><cell>ours</cell><cell></cell><cell>-</cell><cell></cell><cell>91.05</cell><cell>97.16</cell><cell>98.57</cell><cell>83.28</cell><cell>68.84</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Thermal → Visible</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Zero-Pad [27]</cell><cell cols="2">ICCV17</cell><cell>16.63</cell><cell>34.68</cell><cell>44.25</cell><cell>17.82</cell><cell>-</cell></row><row><cell cols="2">HCML [31]</cell><cell cols="2">AAAI18</cell><cell>21.70</cell><cell>45.02</cell><cell>55.58</cell><cell>22.24</cell><cell>-</cell></row><row><cell cols="2">eBDTR [33]</cell><cell cols="2">TIFS20</cell><cell>34.21</cell><cell>58.74</cell><cell>68.64</cell><cell>32.49</cell><cell>-</cell></row><row><cell cols="2">MAC [30]</cell><cell cols="2">MM19</cell><cell>36.20</cell><cell>61.68</cell><cell>70.99</cell><cell>39.23</cell><cell>-</cell></row><row><cell cols="2">HSME [6]</cell><cell cols="2">AAAI19</cell><cell>50.15</cell><cell>72.40</cell><cell>81.07</cell><cell>46.16</cell><cell>-</cell></row><row><cell cols="2">EDFL [12]</cell><cell cols="2">Neuro20</cell><cell>51.89</cell><cell>72.09</cell><cell>81.04</cell><cell>52.13</cell><cell>-</cell></row><row><cell cols="2">AliGAN [20]</cell><cell cols="2">ICCV19</cell><cell>56.30</cell><cell>-</cell><cell>-</cell><cell>53.40</cell><cell>-</cell></row><row><cell cols="2">expAT [29]</cell><cell cols="2">Arxiv20</cell><cell>67.45</cell><cell>-</cell><cell>-</cell><cell>66.51</cell><cell>-</cell></row><row><cell cols="2">MPMN [23]</cell><cell cols="2">TMM20</cell><cell>84.62</cell><cell>95.51</cell><cell>97.33</cell><cell>79.49</cell><cell>-</cell></row><row><cell>ours</cell><cell></cell><cell>-</cell><cell></cell><cell>89.30</cell><cell>96.41</cell><cell>98.16</cell><cell>81.46</cell><cell>64.81</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE X COMPARISON</head><label>X</label><figDesc>TO THE STATE-OF-THE-ART METHODS ON THE SYSU-MM01 DATASETS. RE-IDENTIFICATION RATES AT RANK R, MAP AND MINP (%).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>All search</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Indoor search</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Venue</cell><cell>r = 1</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell><cell>mINP</cell><cell>r = 1</cell><cell>r = 10</cell><cell>r = 20</cell><cell>mAP</cell><cell>mINP</cell></row><row><cell>Zero-Pad [27]</cell><cell>ICCV17</cell><cell>14.80</cell><cell>54.12</cell><cell>71.33</cell><cell>15.95</cell><cell>-</cell><cell>20.58</cell><cell>68.38</cell><cell>85.79</cell><cell>26.92</cell><cell>-</cell></row><row><cell>cmGAN [2]</cell><cell>IJCAI18</cell><cell>26.97</cell><cell>67.51</cell><cell>80.56</cell><cell>27.80</cell><cell>-</cell><cell>31.63</cell><cell>77.23</cell><cell>89.18</cell><cell>42.19</cell><cell>-</cell></row><row><cell>HCML [31]</cell><cell>AAAI18</cell><cell>14.32</cell><cell>53.16</cell><cell>69.17</cell><cell>16.16</cell><cell>-</cell><cell>24.52</cell><cell>73.25</cell><cell>86.73</cell><cell>30.08</cell><cell>-</cell></row><row><cell>HSME [6]</cell><cell>AAAI19</cell><cell>20.68</cell><cell>62.74</cell><cell>77.95</cell><cell>23.12</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>D 2 RL [24]</cell><cell>CVPR19</cell><cell>28.90</cell><cell>70.60</cell><cell>82.40</cell><cell>29.20</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MAC [30]</cell><cell>MM19</cell><cell>33.26</cell><cell>79.04</cell><cell>90.09</cell><cell>36.22</cell><cell>-</cell><cell>36.43</cell><cell>62.36</cell><cell>71.63</cell><cell>37.03</cell><cell>-</cell></row><row><cell>AliGAN [20]</cell><cell>ICCV19</cell><cell>42.40</cell><cell>85.00</cell><cell>93.70</cell><cell>40.70</cell><cell>-</cell><cell>45.90</cell><cell>87.60</cell><cell>94.40</cell><cell>54.30</cell><cell>-</cell></row><row><cell>HPILN [38]</cell><cell>TIP19</cell><cell>41.36</cell><cell>84.78</cell><cell>94.51</cell><cell>42.95</cell><cell>-</cell><cell>45.77</cell><cell>91.82</cell><cell>98.46</cell><cell>56.52</cell><cell>-</cell></row><row><cell>DFE [5]</cell><cell>MM19</cell><cell>48.71</cell><cell>88.86</cell><cell>95.27</cell><cell>48.59</cell><cell>-</cell><cell>52.25</cell><cell>89.86</cell><cell>95.85</cell><cell>59.68</cell><cell>-</cell></row><row><cell>Hi-CMD [1]</cell><cell>CVPR20</cell><cell>34.94</cell><cell>77.58</cell><cell>-</cell><cell>35.94</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>EDFL [12]</cell><cell>Neuro20</cell><cell>36.94</cell><cell>85.42</cell><cell>93.22</cell><cell>40.77</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CDP [3]</cell><cell>Arxiv20</cell><cell>38.00</cell><cell>82.30</cell><cell>91.70</cell><cell>38.40</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>expAT [29]</cell><cell>Arxiv20</cell><cell>38.57</cell><cell>76.64</cell><cell>86.39</cell><cell>38.61</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>XIV [11]</cell><cell>AAAI20</cell><cell>49.92</cell><cell>89.79</cell><cell>95.96</cell><cell>50.73</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>eBDTR [33]</cell><cell>TIFS20</cell><cell>27.82</cell><cell>67.34</cell><cell>81.34</cell><cell>28.42</cell><cell>-</cell><cell>32.46</cell><cell>77.42</cell><cell>89.62</cell><cell>42.46</cell><cell>-</cell></row><row><cell>MSR [4]</cell><cell>TIP20</cell><cell>37.35</cell><cell>83.40</cell><cell>93.34</cell><cell>38.11</cell><cell>-</cell><cell>39.64</cell><cell>89.29</cell><cell>97.66</cell><cell>50.88</cell><cell>-</cell></row><row><cell>JSIA [21]</cell><cell>AAAI20</cell><cell>38.10</cell><cell>80.70</cell><cell>89.90</cell><cell>36.90</cell><cell>-</cell><cell>43.80</cell><cell>86.20</cell><cell>94.20</cell><cell>52.90</cell><cell>-</cell></row><row><cell>CMSP [26]</cell><cell>IJCV20</cell><cell>43.56</cell><cell>86.25</cell><cell>-</cell><cell>44.98</cell><cell>-</cell><cell>48.62</cell><cell>89.50</cell><cell>-</cell><cell>57.50</cell><cell>-</cell></row><row><cell>Attri [36]</cell><cell>JEI20</cell><cell>47.14</cell><cell>87.93</cell><cell>94.45</cell><cell>47.08</cell><cell>-</cell><cell>48.03</cell><cell>88.13</cell><cell>95.14</cell><cell>56.84</cell><cell>-</cell></row><row><cell>HAT [34]</cell><cell>TIFS20</cell><cell>55.29</cell><cell>92.14</cell><cell>97.36</cell><cell>53.89</cell><cell>-</cell><cell>62.10</cell><cell>95.75</cell><cell>99.20</cell><cell>69.37</cell><cell>-</cell></row><row><cell>HC [39]</cell><cell>Neuro20</cell><cell>56.96</cell><cell>91.50</cell><cell>96.82</cell><cell>54.95</cell><cell>-</cell><cell>59.74</cell><cell>92.07</cell><cell>96.22</cell><cell>64.91</cell><cell>-</cell></row><row><cell>AGW [33]</cell><cell>Arxiv20</cell><cell>47.50</cell><cell>-</cell><cell>-</cell><cell>47.65</cell><cell>35.30</cell><cell>54.17</cell><cell>-</cell><cell>-</cell><cell>62.97</cell><cell>59.23</cell></row><row><cell>ours</cell><cell>-</cell><cell>61.68</cell><cell>93.10</cell><cell>97.17</cell><cell>57.51</cell><cell>39.54</cell><cell>63.41</cell><cell>91.69</cell><cell>95.28</cell><cell>68.17</cell><cell>64.26</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/hijune6/Hetero-center-triplet-loss-for-VT-Re-ID</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that to simply show the effectiveness of different components, during the ablation experiments, we only reported one trial experimental results, rather than the mean results of 10 trials.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Note that in this subsection, we reported the mean results of 10 trials following the standard dataset settings.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hi-cmd: Hierarchical cross-modality disentanglement for visible-infrared person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cross-modality person re-identification with generative adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="677" to="683" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cross-spectrum dual-subspace pairing for rgb-infrared cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.00213</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning modality-specific representations for visible-infrared person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="579" to="590" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dual-alignment feature embedding for cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hsme: Hypersphere manifold embedding for visible thermal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8385" to="8392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">In defense of the triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07737</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Person re-identification between visible and thermal camera images based on deep residual cnn using single input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="57" to="972" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Thermalgan: Multimodal color-to-thermal image translation for person re-identification in multispectral dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Kniaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">A</forename><surname>Knyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hladuvka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">G</forename><surname>Kropatsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mizginov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Infrared-visible cross-modal person re-identification with an x modality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Enhancing the discriminative feature learning for visible-thermal cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">398</biblScope>
			<biblScope unit="page" from="11" to="19" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Crossmodality person re-identification with shared-specific feature transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A strong baseline and batch normalization neck for deep person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Person recognition system based on a combination of body images from visible light and thermal cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">605</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fine-tuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling (and a strong convolutional baseline)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="501" to="518" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Concentrated local part discovery with fine-grained part representation for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1605" to="1618" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rgb-infrared cross-modality person re-identification via joint pixel and feature alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3623" to="3632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cross-modality paired-images generation for rgb-infrared person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning discriminative features with multiple granularities for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep multi-patch matching network for visible thermal person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to reduce dual-level discrepancy for infrared-visible person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rgb-ir person reidentification by cross-modality similarity preservation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<biblScope unit="page" from="1765" to="1785" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Rgb-infrared cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5380" to="5389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Part-aware progressive unsupervised domain adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMM</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bi-directional exponential angular triplet loss for rgb-infrared person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.00878</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Modality-aware collaborative learning for visible thermal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Leng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Hierarchical discriminative learning for visible thermal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bi-directional centerconstrained top-ranking for visible thermal person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIFS</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="407" to="419" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep learning for person re-identification: A survey and outlook</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.04193</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visible-infrared person re-identification via homogeneous augmented tri-modal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIFS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visible thermal person re-identification via dual-constrained top-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="1092" to="1099" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep feature learning with attributes for cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Attend to the difference: Cross-modality person re-identification via contrastive correlation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11656</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hpiln: a feature learning framework for cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="2897" to="2904" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hetero-center loss for cross-modality person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">386</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
