<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">An End-to-End Multi-Task Learning to Link Framework for Emotion-Cause Pair Extraction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haolin</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuchi</forename><surname>Li</surname></persName>
							<email>qiuchili@dei.unipd.it</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Padua</orgName>
								<address>
									<settlement>Padua</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Song</surname></persName>
							<email>dwsong@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">An End-to-End Multi-Task Learning to Link Framework for Emotion-Cause Pair Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion-cause pair extraction (ECPE), as an emergent natural language processing task, aims at jointly investigating emotions and their underlying causes in documents. It extends the previous emotion cause extraction (ECE) task, yet without requiring a set of pre-given emotion clauses as in ECE. Existing approaches to ECPE generally adopt a twostage method, i.e., (1) emotion and cause detection, and then (2) pairing the detected emotions and causes. Such pipeline method, while intuitive, suffers from two critical issues, including error propagation across stages that may hinder the effectiveness, and high computational cost that would limit the practical application of the method. To tackle these issues, we propose a multi-task learning model that can extract emotions, causes and emotion-cause pairs simultaneously in an end-to-end manner. Specifically, our model regards pair extraction as a link prediction task, and learns to link from emotion clauses to cause clauses, i.e., the links are directional. Emotion extraction and cause extraction are incorporated into the model as auxiliary tasks, which further boost the pair extraction. Experiments are conducted on an ECPE benchmarking dataset. The results show that our proposed model outperforms a range of state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Emotion cause extraction (ECE)  aims to extract possible causes for a given emotion clause. While ECE has attracted an increasing attention due to its theoretical and practical significance, it requires that the emotion signals should be given. In practice emotion annotation is rather labor intensive, limiting the applicability of ECE in practical settings. To address the limitation of ECE, the emotion-cause pair extraction (ECPE) task was recently proposed in <ref type="bibr" target="#b19">(Xia and Ding 2019)</ref>. Unlike ECE , ECPE aims to extract emotions and causes without any given emotion signals, and thus better aligns with real-world applications. An illustrative example is given in <ref type="figure" target="#fig_0">Figure 1(a)</ref>, showing that clause 4 serves as the emotion and clauses 2 and 3 are the corresponding causes. Typically, ECPE is formulated as extracting emotion-cause pairs, e.g., (clause 4, clause 2) and (clause 4, clause 3), directly from provided documents. ECPE is a challenging task, as it requires the extraction of emotions, causes and emotion-cause pairs. Existing work in ECPE mainly focuses on how to collaboratively extract emotions and causes and combine them in an appropriate way. Thus, a two-stage method <ref type="bibr" target="#b19">(Xia and Ding 2019)</ref> is typically adopted, which divides pair extraction into two steps: firstly detecting emotions and causes, and then pairing them based on the likelihood of cartesian products between them. Such pipeline method is intuitive and straightforward. However, one critical issue arises, which is the error propagation from the first step to the second.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emotion Cause Pair</head><p>To tackle the issue, we propose an end-to-end multi-task learning model for predicting emotion-cause pairs, namely E2EECPE. Our model takes an innovative perspective by viewing ECPE as a link prediction problem, and connects the emotion/cause extraction and link prediction within one single stage. Particularly, as shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>, the model predicts whether there exists a directional link from an emotion clause to a cause clause. Meanwhile, we incorporate into the model two auxiliary tasks, namely emotion extraction and cause extraction, which are oriented to further en-hance the expressiveness of the intermediate emotion and cause representations. These are placed in a carefully designed end-to-end multi-task learning architecture, which helps resolving the issue of error propagation.</p><p>Extensive experiments are carried out on a benchmarking ECPE dataset. The experimental results demonstrate the effectiveness of the proposed E2EECPE model, in comparison with a variety of state-of-the-art baselines. Moreover, a further ablation study indicates that the auxiliary tasks are beneficial. We have also shown that the model can be applied to an extended task: emotion-cause triplet extraction, and achieves a promising pewrformance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>First of all, our work is related to extracting causes based on emotions explicitly presented in documents, i.e., ECE . Earlier work views ECE as a wordlevel sequence tagging problem and tries to solve it with corresponding tagging techniques. Therefore, primary efforts have been made on discovering refined linguistic features <ref type="bibr" target="#b14">Lee et al. 2013)</ref>, yielding improved performance. In line with other tagging related tasks such as named entity recognition (NER), support vector machines (SVMs) <ref type="bibr" target="#b8">(Gui et al. 2014</ref>) and conditional random fields (CRFs) <ref type="bibr" target="#b12">(Lafferty, McCallum, and Pereira 2001)</ref> have been used for ECE. More recently, instead of concentrating on word-level cause detection, clause-level extraction <ref type="bibr" target="#b7">(Gui et al. 2016)</ref> is put forward in that the impact of individual words in a cause can span over the whole sequence in the clause.</p><p>With the emergence and development of deep representation learning, neural models has also been utilized in ECE. <ref type="bibr" target="#b4">(Cheng et al. 2017</ref>) leverages long short-term memory networks (LSTMs) <ref type="bibr" target="#b10">(Hochreiter and Schmidhuber 1997)</ref> to promote the context awareness of clause modelling. <ref type="bibr" target="#b6">(Gui et al. 2017)</ref> views the information extraction problem as the retrieval task in question answering (QA) and examines the effect of memory networks <ref type="bibr" target="#b18">(Sukhbaatar et al. 2015)</ref> for extraction. Likewise, taking advantage of attention mechanism <ref type="bibr" target="#b0">(Bahdanau, Cho, and Bengio 2014)</ref>, ) employs a co-attention based model and achieves the stateof-the-art performance.</p><p>In light of recent advances in multi-task learning, joint extraction of emotions and causes is investigated <ref type="bibr" target="#b2">(Chen et al. 2018</ref>) to exploit the mutual information between two correlated tasks. However, these works do not explicitly combine two tasks into one. Thereafter, <ref type="bibr" target="#b19">(Xia and Ding 2019)</ref> argues that, while co-extraction of emotions and causes is important, emotion-cause pair extraction (ECPE) is a more challenging problem that is worth putting more emphasis on. Nevertheless, (Xia and Ding 2019) adopts a two-stage approach, which performs emotion and cause extraction first and then pairs the extracted emotions and causes. As discussed in the previous section, such two stage approach suffers from error propagation and high computation cost.</p><p>Our work aims to tackle these challenges in ECPE. Rather than processing emotion-cause pair extraction as a two stage task (as used in the existing work), we consolidate two stages into a unified multi-task learning framework, and fur-ther consider it as a link prediction task which could be solved in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>To solve ECPE in an end-to-end fashion, we take inspiration from the link prediction problem in graph learning, which aims at predicting potential edges between unconnected vertices in a graph. Essentially, if we consider emotion-cause pairs in a document as triplets in a graph, then the extraction of such pairs is a sort of link prediction from a graph that is at first armed with no edges but only vertices. In order to achieve above procedure, we borrow the idea of learning a graph-based dependency parser <ref type="bibr" target="#b5">(Dozat and Manning 2016)</ref> and adapt it to our target task. Coupling link prediction with auxiliary emotion extraction and cause extraction tasks, our model is capable of jointly, and more effectively, extracting emotions, causes, and emotion-cause pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Formulation</head><p>Generally, for any provided document D, we could split it into a sequence of clauses based on punctuation, i.e.,</p><formula xml:id="formula_0">D = {c i } |D| i=1 , where c i could further be decomposed into words, i.e., c i = {w j } |ci| j=1 .</formula><p>Here, |D| is the number of clauses in the document and |c i | is the number of words in the i-th clause. ECPE aims to extract a set of |P | emotion-</p><formula xml:id="formula_1">cause pairs P = {(c e k , c c k )} |P | k=1 from the document D,</formula><p>where c e k , c c k represents the emotion clause and the cause clause in the k-th pair, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Overall Architecture</head><p>An overview of the proposed E2EECPE approach is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The bottom layer is a clause encoder (Section 3.3) and a document modelling layer (Section 3.4) which transform the word embeddings into the contextualized clause representations. The middle part consists of auxiliary tasks (Section 3.7), i.e., emotion extraction and cause extraction. The top most part is a biaffine attention layer (Section 3.5) which first encodes interaction between the emotion representation and cause representation, and then outputs a postion-weighted pair matrix for pair extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Embedding &amp; Clause Encoder</head><p>With the purpose of integrating words into clause-level neural models, we embed each word in a clause into lowdimensional vectors <ref type="bibr" target="#b1">(Bengio et al. 2003)</ref>, by which we could represent each word in the clause with its vector representation 1 c i = {w j } |ci| j=1 , where w j ∈ R de and d e is the dimensionality of the embedding.</p><p>After that, we need to attain contextualized representations of clauses. Owing to the recognized performance and local context awareness of the convolutional neural networks (CNNs) on text classification benchmarks <ref type="bibr" target="#b11">(Kim 2014)</ref>, we adopt CNNs as the backbone of our clause encoder. For an embedded clause c = {w j } |c| j=1 , we apply onedimensional convolution operations with kernels of different sizes over the word sequence:</p><formula xml:id="formula_2">C t = σ(conv t (w 1 , · · · , w |c| ))<label>(1)</label></formula><p>where conv t denotes the t-th convolution operation and C t ∈ R |c|×dc is the output of the operation. d c is the number of filters employed in one convolution operation and σ(·) used here is actually max(0, ·).</p><p>Then max-pooling is used to distill the features for concatenation. Hence, we finally get context-aware features for the clause:c</p><formula xml:id="formula_3">t = maxpool(C t ) (2) c = [⊕ tct ]<label>(3)</label></formula><p>where c ∈ R |t|·dc is the convoluted feature and ⊕ means vector concatenation. |t| is the total number of convolution operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Document Modelling</head><p>Since we have sequential clauses in a document, the influences brought by document-level structures become a crucial part that we should fit into our model. A straightforward idea is to leverage temporal relations among clauses with LSTMs. Specifically, provided with the encoded clause representations {c i } |c| i=1 , we employ a bidirectional LSTM to update clause-level features and get h i ∈ R 2d h :</p><formula xml:id="formula_4">h i = [LSTM f (c i ) ⊕ LSTM b (c i )]<label>(4)</label></formula><p>where LSTM f (·) and LSTM b (·) denote the forward and backward unidirectional LSTMs, respectively. d h is the dimensionality of hidden states for a unidirectional LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Biaffine Attention</head><p>Motivated by advances in link prediction <ref type="bibr" target="#b17">(Schlichtkrull et al. 2018)</ref>, we can directly compute the similarity scores among vertex representations (clause representations in our task), e.g., σ(z p z q ) for any representations of vertex p and q, to make predictions. However, the above predictions are only concerned with undirectional circumstances since z p z q = z q z p , which is not adequate for emotion-cause pair extraction. To solve the problem, we utilize biaffine transform to complete the filling of adjacent matrices, which are called pair matrices in our work. This idea is similar to dependency parsing <ref type="bibr" target="#b5">(Dozat and Manning 2016)</ref> that is also directional.</p><p>Emotion &amp; Cause Representation According to biaffine attention mechanism, each vertex in the graph should have two independent representations, i.e., one is for pointing out and the other for pointed in. In doing so, the pair matrix output by the transformation is asymmetric and directionaware.</p><p>The emotion representation and cause representation are separately offered as below:</p><formula xml:id="formula_5">z e i = σ(W e h i + b e )<label>(5)</label></formula><formula xml:id="formula_6">z c i = σ(W c h i + b c )<label>(6)</label></formula><p>where</p><formula xml:id="formula_7">W e ∈ R dz×2d h , b e ∈ R dz and W c ∈ R dz×2d h , b c ∈ R dz</formula><p>are two sets of trainable weights and biases, respectively for the emotion and cause representations.</p><p>Biaffine Transform Then, we implement biaffine transform on the collected emotion and cause representations. In other words, with the purpose of merging these two kinds of representations into our aimed pair matrices, we fold emotion-cause dynamics into two components. On the one hand, we need to perform a bilinear like operation on each possible pair of emotion and cause. On the other hand, we believe bilinear transform is not enough to deal with such complicated interactions, and thus we facilitate it by injecting bias. More specifically, we calculate each entry in the expected pair matrix as follows:</p><formula xml:id="formula_8">M p,q = (W m z e p + b m ) z c q (7)</formula><p>where W m ∈ R dz×dz and b m ∈ R dz are learnable parameters of affine transform, while M p,q indicates an entry of the pair matrix in the p-th row, q-th column.</p><p>Constrained by the inherent property of an adjacent matrix, we further activate the pair matrix with the sigmoid function g(·):M</p><formula xml:id="formula_9">p,q = g(M i,j )<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Position Weight Matrix</head><p>A trivial observation on the co-occurrence patterns of emotions and causes is that the emotion and the cause in a unique pair appear near each other in term of their absolute positions in the document. Thus, position embeddings are introduced to directly encode positions into vectors <ref type="bibr" target="#b19">(Xia and Ding 2019)</ref>. Different from the existing approaches, our work is based on graph learning, and thereby can not be aided by manipulation of embeddings. Instead, we apply proximity weights on features as in <ref type="bibr" target="#b20">(Zhang, Li, and Song 2019)</ref>, but extent it to matrices. Moreover, we notice that in reality people are more likely to inform the causes before expressing emotions. This is verified in <ref type="figure" target="#fig_2">Figure 3</ref>, where the matrices of ground truth pairs in all documents with document length less than 20 are visualized. In the matrix of <ref type="figure" target="#fig_2">Figure 3</ref>, the horizontal axis represents the sequence number of the emotion clauses, the vertical axis represents the sequence number of the cause clauses, and each element represents the number of emotioncause pairs. The diagonal position indicates the number of emotion-cause pairs in the same clauses. The brighter the color, the more pairs there are. It can be seen from the figure that causes tend to appear in the clause before emotions. It implies that we should consider constraining the original matrix representations with the position bias.</p><p>Therefore we propose two position weight methods: (1) Asymmetric Position Weight Matrix, and (2) Ground Truth Weight Matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Asymmetric Position Weight Matrix</head><p>We calculate asymmetric position weight matrix with all training documents:</p><formula xml:id="formula_10">A asw p,q = |D| − |p − q − 1| + asw |D| + asw<label>(9)</label></formula><p>where asw is a small number for smoothing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth Weight Matrix</head><p>We directly use ground truth pairs in training documents to construct the weight matrix:</p><formula xml:id="formula_11">A gtw p,q = σ( |D| i=1 pairs i + gtw )<label>(10)</label></formula><p>where gtw is a small number for smoothing and pairs i is all ground truth pairs in document i , if there is pair (p, q) in document i , then pairs i = 1. </p><p>where denotes element-wise multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Multi-task Setting</head><p>As aforementioned, emotion extraction and cause extraction can be viewed as auxiliary tasks to augment the emotion and cause representations for constructing more expressive pair matrix. Hence we develop a multi-task paradigm, which shares the fundamental part of network structure for the main task with auxiliary tasks. To achieve this goal, we first acquire features dedicated for classification with following procedure:z</p><formula xml:id="formula_13">e i = σ(W e h i +b e )<label>(12)</label></formula><formula xml:id="formula_14">z c i = σ(W c h i +b c )<label>(13)</label></formula><p>whereW e ∈ R dz×2d h ,b e ∈ R dz andW c ∈ R dz×2d h , b c ∈ R dz are again two sets of trainable weights and biases, respectively. Subsequently, predictions are produced by two fully connected layers followed by softmax normalization layers:</p><formula xml:id="formula_15">y e = softmax(Ŵ eze i +b e )<label>(14)</label></formula><formula xml:id="formula_16">y c = softmax(Ŵ czc i +b c )<label>(15)</label></formula><p>whereŴ e ∈ R 2×dz ,b e ∈ R 2 andŴ c ∈ R 2×dz ,b c ∈ R 2 are weights and biases for learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8">Training Objective</head><p>Eventually, the whole structure can be trained by standard gradient descent. Accordingly, the objective function is a combination of cross entropy with L 2 -norm regularization, formulated as below: <ref type="formula" target="#formula_2">(17)</ref> where (p, q) and i, k serve as enumerators over all elements. y e , y c , and Y are correspondingly the ground truth. Furthermore, we add two coefficients to balance the influences of above two objective functions. The ultimate training objective then becomes:</p><formula xml:id="formula_17">L pair = − p,q Y p,q log(M p,q ) − p,q (1 − Y p,q )log(1 −M p,q ) (16) L aux = − i [ k y e k log(ŷ e k ) + k y c k log(ŷ c k )]</formula><formula xml:id="formula_18">L = L pair + βL aux + λ||θ|| 2<label>(18)</label></formula><p>where the term β is used to adjust the potential influences brought by multi-task learning, which is refined according to a pilot study. θ stands for all parameters that need to be optimized, while λ is a coefficient for L 2 -norm regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9">Inference</head><p>With the well trained model, we can infer emotion-cause pairs by comparing each entry inM with a predefined threshold ηŶ</p><formula xml:id="formula_19">p,q = 1,M p,q &gt; η 0,M p,q ≤ η<label>(19)</label></formula><p>whereŶ is the inference result matrix with binary (1-0) indicators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setting</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We carry out experiments on a publicly available dataset released by(Xia and Ding 2019) for emotion-cause pair extraction, which is the only publically available dataset we can currently access, and most of published papers in the area are experimented based on it. Consisting of news crawled from web, the dataset is referred to as NEWS in the rest of the paper. To facilitate a fair comparison, we use the same data split method as in <ref type="bibr" target="#b19">(Xia and Ding 2019)</ref>. The dataset is randomly split into ten folds, each of them has a similar amount of data. In our experiments, the evaluation is done in 10 runs where each run uses 9 folds as training data and the remaining one as test data (different for different runs), which respectively take 90% and 10% of the data. <ref type="table">Table 1</ref> shows some basic statistics of the dataset. A key observation is that most documents only contain one emotion-cause pair therein, implying the sparsity of the pair matrix. Therefore the issue of label imbalance will be elaborated in following discussions. Moreover, a large amount of emotion-cause pairs have the emotion and the cause within 1 relative offset, suggesting the necessity of using proximity constraints (exactly what position weight matrix does) in the predicted pair matrix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We use a RTX 2080Ti GPU with 11GB of memory as the experimental hardware base and conduct experiments on the Linux system. The programming language and deep learning framework we use are Python 3.6.3 and Pytorch 1.2.0 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Parameter Settings</head><p>For all our experiments, pre-trained word vectors on Weibo (a Chinese micro-bloging website) using Word2Vec <ref type="bibr" target="#b16">(Mikolov et al. 2013</ref>) are leveraged to initialize the word embeddings. Specifically, the skip-gram used, which is the same as used in the baseline approaches <ref type="bibr" target="#b19">(Xia and Ding 2019)</ref>. The dimensionality of the embeddings (i.e., d e ) is set to 200. We use 4 convolutional layers (i.e., |t|) whose kernel sizes are {2,3,4,5} for the clause encoder and the number of filters for all the convolutional layers (i.e., d c ) is 50, for capturing gram-level features. In order to avoid overfitting, we apply dropout to embeddings and outputs of the clause encoder, yielding 0.5 probability of randomized zeroes on features. The dimensionality for hidden states of a unidirectional LSTM (i.e., d h ) is 300. The dimensionalities for all fully connected layers in the main task and auxiliary tasks (i.e., d z ) are 100. Moreover, the batch size and learning rate are determined through grid parameter search, which are 32 and 10 -3 , respectively. The number of epoch is 100 with early stop. The coefficient for L 2 -norm regularization (i.e., λ) is 10 -5 . Based on a pilot study, we find the best value for the threshold (i.e., η) in the inference stage is 0.3, which will be detailed in next section. The coefficient for the trade-off in objective function (i.e., β) is 1. In addition, the smoothing term in the calculation of position weight matrix (i.e., asw and gtw ) are 1 and 0.01, respectively. Furthermore, Adam is used as the optimizer and all trainable parameters are randomly initialized with uniform distribution <ref type="bibr" target="#b9">(He et al. 2015)</ref>.  <ref type="table">Table 2</ref>: Comparison results of emotion extraction, cause extraction, and emotion-cause pair extraction with precision, recall, and F1-measure as metrics. The results in bold are the best performing ones under each column. The results of emotion extraction and cause extraction for one-stage and two-stage models are exactly the same because one-stage models are ablated ones of two-stage models. † indicates results that are significantly better than best performing baseline Inter-EC with paired t-test (p is smaller than 0.05).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Baselines &amp; Evaluation Metrics</head><p>Our approach 2 is compared with a range of strong baselines, which are the state-of-the-art methods proposed by <ref type="bibr" target="#b19">(Xia and Ding 2019)</ref> for emotion-cause pair extraction. These baselines are either one-stage or two-stage models.</p><p>The two-stage models are listed below. They first extract emotions and causes with multi-task architectures independently or interactively, then classify the cartesian products of emotions and causes extracted in the first stage into pairs or non-pairs.</p><p>• Indep firstly considers emotion extraction and cause extraction as independent tasks and extract emotions and causes with multi-task learning, then pairs the extracted emotions and causes with a classifier.</p><p>• Inter-CE follows the procedure of Indep, however, uses cause extraction to assist emotion extraction in the first stage.</p><p>• Inter-EC is same as Inter-CE except utilizing emotion extraction to improve cause extraction.</p><p>We also include three one-stage baseline models. Basically they drop the second stage in the two-stage models, and use the cartesian products as predictions instead of using the classifier in the second stage. The resultant one-stage baseline models are listed as follows.</p><p>• Indep w/o classifier removes the classifier in the second stage of Indep.</p><p>• Inter-CE w/o classifier removes the classifier in the second stage of Inter-CE.</p><p>• Inter-EC w/o classifier removes the classifier in the second stage of Inter-EC.</p><p>Precision, recall, and macro F1 measures are adopted as effectiveness metrics in our experiments. The final results are obtained by averaging the ten folds results.</p><p>2 Code is available in https://github.com/shl5133/E2EECPE 5 Result &amp; Analysis</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Results in Effectiveness</head><p>We perform a comparison of E2EECPE-asw (E2ECPE with asymmetric position weight matrix) and E2EECPE-gtw (E2ECPE with ground truth position weight matrix) with one-stage and two-stage baseline models to quantitatively understand in what ways E2EECPE is more effective than the baselines. <ref type="table">Table 2</ref> gives the results in terms of precision, recall and macro-F1 measures. The comparison results demonstrate that our model E2EECPE-asw and E2EECPE-gtw consistently outperforms the baselines for the main task (emotioncause pair extraction) with regard to recall and F1, indicating the representation power and the effectiveness of our model. Nevertheless, we also observe that our model performs less well in precision than the two-stage baseline models. With additional observation that the baseline models are performing poorly on recall, we conjecture the existing models suffer from predicting only few testing instances as pairs. Furthermore, E2EECPE is superior on the two auxiliary tasks (emotion extraction and cause extraction). We attribute the improvement to multi-task structure in our model which combines auxiliary tasks and the main task. Apart from that, the one-stage models yield lower results than E2EECPE on cause extraction and emotion extraction, suggesting that error propagation is a comparably severe issue in the existing models but is alleviated in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Ablation Study</head><p>To understand the efficacy of auxiliary tasks and position weight matrix, we conduct an ablation study on E2EECPEasw and E2EECPE-gtw. Specifically, we separately ablate auxiliary tasks (i.e., emotion extraction and cause extraction) and position weight matrix from E2EECPE, and call them E2EECPE w/o aux and E2EECPE w/o position, respectively.</p><p>The results in <ref type="table" target="#tab_3">Table 3</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effect of Threshold in Inference Stage</head><p>Inference based on pair matrix is powerful, yet we do not exactly know what threshold (i.e., η) is the most suitable one for its expressiveness. It is therefore helpful to explore the effect of the threshold by altering it and examining the results.  From <ref type="table" target="#tab_4">Table 4</ref>, we conclude that 0.3 is the most appropriate one for our studied task. With increases of η, drops of F1 are noted, implying potential loss of extracted pairs. In addition, we also speculate that the reason why the best value is not around 0.5 (the expectation of random variables ranging uniformly from 0 to 1) is that the element-wise multiplication of a position weight matrix with the sigmoid-activated pair matrix produces a smaller expectation (as upper bound decreases).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Issue of Label Imbalance</head><p>In order to measure the impact brought by label imbalance, typically in the form of pair matrix sparsity, we remove the examples containing more than one pair for test set in each fold to make up a Hard dataset, then record the mean results across ten folds correspondingly.</p><p>We can observe in <ref type="table" target="#tab_6">Table 5</ref> that our model encounters a failure on the Hard dataset with decreases on precision and F1 measure, suggesting that further investigation is needed to solve this problem.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">An Extended Task: Emotion-Cause Triplet Extraction</head><p>Our model can also be extended to emotion-cause triplet extraction setting with minor modifications. Specifically, emotion-cause triplet differs from emotion-cause pair only on that a triplet should contain types of emotions while a pair does not. The emotion type extraction is also very important, just like in the Sentiment Analysis task. So we adjust the output of the Biaffine Transform from a two-dimensional tensor to a three-dimensional tensor. The extra dimension represents the emotion type.</p><p>The experimental results are shown in <ref type="table">Table 6</ref>. It can be seen that using the E2EECPE model for the triplet extraction task can also achieve promising results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>P R F1</head><p>triplet extraction 0.5824 0.4304 0.4940 emotion extraction 0.8417 0.7875 0.8131 cause extraction 0.6577 0.5770 0.6133 <ref type="table">Table 6</ref>: Results of triplet extraction, emotion extraction, and cause extraction with precision, recall, and F1-measure as metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>The emotion-cause pair extraction task is a new and more realistic task that seeks to identify emotion-cause pairs in documents. However, previous models are inherently limited by the idea of solving the task via two stages. To this end, we propose an end-to-end multi-task learning model that regards the problem as predicting directional links between emotions and causes via biaffine attention. Additionally, we also aid the model with auxiliary tasks and position weight matrix. Experimental results prove the superiority of our model over a series of baselines. We believe there are some promising directions yet to be explored. Firstly, the position weigh matrix used in our model is directly obtained from document information. Further models such as graph neural networks are expected to be developed to incorporate with learned position information instead of refined one. Secondly, triplet extraction is a brand new direction and is worth researching. Thirdly, the label imbalance issue should be addressed with task-specific tactics.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Extracting emotion-cause pairs from the given document via learning to link.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>An overview of E2EECPE. EE, CE stands for emotion extraction, cause extraction respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Visualization of values for ground truth pair matrices with document length less than 20 in all documents.Finally, we obtain the features for indicating pair links as follows:M p,q =M p,q A p,q</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study results. The results in bold are the best performing ones under each column.</figDesc><table><row><cell>drop of E2EECPE w/o position compared with E2EECPE-</cell></row><row><cell>asw and E2EECPE-gtw, verifying the remarkable bene-</cell></row><row><cell>fit of the multi-task learning schema. Meanwhile, the re-</cell></row><row><cell>sults that E2EECPE w/o position only differs slightly from</cell></row><row><cell>E2EECPE based on all metrics, indicating that imposing</cell></row><row><cell>position information is still of importance.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Effect of threshold. The results in bold are the best performing ones under each column.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Full 0.6478 / 0.6491 0.6105 / 0.6195 0.6280 / 0.6315 Hard 0.6002 / 0.6322 0.6479 / 0.6078 0.6226 / 0.6186</figDesc><table><row><cell>Type</cell><cell cols="2">E2EECPE-asw / E2EECPE-gtw</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The results for verifying the issue of label imbalance.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">If not specified, we use notations in bold as the vector representations of their original concepts.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint learning for emotion classification and emotion cause detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="646" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotion cause detection with linguistic constructions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Computational Linguistics</title>
		<meeting>the 23rd International Conference on Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An emotion cause corpus for chinese microblogs with multiple-user structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Asian and Low-Resource Language Information Processing (TALLIP)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01734</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Question Answering Approach for Emotion Cause Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1593" to="1602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Event-Driven Emotion Cause Extraction with Corpus Construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1639" to="1649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Emotion cause detection with linguistic construction in chinese weibo text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Natural Language Processing and Chinese Computing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="457" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Machine Learning</title>
		<meeting>the Eighteenth International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A textdriven rule-based system for emotion cause detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</title>
		<meeting>the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="45" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">DETECTING EMOTION CAUSES WITH A LINGUISTIC RULE-BASED APPROACH 1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="390" to="416" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A co-attention neural network model for emotion cause analysis with emotional context awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4752" to="4757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">End-toend memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Emotion-Cause Pair Extraction: A New Task to Emotion Analysis in Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1096</idno>
		<ptr target="https://www.aclweb.org/anthology/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="19" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Syntax-Aware Aspect-Level Sentiment Classification with Proximity-Weighted Convolution Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1145" to="1148" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
