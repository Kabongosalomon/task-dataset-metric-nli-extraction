<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DSRN: AN EFFICIENT DEEP NETWORK FOR IMAGE RELIGHTING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourya</forename><forename type="middle">Dipta</forename><surname>Das</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Jadavpur University</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nisarg</forename><forename type="middle">A</forename><surname>Shah</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IIT Jodhpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Equal contribution</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saikat</forename><surname>Dutta</surname></persName>
							<affiliation key="aff2">
								<address>
									<settlement>Madras</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IIT Jodhpur</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DSRN: AN EFFICIENT DEEP NETWORK FOR IMAGE RELIGHTING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-CNN</term>
					<term>Image relighting</term>
					<term>End-to-end</term>
					<term>Lightweight</term>
					<term>DSRN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Custom and natural lighting conditions can be emulated in images of the scene during post-editing. Extraordinary capabilities of the deep learning framework can be utilized for such purpose. Deep image relighting allows automatic photo enhancement by illumination-specific retouching. Most of the state-of-the-art methods for relighting are run-time intensive and memory inefficient. In this paper, we propose an efficient, real-time framework Deep Stacked Relighting Network (DSRN) for image relighting by utilizing the aggregated features from input image at different scales. Our model is very lightweight with total size of about 42 MB and has an average inference time of about 0.0116s for image of resolution 1024 × 1024 which is faster as compared to other multi-scale models. Our solution is quite robust for translating image color temperature from input image to target image and also performs moderately for light gradient generation with respect to the target image. Additionally, we show that if images illuminated from opposite directions are used as input, the qualitative results improve over using a single input image.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Appropriate illumination is essential to obtain the desired images of a scene. But this cannot be achieved efficiently as one has very little or no control over the lighting. However, images can be modified to emulate the desired illumination conditions of the scene after the imaging process. There exist many digital tools for illumination manipulation but most of the tools have limited ability to manipulate the for small fluctuations in intensity or color. Whereas, the state-of-the-art <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> methods for illumination editing, which are based on Deep learning framework have much better illumination-editing capabilities compared to professionally available tools, however they are computationally and memory intensive. This paper aims to develop an effective, lightweight framework for illumination editing that can also be deployed in the low-resource computing devices such as mobile.</p><p>The image of a scene for a given illumination depends upon the various factors such as number and position of sources, surface properties, image plane position and properties of illumination. The radiant energy I p due to the point source I s at a given point at distance r is given as in Equation <ref type="bibr" target="#b0">1</ref>.</p><formula xml:id="formula_0">I p = I s 4πr 2r<label>(1)</label></formula><p>Similarly, interaction of light with objects depends upon the properties of the surface such as orientation, texture and colour. The Phong model <ref type="bibr" target="#b3">[4]</ref> given in Equation 2 can be utilized to describe the reflection of the various surfaces in such an illumination condition and thus re-target the image appropriately for the case when the illumination source has been shifted to other locations.</p><formula xml:id="formula_1">I = I p K dN .L + K a I a<label>(2)</label></formula><p>Here, K d is surface diffuse reflectivity, I p is point source intensity,N is surface normal,L is the light direction, K a is the ambient reflectivity and I a is the ambient light intensity. In this work, we have considered only the diffused part of reflected light. Shadows are also one of the most important signs of the light source position. Previous methods such as <ref type="bibr" target="#b4">[5]</ref> produce the most aesthetic result but are dependent on 3D models and priors for ascertaining the shadows and casting of shadows from the direction of the new light source.</p><p>Here, the problem description is to transfer one fixed set of illumination settings which is illumination direction and color temperature of the light source, from an input image taken at a scene to another fixed set of illumination settings as target image at the same scene. In this paper, we are considering this case as single view problem. Along with this single view problem , we have also shown a study on multi-illumination approach where images having same colour temperature but opposite illumination direction are fused together to create a new average input image which can be used as input to reconstruct the output by our proposed method having illumination settings as same as the target image. For both of the approaches, we are using a fast and efficient network called Deep Stacked Relighting Network (DSRN). We have done two separate studies on two approaches and have concluded that using two stage training process improved model's performance and using multi view approach to this relighting problem, have solved lot of discrepancy in the results which we can find often for single view approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Deep learning based methods have played a vital role in relighting real-life and synthetic scenarios with great efficiency. Gafton et al. <ref type="bibr" target="#b2">[3]</ref> proposed a GAN based Image translation methods like pix2pix <ref type="bibr" target="#b5">[6]</ref>. There is no additional knowledge requirement of the view geometry and parameters to generate proper lighting effects in this type of approach. Zhou et al. <ref type="bibr" target="#b6">[7]</ref> proposed the use of Spherical Harmonics lighting parameters <ref type="bibr" target="#b7">[8]</ref> in the bottleneck layer of the U-Net hourglass network to achieve state-of-the-art results over the portrait relighting task. Ren et al. <ref type="bibr" target="#b8">[9]</ref> introduces a regression-based neural network for relighting real-world scenes from a small number of images. We infer from these methods that the relighting problem can be reformulated as a image color translation from input image to target image and relighting based upon the explicit geometry information is computation intensive. Xu et al. <ref type="bibr" target="#b9">[10]</ref> proposed a deep learning-based framework to generate images under novel illumination using only five images captured under predefined directional lights. Their framework included a fully convolution neural network to predict the relighting function and input light direction. Murmann et al. <ref type="bibr" target="#b10">[11]</ref> introduced a new multi-illumination dataset of real scenes captured under varying lighting conditions in high dynamic range and high resolution. They also proposed the use of U-net <ref type="bibr" target="#b11">[12]</ref> for singleimage relighting, treating as an image-to-image translation problem. It is a general observation that a minor change in lighting conditions alters many features in a scene and leads to deep neural network performance degradation. The approach proposed by Carlson et al. <ref type="bibr" target="#b1">[2]</ref> shows an application to relight outdoor scenes by adding a realistic shadow, shading, and other lighting effects onto an image used for tasks such as detection, recognition, and segmentation in urban driving scenes. This approach would be beneficial to make models for such tasks more robust. The notion of geometry can be encoded in much efficiency in the encoder-decoder framework and by usage of multiple views of the scene. We encapsulate this information by using the stacked deep architecture as opposed to state of the art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED APPROACH</head><p>Base Network: We use a multi-scale network which works on a three-level image pyramid. In each level of the network, we have an encoder and a decoder. Decoder output of each level is upscaled and added to input image in next level and passed to encoder of next level. To capture global contextual information better, encoder output of each level is added with encoder output of previous level before feeding it to decoder.</p><p>Let the input of encoder at i-th level (enc i ) be in i and output of decoder at i-th level (out i ) be out i . in i and out i are given by,</p><formula xml:id="formula_2">in i = I i + up(out l+1 ) (3) F i = enc i (in i ) (4) G i = F i + up(G i+1 ) (5) out i = dec i (G i )<label>(6)</label></formula><p>where, I i is the image at i-th level of image pyramid. We can not improve performance of our model by adding additional lower levels in the base network as discussed in <ref type="bibr" target="#b12">[13]</ref>. Thus, we cascade the same network twice to increase the performance of our network. We refer our final model as Deep Stacked Relighting Network (DSRN). The model architecture diagram is shown in <ref type="figure">Fig. 1</ref>. <ref type="figure" target="#fig_0">Fig. 2</ref> shows details of Encoder and Decoder architecture. Both encoder and decoder have a three-level hierarchy. Each hierarchy contains two residual blocks. Strided convolution is used for reducing spatial dimension in encoder and Transposed Convolution is used for increasing spatial dimension in decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1. Architecture Diagram of proposed DSRN</head><p>Single-illumination Approach: In case of Single illumination data, images where illumination in the scene is coming from one particular direction and have certain color temperature is taken as input image. As a target image, image with different colour temperature than input image and direction of illumination is also changed from the input image.</p><p>Multi-illumination Approach: It is difficult to recover details in the dark regions of a single input image. On the other hand, if we have another input image of the same scene  illuminated from opposite direction, most regions in the scene will appear to be bright in at least one of the input images. In multi-illumination approach, we leverage this extra information for image relighting. In case of multi-illumination, two images where illumination is coming from two opposite direction in the scene having same colour temperature, are weighted averaged together to get input overlayed image for the model. Target image for this case is same as for the single-illumination case.</p><formula xml:id="formula_3">I input Avg = W 1 * I input (D,T ) + W 2 * I input (D * ,T )<label>(7)</label></formula><p>Here, I input (D,T ) is an image having illumination direction D and color temperature T , I input (D * ,T ) is an image having opposite illumination direction D * and color temperature T , and I input Avg is input overlayed or average image. Initially for our experiments, we have used T = 6500 for input image, and D and D * are North (N) and South (S) directions respectively. The weight values 0.5 is used for both W 1 and W 2 .</p><p>Loss function: We have done two stage training to enhance our results. First, we are using only L 2 loss as a reconstruction loss to train our model. Then, we used a linearly weighted loss function as objective loss function in second phase of training to get our final trained model. The combined loss function, L CL is given by,</p><formula xml:id="formula_4">L CL = λ 1 * L 1 + λ 2 * L SSIM + λ 3 * L p + λ 4 * L tv (8)</formula><p>where, L 1 is Mean Absolute Error (MAE) loss, L SSIM is SSIM loss <ref type="bibr" target="#b13">[14]</ref> , L p is Perceptual loss <ref type="bibr" target="#b14">[15]</ref> and L tv is TV loss <ref type="bibr" target="#b15">[16]</ref>. During training, values of λ 1 , λ 2 , λ 3 and λ 4 are chosen to be 1, 5 × 10 −3 , 6 × 10 −3 and 2 × 10 −8 respectively. We optimized the proposed DSRN by imploying the performance test with different loss function and architecture.</p><p>We utilized L 2 loss (L M SE ) and Combined Loss (L CL ) to train our networks. <ref type="table" target="#tab_0">Table 1</ref> presents the performance of the proposed Base and Stacked Networks trained with different loss functions. We observe from the table that our stacked model, DSRN, provides optimal performance for Combined Loss (L CL ) function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS AND DISCUSSION</head><p>System Description: Our performance evaluation framework includes Pytorch in Linux environment with AMD 1950X processor along with 64GB RAM and 11 GB NVIDIA GTX 1080 Ti GPU. In all of our experiments, an initial learning rate of 2 × 10 −3 is selected which is gradually decreased to 5 × 10 −5 . During training, the Batch Size is 2 and we are resizing input image from 1024 × 1024 to 512 × 512.</p><p>Dataset Details: We have utilized VIDIT dataset <ref type="bibr" target="#b16">[17]</ref> for our experiments. The dataset contains 15600 images from 390 different scenes in 40 different illumination settings. The scenes are illuminated with 5 sources of different color temperatures from 8 azimuthal angle directions one at a time. We have used 300 scene images of size 512 × 512 for training and 45 scene images for validation and testing each. For multiillumination approach, we need multi-illumination data for both training and validation but as multi-illumination data for validation set is not available, we split up our training data into two sets of custom training and custom test set. For custom test set, we sampled 60 images randomly from whole training data.</p><p>Evaluation: We report the performance comparison over validation dataset as ground truth for test set is not made publicly available. We compare the performance using metrics PSNR, SSIM and LPIPS <ref type="bibr" target="#b17">[18]</ref>.  <ref type="table" target="#tab_1">Table 2</ref> presents the performance comparison of the proposed DSRN framework for relighting with the state of the art methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b0">[1]</ref>. We observe from the table that the proposed DSRN based relighting framework produces more accurate output with more than 25x speedup in the computation time. This highlights the efficacy of the proposed DSRN for the real-time relighting.</p><p>We also present the qualitative comparison of image relighting of proposed DSRN framework with state-of-the art methods as shown in the <ref type="figure" target="#fig_2">Fig. 4</ref>. We observe from the figure that the proposed method produces relighting output of higher  quality compared to state of the art methods <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref> and <ref type="bibr" target="#b0">[1]</ref> which suffer from the various relighting-artifacts.</p><p>We have also extended the proposed DSRN framework for multi-illumination case to improve the results in the saturation regions. We evaluated the performance of the multiillumination based approach on custom dataset. <ref type="table" target="#tab_2">Table 3</ref> shows the performance metrics for the multi-illumination based image relighting frameworks. We observe from the <ref type="table" target="#tab_0">Table 1 and 3</ref> that DSRN leverages the most from the additional information and results in significant improvement in PSNR, SSIM and LPIPS. This validates the efficacy of the proposed DSRN for image relighting task. We also present the qualitative per- formance improvement in the image relighting results due to usage of the multi-illumination data as shown in the <ref type="figure" target="#fig_3">Fig. 5</ref>. We observe from the figure that multi-illumination data improves the quality of relighted images in the saturation regions. We also tested the performance of the multi-illumination based image relighting for various combinations of the illumination and target illumination as given in <ref type="table" target="#tab_3">Table 4</ref>. We observe from the table that the proposed DSRN is robust with respect to different illumination conditions. Also, illumination com-bination covering the entire depth of the scene (N and S) is usually more preferable compared to other combination. Thus, we conclude from the results that the proposed framework of DSRN is an effective framework for real-time image relighting. The utilization of multi-illumination data further improves the performance of the proposed framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we have proposed a fast and lightweight stacked network, DSRN for image relighting. The proposed network achieves state-of-the-art results with more than 25x speedup. We have adopted a two-stage training process to improve results both quantitatively and qualitatively. Through extensive experiments, we have demonstrated the capability of the proposed DSRN in both single-illumination and multiillumination settings. In future, we would like to explore these aspect to develop a more complete image relighting framework. Also, we would also like to include the both diffused and specular components of the reflected illumination for better and more realistic scene relighting in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Architecture diagram of Encoder and Decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of images at different stages during preprocessing for multi-illumination approach. From left: (a) Input image,I input (D,T ) , (b) Input image,I input (D * ,T ) , (c) Processed weighted average or overlayed Image, I input Avg , (d) Target Image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Qualitative comparison of proposed DSRN with state of the art methods. From left: (a) Input Image (I input (D,T ) ), (b) SRN [20], (c) Dense-GridNet [19], (d) DRN[1], (e) Base Network (Proposed), (f) DSRN (Proposed), (g) Ground Truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Qualitative Comparison of our method for Multi-illumination Approach against Ground Truth on custom test data. From left: (a) Input Image (I input (D,T ) ), (b) Input Image (I input Avg ), (d) Base Network with L 2 Loss, (e) Base Network with Combined Loss, (f) DSRN with L 2 Loss, (g) DSRN with Combined Loss, (h) Ground Truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance of the proposed relighting frameworks for various loss functions.</figDesc><table><row><cell>Method</cell><cell>PSNR ↑ SSIM ↑ LPIPS ↓</cell></row><row><cell cols="2">Base Network (LMSE) 16.94 0.5659 0.4933</cell></row><row><cell>Base Network (LCL)</cell><cell>17.20 0.5696 0.3712</cell></row><row><cell>DSRN (LMSE)</cell><cell>17.53 0.5673 0.4253</cell></row><row><cell>DSRN (LCL)</cell><cell>17.89 0.5899 0.4088</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison of proposed DSRN with state of the art methods on<ref type="bibr" target="#b16">[17]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="4">PSNR ↑ SSIM ↑ LPIPS ↓ Runtime (s)</cell></row><row><cell>Pix2Pix [6]</cell><cell>16.28</cell><cell>0.553</cell><cell>0.482</cell><cell>0.2504</cell></row><row><cell cols="4">Dense-GridNet [19] 16.67 0.2811 0.3691</cell><cell>0.9326</cell></row><row><cell>SRN [20]</cell><cell cols="3">16.94 0.5660 0.4319</cell><cell>0.8710</cell></row><row><cell>DRN [1]</cell><cell>17.59</cell><cell>0.596</cell><cell>0.440</cell><cell>0.5012</cell></row><row><cell>DSRN (Proposed)</cell><cell cols="3">17.89 0.5899 0.4088</cell><cell>0.0116</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance metrics for the multi-illumination based relighting using proposed DSRN on<ref type="bibr" target="#b16">[17]</ref>.</figDesc><table><row><cell>Method</cell><cell cols="3">PSNR ↑ SSIM ↑ LPIPS ↓</cell></row><row><cell>Base Network (LMSE)</cell><cell>17.88</cell><cell>0.6372</cell><cell>0.2547</cell></row><row><cell>Base Network (LCL)</cell><cell>17.87</cell><cell>0.6445</cell><cell>0.2532</cell></row><row><cell>DSRN (LMSE)</cell><cell>19.02</cell><cell>0.6725</cell><cell>0.3913</cell></row><row><cell>DSRN (LCL)</cell><cell>19.25</cell><cell>0.7038</cell><cell>0.3265</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Performance metrics for proposed relighting using DSRN for multi-illumination conditions.</figDesc><table><row><cell>Source Settings</cell><cell>Target Settings</cell><cell cols="3">PSNR ↑ SSIM ↑ LPIPS ↓</cell></row><row><cell>N and S</cell><cell>E</cell><cell>19.25</cell><cell>0.7038</cell><cell>0.3265</cell></row><row><cell>N and S</cell><cell>W</cell><cell>19.55</cell><cell>0.6955</cell><cell>0.2860</cell></row><row><cell>E and W</cell><cell>S</cell><cell>19.55</cell><cell>0.6797</cell><cell>0.2839</cell></row><row><cell>E and W</cell><cell>N</cell><cell>17.53</cell><cell>0.6556</cell><cell>0.3169</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep relighting networks for image light source manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Wen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Chi</forename><surname>Siu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu-Tak</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel Pk</forename><surname>Lun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08298</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Shadow transfer: Single image relighting for urban road scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson-Roberson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10363</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Gafton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erick</forename><surname>Maraz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07816</idno>
		<title level="m">2d image relighting with image-to-image translation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Illumination for computer generated pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Bui Tuong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="311" to="317" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Multi-view relighting using a geometry-aware network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Drettakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE confer</title>
		<meeting>the IEEE confer</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep single-image portrait relighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7194" to="7202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sfsnet: Learning shape, reflectance and illuminance of facesin the wild&apos;</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumyadip</forename><surname>Sengupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6296" to="6305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image based relighting using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (ToG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep image-based relighting from optimal sparse samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zexiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunil</forename><surname>Hadap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A dataset of multi-illumination images in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Murmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredo</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4080" to="4089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep stacked hierarchical multi-patch network for image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eero P</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravindh</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5188" to="5196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Vidit: Virtual image dataset for illumination transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Majed</forename><forename type="middle">El</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><surname>Barthas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabine</forename><surname>Süsstrunk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.05460</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Griddehazenet: Attention-based multi-scale network for image dehazing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongrui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scale-recurrent network for deep image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8174" to="8182" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
