<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HybridSN: Exploring 3D-2D CNN Feature Hierarchy for Hyperspectral Image Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swalpa</forename><surname>Kumar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE, Gopal</roleName><forename type="first">Roy</forename><surname>Krishna</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Shiv</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Bidyut</forename><forename type="middle">B</forename><surname>Dubey</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Life Fellow, IEEE</roleName><surname>Chaudhuri</surname></persName>
						</author>
						<title level="a" type="main">HybridSN: Exploring 3D-2D CNN Feature Hierarchy for Hyperspectral Image Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/LGRS.2019.2918719)</idno>
					<note>PUBLISHED IN IEEE GEOSCIENCE AND REMOTE SENSING LETTERS (DOI: 10.1109/LGRS.2019.2918719) 1 This paper is a preprint. IEEE copyright notice. &quot;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep Learning</term>
					<term>Convolutional Neural Networks</term>
					<term>Spectral-Spatial</term>
					<term>3D-CNN</term>
					<term>2D-CNN</term>
					<term>Remote Sensing</term>
					<term>Hyperspec- tral Image Classification</term>
					<term>HybridSN</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Hyperspectral image (HSI) classification is widely used for the analysis of remotely sensed images. Hyperspectral imagery includes varying bands of images. Convolutional Neural Network (CNN) is one of the most frequently used deep learning based methods for visual data processing. The use of CNN for HSI classification is also visible in recent works. These approaches are mostly based on 2D CNN. Whereas, the HSI classification performance is highly dependent on both spatial and spectral information. Very few methods have utilized the 3D CNN because of increased computational complexity. This letter proposes a Hybrid Spectral Convolutional Neural Network (HybridSN) for HSI classification. Basically, the HybridSN is a spectral-spatial 3D-CNN followed by spatial 2D-CNN. The 3D-CNN facilitates the joint spatial-spectral feature representation from a stack of spectral bands. The 2D-CNN on top of the 3D-CNN further learns more abstract level spatial representation. Moreover, the use of hybrid CNNs reduces the complexity of the model compared to 3D-CNN alone. To test the performance of this hybrid approach, very rigorous HSI classification experiments are performed over Indian Pines, Pavia University and Salinas Scene remote sensing datasets. The results are compared with the state-of-the-art handcrafted as well as end-to-end deep learning based methods. A very satisfactory performance is obtained using the proposed HybridSN for HSI classification. The source code can be found at https://github.com/gokriznastic/HybridSN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HE research in hyperspectral image analysis is important due to its potential applications in real life <ref type="bibr" target="#b0">[1]</ref>. Hyperspectral imaging results in multiple bands of images which makes the analysis challenging due to increased volume of data. The spectral, as well as the spatial correlation between different bands convey useful information regarding the scene of interest. Recently, Camps-Valls et al. have surveyed the advances in hyperspectral image (HSI) classification <ref type="bibr" target="#b1">[2]</ref>. The HSI classification is tackled in two ways, one with handdesigned feature extraction technique and another with learning based feature extraction technique.</p><p>Several HSI classification approaches have been developed using the hand-designed feature description <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. Yang and S.K. Roy and G. Krishna are with Computer Science and Engineering Department at Jalpaiguri Government Engineering College, Jalpaiguri, West Bengal-735102, India (email: swalpa@cse.jgec.ac.in; gk1948@cse.jgec.ac.in).</p><p>S.R. Dubey is with Computer Vision Group, Indian Institute of Information Technology, Sri City, Chittoor, Andhra Pradesh-517646, India (e-mail: srdubey@iiits.in).</p><p>B.B. Chaudhuri is with Computer Vision and Pattern Recognition Unit at Indian Statistical Institute, Kolkata-700108, India (email: bbc@isical.ac.in).</p><p>Qian have proposed a joint collaborative representation by using the locally adaptive dictionary <ref type="bibr" target="#b2">[3]</ref>. It reduces the adverse impact of useless pixels and improves the HSI classification performance. Fang et al. have utilized the local covariance matrix to encode the relationship between different spectral bands <ref type="bibr" target="#b3">[4]</ref>. They used these matrices for HSI training and classification using Support Vector Machine (SVM). A composite kernel is used to combine spatial and spectral information for HSI classification <ref type="bibr" target="#b4">[5]</ref>. Li et al. have applied the learning over the combination of multiple features for the classification of hyperspectral scenes <ref type="bibr" target="#b5">[6]</ref>. Some other hand crafted approaches are Joint Sparse Model and Discontinuity Preserving Relaxation <ref type="bibr" target="#b6">[7]</ref>, Boltzmann Entropy-Based Band Selection <ref type="bibr" target="#b7">[8]</ref>, Sparse Self-Representation <ref type="bibr" target="#b8">[9]</ref>, Fusing Correlation Coefficient and Sparse Representation <ref type="bibr" target="#b9">[10]</ref>, Multiscale Superpixels and Guided Filter <ref type="bibr" target="#b10">[11]</ref>, and etc.</p><p>Recently, the Convolutional Neural Network (CNN) has become very popular due to drastic performance gain over the hand-designed features <ref type="bibr" target="#b11">[12]</ref>. The CNN has shown very promising performance in many applications where visual information processing is required, such as image classification <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, object detection <ref type="bibr" target="#b14">[15]</ref>, semantic segmentation <ref type="bibr" target="#b15">[16]</ref>, colon cancer classification <ref type="bibr" target="#b16">[17]</ref>, depth estimation <ref type="bibr" target="#b17">[18]</ref>, face anti-spoofing <ref type="bibr" target="#b18">[19]</ref>, etc. In recent years, a huge progress is also made in deep learning for hyperspectral image analysis. A dual-path network (DPN) by combining the residual network and dense convolutional network is proposed for the HSI classification <ref type="bibr" target="#b19">[20]</ref>. <ref type="bibr">Yu</ref>   <ref type="bibr" target="#b30">[31]</ref>. It is evident from the literature that using just 2D-CNN or 3D-CNN had a few shortcomings such as missing channel relationship information or very complex model, respectively. It also prevented these methods from achieving a better accuracy on hyperspectral images. The main reason is due to the fact that hyperspectral images are volumetric data and have a spectral dimension as well. The 2D-CNN alone isn't able to extract good discriminating feature maps from the spectral dimensions. Similarly, a deep 3D-CNN is more computationally complex and alone seems to perform worse for classes having similar textures over many spectral bands. This is the motivation for us to propose a hybrid-CNN model which overcomes these shortcomings of the previous models. The 3D-CNN and 2D-CNN layers are assembled for the proposed model in such a way that they utilise both the spectral as well as spatial feature maps to their full extent to achieve maximum possible accuracy.</p><p>This letter proposes the HybridSN in Section 2; presents the experiments and analysis in Section 3; and highlights the concluding remarks in section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED HYBRIDSN MODEL</head><p>Let the spectral-spatial hyperspectral data cube be denoted by I ∈ R M ×N ×D , where I is the original input, M is the width, N is the height, and D is the number of spectral bands/depth. Every HSI pixel in I contains D spectral measures and forms a one-hot label vector Y = (y 1 , y 2 , . . . y C ) ∈ R 1×1×C , where C represents the land-cover categories. However, the hyperspectral pixels exhibit the mixed land-cover classes, introducing the high intra-class variability and interclass similarity into I. It is of great challenge for any model to tackle this problem. To remove the spectral redundancy first the traditional principal component analysis (PCA) is applied over the original HSI data (I) along spectral bands. The PCA reduces the number of spectral bands from D to B while maintaining the same spatial dimensions (i.e., width M and height N ). We have reduced only spectral bands such that it preserves the spatial information which is very important for recognising any object. We represent the PCA reduced data cube by X ∈ R M ×N ×B , where X is the modified input after PCA, M is the width, N is the height, and B is the number of spectral bands after PCA. In order to utilize the image classification techniques, the HSI data cube is divided into small overlapping 3D-patches, the truth labels of which are decided by the label of the centred pixel. We have created the 3D neighboring patches P ∈ R S×S×B from X, centered at the spatial location (α, β), covering the S ×S window or spatial extent and all B spectral bands. The total number of generated 3D-patches (n) from X is given by (M − S + 1) × (N − S + 1). Thus, the 3D-patch at location (α, β), denoted by P α,β , covers the width from α − (S − 1)/2 to α + (S − 1)/2, height from β − (S − 1)/2 to β + (S − 1)/2 and all B spectral bands of PCA reduced data cube X.</p><p>In 2D-CNN, the input data are convolved with 2D kernels. The convolution happens by computing the sum of the dot product between input data and kernel. The kernel is strided over the input data to cover full spatial dimension. The convolved features are passed through the activation function to introduce the non-linearity in the model. In 2D convolution, the activation value at spatial position (x, y) in the j th feature map of the i th layer, denoted as v x,y i,j , is generated using the following equation,</p><formula xml:id="formula_0">v x,y i,j = φ(b i,j + d l−1 τ =1 γ ρ=−γ δ σ=−δ w σ,ρ i,j,τ × v x+σ,y+ρ i−1,τ )<label>(1)</label></formula><p>where φ is the activation function, b i,j is the bias parameter for the j th feature map of the i th layer, d l−1 is the number of feature map in (l − 1) th layer and the depth of kernel w i,j for the j th feature map of the i th layer, 2γ + 1 is the width of kernel, 2δ + 1 is the height of kernel, and w i,j is the value of weight parameter for the j th feature map of the i th layer. The 3D convolution <ref type="bibr" target="#b31">[32]</ref> is done by convolving a 3D kernel with the 3D-data. In the proposed model for HSI data, the feature maps of convolution layer are generated using the 3D kernel over multiple contiguous bands in the input layer; this captures the spectral information. In 3D convolution, the activation value at spatial position (x, y, z) in the j th feature map of the i th layer, denoted as v x,y,z i,j , is generated as follows, v x,y,z</p><formula xml:id="formula_1">i,j = φ(b i,j + d l−1 τ =1 η λ=−η γ ρ=−γ δ σ=−δ w σ,ρ,λ i,j,τ × v x+σ,y+ρ,z+λ i−1,τ )</formula><p>(2) where 2η + 1 is the depth of kernel along spectral dimension and other parameters are the same as in (Eqn. 1).</p><p>The parameters of CNN, such as the bias b and the kernel weight w, are usually trained using supervised approaches <ref type="bibr" target="#b11">[12]</ref> with the help of a gradient descent optimization technique. In conventional 2D CNNs, the convolutions are applied over the spatial dimensions only, covering all the feature maps of the previous layer, to compute the 2D discriminative feature maps. Whereas, for the HSI classification problem, it is desirable to capture the spectral information, encoded in multiple bands along with the spatial information. The 2D-CNNs are not able to handle the spectral information. On the other hand, the 3D-CNN kernel can extract the spectral and spatial feature representation simultaneously from HSI data, but at the cost of increased computational complexity. In order to take the advantages of the automatic feature learning capability of both 2D and 3D CNN, we propose a hybrid feature learning framework called HybridSN for HSI classification. The flow diagram of the proposed HybridSN network is shown in <ref type="figure">Fig. 1</ref>. It comprises of three 3D convolutions (Eqn. 2), one 2D convolution (Eqn. 1) and three fully connected layers.</p><p>In HybridSN framework, the dimensions of 3D convolution kernels are 8 × 3 × 3 × 7 × 1 (i.e., K 1 1 = 3, K 1 2 = 3, and K 1 3 = 7 in <ref type="figure">Fig. 1</ref>), 16×3×3×5×8 (i.e., K 2 1 = 3, K 2 2 = 3, and K 2 3 = 5 in <ref type="figure">Fig. 1</ref>) and 32×3×3×3×16 (i.e., K 3 1 = 3, K 3 2 = 3, and K 3 3 = 3 in <ref type="figure">Fig. 1</ref>) in the subsequent 1 st , 2 nd and 3 rd convolution layers, respectively, where 16×3×3×5×8 means 16 3D-kernels of dimension 3×3×5 (i.e., two spatial and one spectral dimension) for all 8 3D input feature maps. Whereas, the dimension of 2D convolution kernel is 64 × 3 × 3 × 576 (i.e., K 4 1 = 3 and K 4 2 = 3 in <ref type="figure">Fig. 1)</ref>, where 64 is the number of 2D-kernels, 3 × 3 represents the spatial dimension of 2D-kernel, and 576 is the number of 2D input feature maps. To increase the number of spectral-spatial feature maps simultaneously, 3D convolutions are applied thrice and can preserve the spectral information of input HSI data in the output volume. The 2D convolution is applied once before the f latten layer by keeping in mind that it strongly discriminates the spatial information within the different spectral bands without substantial loss of spectral information, which is very important for HSI data. A detailed summary of the proposed model in terms of the layer types, output map dimensions and number of parameters is given in <ref type="table" target="#tab_1">Table I</ref>. It can be seen that the highest number of parameters are present in the 1 st dense layer. The number of node in the last dense layer is 16, which is same as the number of classes in Indian Pines dataset. Thus, the total number of parameters in the proposed model depends on the number of classes in a dataset. The total number of trainable weight parameters in HybridSN is 5, 122, 176 for Indian Pines dataset. All weights are randomly initialised and trained using back-propagation algorithm with the Adam optimiser by using the sof tmax loss. We use minibatches of size 256 and train the network for 100 epochs with no batch normalization and data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS AND DISCUSSION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset Description and Training Details</head><p>We have used three publicly available hyperspectral image datasets 1 , namely Indian Pines, University of Pavia and Salinas Scene. The Indian Pines (IP) dataset has images with 145 × 145 spatial dimension and 224 spectral bands in the wavelength range of 400 to 2500 nm, out of which 24 spectral bands covering the region of water absorption have been discarded. The ground truth available is designated into 16 classes of vegetation. The University of Pavia (UP) dataset consists of 610×340 spatial dimension pixels with 103 spectral bands in the wavelength range of 430 to 860 nm. The ground truth is divided into 9 urban land-cover classes. The Salinas Scene (SA) dataset contains the images with 512×217 spatial dimension and 224 spectral bands in the wavelength range of 360 to 2500 nm. The 20 water absorbing spectral bands have been discarded. In total 16 classes are present in this dataset.</p><p>All experiments are conducted on an Acer Predator-Helios laptop with the GTX 1060 Graphical Processing Unit (GPU) and 16 GB of RAM. We have chosen the optimal learning rate of 0.001, based on the classification outcomes. In order to make the fair comparison, we have extracted the same spatial dimension in 3D-patches of input volume for different datasets, such as 25 × 25 × 30 for IP and 25 × 25 × 15 for UP and SA, respectively.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Classification Results</head><p>In this letter, we have used the Overall Accuracy (OA), Average Accuracy (AA) and Kappa Coefficient (Kappa) evaluation measures to judge the HSI classification performance. Here, OA represents the number of correctly classified samples out of the total test samples; AA represents the average of class-wise classification accuracies; and Kappa is a metric of statistical measurement which provides mutual information regarding a strong agreement between the ground truth map and classification map. The results of the proposed HybridSN model are compared with the most widely used supervised methods, such as SVM <ref type="bibr" target="#b32">[33]</ref>, 2D-CNN <ref type="bibr" target="#b33">[34]</ref>, 3D-CNN <ref type="bibr" target="#b34">[35]</ref>, M3D-CNN <ref type="bibr" target="#b35">[36]</ref>, and SSRN <ref type="bibr" target="#b26">[27]</ref>. The 30% and 70% of the data are randomly divided into training and testing groups, respectively 2 . We have used the publicly available code 3 of the compared methods to compute the results. <ref type="table" target="#tab_1">Table II</ref> shows the results in terms of the OA, AA, and Kappa coefficient for different methods <ref type="bibr" target="#b3">4</ref> . It can be observed from <ref type="table" target="#tab_1">Table II</ref> that the HybridSN outperforms all the compared methods over each dataset while maintaining the minimum standard deviation. The proposed HybridSN is based on the hierarchical representation of spectral-spatial 3D CNN followed by a spatial 2D CNN, which are complementary to each other. It is also observed from these results that the performance of 3D-CNN is poor than 2D-CNN over Salinas Scene dataset. To the best of the our <ref type="bibr" target="#b1">2</ref> More details of dataset are provided in the supplementary material 3 https://github.com/eecn/Hyperspectral-Classification <ref type="bibr" target="#b3">4</ref> The class-wise accuracy is provided in the supplementary material.   knowledge, this is probably due to the presence of two classes in the Salinas dataset (namely Grapes-untrained and Vinyarduntrained) which have very similar textures over most spectral bands. Hence, due to the increased redundancy among the spectral bands, the 2D-CNN outperforms the 3D-CNN over Salinas Scene dataset. Moreover, the performance of SSRN and HybridSN is always far better than M3D-CNN. It is evident that 3D or 2D convolution alone is not able to represent the highly discriminative feature compared to hybrid 3D and 2D convolutions. The classification map for an example hyperspectral image is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref> using SVM, 2D-CNN, 3D-CNN, M3D-CNN, SSRN and HybridSN methods. The quality of classification map of SSRN and HybridSN is far better than other methods. Among SSRN and HybridSN, the maps generated by HybridSN in small segment are better than SSRN. <ref type="figure">Fig. 3</ref> shows the confusion matrix for the HSI classification performance of the proposed HybridSN over IP, UP and SA datasets, respectively. The accuracy and loss convergence for 100 epochs of training and validation sets are portrayed in <ref type="figure">Fig. 4</ref> for the proposed method. It can be seen that the convergence is achieved in approximately 50 epochs which points out the fast convergence of our method. The computational efficiency of HybridSN model appears in term of training and testing times in <ref type="table" target="#tab_1">Table III</ref>. The proposed model is more efficient than 3D-CNN. The impact of spatial dimension over the performance of HybridSN model is reported in <ref type="table" target="#tab_1">Table IV</ref>. It has been found that the used 25 × 25 spatial dimension is most suitable for the proposed method. We have also computed the results with an even less training data, i.e., only 10% of total samples and have summarized the results in <ref type="table" target="#tab_5">Table V</ref>. It is observed from this experiment that the performance of each model decreases slightly, whereas the proposed method is still able to outperform other methods in almost all cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>This letter has introduced a hybrid 3D and 2D model for hyperspectral image classification. The proposed HybridSN model basically combines the complementary information of spatio-spectral and spectral in the form of 3D and 2D convolutions, respectively. The experiments over three benchmark datasets compared with recent state-of-the-art methods confirm the superiority of the proposed method. The proposed model is computationally efficient than the 3D-CNN model. It also shows the superior performance for small training data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>The classification map for Indian Pines, (a) False color image (b) Ground truth (c)-(h) Predicted classification maps for SVM, 2D-CNN, 3D-CNN, M3D-CNN, SSRN, and proposed HybridSN, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :Fig. 4 :</head><label>34</label><figDesc>The confusion matrix using proposed method over Indian Pines, University of Pavia, and Salinas Scene datasets in 1 st , 2 nd , and 3 rd matrix, respectively. The accuracy and loss convergence vs epochs over Indian Pines dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>et al. have proposed a greedy layer-wise approach for unsupervised training to represent the remote sensing images [21]. Li et al. introduced a pixel-block pair (PBP) based data augmentation technique to generalize the deep learning for HSI classification [22]. Song et al. have proposed deep feature fusion network [23] and Cheng et al. have used the off-the-shelf CNN models for HSI classification [24]. Basically, they extracted the hierarchical deep spatial features and used with SVM for training and classification. Recently, the low power consuming hardwares for deep learning based HSI classification is also explored [25]. Chen et al. have used the deep feature extraction of 3D-CNN for HSI classification [26]. Zhong et al. have proposed the spectral-spatial residual</figDesc><table><row><cell></cell><cell cols="4">Spectral-Spatial Feature Learning</cell><cell></cell><cell></cell><cell cols="3">Spatial Feature Learning</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>3 D</cell><cell>C o n v</cell><cell>3 D</cell><cell>C o n v</cell><cell>3 D</cell><cell cols="2">C o n v</cell><cell>2 D</cell><cell>C o n v</cell><cell>Flatten</cell><cell>F C</cell><cell>F C</cell><cell>C la s s ifi c a t io n</cell><cell>1 x 1 x C</cell></row><row><cell></cell><cell cols="2">Neighbourhood Hyperspectral Image Extraction MxNxB</cell><cell>SxSxB 8 @ K 1 1 x K 1 2 x K 1 3</cell><cell cols="3">Feature 1 1 6 @ K 2 1 x K 2 2 x K 2 3</cell><cell cols="2">Feature 2 3 2 @ K 3 1 x K 3 2 x K 3 3</cell><cell>Feature 3 6 4 @ K 4 1 x K 4 2</cell><cell>Feature 4</cell><cell>D r o p O u t</cell><cell cols="2">Softmax Feature 5 Feature 6 D r o p O u t</cell></row><row><cell></cell><cell cols="13">Fig. 1: Proposed HybridSpectralNet (HybridSN) Model which integrates 3D and 2D convolutions for hyperspectral image (HSI) classification.</cell></row><row><cell>arXiv:1902.06701v3 [cs.CV] 3 Jul 2019</cell><cell cols="4">capsule networks to learn the hyperspectral features [30], whereas Fang et al. introduced deep hashing neural networks for hyperspectral image feature extraction</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">network (SSRN) [27]. The residual blocks in SSRN use the</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">identity mapping to connect every other 3-D convolutional</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">layer. Mou et al. have investigated the residual conv-deconv</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">network, an unsupervised model, for HSI classification [28].</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">Recently, Paoletti et al. have proposed the Deep Pyramidal</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">Residual Networks (DPRN) specially for the HSI data [29].</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">Very recently, Paoletti et al. have also proposed spectral-spatial</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>The layer wise summary of the proposed HybridSN architecture with window size 25×25. The last layer is based on the Indian Pines dataset.</figDesc><table><row><cell>Layer (type)</cell><cell>Output Shape</cell><cell># Parameter</cell></row><row><cell>input 1 (InputLayer)</cell><cell>(25, 25, 30, 1)</cell><cell>0</cell></row><row><cell>conv3d 1 (Conv3D)</cell><cell>(23, 23, 24, 8)</cell><cell>512</cell></row><row><cell>conv3d 2 (Conv3D)</cell><cell>(21, 21, 20, 16)</cell><cell>5776</cell></row><row><cell>conv3d 3 (Conv3D)</cell><cell>(19, 19, 18, 32)</cell><cell>13856</cell></row><row><cell>reshape 1 (Reshape)</cell><cell>(19, 19, 576)</cell><cell>0</cell></row><row><cell>conv2d 1 (Conv2D)</cell><cell>(17, 17, 64)</cell><cell>331840</cell></row><row><cell>flatten 1 (Flatten)</cell><cell>(18496)</cell><cell>0</cell></row><row><cell>dense 1 (Dense)</cell><cell>(256)</cell><cell>4735232</cell></row><row><cell>dropout 1 (Dropout)</cell><cell>(256)</cell><cell>0</cell></row><row><cell>dense 2 (Dense)</cell><cell>(128)</cell><cell>32896</cell></row><row><cell>dropout 2 (Dropout)</cell><cell>(128)</cell><cell>0</cell></row><row><cell>dense 3 (Dense)</cell><cell>(16)</cell><cell>2064</cell></row><row><cell>Total Trainable Parameters: 5, 122, 176</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>The classification accuracies (in percentages) on Indian Pines, University of Pavia, and Salinas Scene datasets using proposed and state-of-the-art methods. ± 2.8 83.<ref type="bibr" target="#b9">10</ref> ± 3.2 79.03 ± 2.7 94.34 ± 0.2 92.50 ± 0.7 92.98 ± 0.4 92.95 ± 0.3 92.11 ± 0.2 94.60 ± 2.3 2D-CNN 89.48 ± 0.2 87.96 ± 0.5 86.14 ± 0.8 97.86 ± 0.2 97.16 ± 0.5 96.55 ± 0.0 97.38 ± 0.0 97.08 ± 0.1 98.84 ± 0.1 3D-CNN 91.10 ± 0.4 89.98 ± 0.5 91.58 ± 0.2 96.53 ± 0.1 95.51 ± 0.2 97.57 ± 1.3 93.96 ± 0.2 93.32 ± 0.5 97.01 ± 0.6 M3D-CNN 95.32 ± 0.1 94.70 ± 0.2 96.41 ± 0.7 95.76 ± 0.2 94.50 ± 0.2 95.08 ± 1.2 94.79 ± 0.3 94.20 ± 0.2 96.25 ± 0.6 SSRN 99.19 ± 0.3 99.07 ± 0.3 98.93 ± 0.6 99.90 ± 0.0 99.87 ± 0.0 99.91 ± 0.0 99.98 ± 0.1 99.97 ± 0.1 99.97 ± 0.0 HybridSN 99.75 ± 0.1 99.71 ± 0.1 99.63 ± 0.2 99.98 ± 0.0 99.98 ± 0.0 99.97 ±</figDesc><table><row><cell>Methods</cell><cell>OA</cell><cell>Indian Pines Dataset Kappa</cell><cell>AA</cell><cell>OA</cell><cell>University of Pavia Dataset Kappa</cell><cell>AA</cell><cell>OA</cell><cell cols="3">Salinas Scene Dataset Kappa</cell><cell>AA</cell></row><row><cell>SVM</cell><cell cols="6">85.30 0.0</cell><cell cols="2">100 ± 0.0</cell><cell>100 ± 0.0</cell><cell>100 ± 0.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>The training time in minutes (m) and test time in seconds (s) over IP, UP, and SA datasets using 2D-CNN, 3D-CNN and HybridSN architectures.</figDesc><table><row><cell>Data</cell><cell cols="2">2D CNN Train(m) Test(s)</cell><cell cols="2">3D CNN Train(m) Test(s)</cell><cell cols="2">HybridSN Train(m) Test(s)</cell></row><row><cell>IP</cell><cell>1.9</cell><cell>1.1</cell><cell>15.2</cell><cell>4.3</cell><cell>14.1</cell><cell>4.8</cell></row><row><cell>UP</cell><cell>1.8</cell><cell>1.3</cell><cell>58.0</cell><cell>10.6</cell><cell>20.3</cell><cell>6.6</cell></row><row><cell>SA</cell><cell>2.2</cell><cell>2.0</cell><cell>74</cell><cell>15.2</cell><cell>25.5</cell><cell>9.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>The impact of spatial window size over the performance of HybridSN .</figDesc><table><row><cell cols="6">Window IP(%) UP(%) SA(%) Window IP(%) UP(%) SA(%)</cell></row><row><cell>19×19 99.74</cell><cell>99.98</cell><cell>99.99</cell><cell>23×23 99.31</cell><cell>99.96</cell><cell>99.71</cell></row><row><cell>21×21 99.73</cell><cell>99.90</cell><cell>99.69</cell><cell>25×25 99.75</cell><cell>99.98</cell><cell>100</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>The classification accuracies (in percentages) using proposed and state-ofthe-art methods on less amount of training data, i.e., 10% only. 78.26 68.32 96.63 95.53 94.84 96.34 95.93 94.36 3D-CNN 82.62 79.25 76.51 96.34 94.90 97.03 85.00 83.20 89.63 M3D-CNN 81.39 81.20 75.22 95.95 93.40 97.52 94.20 93.61 96.66 SSRN 98.45 98.23 86.19 99.62 99.50 99.49 99.64 99.60 99.76 HybridSN 98.39 98.16 98.01 99.72 99.64 99.20 99.98 99.98 99.98</figDesc><table><row><cell>Methods</cell><cell>Indian Pines OA Kappa AA</cell><cell>Univ. of Pavia OA Kappa AA</cell><cell>Salinas Scene OA Kappa AA</cell></row><row><cell>2D-CNN</cell><cell>80.27</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">www.ehu.eus/ccwintco/index.php/Hyperspectral Remote Sensing Scenes</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Hyperspectral imaging: techniques for spectral detection and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-I</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Advances in hyperspectral image classification: Earth monitoring with statistical learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="54" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification via multiscale joint collaborative representation with locally adaptive dictionary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="112" to="116" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A new spatialspectral feature extraction method for hyperspectral images using local covariance matrix representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3534" to="3546" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Composite kernels for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Camps-Valls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Chova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Muñoz-Marí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vila-Francés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Calpe-Maravilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="97" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiple feature learning for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gamba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Benediktsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1592" to="1606" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification using joint sparse model and discontinuity preserving relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="78" to="82" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Boltzmann entropy-based unsupervised band selection for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Band selection of hyperspectral images using multiobjective optimization-based sparse self-representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification via fusing correlation coefficient and joint sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="340" to="344" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sparse representation-based hyperspectral image classification using multiscale superpixels and guided filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Lisht: Nonparametric linearly scaled hyperbolic tangent activation function for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Manna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">B</forename><surname>Chaudhuri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.05894</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rccnet: An efficient convolutional neural network for histological routine colon cancer nuclei classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Basha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Babu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pulabaigari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th International Conference on Control, Automation, Robotics and Vision (ICARCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1222" to="1227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dual cnn models for unsupervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Repala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Dubey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06324</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A performance evaluation of convolutional neural networks for face anti spoofing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nagpal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Dubey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dual-path network-based hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An unsupervised convolutional feature fusion network for deep representation of remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="23" to="27" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data augmentation for hyperspectral image classification with deep cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hyperspectral image classification with deep feature fusion network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3173" to="3184" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploring hierarchical convolutional features for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Low-high-power consumption architectures for deeplearning models applied to hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Haut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bernabé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Paoletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fernandez-Beltran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep feature extraction and classification of hyperspectral images based on convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="6232" to="6251" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spectral-spatial residual network for hyperspectral image classification: A 3-d deep learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chapman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="847" to="858" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised spectral-spatial feature learning via deep residual conv-deconv network for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghamisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="391" to="406" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep pyramidal residual networks for spectral-spatial hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Paoletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Haut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fernandez-Beltran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Capsule networks for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Paoletti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Haut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fernandez-Beltran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep hashing neural networks for hyperspectral image feature extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Classification of hyperspectral remote sensing images with support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Melgani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bruzzone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on geoscience and remote sensing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1778" to="1790" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep supervised learning for hyperspectral data classification through convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Makantasis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karantzalos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doulamis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Doulamis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Geoscience and Remote Sensing Symposium (IGARSS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4959" to="4962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">3-d deep learning approach for remote sensing image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ben Hamida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benoit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Ben</forename><surname>Amar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on geoscience and remote sensing</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4420" to="4434" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-scale 3d deep convolutional neural network for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3904" to="3908" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
