<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Interpretable Visual Reasoning via Induced Symbolic Space</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhonghao</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IBM Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Mei</forename><surname>Hwu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UIUC</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Oregon</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Interpretable Visual Reasoning via Induced Symbolic Space</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:51+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of concept induction in visual reasoning, i.e., identifying concepts and their hierarchical relationships from question-answer pairs associated with images; and achieve an interpretable model via working on the induced symbolic concept space. To this end, we first design a new framework named object-centric compositional attention model (OCCAM) to perform the visual reasoning task with object-level visual features. Then, we come up with a method to induce concepts of objects and relations using clues from the attention patterns between objects' visual features and question words. Finally, we achieve a higher level of interpretability by imposing OCCAM on the objects represented in the induced symbolic concept space. Experiments on the CLEVR dataset demonstrate: 1) our OCCAM achieves a new state of the art without humanannotated functional programs; 2) our induced concepts are both accurate and sufficient as OCCAM achieves an on-par performance on objects represented either in visual features or in the induced symbolic concept space. Our code will be made available at https://github.com/SHI-Labs/Interpretable-Visual-Reasoning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent advances in Visual Question Answering (VQA) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b24">25]</ref> usually rely on carefully designed neural attention models over images, and rely on pre-defined lists of concepts to enhance the compositional reasoning ability of the attention modules. Human prior knowledge plays an essential role in the success of the model design.</p><p>We focus on a less-studied problem in this field -given only question-answer pairs and images, whether it is possible to induce the visual concepts that are sufficient for completing the visual reasoning tasks. By sufficiency, we hope to maintain the predictive accuracy of the visual reasoning process, when using the induced concepts in place of the original visual features. We consider concepts that There is a big sphere behind the brown cylinder; does it have the same color as the small rubber sphere on the left side of the small red rubber ball? are important for visual reasoning, including properties of objects (e.g., red, cube) and relations between objects (e.g., left, front). The aforementioned scope and sufficiency criterion require accurately associating the induced symbols of concepts to both visual features and words that can express the meanings of concepts, so that each new instance of question-image pair can be transformed into the induced concept space for further computations. Additionally, it is necessary to identify super concepts, i.e., hypernyms of concept subsets (e.g., shape). The concepts inside a super concept are exclusive, so that the system knows each object can only possess one value in each subset. This introduces structural information to the concept space (multiple onehot vectors for each visual object) and further guarantees the accuracy of the aforementioned transformation. The value of the study has two folds. First, our pro-posed problem aims to identify visual concepts, their argument patterns (properties or relations) and their hierarchy (super concepts) without using any concept-level supervision. Solving the problem frees both the efforts of human annotations and human designs of concept schema required in previous visual reasoning works. At the same time, the problem is technically more challenging compared to the related existing problem like unsupervised or weaklysupervised visual grounding <ref type="bibr" target="#b35">[35]</ref>. Second, by constraining the visual reasoning models to work over the induced concepts, the ability of concept induction improves the interpretability of visual reasoning models. Unlike previous interpretable visual reasoning models that rely on humanwritten rules to associate neural modules with given concept definitions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b31">31]</ref>, our method resolves the concept definitions and associations interpretability automatically in the learning process, without the need of trading off for hand-crafted model designs. We achieve the proposed challenge in three steps. First, we propose a new model architecture, object-centric compositional attention model (OCCAM), that performs objectlevel visual reasoning instead of pixel-level by extracting object-level visual features with ResNet <ref type="bibr" target="#b11">[12]</ref> and pooling the features according to each object's bounding box. The object-level reasoning not only improves over the stateof-the-art methods, but also provides a higher-level interpretability for concept association and identification. In our second step, we benefit from this model's attention values over objects to create classifiers mapping visual objects to words; then derive the concepts and super concepts from the object-word cooccurrence matrices as shown in <ref type="figure" target="#fig_0">Figure  1</ref>. Finally, our concept-based visual reasoning framework predicts the concepts of objects and object relations and performs compositional reasoning using the predicted symbolic concept embeddings instead of the original visual features as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Experiments on the CLEVR dataset demonstrate that our overall approach improves the level of interpretability of neural visual reasoning models, and maintains the predictive accuracy: (1) our object-level visual reasoning model improves over the previous state-of-the-art methods; (2) our induced concepts and concept hierarchy is accurate in human study; and (3) our induced concepts are sufficient for visual reasoning -replacing visual features with concepts leads to only 0.7% performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Visual Question Answering (VQA) requires models to reason a question about an image to infer an answer. Recent works on this task can be partitioned into two groups: holistic models <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b15">16]</ref> and modular models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b24">25]</ref>, according to whether the approach has explicit sub-task structures. A typical holistic model, MAC <ref type="bibr" target="#b15">[16]</ref>, perform iterative reasoning steps with an attention mechanism on the image. A modular framework, NS-CL <ref type="bibr" target="#b24">[25]</ref>, designs multiple principle functions over the extracted features to explain the reasoning process.</p><p>Model interpretability. Plenty of previous works attempt to explain the decision rules of the learned models. For instance, Bau et al. <ref type="bibr" target="#b4">[5]</ref> proposed network dissection to quantify interpretability of CNNs. Zhang et al. <ref type="bibr" target="#b38">[38]</ref> explained a CNN prediction at the semantic level with decision trees. Shi et al. <ref type="bibr" target="#b31">[31]</ref> generated scene graphs from images to explicitly trace the reasoning-flow. Whereas some works <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b13">14]</ref> focused on visual attentions to provide enhanced interpretability. Our work is closely related to the self-explaining systems via rationalization <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b37">37]</ref>. Different from previous works that extract subsets of inputs as explanations, our work moves one-step further by learning parts of the structural explanation definitions (i.e., the concepts hierarchy) together with explanations.</p><p>Visual concept learning contributes to many visuallinguistic applications, such as cross-modal retrieval <ref type="bibr" target="#b21">[22]</ref>, visual captioning <ref type="bibr" target="#b19">[20]</ref>, and visual-question answering <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b3">4]</ref>. Some recent papers <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b24">25]</ref> attempt to disentangle visual concept learning and reasoning. Based on the visual concepts learned from VQA, Han et al. <ref type="bibr" target="#b9">[10]</ref> learned metaconcepts, i.e., relational concepts about concepts, with augmented questions and answers. In contrast, our model learns super concepts without external knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Object-Centric Compositional Reasoning</head><p>In this section, we introduce a new neural architecture for neural reasoning. Our model adopts an object-centric approach, which performs compositional reasoning over the extracted object-level visual features. This proposed objectcentric reasoning approach not only achieves state-of-theart performance, but also plays a key role in the task of inducing object-wise or relational concepts as will be described in section 4. <ref type="figure" target="#fig_1">Figure 2</ref> shows our general framework. It includes two training phases involving the process from feeding the input images and questions to attain the final answers. Phase 1 (black-colored paths) corresponds to the training of our object-centric neural model, in which we train the objectlevel feature extractor, the compositional reasoning module and the question embedding LSTM; Phase 2 (red-colored paths) corresponds to the induction of symbolic concepts based on the aforementioned trained neural modules, as well as the training of a concept projection module so that the induced concepts can be accommodated in our reasoning pipeline. The figure shows the central role that the object-centric model plays in our framework.</p><p>In the following sections, we will first review the background of the compositional reasoning framework which was first proposed by Hudson and Manning <ref type="bibr" target="#b15">[16]</ref> (section What is the material of the small cylinder that is the same color as the matte cube? 3.1); then, we will introduce the proposed object-level compositional attention network (section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Background on compositional reasoning</head><p>Notations As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, we name the visual vectors as vs, the output memory vector from the compositional reasoning module as m, the embedded word vectors for questions as ws, and the question embedding as q.</p><p>Baseline visual reasoning framework The original compositional reasoning framework <ref type="bibr" target="#b15">[16]</ref> is similar to the phase 1 of our framework in <ref type="figure" target="#fig_1">Figure 2</ref>, except that it works on pixel-level instead of object-level features. To generate vs, it feeds the image to a ResNet101 <ref type="bibr" target="#b11">[12]</ref> pretrained on Ima-geNet <ref type="bibr" target="#b7">[8]</ref> and flatten the last feature maps across the width and height as vs. For the question inputs, we first convert each question word to its word embedding vector (ws), then input ws to a bidirectional LSTM <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b8">9]</ref> to extract the question embedding vector q. The compositional reasoning module takes vs, ws and q as inputs and performs multistep reasoning to attain m, the final step memory output. Finally, the classifier outputs the probability for each answer choice with a linear classifier over the concatenation of m and q. The loss function can thus be written as:</p><formula xml:id="formula_0">L(ws, vs, q) = − k∈K y k log F(q k , G(ws k , vs k , q k )) q k = Q(ws k ), vs k = I(im k ).<label>(1)</label></formula><p>K is the total number of image-question pairs, y is the onehot ground truth vector, F is the classifier, G is the compositional module, Q is the question embedding LSTM, I is the visual feature extractor and im is the image input.</p><p>The MAC reasoning module The aforementioned framework employs the MAC (i.e., Memory, Attention, and Composition) cells <ref type="bibr" target="#b15">[16]</ref> as the compositional reasoning  module. This module processes visual and language inputs in a sequential way. Shown in <ref type="figure" target="#fig_1">Figure 2</ref> (right), each MAC cell contains a control unit and an R/W (Read/Write) unit; the blue diagrams labeled with w stand for fully connected layers and the symbol stands for Hadamard product. At each step, the i-th MAC cell receives the control signal c i−1 and the memory output from the previous step, m i−1 , and outputs the new memory vector m i . The control unit computes the single c i to control reading of vs in the R/W unit. Specifically, it computes the interactions among c i−1 , q i , and each vector in ws to produce the attention weights, and weighted averages ws to produce c i . The control unit of each MAC cell has a unique question embedding projection layer, while all other layers are shared. The R/W unit aims to read the useful vs and store the read information into m i . It first computes the interactions among m i−1 , c i−1 and each vector in vs to attain the attention weights, weighted averages vs to produce a read vector r i , and finally computes the interaction of r i and m i−1 to produce m i . The weights of the R/W units are shared across all MAC cells. The initial control signal and memory c 0 and m 0 are learnable parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Object-level compositional attention network</head><p>The object-level compositional attention network is shown in <ref type="figure" target="#fig_1">Figure 2</ref> with phase 1 path, and optimizes Eqn (1) with vs generated by our object-level feature extractor.</p><p>Object-level feature extraction Fed with an image, the object-level feature extractor produces a set of vectors, each of which encodes both a single object's unary visual features and its interactions with other objects. The structure of this module is shown in <ref type="figure" target="#fig_3">Figure 3</ref>. Following <ref type="bibr" target="#b24">[25]</ref>, we use Mask-RCNN <ref type="bibr" target="#b10">[11]</ref> to detect all objects in an image and output the bounding boxes for them. The image is fed to a ResNet34 network <ref type="bibr" target="#b11">[12]</ref> pretrained on ImageNet <ref type="bibr" target="#b7">[8]</ref> to generate the feature maps in parallel.</p><p>On top of the ResNet34 feature maps, we apply a global average pooling to get a single global feature vector (the gray vector in the figure). We concatenate this global vector with the feature map at each location, followed by three convolution layers. This global vector is crucial since it allows the visual features to encode the interaction among objects; and the three convolution layers fuse the local and global features into a single visual vector at each position.</p><p>Finally, to get object-level features from the above pixellevel fused features, we utilize RoI align <ref type="bibr" target="#b10">[11]</ref> to project the objects' bounding boxes onto the fused feature vectors to generate the RoI feature maps, and average pool the RoI feature maps for each object to produce the object-level vs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Concept Induction and Reasoning</head><p>In this section, we describe how we achieve our goal of inducing symbolic concepts for objects and performing compositional reasoning on the induced concepts. We first formalize the problem of concept induction (section 4.1). Then building on the learned object-centric reasoning model introduced in the previous section, we propose to induce concepts of both unary object properties or the binary relations between objects (section 4.2). Finally we introduce how to achieve compositional reasoning over symbolic concepts by substituting the object-level features with the induced concepts (section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Problem definition</head><p>We consider identifying three types of concepts: (1) the unary concepts C u that are properties of objects (e.g., red, cube, etc.); (2) the binary concepts C b that are relation descriptions between any two objects (e.g., left, front etc.); and (3) the super concepts C sup that are hypernyms of certain subsets of concepts (e.g., color, shape, etc.), subject to that each object can only possess one concept under each super concept, e.g., cube and sphere.</p><p>As questions refer to objects and describe object relations in images and, more importantly, include all the semantic information to reach an answer, it is natural to induce the concepts from question words. Therefore we assume that all the unary and binary concepts have their corresponding words; and these words are a subset of the nouns  <ref type="figure" target="#fig_5">Figure 4</ref>. The structure of the concept regression module. v1 and v2 are the object-level visual vectors representing two objects respectively, and cw is the word vector. m0 is a fixed vector and mw equals to m0 for the unary concept classifier. or adjectives from all the training questions. We denote the sets of words that describe unary concepts and binary concepts as M u and M b respectively. Therefore, the goal of concept induction consists of the following tasks: • Visual mapping: for each concept c ∈ C u or C b , learning a mapping from the visual feature v to c. In other words, a prediction function f c (v) ∈ {0, 1} is learned to predict the existence of concept c from the visual feature v of an object. • Word mapping: for each concept c ∈ C u or C b , identifying a subset of words S c ⊂ M u or M b that are synonyms representing the same concept, e.g., the concept of 'cube' corresponds to set of words {cube, cubes, block, blocks, ...}.</p><p>• Super concept induction: clustering of concepts to form super concepts. Each super concept c contains a set of con-</p><formula xml:id="formula_1">cepts {c 1 , · · · , c k } ⊂ C u or C b .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Concept induction</head><p>This section describes how we achieve the aforementioned tasks of concept induction. The key idea of our approach includes: (1) benefiting from the R/W unit from the trained MAC cells to achieve the visual mapping to textual words; (2) utilizing the inclusiveness of words' visual mapping to induce each concept's multiple word descriptions;</p><p>(3) clustering super concepts from the mutual exclusiveness between concepts. To achieve the above, we first train two binary classifiers that can determine if a word correctly describes an object's unique feature and if a word correctly describes a relation between two objects respectively. Then, with the help of these classifiers, we produce zero-one vectors for words that properly describe the unique features for each object and the relations between any pair of objects in single images across the dataset. Finally, we perform a clustering method on the word vectors to generalize unary and binary concepts, and the super concept sets.  We generate training data points</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual mapping via regression from MAC cells The concept regression module is shown in</head><formula xml:id="formula_2">P u = {(v 1 u i , c w u i , y u i )} and P b = {(v 1 b i , v 2 b i , c w b i , y b i )} for B u and B b</formula><p>by utilizing the Read/Write unit <ref type="figure" target="#fig_1">(Figure 2(right)</ref>) in the reasoning module after phase 1 training. The whole generation process is described in Algorithm 1. We denote R(vs, c i , m i−1 ) ∈ R |O| for the sequence of functions before the softmax operation in the Read unit and W(m i−1 , r i ) ∈ R D for the function of the Write unit, where O is the set of objects in an image and D is the vector dimension.</p><p>Specifically, our algorithm first uses R(·, ·, ·) and W(·, ·) to find the attention logits on the objects corresponding to words describing the unary and binary concepts in a question as shown in <ref type="figure" target="#fig_6">Figure 5</ref>(a&amp;b). We then use the values of logits to determine if the object possesses the concept of the word (positive) or not (negative). Noticing the attention logit distribution of the sampled objects for each word is a two-peak distribution ( <ref type="figure" target="#fig_6">Figure 5(c)</ref>), we use a GMM <ref type="bibr" target="#b33">[33]</ref> with two Gaussian components to model the distribution and find the decision boundary for each word's attention logit distribution. It is worth mentioning that the attention logit distribution for a unary concept word has two waves that do not interfere each other; while for a binary concept word the two waves interfere. This is because in some cases it is hard to tell if two objects have the relation described by a word. For example, it is hard to describe the "in front of" relation between two objects on the same horizontal axis (e.g., the blue and yellow objects in <ref type="figure" target="#fig_6">Figure 5(b)</ref>). Finally, P u and P b are generated by classifying the data points to Algorithm 1: Classifier data points generation. ST(·) splits a vector α ∈ R β to a set of β values. GMM(·) uses Gaussian Mixture Model to cluster a set of data points. FB(·) finds the decision boundary for the 2 Gaussian components. 1 is the indicator function. </p><formula xml:id="formula_3">Result: P u , P b P u = {}, P b = {} for x ∈ M u ∪ M b do Sx = {}, bdx = 0 for vs, ws ∈ DATASET do for cw ∈ ws ∩ M u do Sc w = Sc w ∪ ST(R(vs, cw, m0)) for cw ∈ ws ∩ M b do for v ∈ vs do Sc w = Sc w ∪ ST(R(vs, cw, W(m0, v))) for x ∈ M u ∪ M b do bdx = FB(GMM(Sx)) for vs, ws ∈ DATASET do for v1 ∈ vs do for cw ∈ ws ∩ M u do y = 1(R(v1, cw, m0) &gt; bdc w ) P u = P u ∪ {(v1, cw, y)} for cw ∈ ws ∩ M b do for v2 ∈ {vs − v1} do y = 1(R(v1, cw, W(m0, v2)) &gt; bdc w ) P b = P b ∪ {(v1,</formula><formula xml:id="formula_4">γ u = 1i&gt;0.5(B u (v1, C u )) γ b = 1i&gt;0.5(B b (v1, v2, C b )),<label>(2)</label></formula><p>where v <ref type="bibr" target="#b0">1</ref>  as shown in <ref type="figure" target="#fig_7">Figure 6</ref>, where N u and N b are the total numbers of objects and co-occurred object pairs. The two matrices summarize each word's corresponding visual objects in the whole dataset.</p><p>Concept/super-concept induction Finally, we group synonym words to unary and binary concepts and generate the super concepts. These two tasks are achieved via exploring the word inclusiveness and the concept exclusiveness captured by Γ u and Γ b : (1) words describing the same concept correspond to similar column vectors, e.g., Γ u small and Γ u tiny ; (2) words describing exclusive concepts have column vectors that usually do not have 1 values on same objects simultaneously, e.g., Γ u cube and Γ u ball . Based on the aforementioned ideas, we define the correlation metric between two words c w1 and c w2 as below:</p><formula xml:id="formula_5">θ cw 1 ,cw 2 = P (γ cw 1 = 1 | γ cw 2 = 1)+ P (γ cw 2 = 1 | γ cw 1 = 1) = |Γ cw 1 Γ cw 2 | 1 1 |Γ cw 2 | 1 1 + |Γ cw 1 Γ cw 2 | 1 1 |Γ cw 1 | 1 1 . (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2: Concept vector generalization. MAX(α)</head><p>returns the greatest value in the vector α and HARDMAX(α) returns a zero-one vector indicating the position of the greatest value in the vector α.</p><formula xml:id="formula_6">Result: K u , K b K u = 0 |O|×|E u | , K b = 0 |O|×|O|×|E b | for i ∈ O do for e u ∈ E u do K u [i][e u ] =MAX(B u (vi, ρeu )) for l u ∈ L u do K u [i][l u ] =HARDMAX(K u [i][l u ]) for j ∈ O − {i} do for e b ∈ E b do K b [i][j][e b ] =MAX(B b (vi, vj, ρ e b )) for l b ∈ L b do K b [i][j][l b ] =HARDMAX(K u [i][j][l b ])</formula><p>This guarantees that θ → 0 + for two synonym words, θ → 2 − for two words corresponding to exclusive concepts and θ ∈ (0, 2) for words corresponding to different nonexclusive concepts. We can produce the correlation sets for the words describing the unary concepts and the binary concepts respectively with Eqn (4).</p><formula xml:id="formula_7">Θ u = {θc w 1 ,cw 2 }; cw 1 , cw 2 ∈ M u Θ b = {θc w 1 ,cw 2 }; cw 1 , cw 2 ∈ M b .<label>(4)</label></formula><p>Our final step fits two GMM on Θ u and Θ b respectively. Each GMM has three components N 0 , N 1 and N 2 , with their mean values initialized with 0,1 and 2. We then induce the unary and binary concepts, where each concept consists of synonym words whose mutual correlation is clustered to the Gaussian component N 0 . Similarly, we induce the super concepts, where each super concept contains multiple concepts and any two words from different concepts have correlation clustered to the Gaussian component of N 2 .</p><p>We denote the set of words corresponding to a concept e as ρ e , the set of the super concept sets as L, the set of all concepts as E. Then, we can represent all the objects in an image with a unary concept matrix K u and represent all the relations between any two objects in an image with a binary concept matrix K b with Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Concept compositional reasoning</head><p>Our ultimate goal is to perform compositional reasoning to answer a question with the generated concept representations K u and K b for an image; so as to confirm that our induced concepts are accurate and sufficient. We achieve this with the phase 2 training process in <ref type="figure" target="#fig_1">Figure 2</ref>. The key idea is to transplant the learned compositional reasoning module from manipulating the visual features to manipulating K u and K b , for attaining the answer to a question. w w conv 1D <ref type="figure">Figure 7</ref>. The structure of the concept projection module. We label the dimensions of matrices near them in the graph.</p><formula xml:id="formula_8">ℝ |"|×|"|×|$ ! | ℝ |"|×(|"||$ ! |) ℝ |"|×' ℝ |"|×((') ℝ |"|×' ℝ |"|× $ " ℝ |"|×' binary concepts ( ! ) unary concepts ( " )</formula><p>To this end, first, we project K u and K b to the same vector space with vs with the concept projection module shown in <ref type="figure">Figure 7</ref>, so that the compositional module can perform the reasoning steps on the projected concept vectors. Specifically, we first reduce the dimension of K b from</p><formula xml:id="formula_9">R |O|×|O|×|E b | to R |O|×|O||E b | , resulted inK b , because K b</formula><p>can be understood as the relations to other objects for each object in an image. Then, we use two separate fully connected networks to project K u andK b respectively, concatenate and use a sequence of 1D convolution layers to project the results to the same dimension of vs's.</p><p>Second, to minimize the discrepancy between the distribution of our projected vectors and that of the original visual vectors vs, we fix the weights of other modules in the framework and only train the concept project module by optimizing the target function Eqn. <ref type="bibr" target="#b0">(1)</ref>. Then, we train the concept projection module and the compositional reasoning module with other modules' weights fixed to better optimize Eqn. <ref type="bibr" target="#b0">(1)</ref>. The result is a compositional reasoning model that works on the induced concepts only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Settings</head><p>Dataset We use the CLEVR <ref type="bibr" target="#b17">[18]</ref> dataset to evaluate our model. The dataset comprises images of synthetic objects of various shapes, colors, sizes and materials and question/answer pairs about these images. The questions require multi-hop reasoning, such as finding the transitive relations, counting numbers, comparing properties, to attain correct answers. Each question is provided a ground truth human-written programs. Because the programs rely on pre-defined concepts thus do not fit our problem, we let our framework learn the compositional reasoning by itself without using the program annotations. There are 70k images and ∼700k questions in the training set; 15k images and ∼150k questions in the validation set. We follow the previous works <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25</ref>] to train our model on the whole training set and test our model on the validation set.</p><p>Training details We set the hidden dimension D to in all modules. We follow <ref type="bibr" target="#b15">[16]</ref> to design the question embed- ding module, the compositional module and the classifier. For the object-level feature extracter, we make the backbone ResNet34 learnable and zero-pad the output vs to 12 vectors in total for any image. Notice that the maximum number of objects in an image is 11, so that the reasoning module is able to read nothing into the memory for some steps. For the concept projection module, to cover the full view of vs, the conv1D consists of five 1D convolution layers with kernel sizes <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b4">5)</ref>, each followed by a Batch Norm layer <ref type="bibr" target="#b16">[17]</ref> and an ELU activation layer <ref type="bibr" target="#b6">[7]</ref>. We use Adam optimizer <ref type="bibr" target="#b20">[21]</ref> with momentum 0.9 and 0.999. Phase 1 and phase 2 share a same training schedule: the learning rate is initiated with 10 −4 for the first 20 epochs and is halved every 5 epochs afterwards until stopped at the 40th epoch. We train the concept regression module separately with learning rate of 10 −4 for 6 epochs. Phase 1 uses 4 Nvidia V100 GPUs with batch size 256. The other training processes use a single GPU with a batch size of 192.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Object-level reasoning</head><p>We first perform the end-to-end phase 1 training shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The performance comparison of our model to the state-of-the-art models is shown in <ref type="table" target="#tab_0">Table 1</ref>. Under the condition that no external human-labeled programs are used, our model achieves 99.4% in accuracy, a new stateof-the-art performance on CLEVR validation set compared to other methods under the same condition. Our model also has an on-par performance with the best model <ref type="bibr" target="#b36">[36]</ref> using external human-labeled programs. Compared to the original MAC <ref type="bibr" target="#b15">[16]</ref> framework which uses image-level attentions and achieves 98.9% in accuracy on CLEVR validation set, our model proves that the constraint of attentions on the objects are useful for improving the performance. We do not use the position embedding to explicitly encode the positions of objects for relational reasoning; however, we use the global features to enhance the model's understanding of inter-object relations. This implicates that the relations among objects are totally learnable concepts without external knowledge for the deep network. <ref type="table">Table 2</ref> further gives an ablation study on the numbers of reasoning steps, i.e., the number of MAC modules, for our model. The reasoning model with 4 steps has a performance gap to the models with 8, 12 or 16 steps, while the latter three models have on-par performances. We conjecture that the model with low reasoning steps may not be able to capture multiple hops of a question and the model performance converges with an increasing number of reasoning steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Concept induction and reasoning</head><p>Results of concept induction To achieve the balance of the performance and the interpretability, we choose the object-level compositional reasoning model with 8 reasoning steps for the concept induction and reasoning. After visual mapping, binary coding and concept/super-concept induction described in section 4.2, the unary concepts and super concepts are induced as shown in <ref type="figure" target="#fig_10">Figure 8</ref>; the binary concepts are 'left', 'right', 'front' and 'behind', and {'left', 'right'} and {'front', 'behind'} form two super concept sets. Appendix A provides more detailed information on how these clusters are generated by presenting the concept correlations. The generated concept hierarchy perfectly recovers the definition in CLEVR data generator and matches human prior knowledge, showing the success of our approach.</p><p>Concept-level reasoning For each image in the dataset, we can now represent each object and the relations between any two objects with the induced unary and binary concepts. With the method described in section 4.3, we project the concepts back to the visual feature space, so that the compositional reasoning module can perform the reasoning steps on the projected vectors(ps). Our concept compositional reasoning model achieves an on-par performance with the one of the object-level compositional reasoning model as shown in <ref type="table" target="#tab_1">Table 3</ref>. We also visualize the reasoning steps for the concept reasoning module as shown in Appendix B.    <ref type="figure">Figure 9</ref>. An multi-modal analogy example enabled by our results.</p><p>Discussion Our concept induction results bridge the visual and symbolic spaces. The results enable to extend word analogy <ref type="bibr" target="#b27">[27]</ref> (e.g., "Madrid" -"Spain" + "France" → "Paris") into the multi-modality setting. <ref type="figure">Figure 9</ref> gives an example, starting with the initial object v 0 and its predicted concepts K 0 , subtracting concepts K 1 and adding new concepts K 2 result in a new concept set K 3 <ref type="figure">(Figure 9 (bottom)</ref>). Then if we retrieve visual object v i with each concept set K i along the path <ref type="figure">(Figure 9</ref> (top)), we have v 0 −v 1 +v 2 ≈ v 3 in the original visual feature space. Similarly, with the exclusiveness inside a super concept, we can quantify the distance between two visual objects with the super concept space. More details are provided in Appendix B.</p><p>For future works, our method can be extended to more sophisticated induction tasks, such as inducing concepts from phrases, with more complicated hierarchy, with degrees of features (e.g., dark blue, light blue) and inducing complicated relations between objects (e.g. a little bigger).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Our proposed OCCAM framework performs pure object-level reasoning and achieves a new state-of-theart without human-annotated functional programs on the CLEVR dataset. Our framework makes the object-word cooccurrence information avaiable, which enables induction of the concepts and super concepts based on the inclusiveness and the mutual exclusiveness of words' visual mappings. When working on concepts instead of visual features, OCCAM achieves comparable performance, proving the accuracy and sufficiency of the induced concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unary concept correlations Θ u</head><p>We present the unary concept correlations Θ u in <ref type="figure" target="#fig_0">Figure  10</ref>. The correlation between any pair of synonyms is close to 0, the correlation between words belonging to the same super concept set is close to 2, and the correlation between words belonging to two different super concept sets is in the middle of the range[0, 2]. Therefore, a GMM with three components whose mean values are initialized with 0,1 and 2 can successfully generate the concepts and super concept sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Derivation from the concept interpretation</head><p>With the induced concepts and super concept sets, each object can be represented with a zero-one vector, k, where the entry is 1 if that object possesses the corresponding concept or 0 otherwise. Notice that the super concept sets split the whole concept set; we thereby name the entries of k corresponding to one super concept set as a super concept. The super concept is thus a zero-one vector with exactly one entry to be 1. We name this pattern as the super concept constraint. Therefore, we can define the semantic distance between two visual objects by the number of different super concepts or by Eqn. <ref type="bibr" target="#b4">(5)</ref>.</p><formula xml:id="formula_10">ζ k1,k2 = |k 1 ⊕ k 2 | 1 1 2 ,<label>(5)</label></formula><p>where k 1 and k 2 are the concept vectors representing two objects and ⊕ is the operation XOR. Studying the concepts and super concept sets induced, we acknowledge that the super concept sets correspond to color, shape, size and material in semantics. Thereby, we give an example of the semantic distances of multiple objects to one object as shown  in <ref type="figure" target="#fig_0">Figure 11</ref>. The circle radii indicate the semantic distances to the object at the centers of these circles. The inner three circles are segmented so that each segment represents what super concepts are different. The outer circle represents all the 4 super concepts are different between the object on that circle and the object at the center. We can further interpret the semantic analogy in the visual feature space with the induced concept vectors. Shown in <ref type="figure" target="#fig_0">Figure 12</ref>, we first generate four images of different objects; then, we use our trained OCCAM structure to extract the object-level features corresponding to the objects bounded by red rectangles. Shown in <ref type="figure" target="#fig_0">Figure 13</ref>(a), we can move the visual feature vector of the leftmost object closer to that of the rightmost object by subtracting and adding visual feature vectors of two other objects. The proximity between pairs of visual feature vectors is measured with cosine similarity as shown in <ref type="figure" target="#fig_0">Figure 13</ref>(b). In the concept vector space, we can define a 'minus' operation, k 1 \k 2 , as eliminate the shared super concepts between k 1 and k 2 from k 1 . We can also define a 'plus' operation, k 1 ⊕ k 2 , between a concept vector template k 1 and a concept vector k 2 as add the super concepts of o 2 that o 1 misses to o 1 . Therefore, The operations in the visual feature space can be explained with the operations we defined in the concept vector space shown in <ref type="figure" target="#fig_0">Figure 14</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Visualization of reasoning steps</head><p>We give an example of the compositional reasoning steps on the induced concept space of OCCAM as shown in <ref type="figure" target="#fig_0">Figure 15</ref>. While the attention is directly imposed on the projected concept vectors in the read unit of the compositional reasoning module, the attention can be equally mapped to the concept vectors and the visual objects as the projected concept vector to the concept vector or the projected concept vector to the visual object is a one-to-one mapping relationship.</p><p>pred: cube answer: cube </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>What is the big thing that is in front of the block that is behind the block that is in front if the large shiny block made of? matte Illustration of our framework. Our model induces the concepts and super concepts with the attention correlation between the objects and question words in image-question pairs as the paths shown in blue arrows. Then, it answers a question about an image via compositional reasoning on the induced symbolic representations of objects and object relations, shown as the orange paths.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>The framework and the compositional reasoning module. The left graph shows the general framework; The phase 1 training path is drawn in purple and the phase 2 training paths are drawn in red. The black paths are shared for both training phases. The structures of our proposed object-level feature extractor, concept regression module and concept projection module are shown in Figures 3, 4 and 7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>The structure of object-level feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>It is composed of a classifier for the unary concept word regression, B u (v 1 , c w ) ∈ [0, 1], and a classifier for the binary concept word regression, B b (v 1 , v 2 , c w ) ∈ [0, 1]. B u is expected to produce 1 if v 1 can be described by the word vector c w . Likewise, B b is expected to produce 1 if the relation of v 1 to v 2 can be described by the word vector c w .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Attention visualization and attention logit distributions. (a) The attention visualization corresponding to the words describing the unary concepts by performing R(vs, cw, m0). Each of the words above the latter 4 images corresponds to a unique cw and the value on each object is the attention logit (the same applies to (b)). (b) The attention visualization corresponding to the words describing the binary concepts by performing R(vs, cw, W(m0, v2)). v2 represents the object bounded by a red rectangle in the first image. (c) the attention logit distribution corresponding to each word describing a concept.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>v2, cw, y)} positives and negatives with the decision boundaries.With the data points P u and P b generated, we can train The zero-one matrices indicating word descriptions of objects and object relations across the dataset.(a) The matrix Γ u indicates what words can describe objects. (b) The matrix Γ b indicates what words can describe the relations object v1's (bounded by green rectangles) are to object v2's (bounded by red rectangles).B u and B b by minimizing the binary cross entropy loss. Binary coding of objects With trained B u and B b , we represent an object o 1 with a binary code vector. Each dimension corresponds a word. A dimension has value 1 if the corresponding word can describe o 1 and 0 otherwise. The binary vectors of object properties and of the relations between two objects, o 1 and o 2 can be computed with the functions γ u ∈ R |M u | and γ b ∈ R |M b | respectively:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>and v 2</head><label>2</label><figDesc>are the object-level visual vectors of o 1 and o 2 , C u ∈ R |M u |×D and C b ∈ R |M b |×D are the stacks of word embeddings in vocabulary M u and M b . 1 α (β) performs elementwise on β: return 1 if the element satisfies condition α or 0 otherwise. By applying γ u and γ b to all the objects and relations in the dataset, we can attain a matrix Γ u ∈ {0, 1} M u ,N u and a matrix Γ b ∈ {0, 1} M b ,N b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>unary concepts/super-concepts (b) binary concepts/super-concepts</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Concepts and super concept sets. Each circle represents a concept described by the words in that circle. A super concept set comprises the concepts represented by circles of the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>The unary concept correlations Θ u .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 11 .</head><label>11</label><figDesc>Illustration of the semantic distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 12 .</head><label>12</label><figDesc>The original images for extracting visual features. The object-level features corresponding to the objects bounded by red rectangles are used for the illustration of semantic operations in the visual feature space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 13 .Figure 14 .</head><label>1314</label><figDesc>Illustration of the semantic analogy in the visual feature space. (a) The operations on the visual features. (b) The cosine similarities between pairs of visual feature vectors. Operations on the concept vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 15 .</head><label>15</label><figDesc>Visualization of reasoning steps. (a) The question, image, prediction and ground truth answer. The index of each object is shown on the upper left of the object. (b) The induced concepts of objects and relations. (c) The stepwise attentions on question words. (d) The stepwise attentions on objects. (e) The concept vector read into the memory of the reasoning module in each step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The comparison of our object-level compositional reasoning framework to the state-of-the-art methods. * indicates the method uses external program annotations.</figDesc><table><row><cell>method</cell><cell cols="4">overall count exist comp</cell><cell>query</cell><cell>comp</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>numb</cell><cell>attr</cell><cell>attr</cell></row><row><cell>Human [19]</cell><cell>92.6</cell><cell cols="4">86.7 96.6 86.5 95.0 96.0</cell></row><row><cell>NMN* [3]</cell><cell>72.1</cell><cell cols="4">52.5 72.7 79.3 79.0 78.0</cell></row><row><cell>N2NMN* [15]</cell><cell>83.7</cell><cell cols="4">68.5 85.7 84.9 90.0 88.7</cell></row><row><cell>IEP* [19]</cell><cell>96.9</cell><cell cols="4">92.7 97.1 98.7 98.1 98.9</cell></row><row><cell>TbD* [26]</cell><cell>99.1</cell><cell cols="4">97.6 99.4 99.2 99.5 99.6</cell></row><row><cell>NS-VQA* [36]</cell><cell>99.8</cell><cell cols="4">99.7 99.9 99.9 99.8 99.8</cell></row><row><cell>RN [30]</cell><cell>95.5</cell><cell cols="4">90.1 93.6 97.8 97.1 97.9</cell></row><row><cell>FiLM [29]</cell><cell>97.6</cell><cell cols="4">94.5 93.8 99.2 99.2 99.0</cell></row><row><cell>MAC [16]</cell><cell>98.9</cell><cell cols="4">97.2 99.4 99.5 99.3 99.5</cell></row><row><cell>NS-CL [25]</cell><cell>98.9</cell><cell cols="4">98.2 99.0 98.8 99.3 99.1</cell></row><row><cell>OCCAM (ours)</cell><cell>99.4</cell><cell cols="4">98.1 99.8 99.0 99.9 99.9</cell></row><row><cell cols="6">Table 2. The ablation study on the choice of reasoning steps for</cell></row><row><cell cols="3">the object-level compositional reasoning.</cell><cell></cell><cell></cell></row><row><cell>steps</cell><cell>4</cell><cell>8</cell><cell>12</cell><cell>16</cell></row><row><cell cols="6">accuracy 94.3 98.6 99.4 99.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>The comparison of our object-level compositional reasoning framework and our concept compositional reasoning framework. The number of reasoning steps is set to 8.</figDesc><table><row><cell cols="3">method overall count exist comp</cell><cell>query</cell><cell>comp</cell></row><row><cell></cell><cell></cell><cell>numb</cell><cell>attr</cell><cell>attr</cell></row><row><cell>object</cell><cell>98.6</cell><cell cols="2">95.9 99.8 96.2 99.8 99.7</cell></row><row><cell>concept</cell><cell>97.9</cell><cell cols="2">95.6 98.7 97.3 98.4 99.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NACACL)</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NACACL)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6541" to="6549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to explain: An information-theoretic perspective on model interpretation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="883" to="892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Framewise phoneme classification with bidirectional lstm and other neural network architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="602" to="610" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual concept-metaconcept learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5001" to="5012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explainable neural computation via stack neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="53" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Compositional attention networks for machine reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clevr: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2901" to="2910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2989" to="2998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rationalizing neural predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="107" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Transparency by design: Closing the gap between performance and interpretability in visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mascharka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Soklaski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Attentive explanations: Justifying decisions and pointing to the evidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Dong Huk Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04757</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Film: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Harm De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Association for the Advancement of Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Explainable and explicit visual reasoning over scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8376" to="8384" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="451" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Em algorithms of gaussian mixture model and hidden markov model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guorong</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiqi</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="145" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised textual grounding: Linking words to image concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><forename type="middle">N</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schwing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6125" to="6134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural-symbolic vqa: Disentangling reasoning from vision and language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1031" to="1042" />
		</imprint>
	</monogr>
	<note>Pushmeet Kohli, and Josh Tenenbaum</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking cooperative rationalization: Introspective extraction and complement control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4085" to="4094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Interpreting cnns via decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanshi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6261" to="6270" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
