<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Attentive Bag-of-Entities Model for Text Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Studio Ousia</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">RIKEN AIP</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya@ousia</forename><surname>Jp</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Nara Institute of Science and Technology</orgName>
								<address>
									<settlement>Nara</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">RIKEN AIP</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shindo@is</forename><forename type="middle">Naist</forename><surname>Jp</surname></persName>
						</author>
						<title level="a" type="main">Neural Attentive Bag-of-Entities Model for Text Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This study proposes a Neural Attentive Bagof-Entities model, which is a neural network model that performs text classification using entities in a knowledge base. Entities provide unambiguous and relevant semantic signals that are beneficial for capturing semantics in texts. We combine simple high-recall entity detection based on a dictionary, to detect entities in a document, with a novel neural attention mechanism that enables the model to focus on a small number of unambiguous and relevant entities. We tested the effectiveness of our model using two standard text classification datasets (i.e., the 20 Newsgroups and R8 datasets) and a popular factoid question answering dataset based on a trivia quiz game. As a result, our model achieved state-of-the-art results on all datasets. The source code of the proposed model is available online at https://github.com/ wikipedia2vec/wikipedia2vec.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Text classification is an important task, and its applications span a wide range of activities such as topic classification, spam detection, and sentiment classification. Recent studies showed that models based on neural networks can outperform conventional models (e.g., na√Øve Bayes) on text classification tasks <ref type="bibr" target="#b21">(Kim, 2014;</ref><ref type="bibr" target="#b18">Iyyer et al., 2015;</ref><ref type="bibr" target="#b38">Tang et al., 2015;</ref><ref type="bibr" target="#b3">Dai and Le, 2015;</ref><ref type="bibr" target="#b19">Jin et al., 2016;</ref><ref type="bibr" target="#b20">Joulin et al., 2017;</ref><ref type="bibr" target="#b37">Shen et al., 2018)</ref>. Typical neural network-based text classification models are based on words. They typically use words in the target documents as inputs, map words into continuous vectors (embeddings), and capture the semantics in documents by using compositional functions over word embeddings such as averaging or summation of word embeddings, convolutional neural networks (CNN), and recurrent neural networks (RNN).</p><p>Apart from the aforementioned approaches, past studies attempted to use entities in a knowledge base (KB) (e.g., Wikipedia) to capture the semantics in documents. These models typically represent a document by using a set of entities (or bag of entities) relevant to the document <ref type="bibr">Markovitch, 2006, 2007;</ref><ref type="bibr" target="#b40">Xiong et al., 2016)</ref>. The main benefit of using entities instead of words is that unlike words, entities provide unambiguous semantic signals because they are uniquely identified in a KB. One key issue here is to determine the way in which to associate a document with its relevant entities. An existing straightforward approach <ref type="bibr" target="#b40">Xiong et al., 2016)</ref> involves creating a set of relevant entities using an entity linking system to detect and disambiguate the names of entities in a document. However, this approach is problematic because (1) entity linking systems produce disambiguation errors <ref type="bibr" target="#b2">(Cornolti et al., 2013)</ref>, and (2) entities appearing in a document are not necessarily relevant to the given document <ref type="bibr" target="#b11">(Gamon et al., 2013;</ref><ref type="bibr" target="#b5">Dunietz and Gillick, 2014)</ref>.</p><p>This study proposes the Neural Attentive Bagof-Entities (NABoE) model, which is a neural network model that addresses the text classification problem by modeling the semantics in the target documents using entities in the KB. For each entity name in a document (e.g., "Apple"), our model first detects entities that may be referred to by this name (e.g., Apple Inc., Apple (food)), and then represents the document using the weighted average of the embeddings of these entities. The weights are computed using a novel neural attention mechanism that enables the model to focus on a small subset of the entities that are less ambiguous in meaning and more relevant to the document. In other words, the attention mechanism is designed to compute weights by jointly addressing entity linking and entity salience detection <ref type="bibr">(Ga-mon et al., 2013;</ref><ref type="bibr" target="#b5">Dunietz and Gillick, 2014)</ref> tasks. Furthermore, the attention mechanism improves the interpretability of the model because it enables us to inspect the small number of entities that strongly affect the classification decisions.</p><p>We validate the effectiveness of our proposed model by addressing two important natural language tasks: a text classification task using two standard datasets (i.e., the 20 Newsgroups and R8 datasets), and a factoid question answering task based on a popular dataset derived from the quiz bowl trivia quiz game. As a result, our model achieved state-of-the-art results on both tasks. The source code of the proposed model is available online at https://github.com/ wikipedia2vec/wikipedia2vec.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head><p>Given a document, our model addresses the text classification task by using the following two steps: it first detects entities from the document, and then classifies the document using the proposed model with the detected entities as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Entity Detection</head><p>In this step, we detect entities that may be relevant to the document. Here, we use a simple method based on an entity dictionary that maps an entity name (e.g., "Washington") to a set of possible referent entities (e.g., Washington, D.C. and George Washington). In particular, we first take all words and phrases in a document, treat them as entity names if they exist in the dictionary, and detect all possible referent entities for each detected entity name. Following past work <ref type="bibr" target="#b15">(Hasibi et al., 2016;</ref><ref type="bibr" target="#b40">Xiong et al., 2016)</ref>, the boundary overlaps of the names are resolved by detecting only those that are the earliest and the longest.</p><p>We use Wikipedia as the target KB, and the entity dictionary is built by using the names and their referent entities of all internal anchor links in Wikipedia <ref type="bibr" target="#b13">(Guo et al., 2013)</ref>. We also collect two statistics from Wikipedia, namely link probability and commonness <ref type="bibr" target="#b28">(Mihalcea and Csomai, 2007;</ref><ref type="bibr" target="#b31">Milne and Witten, 2008)</ref>. The former is the probability of a name being used as an anchor link in Wikipedia, whereas the latter is the probability of a name referring to an entity in Wikipedia.</p><p>We generate a list of entities by concatenating all possible referent entities contained in the dictionary for each detected entity name, and feed it to the model presented in the next section. Note that we do not disambiguate entity names here, but detect all possible referent entities of the entity names. <ref type="figure" target="#fig_0">Figure 1</ref> shows the architecture of our model. Given words w 1 , ..., w N , and entities e 1 , ..., e K detected from target document D, we first compute the word-based representation of D:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Model</head><formula xml:id="formula_0">z word = 1 N N i=1 v w i ,<label>(1)</label></formula><p>where v w ‚àà R d is the embedding of word w. We then derive the entity-based representation of D as a weighted average of the embeddings of the entities:</p><formula xml:id="formula_1">z entity = K i=1 a e i v e i ,<label>(2)</label></formula><p>where v e ‚àà R d is the embedding of entity e and a e the normalized attention weight corresponding to e computed using the following softmax-based attention function:</p><formula xml:id="formula_2">a e = exp(w a Œ¶(e, D) + b a ) K i=1 exp(w a Œ¶(e i , D) + b a ) ,<label>(3)</label></formula><p>where w a ‚àà R l is a weight vector, b a ‚àà R is the bias, and Œ¶(e, D) is a function that generates an l-dimensional vector consisting of the features of the attention function.</p><p>We use the following two features in the attention function:</p><p>‚Ä¢ Cosine: the cosine similarity between the embedding of the entity v e and the wordbased representation of the document z word .</p><p>‚Ä¢ Commonness: the probability that the entity name refers to the entity in KB.</p><p>Here, our aim is to capture the relevance and the unambiguity of entity e in document D using the attention function. Thus, the problem is related to the tasks of entity salience detection <ref type="bibr" target="#b11">(Gamon et al., 2013;</ref><ref type="bibr" target="#b5">Dunietz and Gillick, 2014)</ref>, which aims to detect entities relevant (or salient) to the document, and entity linking, which aims to resolve the ambiguity of entities. The key assumption relating to these two tasks in the literature is that if an entity is semantically related to the given document, it is relevant to the document <ref type="bibr" target="#b5">(Dunietz and Gillick, 2014)</ref>, and it is likely to appear in the document <ref type="bibr" target="#b31">(Milne and Witten, 2008;</ref><ref type="bibr" target="#b36">Ratinov et al., 2011)</ref>. With this in mind and following past work <ref type="bibr" target="#b43">(Yamada et al., 2016)</ref>, we use the cosine similarity between v e and z word as a feature. Further, as in past entity linking studies, we also use the commonness of the name referring to the entity. Moreover, we derive a representation based both on entities and words by simply adding z entity and z word 1 :</p><formula xml:id="formula_3">z f ull = z entity + z word .<label>(4)</label></formula><p>We then solve the task using a multiclass logistic regression classifier with the computed representation (i.e., with z entity or z f ull ) as features. In the remainder of this paper, we denote our models based on z entity and z f ull by NABoE-entity and NABoE-full, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>In this section, we describe our experimental setup used both in the text classification and the factoid question answering experiments presented below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Entity Detection</head><p>As the target KB, we used the September 2018 version of Wikipedia, which contains a total of 7,333,679 entities. 2 Regarding the entity dictionary described in Section 2.1, we excluded an entity name if its link probability was lower than 1% and a referent entity if its commonness given the entity name was lower than 3% for computational efficiency. Entity names were treated as case-insensitive. As a result, the dictionary contained 18,785,550 entity names, and each name had 1.14 referent entities on average. Furthermore, to detect entities from a document, we also tested two publicly available entity linking systems, Wikifier <ref type="bibr" target="#b36">(Ratinov et al., 2011;</ref><ref type="bibr" target="#b1">Cheng and Roth, 2013)</ref> and TAGME <ref type="bibr" target="#b8">(Ferragina and Scaiella, 2012)</ref>, instead of using dictionarybased entity detection. <ref type="bibr">3</ref> We selected these systems because they are capable of detecting non-named entities (e.g., technical terms) that are useful for addressing the text classification task. 4 Here, we used the entities detected and disambiguated by these systems as inputs to our neural network model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pretrained Embeddings</head><p>We initialized the embeddings of words (v w ) and entities (v e ) using pretrained embeddings trained on KB. To learn embeddings from the KB, we used the method adopted in the open source Wikipedia2Vec tool <ref type="bibr" target="#b43">(Yamada et al., 2016</ref><ref type="bibr" target="#b42">(Yamada et al., , 2018a</ref>. In particular, we generated an entity-annotated corpus from Wikipedia by treating entity links in Wikipedia articles as entity annotations, and trained skip-gram embeddings <ref type="bibr">(Mikolov et al., 2013a,b)</ref> of 300 dimensions with negative sampling using the generated corpus as inputs. The learned embeddings place similar words and entities close to one another in a unified vector space. Here, we used the same version of Wikipedia described in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Text Classification</head><p>To evaluate the effectiveness of our proposed model, we first conducted the text classification task on two standard datasets, namely the 20 Newsgroups (20NG) <ref type="bibr" target="#b24">(Lang, 1995)</ref> and R8 datasets <ref type="bibr" target="#b4">(Debole and Sebastiani, 2005)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>Our experimental setup described in this section follows that in past work <ref type="bibr" target="#b19">Jin et al., 2016;</ref><ref type="bibr" target="#b45">Yamada et al., 2018b)</ref>. In particular, we used the 20NG and R8 datasets to train and test the proposed model. The 20NG dataset was created using the documents obtained from 20 Newsgroups and contained 11,314 training documents and 7,532 test documents. 5 The R8 dataset consisted of news documents from the eight most popular classes of the Reuters-21578 corpus <ref type="bibr" target="#b26">(Lewis, 1992)</ref> and comprised 5,485 training documents and 2,189 test documents. We created the development set for each dataset by selecting 5% of the documents for training. Note that the class distribution of the R8 dataset is highly imbalanced. For example, the number of documents in the largest and smallest classes is 3,923 documents and 51 documents, respectively.</p><p>We report the accuracy and macro-average F1 scores. The model was trained using mini-batch stochastic gradient descent (SGD) with its batch size set to 32 and its learning rate controlled by Adam <ref type="bibr" target="#b22">(Kingma and Ba, 2014)</ref>. We used words and entities that were detected three times or more in the dataset and ignored the other words and entities. The size of the embeddings of words and entities was set to d = 300. We used early stopping based on the accuracy of the development set of each dataset to avoid overfitting of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We used the following models as our baselines: The performance of this model was superior to that of many state-of-the-art models, including those based on the skip-gram and CBOW models <ref type="bibr" target="#b30">(Mikolov et al., 2013b)</ref>, and the paragraph vector model (Le and <ref type="bibr" target="#b25">Mikolov, 2014)</ref>.</p><p>‚Ä¢ SWEM-concat <ref type="bibr" target="#b37">(Shen et al., 2018)</ref>: This model is based on a neural network model with simple pooling operations (i.e., average and max pooling) over pretrained word embeddings. 6 Despite its simplicity, it outperformed many neural network-based models such as the word-based CNN model <ref type="bibr" target="#b21">(Kim, 2014)</ref> and RNN model with LSTM units <ref type="bibr" target="#b37">(Shen et al., 2018)</ref>.</p><p>‚Ä¢ TextEnt <ref type="bibr" target="#b45">(Yamada et al., 2018b)</ref>: This model learns entity-aware document embeddings from Wikipedia, and uses a neural network model with the learned embeddings as pretrained parameters to address text classification.</p><p>As described in Section 2.1, we also tested the variants of our NABoE-entity and NABoEfull models for which Wikifier and TAGME were used as the entity detection methods. <ref type="table">Table 1</ref> shows the results of our models and those of our baselines. Here, w/o att. and w/o emb. signify the model without the neural attention mechanism (all attention weights a e are set to 1 K , where K is the number of entities in the document) and the model without the pretrained embeddings (the embeddings are initialized randomly), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>Relative to the baselines, our models yielded enhanced overall performance on both datasets. The NABoE-full model outperformed all baseline models in terms of both measures on both datasets. Furthermore, the NABoE-entity model outperformed all the baseline models in terms of both measures on the 20NG dataset, and the F1 score on the R8 dataset. Moreover, our attention mechanism consistently improved the performance. These results clearly highlighted the effectiveness of our approach, which addresses text classification by using a small number of unambiguous and relevant entities detected by the proposed attention mechanism. Moreover, the pretrained embeddings improved the performance on both datasets. Further, the models based on the dictionarybased entity detection (see Section 2.1) generally outperformed the models based on the entity linking systems (i.e., Wikifier and TAGME). We consider that this is because these entity linking systems failed to detect or disambiguate entity names that were useful to address the text classification task. Moreover, our attention mechanism consistently improved the performance for Wikifierand TAGME-based models because the attention mechanism enabled the model to focus on entities that were relevant to the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>In this section, we provide a detailed analysis of the performance of our model in terms of conducting the text classification task. We first provide a comparison of the SWEM-concat, NABoEentity, and NABoE-full models using class-level F1 scores on both of the datasets (see <ref type="table" target="#tab_2">Table 2</ref>). Here, we aim to compare the detailed performance of the word-based model (SWEM-concat), entitybased model (NABoE-entity), and the model based on both words and entities (NABoE-full). Compared with the SWEM-concat model, the NABoE-full and NABoE-entity models performed   more accurately in 23 out of 28 and 17 out of 28 classes, respectively. This result clearly demonstrates the ability of the model to successfully capture strong semantic signals that can only be obtained from entities. Moreover, we observed that the NABoE-entity model achieved weaker performance especially for the misc.forsale class in the 20NG dataset and several classes in the R8 dataset. Regarding the misc.forsale class, because documents in this class contain a wider variety of entities (i.e., objects users want to sell) than other classes, the model failed to capture the effective semantic signals from the entities. Further, as described in the error analysis provided below, it often appeared to be difficult to distinguish pairs of Class  similar classes in the R8 dataset based only on entities.</p><p>Next, we conducted a feature study of the attention mechanism by excluding one feature at a time from the NABoE-entity model <ref type="table" target="#tab_3">(Table 3)</ref>. We found both of the features to make an important contribution to the performance. Furthermore, to investigate the attention mechanism in more detail, we computed the top influential entities in the attention mechanism for each class on the 20NG and R8 datasets. In particular, we calculated the number of times each entity obtained the highest attention weight in the test documents in each class and selected the five most frequent ones. <ref type="table" target="#tab_5">Table 4</ref> presents the results. Overall, our attention mechanism successfully selected entities that were highly relevant to each class. For example, Cryptography, Algorithm, Escrow, Considered harmful, and Encryption were selected for the sci.crypt class. Furthermore, although we did not explicitly perform entity disambiguation, the model successfully overcame the ambiguity issues in the entity names and attended to the entities that were relevant to the classes.</p><p>Subsequently, we conducted an error analysis by selecting 50 random test documents for which the NABoE-entity model made wrong predictions. Most of the errors were caused by two pairs of classes: 22 errors were caused by misclassifying documents of acq (corporate acquisitions) and those of earn (corporate earnings), and 13 errors were caused by misclassifying documents of interest and those of money-fx. Furthermore, the model tended to perform poorly if a document contained entities that strongly indicate an incorrect class. For example, a money-fx document containing the entity interest rate multiple times was classified into the interest class, and a document in the acq class reporting news related to oil companies (i.e., ExxonMobil and ZENEX) was classified into the crude class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Factoid Question Answering</head><p>In this section, we address factoid question answering based on a dataset consisting of questions of the quiz bowl trivia quiz game. Factoid ques-tion answering is one of the common settings of question answering that aims to predict an entity (e.g., events, authors, and books) that is described in a given question. The players of quiz bowl solve questions consisting of sentences that describe an entity. Quiz bowl questions have frequently been used for evaluating neural network-based models in recent studies <ref type="bibr" target="#b17">(Iyyer et al., 2014</ref><ref type="bibr" target="#b18">(Iyyer et al., , 2015</ref>.</p><p>This task has a significantly larger number of target classes compared to the task addressed in the previous experiment. Our main aim here is to evaluate the effectiveness of using entities to capture the finer-grained semantics required to perform the task of factoid question answering effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Setup</head><p>Our experimental setup described in this section follows that in past work <ref type="bibr" target="#b41">(Xu and Li, 2016;</ref>. We address this task as a text classification problem that selects the most relevant answer from the possible answers observed in the dataset. We obtained the dataset proposed in <ref type="bibr">Iyyer et al. (2014) 7</ref> . We only used questions in the history and literature categories. Furthermore, we excluded questions of which the answers appear fewer than six times in the dataset. As a result, the number of candidate answers was 303 and 424 in the history and literature categories, respectively. We used 20% of questions each for the development set and test sets, and the remaining 60% for the training set. As a result, the training, development, and test sets consisted of 1,535, 511, and 511 questions for the history category, and 2,524, 840, and 840 questions for the literature category.</p><p>The settings we used to train the model were the same as those in the previous experiment (see Section 4.1). The model was trained using mini-batch SGD with its learning rate controlled by Adam <ref type="bibr" target="#b22">(Kingma and Ba, 2014)</ref> and its mini-batch size set to 32. We used words and entities that were detected three times or more in the dataset, and ignored the other words and entities. The size of the embeddings of words and entities was set to d = 300. As in past work, we report the accuracy score, and the score on the development set was used for early stopping.  <ref type="table">Table 5</ref>: Accuracy of the proposed and baseline methods for the factoid QA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Baselines</head><p>We used the following baseline models:</p><p>‚Ä¢ BoW <ref type="bibr" target="#b41">(Xu and Li, 2016)</ref> This model is based on a logistic regression classifier with conventional binary BoW features.</p><p>‚Ä¢ FTS-BRNN <ref type="bibr" target="#b41">(Xu and Li, 2016)</ref> This model is based on a bidirectional RNN with gated recurrent units (GRU). It uses the logistic regression classifier with the features derived by the RNN.</p><p>‚Ä¢ NTEE  This model is a state-of-the-art model that uses a multi-layer perceptron classifier with the features computed using the embeddings of words and entities trained on Wikipedia using the neural network model proposed in their paper.</p><p>Similar to our previous experiment, we also add SWEM-concat, and the variants of our NABoEentity and NABoE-full models based on Wikifier and TAGME (see Section 4.2). Note that all the baselines address the task as a text classification problem. <ref type="table">Table 5</ref> provides the results of our models and those of our baselines. Overall, our models achieved enhanced performance on this task. In particular, the NABoE-full model successfully outperformed all the baseline models, and the NABoE-entity model achieved competitive performance and outperformed all the baseline models in the literature category. These results clearly highlighted the effectiveness of our model for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and Analysis</head><p>Furthermore, similar to the previous text classification experiment, the attention mechanism and the pretrained embeddings consistently improved the performance. Moreover, the models based on dictionary-based entity detection outperformed the models based on the entity linking systems.</p><p>We also conducted an error analysis using the NABoE-entity model and the test questions in the history category. We found nearly 70% of the errors to be caused by questions of which the answers were country names. This is because these questions tended to provide indirect clues (e.g., describing a notable person born in the country) and most entities used in these clues do not directly indicate the answer (i.e., country names). Furthermore, our model failed in difficult cases such as predicting Tokugawa shogunate instead of Tokugawa Ieyasu.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related Work</head><p>KB entities have been conventionally used to model the semantics in texts. A representative example is Explicit Semantic Analysis (ESA) <ref type="bibr">Markovitch, 2006, 2007)</ref>, which represents a document using a bag of entities, namely a sparse vector of which each dimension corresponds to the relevance score of the text to each entity. This simple method is shown to be effective for various NLP tasks including text classification <ref type="bibr" target="#b9">(Gabrilovich and Markovitch, 2006;</ref><ref type="bibr" target="#b14">Gupta and Ratinov, 2008;</ref><ref type="bibr" target="#b32">Negi and Rosner, 2013)</ref> and information retrieval <ref type="bibr" target="#b6">(Egozi et al., 2011;</ref><ref type="bibr" target="#b40">Xiong et al., 2016)</ref>, Several neural network models that use KB entities to capture the semantics in texts have been proposed. These models typically depend on an additional preprocessing step that extracts the relevant entities from the target texts. For example, <ref type="bibr" target="#b39">Wang et al. (2017)</ref> used the Probase conceptualization API for short text classification by retrieving the Probase entities that were relevant to the target text and used them in a model based on <ref type="bibr">CNN. Pilehvar et al. (2017)</ref> also extracted entities using a graph-based linking algorithm and used these entities in a neural network model. A similar approach was adopted in <ref type="bibr">Yamada et al. (2018b,c)</ref>; they extracted entities from the target text using an entity linking system and simply used the detected entities in a neural network model. However, un-like these models, our proposed model addresses the task in an end-to-end manner; i.e., entities that are relevant to the target text are automatically selected using our neural attention mechanism. Furthermore, we also used the model proposed by <ref type="bibr" target="#b45">Yamada et al. (2018b)</ref> as a baseline in our text classification experiments.</p><p>Additionally, our work is also related to studies on entity linking. Entity linking models can be roughly classified into two groups: local models, which resolve entity names independently using the contextual relevance of the entity given a document, and global models, in which all the entity names in a document are resolved simultaneously to select a topically coherent set of results <ref type="bibr" target="#b36">(Ratinov et al., 2011)</ref>. Recent state-of-the-art models typically combine both of these models <ref type="bibr" target="#b43">(Yamada et al., 2016;</ref><ref type="bibr" target="#b12">Ganea and Hofmann, 2017;</ref><ref type="bibr" target="#b0">Cao et al., 2018;</ref><ref type="bibr" target="#b23">Kolitsas et al., 2018)</ref>. However, several studies also showed that the local model alone can achieve results competitive to those of the global and combined models <ref type="bibr" target="#b7">(Eshel et al., 2017;</ref><ref type="bibr" target="#b12">Ganea and Hofmann, 2017;</ref><ref type="bibr" target="#b0">Cao et al., 2018;</ref><ref type="bibr" target="#b23">Kolitsas et al., 2018)</ref>. In this study, we adopt a simple but effective local model, which uses cosine similarity between the embedding of the target entity and the word-based representation of the document to capture the relevance of an entity given a document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This study proposed NABoE, which is a neural network model that performs text classification using entities in Wikipedia. We combined simple dictionary-based entity detection with a neural attention mechanism to enable the model to focus on a small number of unambiguous and relevant entities in a document. We achieved state-of-theart results on two important NLP tasks, namely text classification and factoid question answering, which clearly verified the effectiveness of our approach. As a future task, we intend to more extensively analyze our model and explore its effectiveness for other NLP tasks. Furthermore, we would also like to test more expressive neural network models for example by integrating global entity coherence information into our neural attention mechanism.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Architecture of the NABoE-entity model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>‚Ä¢</head><label></label><figDesc>BoW-SVM<ref type="bibr" target="#b19">(Jin et al., 2016)</ref>: This model is based on a conventional linear support vector machine (SVM) with bag of words (BoW) features. It outperformed the conventional na√Øve Bayes-based model.‚Ä¢ BoE (Jin et al., 2016): This model extends the skip-gram model; It learns different word embeddings per target class from the dataset, and a linear model based on learned word embeddings is used to classify the documents.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>20NG R8 Acc. F1 Acc. F1 NABoE-entity .863 .856 .962 .915 NABoE-entity w/o att. .822 .817 .943 .869 NABoE-entity w/o emb. .844 .838 .957 .892 NABoE-full .868 .862 .971 .917 Wikifier (NABoE-entity) .735 .729 .896 .803 Wikifier (NABoE-entity w/o att.) .728 .723 .844 .782 Wikifier (NABoE-entity w/o emb.) .727 .722 .861 .755</figDesc><table><row><cell>Wikifier (NABoE-full)</cell><cell>.797 .789 .953 .839</cell></row><row><cell>TAGME (NABoE-entity)</cell><cell>.844 .838 .942 .871</cell></row><row><cell cols="2">TAGME (NABoE-entity w/o att.) .826 .821 .924 .857</cell></row><row><cell cols="2">TAGME (NABoE-entity w/o emb.) .842 .836 .942 .865</cell></row><row><cell>TAGME (NABoE-full)</cell><cell>.860 .853 .958 .889</cell></row><row><cell>BoW-SVM</cell><cell>.790 .783 .947 .851</cell></row><row><cell>BoE</cell><cell>.831 .827 .965 .886</cell></row><row><cell>SWEM-concat</cell><cell>.853 .855 .967 .898</cell></row><row><cell>TextEnt</cell><cell>.845 .839 .967 .910</cell></row><row><cell cols="2">Table 1: Results of the text classification task on the</cell></row><row><cell cols="2">20NG and R8 datasets. Here, w/o att. and w/o emb.</cell></row><row><cell cols="2">represent the model without the neural attention mech-</cell></row><row><cell cols="2">anism and the model without the pretrained embed-</cell></row><row><cell>dings, respectively.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Class-level F1 scores in each class on the 20NG and R8 datasets.</figDesc><table><row><cell></cell><cell>20NG</cell><cell>R8</cell></row><row><cell></cell><cell cols="2">Acc. F1 Acc. F1</cell></row><row><cell cols="3">Commonness only .849 .843 .949 .894</cell></row><row><cell>Cosine only</cell><cell cols="2">.846 .840 .956 .898</cell></row><row><cell>Both</cell><cell cols="2">.863 .856 .962 .915</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Feature study of the neural attention mecha- nism of the NABoE-entity model.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Ray tracing (graphics), Framebuffer, Image file formats, TIFF comp.os.ms-windows.misc Windows 3.1x, Microsoft Windows, Windows NT, CONFIG.SYS, BMP file format comp.sys.ibm.pc.hardware BIOS, Don't Copy That Floppy, SCSI host adapter, Nonvolatile BIOS memory, Parallel SCSI comp.sys.mac.hardware PowerBook, Macintosh Quadra 610, Macintosh Quadra 650, FirstClass, Macintosh SE/30 comp.windows.x X-Perts, Xterm, OPEN LOOK, OpenWindows, Man page</figDesc><table><row><cell></cell><cell>Top entities</cell></row><row><cell>20NG:</cell><cell></cell></row><row><cell>alt.atheism</cell><cell>Christian ethics, Atheism, Moral agency, Gregg Jaeger, Fred Rice</cell></row><row><cell cols="2">comp.graphics Algorithm, misc.forsale Freight transport, Make Me an Offer, AC adapter, Plaque reduction neutralization test, Outline</cell></row><row><cell></cell><cell>of working time and conditions</cell></row><row><cell>rec.autos</cell><cell>Manual Shift, Chassis, Automotive industry, Nissan, Ford Probe</cell></row><row><cell>rec.motorcycles</cell><cell>United States Department of Defense, Motorcycle, ZX8302, Honda motorcycles, Pillion, Hawk</cell></row><row><cell></cell><cell>GT</cell></row><row><cell>rec.sport.baseball</cell><cell>Pitcher, Inning, The Jays, Home run, Bullpen</cell></row><row><cell>rec.sport.hockey</cell><cell>National Hockey League, Goaltender, ESPN, The Penguins, Achkar</cell></row><row><cell>sci.crypt</cell><cell>Cryptography, Algorithm, Escrow, Considered harmful, Encryption</cell></row><row><cell>sci.electronics</cell><cell>Solvent, Copy protection, Electronics, Leadacid battery, Printed circuit board</cell></row><row><cell>sci.med</cell><cell>Infection, Antibiotics, Kirlian photography, Allergy, Kirlian</cell></row><row><cell>sci.space</cell><cell>Spacecraft, SunOS, Vandalism, VIA International, Space station</cell></row><row><cell>soc.religion.christian</cell><cell>Rutgers University, Geneva, Byler, Immaculate Conception, Original sin</cell></row><row><cell>talk.politics.guns</cell><cell>Ranch, BD's Mongolian Grill, Firearm, Second Amendment to the United States Constitution,</cell></row><row><cell></cell><cell>Feustel</cell></row><row><cell>talk.politics.mideast</cell><cell>Serdar Argic, Israelis, Palestinians, Palestine Liberation Organization, Arabs</cell></row><row><cell>talk.politics.misc</cell><cell>Clayton Cramer, Janet Reno, Police state, Ronzone, Federal Bureau of Investigation</cell></row><row><cell>talk.religion.misc</cell><cell>Christian ethics, Thomas George Lanphier, David Koresh, Albert Sabin, Josephus</cell></row><row><cell>R8:</cell><cell></cell></row><row><cell>grain</cell><cell>Grain, Tonne, Price support, Oil reserves, United States Senate</cell></row><row><cell>ship</cell><cell>Freight transport, Shipbuilding, Flag of convenience, Cargo, Persian Gulf</cell></row><row><cell>trade</cell><cell>Balance of trade, Export, International trade, Economic sanctions, Import</cell></row><row><cell>interest</cell><cell>Interest rate, Prime rate, Repurchase agreement, Balance of trade, Money market</cell></row><row><cell>money-fx</cell><cell>Exchange rate, Currency, Money market, Foreign exchange market, Monetary policy</cell></row><row><cell>crude</cell><cell>Petroleum, West Texas Intermediate, Price of oil, OPEC, Oil platform</cell></row><row><cell>acq</cell><cell>Common stock, Tender offer, Privately held company, Preferred stock, Shares outstanding</cell></row><row><cell>earn</cell><cell>QTR, Dividend, Stock split, Net profit, Income fund</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Top five influential entities for each class of the NABoE-entity model in the 20NG and R8 datasets.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We also tested concatenating zentity and z word to derive z f ull ; however, adding them generally achieved enhanced performance in our experiments presented below.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We downloaded the Wikipedia dump from Wikimedia Downloads: https://dumps.wikimedia.org/ 3 In our experiments, we simply used all entities detected by the entity linking systems.4  In our preliminary experiments, we also tested three other state-of-the-art entity linking systems: AIDA (Hoffart et al., 2011), WAT<ref type="bibr" target="#b34">(Piccinno and Ferragina, 2014)</ref>, and the commercial Entity Analysis API in Google's Cloud Language service. However, these systems achieved lower overall performance compared to Wikifier and TAGME because they tended to ignore non-named entities.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We used the by-date version downloaded from the author's web site:http://qwone.com/Àújason/ 20Newsgroups/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">We also tested all four models proposed in<ref type="bibr" target="#b37">Shen et al. (2018)</ref> (i.e., SWEM-aver, SWEM-max, SWEM-concat, and SWEM-hier). These models generally delivered comparable performance, with SWEM-concat slightly outperforming the other models on average.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">This dataset was downloaded from the authors' web page: https://cs.umd.edu/oemiyyer/qblearn/.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural Collective Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="675" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Relational Inference for Wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1787" to="1796" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Framework for Benchmarking Entity-annotation Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cornolti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Ciaramita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on World Wide Web</title>
		<meeting>the 22nd International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="249" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semi-supervised Sequence Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="3079" to="3087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An Analysis of the Relative Hardness of Reuters-21578 Subsets: Research Articles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franca</forename><surname>Debole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrizio</forename><surname>Sebastiani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science and Technology</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="584" to="596" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A New Entity Salience Task with Millions of Training Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Dunietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gillick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="205" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Concept-Based Information Retrieval Using Explicit Semantic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ofer</forename><surname>Egozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Named Entity Disambiguation for Noisy Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yotam</forename><surname>Eshel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Conference on Computational Natural Language Learning</title>
		<meeting>the 21st Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="58" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fast and Accurate Annotation of Short Texts with Wikipedia Pages. Software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Scaiella</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IEEE</publisher>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="70" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Overcoming the Brittleness Bottleneck using Wikipedia: Enhancing Text Categorization with Encyclopedic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st National Conference on Artificial Intelligence</title>
		<meeting>the 21st National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1301" to="1306" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Computing Semantic Relatedness Using Wikipedia-Based Explicit Semantic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identifying Salient Entities in Web Pages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gamon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae</forename><surname>Yano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinying</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename><surname>Apacible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 22nd ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2375" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Joint Entity Disambiguation with Local Neural Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Octavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2619" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">To Link or Not to Link? A Study on Endto-End Tweet Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emre</forename><surname>Kiciman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1020" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text Categorization with Knowledge Transfer from Heterogeneous Data Sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd National Conference on Artificial Intelligence</title>
		<meeting>the 23rd National Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="842" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting Entity Linking in Queries for Entity Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faegheh</forename><surname>Hasibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krisztian</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svein</forename><forename type="middle">Erik</forename><surname>Bratsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval</title>
		<meeting>the 2016 ACM International Conference on the Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="209" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust Disambiguation of Named Entities in Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Amir</forename><surname>Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilaria</forename><surname>Bordino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hagen</forename><surname>F√ºrstenau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Spaniol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilyana</forename><surname>Taneva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Thater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="782" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A Neural Network for Factoid Question Answering over Paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Claudino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="633" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep Unordered Composition Rivals Syntactic Methods for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum√©</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1681" to="1691" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bag-of-embeddings for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqing</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Fifth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2824" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bag of Tricks for Efficient Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Conference of the European Chapter</title>
		<meeting>the 15th Conference of the European Chapter<address><addrLine>Short Papers</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="427" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1746" to="1751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-End Neural Entity Linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Kolitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Octavian-Eugen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="519" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">NewsWeeder: Learning to Filter Netnews</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Lang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th International Conference on Machine Learning</title>
		<meeting>the 12th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="331" to="339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed Representations of Sentences and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th International Conference on Machine Learning</title>
		<meeting>the 31th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An Evaluation of Phrasal and Clustered Representations on a Text Categorization Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="37" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Tat-Seng Chua, and Maosong Sun</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2418" to="2424" />
		</imprint>
	</monogr>
	<note>Topical Word Embeddings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Wikify!: Linking Documents to Encyclopedic Knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Csomai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixteenth ACM Conference on Information and Knowledge Management</title>
		<meeting>the Sixteenth ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="233" to="242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Efficient Estimation of Word Representations in Vector Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Conference on Learning Representations</title>
		<meeting>the 2013 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to Link with Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 17th ACM Conference on Information and Knowledge Management</title>
		<meeting>eeding of the 17th ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">UoM: Using Explicit Semantic Analysis for Classifying Sentiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sapna</forename><surname>Negi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rosner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Workshop on Semantic Evaluation</title>
		<meeting>the Seventh International Workshop on Semantic Evaluation</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="535" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">News Citation Recommendation with Implicit and Explicit Semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="388" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">From TagME to WAT: A New Entity Annotator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Piccinno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Ferragina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Workshop on Entity Recognition and Disambiguation</title>
		<meeting>the First International Workshop on Entity Recognition and Disambiguation</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards a Seamless Integration of Word Senses into Downstream NLP Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Mohammad Taher Pilehvar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nigel</forename><surname>Navigli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1857" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Local and Global Algorithms for Disambiguation to Wikipedia</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doug</forename><surname>Downey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1375" to="1384" />
		</imprint>
	</monogr>
	<note>and Mike Anderson</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Baseline Needs More Love: On Simple Word-Embedding-Based Models and Associated Pooling Mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenlin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinliang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="440" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Document Modeling with Gated Recurrent Neural Network for Sentiment Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1422" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Combining Knowledge with Deep Convolutional Neural Networks for Short Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2915" to="2921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Bag-of-Entities Representation for Ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM International Conference on the Theory of Information Retrieval</title>
		<meeting>the 2016 ACM International Conference on the Theory of Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="181" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Full-Time Supervision based Bidirectional RNN for Factoid Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wu-Jun</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05854v2</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.06280v2</idno>
		<title level="m">Wikipedia2Vec: An Optimized Tool for Learning Embeddings from Wikipedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>the 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Distributed Representations of Texts and Entities from Knowledge Base</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="397" to="411" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Representation Learning of Entities and Documents from Knowledge Base Descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="190" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Studio Ousia&apos;s Quiz Bowl Question Answering System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryuji</forename><surname>Tamaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The NIPS &apos;17 Competition: Building Intelligent Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="181" to="194" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
