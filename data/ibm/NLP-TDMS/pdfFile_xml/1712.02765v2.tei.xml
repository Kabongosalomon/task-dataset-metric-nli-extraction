<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Bulat</surname></persName>
							<email>adrian.bulat@nottingham.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">The University of Nottingham</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Tzimiropoulos</surname></persName>
							<email>yorgos.tzimiropoulos@nottingham.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Laboratory</orgName>
								<orgName type="institution">The University of Nottingham</orgName>
								<address>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Super-FAN: Integrated facial landmark localization and super-resolution of real-world low resolution faces in arbitrary poses with GANs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Input SRGAN Ours Figure 1: A few examples of visual results produced by our system on real-world low resolution faces from WiderFace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>This paper addresses 2 challenging tasks: improving the quality of low resolution facial images and accurately locating the facial landmarks on such poor resolution images. To this end, we make the following 5 contributions: (a) we propose Super-FAN: the very first end-to-end system that addresses both tasks simultaneously, i.e. both improves face resolution and detects the facial landmarks. The novelty or Super-FAN lies in incorporating structural information in a GAN-based super-resolution algorithm via integrating a sub-network for face alignment through heatmap regression and optimizing a novel heatmap loss. (b) We illustrate the benefit of training the two networks jointly by reporting good results not only on frontal images (as in prior work) but on the whole spectrum of facial poses, and not only on synthetic low resolution images (as in prior work) but also on real-world images. (c) We improve upon the state-of-the-art in face super-resolution by proposing a new residual-based architecture. (d) Quantitatively, we show large improvement over the state-of-the-art for both face super-resolution and alignment. (e) Qualitatively, we show for the first time good results on real-world low resolution images like the ones of <ref type="figure">Fig. 1.</ref> </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The aim of this paper is to improve upon the quality and understanding of very low resolution facial images. This is important in many applications, like face editing surveillance/security. In terms of quality, our aim is to increase the resolution and recover the details of real-world low resolution facial images like the ones shown in the first row of <ref type="figure">Fig. 1</ref>; this task is also known as face super-resolution (when the input resolution is too small this task is sometimes called face hallucination). In terms of understanding, we wish to extract mid-and high-level facial information by localizing a set a predefined facial landmarks with semantic meaning like the tip of the nose, the corners of the eyes etc.; this task is also known as face alignment.</p><p>Attempting to address both tasks simultaneously is really a chicken-and-egg problem: On one hand, being able to detect the facial landmarks has already been shown beneficial for face super-resolution <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b29">30]</ref>; however how to accomplish this for low resolution faces in arbitrary poses is still an open problem <ref type="bibr" target="#b3">[4]</ref>. On the other hand, if one could effectively super-resolve low quality and low resolution faces across the whole spectrum of facial poses, then facial landmarks can be localized with high accuracy.</p><p>Because it is difficult to detect landmarks in very low resolution faces (as noticed in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> and validated in this work), prior super-resolution methods based on this idea <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b29">30]</ref> produce blurry images with artifacts when the facial landmarks are poorly localized. Our main contribution is to show that actually one can jointly perform facial landmark localization and super-resolution even for very low resolution faces in completely arbitrary poses (e.g. profile images, see also Figs. 1 and 5). In summary, our contributions are: 1. We propose Super-FAN: the very first end-to-end system that addresses face super-resolution and alignment simultaneously, via integrating a sub-network for facial landmark localization through heatmap regression into a GAN-based super-resolution network, and incorporating a novel heatmap loss. See also <ref type="figure" target="#fig_0">Fig. 2</ref>. 2. We show the benefit of training the two networks jointly on both synthetically generated and real-world lowresolution faces of arbitrary facial poses. 3. We also propose an improved residual-based architecture for super-resolution. 4. Quantitatively, we report, for the first time, results across the whole spectrum of facial poses on the LS3D-W dataset <ref type="bibr" target="#b3">[4]</ref>, and show large improvement over the stateof-the-art on both super-resolution and face alignment. 5. Qualitatively, we show, for the first time, good visual results on real-world low resolution facial images taken from the WiderFace dataset <ref type="bibr" target="#b30">[31]</ref> (see Figs. 1 and 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Closely related work</head><p>This section reviews related work in image and face super-resolution, and facial landmark localization.</p><p>Image super-resolution. Early attempts on superresolution using CNNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b15">16]</ref> used standard L p losses for training which result in blurry super-resolved images. To alleviate this, rather than using an MSE over pixels (between the super-resolved and the ground truth HR image), the authors of <ref type="bibr" target="#b14">[15]</ref> proposed an MSE over feature maps, coined perceptual loss. Notably, we also use a perceptual loss in our method. More recently, in <ref type="bibr" target="#b19">[20]</ref>, the authors presented a GAN-based <ref type="bibr" target="#b6">[7]</ref> approach which uses a discriminator to differentiate between the super-resolved and the original HR images and the perceptual loss. In <ref type="bibr" target="#b25">[26]</ref>, a patch-based texture loss is proposed to improve reconstruction quality.</p><p>Notice that all the aforementioned image superresolution methods can be applied to all types of images and hence do not incorporate face-specific information, as proposed in our work. Also, in most cases, the aim is to produce high-fidelity images given an image which is already of good resolution (usually 128 × 128) while face superresolution methods typically report results on very low resolution faces <ref type="bibr">(16 × 16 or 32 × 32)</ref>.</p><p>From all the above mentioned methods, our work is more closely related to <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b19">[20]</ref>. In particular, one of our contributions is to describe an improved GAN-based architecture for super-resolution, which we used as a strong baseline on top of which we built our integrated face superresolution and alignment network.</p><p>Face super-resolution. The recent work of <ref type="bibr" target="#b31">[32]</ref> uses a GAN-based approach (like the one of <ref type="bibr" target="#b19">[20]</ref> without the perceptual loss) to super-resolve very low-resolution faces. The method was shown to work well for frontal and prealigned faces taken from the CelebA dataset <ref type="bibr" target="#b20">[21]</ref>. In <ref type="bibr" target="#b32">[33]</ref>, the same authors proposed a two-step decoder-encoderdecoder architecture which incorporates a spatial transformer network to undo translation, scale and rotation misalignments. Their method was tested on pre-aligned, synthetically generated LR images from the frontal dataset of CelebA <ref type="bibr" target="#b20">[21]</ref>. Notably, our network does not try to undo misalignment but simply learns how to super-resolve, respecting at the same time the structure of the human face by integrating a landmark localization sub-network.</p><p>The closest work to our method is <ref type="bibr" target="#b33">[34]</ref> which performs face super-resolution and dense facial correspondence in an alternating manner. Their algorithm was tested on the frontal faces of PubFig <ref type="bibr" target="#b17">[18]</ref> and Helen <ref type="bibr" target="#b18">[19]</ref> while few results on real images (4 in total) were also shown with less success. The main difference with our work is that, in <ref type="bibr" target="#b33">[34]</ref>, the dense correspondence algorithm is not based on neural networks, but on cascaded regression, is pre-learned disjointly from the super-resolution network and remains fixed. As such, <ref type="bibr" target="#b33">[34]</ref> suffers from the same problem of having to detect landmarks on blurry faces which is particularly evident for the first iterations of the algorithm. On the contrary, we propose learning both super-resolution and facial landmark localization jointly in an end-to-end fashion, and use just one shot to jointly super-resolve the image and localize the facial landmarks. See <ref type="figure" target="#fig_0">Fig. 2</ref>. As we show, this results in large performance improvement and generates images of high fidelity across the whole spectrum of facial poses.</p><p>It is worth noting that we go beyond the state-of-theart and rigorously evaluate super-resolution and facial landmark localization across facial pose both quantitatively and qualitatively. As opposed to prior work which primarily uses frontal datasets <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b29">30]</ref> (e.g. CelebA, Helen, LFW, BioID) to report results, the low resolution images in our experiments were generated using the newly created LS3D-W balanced dataset <ref type="bibr" target="#b3">[4]</ref> which contains an even number of facial images per facial pose. We also report qualitatively results on more than 200 real-world low resolution facial images taken from the WiderFace dataset <ref type="bibr" target="#b30">[31]</ref>. To our knowledge, this is the most comprehensive evaluation of face super-resolution algorithms on real images.</p><p>Face alignment. A recent evaluation of face alignment <ref type="bibr" target="#b3">[4]</ref> has shown that when resolution drops down to 30 pixels, the performance drop of a state-of-the-art network trained on standard facial resolution (192 × 192) for medium and large poses is more than 15% and 30%, respectively. This result is one of the main motivations behind our work. As our aim is not to propose a new architecture for face alignment, we employed the Face Alignment Network (FAN) of <ref type="bibr" target="#b3">[4]</ref>, built by combining the Hourglass network of <ref type="bibr" target="#b21">[22]</ref> with the residual block of <ref type="bibr" target="#b2">[3]</ref>. As shown in <ref type="bibr" target="#b3">[4]</ref>, FAN provides excellent performance across the whole spectrum of facial poses for good resolution images. As we show in this paper, a FAN specifically trained to localize the landmarks in low resolution images performs poorly. One of our contributions is to show that a FAN when integrated and jointly trained with a super-resolution network can localize facial landmarks in low resolution images with high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Datasets</head><p>To systematically evaluate face super-resolution across pose, we constructed a training dataset from 300W-LP <ref type="bibr" target="#b34">[35]</ref>, AFLW <ref type="bibr" target="#b16">[17]</ref>, Celeb-A <ref type="bibr" target="#b20">[21]</ref> and a portion of LS3D-W balanced <ref type="bibr" target="#b3">[4]</ref>. For testing, we used the remaining images from LS3D-W balanced, in which each pose range</p><formula xml:id="formula_0">([0 o − 30 o ], [30 o − 60 o ], [60 o − 90 o ]) is equally represented.</formula><p>300W-LP is a synthetically expanded dataset obtained by artificially rendering the faces from 300W <ref type="bibr" target="#b24">[25]</ref> into large poses (−90 0 to 90 0 ). While the dataset contains 61,225 images, there are only about 3,000 unique faces. Also, the images are affected by artifacts caused by the warping procedure. We included the entire dataset in our training set.</p><p>AFLW is a large-scale face alignment dataset that contains faces in various poses and expressions collected from Flickr. All 25,993 faces were included in our training set.</p><p>Celeb-A is a large-scale facial attribute dataset containing 10,177 unique identities and 202,599 facial images in total. Most of the images are occlusion-free and in frontal or near-frontal poses. To avoid biasing the training set towards frontal poses, we only used a randomly selected subset of approx. 20,000 faces. WiderFace is a face detection dataset containing 32,203 images with faces that exhibit a high degree of variability in pose, occlusion and quality. In order to assess the performance of our super-resolution method on in-the-wild, realworld images, we randomly selected 200 very low resolution, heavily blurred faces for qualitative evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>In this section, we describe the proposed architecture comprising of three connected networks: the first network is a Super-resolution network used to super-resolve the LR images. The second network is a discriminator used to distinguish between the super-resolved and the original HR images. The third network is FAN: the face alignment network for localizing the facial landmarks on the super-resolved facial images. Note that at test time the discriminator is not used. Overall, we call our network Super-FAN. See <ref type="figure" target="#fig_0">Fig. 2</ref> Notably, for super-resolution, we propose a new architecture, shown in <ref type="figure" target="#fig_3">Fig. 3a</ref>, and detailed, along with the loss functions to train it, in sub-section 4.1. Our discriminator, based on Wasserstein GANs <ref type="bibr" target="#b0">[1]</ref>, is described in subsection 4.2. Our integrated FAN along with our newlyintroduced heatmap regression loss for super-resolution is described in sub-section 4.3. Sub-section 4.4 provides the overall loss for training Super-FAN. Finally, sub-section 4.5 describes the complete training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Super-resolution network</head><p>In this section, we propose a new residual-based architecture for super-resolution, inspired by <ref type="bibr" target="#b19">[20]</ref>, and provide the intuition and motivation behind our design choices. Our network as well as the one of <ref type="bibr" target="#b19">[20]</ref> are shown in Figs. 3a and 3b, respectively. Their differences are detailed below. Following recent work <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b31">32]</ref>, the input and output resolutions are 16 × 16 and 64 × 64, respectively. Per-block layer distribution. The architecture of <ref type="bibr" target="#b19">[20]</ref>, shown in <ref type="figure" target="#fig_3">Fig. 3b</ref>, uses 16, 1 and 1 blocks (layers) operating at the original, twice the original, and 4 times the original resolution, respectively; in particular, 16 blocks operate at a resolution 16 × 16, 1 at 32 × 32 and another 1 at 64 × 64. Let us denote this architecture as 16 − 1 − 1. We propose a generalized architecture of the form N 1 − N 2 − N 3 , where N 1 , N 2 and N 3 are the number of blocks used at the original, twice the original, and 4 times the original resolution, respectively. As opposed to the architecture of <ref type="bibr" target="#b19">[20]</ref> where most of the blocks (i.e. 16) work at the input resolution, we opted for a more balanced distribution: 12-3-2, shown in <ref type="figure" target="#fig_3">Fig. 3a</ref>. Our motivation behind this change is as follows: since the main goal of the network is to super-resolve its input via hallucination, using only a single block at higher resolutions (as in <ref type="bibr" target="#b19">[20]</ref>) is insufficient for the generation of sharp details, especially for images found in challenging scenarios (e.g. <ref type="figure">Fig. 1</ref>). Building block architecture. While we experimented with a few variants of residual blocks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, similarly to <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref>, we used the one proposed in <ref type="bibr" target="#b8">[9]</ref>. The block contains two 3 × 3 convolutional layers, each of them followed by a batch normalization layer <ref type="bibr" target="#b13">[14]</ref>. While <ref type="bibr" target="#b19">[20]</ref> uses a PReLU activation function, in our experiments, we noticed no improvements compared to ReLU, therefore we used Re-LUs throughout the network. See <ref type="figure" target="#fig_3">Fig. 3a</ref>.</p><p>On the "long" skip connection. The SR-ResNet of <ref type="bibr" target="#b19">[20]</ref> groups its 16 modules operating at the original resolution in a large block, equipped with a skip connection that links the first and the last block, in an attempt to improve the gradient flow. We argue that the resolution increase is a gradual process in which each layer should improve upon the representation of the previous one, thus the infusion of lower level futures will have a small impact on the overall performance. In practice, and at least for our network, we found very small gains when using it. See supplementary material for additional results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Pixel and perceptual losses</head><p>Pixel loss. Given a low resolution image I LR (of resolution 16 × 16) and the corresponding high resolution image I HR (of resolution 64 × 64), we used the pixel-wise MSE loss to minimize the distance between the high resolution and the super-resolved image. It is defined as follows:</p><formula xml:id="formula_1">l pixel = 1 r 2 W H rW x=1 rH y=1 (I HR x,y − G θ G (I LR ) x,y ) 2 ,<label>(1)</label></formula><p>where W and H denote the size of I LR and r is the upsampling factor (set to 4 in our case). Perceptual loss. While the pixel-wise MSE loss achieves high PSNR values, it often results in images which lack fine details, are blurry and unrealistic (see <ref type="figure" target="#fig_4">Fig. 4</ref>). To address this, in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref>, a perceptual loss is proposed in which the super-resolved image and the original image must also be close in feature space. While <ref type="bibr" target="#b19">[20]</ref> defines this loss over the activations of layer 5 4 (the one just before the FC layers) of VGG-19 <ref type="bibr" target="#b26">[27]</ref>, we instead used a combination of low, middle and high level features computed after the B1, B2 and B3 blocks of ResNet-50 <ref type="bibr" target="#b10">[11]</ref>. The loss over the ResNet features at a given level i is defined as:</p><formula xml:id="formula_2">l f eature/i = 1 W i H i Wi x=1 Hi y=1 (φ i (I HR ) x,y − φ i (G θ G (I LR )) x,y ) 2 ,<label>(2)</label></formula><p>where φ i denotes the feature map obtained after the last convolutional layer of the i−th block and W i , H i its size.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Adversarial network</head><p>The idea of using a GAN <ref type="bibr" target="#b6">[7]</ref> for face super-resolution is straightforward: the generator G in this case is the superresolution network which via a discriminator D and an adversarial loss is enforced to produce more realistic super-resolved images lying in the manifold of facial images. Prior work in image super-resolution <ref type="bibr" target="#b19">[20]</ref> used the GAN formulation of <ref type="bibr" target="#b23">[24]</ref>. While in our work, we do not make an attempt to improve the GAN formulation per se, we are the first to make use of recent advances within super-resolution and replace <ref type="bibr" target="#b23">[24]</ref> with the Wasserstein GAN of (WGAN) <ref type="bibr" target="#b0">[1]</ref>, as also improved in <ref type="bibr" target="#b9">[10]</ref> (see also Eq. <ref type="formula" target="#formula_3">(3)</ref>).</p><p>We emphasize that our finding is that the improvement over <ref type="bibr" target="#b23">[24]</ref> is only with respect to the stability and easiness of training and not with the quality of the super-resolved facial images: while training from scratch with the GAN loss of <ref type="bibr" target="#b23">[24]</ref> is tricky and often leads to an unsatisfactory solution, by using a WGAN loss, we stabilized the training and allowed for the introduction of the GAN loss at earlier stages in the training process, thus reducing the overall training time. Finally, in terms of network architecture, we used the DCGAN <ref type="bibr" target="#b23">[24]</ref> without batch normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Adversarial loss</head><p>Following <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b9">[10]</ref>, the WGAN loss employed in our face super-resolution network is defined as:</p><formula xml:id="formula_3">lW GAN = Ê I∼Pg [D(Î)] − E I∼Pr [D(I HR )] + λ Ê I∼PÎ [ ( ∇Î D(Î) 2 − 1) 2 ],<label>(3)</label></formula><p>where P r is the data distribution and P g is the generator G distribution defined byÎ = G(I LR ). PÎ is obtained by uniformly sampling along straight lines between pairs of samples from P r and P g .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Face Alignment Network</head><p>The losses defined above (pixel, perceptual and adversarial) have been used in general purpose super-resolution and although alone do provide descent results for facial superresolution, they also fail to incorporate information related to the structure of the human face into the super-resolution process. We have observed that when these losses are used alone pose or expression related details may be missing or facial parts maybe incorrectly located (see <ref type="figure" target="#fig_4">Fig. 4</ref>).</p><p>To alleviate this, we propose to enforce facial structural consistency between the low and the high resolution image via integrating a network for facial landmark localization through heatmap regression into the super-resolution process and optimizing an appropriate heatmap loss.</p><p>To this end, we propose to use the super-resolved image as input to a FAN and train it so that it produces the same output as that of another FAN applied on the original high resolution image. We note that FAN uses the concept of heatmap regression to localize the landmarks: rather than training a network to regress a 68×2 vector of x and y coordinates, each landmark is represented by an output channel containing a 2D Gaussian centered at the landmark's location, and then the network is trained to regress the 2D Gaussians, also known as heatmaps. As a number of works have shown (e.g. <ref type="bibr" target="#b1">[2]</ref>), these heatmaps capture shape information (e.g. pose and expression), spatial context and structural part relationships. Enforcing the super-resolved and the corresponding HR image to yield the same heatmaps via minimization of their distance is a key element of our approach: not only are we able to localize the facial landmarks but actually we impose these two images to have similar facial structure. In terms of architecture, we simply used FAN <ref type="bibr" target="#b3">[4]</ref> with 2 Hourglass modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Heatmap loss</head><p>Based on the above discussion, we propose to enforce structural consistency between the super-resolved and the corresponding HR facial image via a heatmap loss defined as:</p><formula xml:id="formula_4">l heatmap = 1 N N n=1 ij ( M n i,j − M n i,j ) 2 ,<label>(4)</label></formula><p>where M n i,j is the heatmap corresponding to the n−th landmark at pixel (i, j) produced by running the FAN integrated into our super-resolution network on the super-resolved im-ageÎ HR and M n i,j is the heatmap obtained by running another FAN on the original image I HR .</p><p>Another key feature of our heatmap loss is that its optimization does not require having access to ground truth landmark annotations just access to a pre-trained FAN. This allows us to train the entire super-resolution network in a weakly supervised manner which is necessary since for some of the datasets used for training (e.g. CelebA) ground truth landmark annotations are not available, anyway.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Overall training loss</head><p>The overall loss used for training Super-FAN is:</p><formula xml:id="formula_5">l SR = αl pixel + βl f eature + γl heatmap + ζl W GAN ,<label>(5)</label></formula><p>where α, β, γ and ζ are the corresponding weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Training</head><p>All images were cropped based on the bounding box such that the face height is 50 px. Input and output resolutions were 16 × 16 px and 64 × 64 px, respectively. To avoid overfitting, we performed random image flipping, scaling (between 0.85 and 1.15), rotation (between −30 o and 30 o ), color, brightness and contrast jittering. All models, except for the one trained with the GAN loss, were trained for 60 epochs, during which the learning rate was gradually decreased from 2.5e-4 to 1e-5. The model trained with the GAN loss was based on a previously trained model which was fine-tunned for 5 more epochs. The ratio between running the generator and the discriminator was kept to 1. Finally, for end-to-end training of the final model (i.e. Super-FAN), all networks (super-resolution, discriminator and FAN) were trained jointly for 5 epochs with a learning rate of 2.5e-4. All models, implemented in PyTorch <ref type="bibr" target="#b22">[23]</ref>, were trained using rmsprop <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we evaluate the performance of Super-FAN. The details of our experiments are as follows: Training/Testing. Unless otherwise stated, all methods, including <ref type="bibr" target="#b19">[20]</ref>, were trained on the training sets of section 3. We report quantitative and qualitative results on the subset of LS3D-W balanced consisting of 3,000 images, with each pose range being equally represented. We report qualitative results for more than 200 images from WiderFace. Performance metrics. In sub-section 5.1, we report results using the standard super-resolution metrics, namely the PSNR and SSIM <ref type="bibr" target="#b28">[29]</ref>, confirming <ref type="bibr" target="#b19">[20]</ref> that both of them are a poor measure of the perceived image quality. In subsection 5.2, we report results on facial landmark localization accuracy. To alleviate the issues with PSNR and SSIM, we also propose another indirect way to assess super-resolution quality based on facial landmarks: in particular, we trained a FAN on high resolution images and then used it to localize the landmarks on the super-resolved images produced by each method. As our test set (LS3D-W balanced) provides the ground truth landmarks, we can use landmark localization accuracy to assess the quality of the super-resolved images: the rationale is that, the better the quality of the superresolved image, the higher the localization accuracy will be, as the FAN used saw only real high resolution images during training. The metric used to quantify performance is the Area Under the Curve (AUC) <ref type="bibr" target="#b3">[4]</ref>. Variants compared. In section 4, we presented a number of networks and losses for super-resolution which are all evaluated herein. These methods are named as follows:</p><p>• Ours-pixel: this is the super-resolution network of subsection 4.1 trained with the pixel loss of Eq. (1).</p><p>• Ours-pixel-feature: this is the super-resolution network of sub-section 4.1 trained with the pixel loss of Eq. (1) and the perceptual loss of Eq. (2).  Comparison with the state-of-the-art. We report results for the method of <ref type="bibr" target="#b19">[20]</ref>, implemented with and without the GAN loss, called SR-GAN and SR-ResNet, respectively, and for the standard baseline based on bilinear in-terpolation. We also show visual results on WiderFace by running the code from [34] 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Super-resolution results</head><p>Our quantitative results on LS3D-W across all facial poses are shown in <ref type="table" target="#tab_0">Table 1</ref>. In terms of PSNR, the best results are achieved by Ours-pixel-feature-heatmap. In terms of SSIM, the best performing method seems to be Ourspixel. From these numbers, it is hard to safely conclude which method is the best. Visually inspecting the superresolved images though in <ref type="figure" target="#fig_4">Fig. 4</ref> clearly shows that the sharper and more detailed facial images are by far produced by Ours-pixel-feature-heatmap and Ours-Super-FAN. Notably, Ours-pixel achieves top performance in terms of SSIM, yet the images generated by it are blurry and unrealistic (see <ref type="figure" target="#fig_4">Fig. 4</ref>), and are arguably less visually appealing than the ones produced by incorporating the other loss terms. We confirm the findings of <ref type="bibr" target="#b19">[20]</ref> that these metrics can sometimes be misleading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Facial landmark localization results</head><p>Herein, we present facial landmark localization results (on LS3D-W), also in light of our proposed way to evaluate super-resolution based on the accuracy of a pre-trained FAN on the super-resolved images (see Performance metrics). We report results for the following methods:</p><p>• FAN-bilinear: this method upsamples the LR image using bilinear interpolation and then runs FAN on it.</p><p>• Retrained FAN-bilinear: this is the same as FAN-bilinear. However, FAN was re-trained to work exclusively with bilinearly upsampled LR images.</p><p>• FAN-SR-ResNet: the LR image is super-resolved using SR-ResNet <ref type="bibr" target="#b19">[20]</ref> and then FAN is run on it.</p><p>• FAN-SR-GAN: the LR image is super-resolved using using SR-GAN <ref type="bibr" target="#b19">[20]</ref> and then FAN is run on it.</p><p>• FAN-Ours-pixel: the LR image is super-resolved using Ours-pixel and then FAN is run on it.</p><p>• FAN-Ours-pixel-feature: the LR image is super-resolved using Ours-pixel-feature and then FAN is run on it.</p><p>• FAN-Ours-pixel-feature-heatmap-GAN: the LR image is super-resolved using Ours-pixel-feature-heatmap-GAN and then FAN is run on it. The FAN is not trained with the rest of the super-resolution network i.e. the same FAN as above was used. This variant is included to highlight the importance of jointly training the face alignment and super-resolution networks as proposed in this work.</p><p>• Super-FAN: this is the same as above however, this time, FAN is jointly trained with the rest of the network.</p><p>• FAN-HR images: this method uses directly the original HR images as input to FAN. This method provides an upper bound in performance.</p><p>The results are summarized in <ref type="figure" target="#fig_4">Fig. 4</ref> and <ref type="table" target="#tab_1">Table 2</ref>. See supplementary material for examples showing the landmark localization accuracy. From the results, we conclude that:</p><p>1. Super-FAN is by far the best performing method being the only method attaining performance close to the upper performance bound provided by FAN-HR images.  <ref type="figure" target="#fig_4">Fig. 4</ref>. This validates our approach to evaluate super-resolution performance indirectly using facial landmark localization accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparison on real-world images</head><p>Most face super-resolution methods show results on synthetically generated LR images. While these results are valuable for assessing performance, a critical aspect of any system is its performance on real-world data captured in unconstrained conditions. To address this, in this section and in our supplementary material, we provide visual results by running our system on more than 200 low resolution blurry images taken from the WiderFace and compare its performance with that of SR-GAN <ref type="bibr" target="#b19">[20]</ref> and CBN <ref type="bibr" target="#b33">[34]</ref>.</p><p>Initially, we found that the performance of our method on real images, when trained on artificially downsampled images, was sub-optimal, with the super-resolved images often lacking sharp details. However, retraining Super-FAN by applying additionally random Gaussian blur (of kernel size between 3 and 7 px) to the input images, and simulating jpeg artifacts and color distortion, seems to largely alleviate the problem. Results of our method, SR-GAN (also retrained in the same way as our method) and CBN can be seen in <ref type="figure" target="#fig_8">Figs. 1 and 5</ref>, while the results on all 200 images can be found in the supplementary material.    <ref type="figure" target="#fig_4">Fig. 4</ref>.</p><p>Our method provides the sharper and more detailed results performing well across all poses. SR-GAN fails to produce sharp results. CBN produces unrealistic results especially for the images that landmark localization was poor.</p><p>A few failure cases of our method are shown in <ref type="figure">Fig. 6</ref>; mainly cases of extreme poses, large occlusions and heavy blurring. With respect to the latter, although our augmentation strategy seems effective, it is certainly far from optimal. Enhancing it is left for interesting future work. Input Output <ref type="figure">Figure 6</ref>: Failure cases of our method on WiderFace. Typically, these include extreme facial poses, large occlusions and heavy blurring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We proposed Super-FAN: the very first end-to-end system for integrated facial super-resolution and landmark localization. Our method incorporates facial structural information in a newly proposed architecture for superresolution, via integrating a sub-network for face alignment and optimizing a novel heatmap loss. We show large improvement over the state-of-the-art for both face superresolution and alignment across the whole spectrum of facial poses. We also show, for the first time, good results on real-world low resolution facial images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1. Ablation studies</head><p>This section describes a series of experiments, further analyzing the importance of particular components on the overall performance. It also provides additional qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.1. On the pixel loss</head><p>In this section, we compare the effect of replacing the L2 loss of Eq. 1 with the L1 loss. While the L1 loss is known to be more robust in the presence of outliers, we found no improvement of using it over the L2 loss. The results are shown in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.2. On the heatmap loss</head><p>Similarly to the above experiment, we also replaced the L2 heatmap loss of Eq. 4 with the L1 loss. The results are shown in <ref type="table" target="#tab_4">Table 5</ref>, showing descent improvement for large poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.3. On the importance of the skip connection</head><p>Herein, we analyzed the impact of the long-skip connections to the overall performance of the generator. The results, shown in <ref type="table">Table 4</ref>, show no improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.4. On network speed</head><p>Besides accuracy, another important aspect of network performance is speed. Compared with SR-GAN <ref type="bibr" target="#b19">[20]</ref>, our generator is only 10% slower, being able to process 1,000 images in 4.6s (vs. 4.3s required by SR-GAN) on an NVIDIA Titan-X GPU. <ref type="figure" target="#fig_11">Fig. 9</ref> shows the results produced by Super-FAN on all of the 200 randomly selected low-resolution images from WiderFace. <ref type="figure" target="#fig_10">Fig. 8</ref> shows the face size distribution Notice that our method copes well with pose variation and challenging illumination conditions. There were a few failure cases, but in most of these cases, it is impossible to tell whether the low-resolution image was actually a face. <ref type="figure" target="#fig_12">Fig. 10</ref> shows a few fitting results produced by Super-FAN on the LS3D-W Balanced dataset. The predictions were plotted on top of the low resolution input images. We observe that our method is capable of producing accurate results even for faces found in arbitrary poses exhibiting various facial expressions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A1.5. Additional qualitative results</head><p>We also tested our system on images from the Surveillance Cameras Face dataset (SCface) <ref type="bibr" target="#b7">[8]</ref>. The dataset contains 4,160 images of 130 unique subjects taken with different cameras from different distances. <ref type="figure" target="#fig_9">Fig. 7</ref> shows a few qualitative results from this dataset.    <ref type="table">Table 4</ref>: PSNR and SSIM for "no-skip" and "with skip" versions. The "no-skip" version indicates the absence of the long skip connection (the network depicted in <ref type="figure" target="#fig_3">Fig. 3a</ref>), while the "with skip" version adds two new long skip connections, similarly to <ref type="bibr" target="#b10">[11]</ref>.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The proposed Super-FAN architecture comprises three connected networks: the first network is a newly proposed Super-resolution network (see sub-section 4.1). The second network is a WGAN-based discriminator used to distinguish between the super-resolved and the original HR image (see sub-section 4.2). The third network is FAN, a face alignment network for localizing the facial landmarks on the super-resolved facial image and improving super-resolution through a newly-introduced heatmap loss (see sub-section 4.3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>LS3D-W balanced is a subset of the LS3D-W [4] dataset containing 7,200 images captured in-the-wild, in which each pose range ([0 0 − 30 0 ], [30 0 − 60 0 ], [60 0 − 90 0 ]) is equally represented (2,400 images each). We used 4,200 images for training, and kept 3000 for testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>A comparison between the proposed superresolution architecture (left) and the one described in<ref type="bibr" target="#b19">[20]</ref> (right). See also sub-section 4.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Visual results on LS3D-W. Notice that: (a) The proposed Ours-pixel-feature already provides better results than those of SR-GAN [20]. (b) By additionally adding the newly proposed heatmap loss (Ours-pixel-feature-heatmap) the generated faces are better structured and look far more realistic. Ours-pixel-feature-heatmap-GAN is Super-FAN which improves upon Ours-pixel-feature-heatmap by adding the GAN loss and by end-to-end training. Best viewed in electronic format.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>•</head><label></label><figDesc>Ours-pixel-feature-heatmap: this is the super-resolution network of sub-section 4.1 trained with the pixel loss of Eq. (1), the perceptual loss of Eq. (2), and the newly proposed heatmap loss of Eq. (4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>•</head><label></label><figDesc>Ours-Super-FAN: this improves upon ours-pixel-featureheatmap by additionally training with the GAN loss of Eq. (3) and by end-to-end training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 .</head><label>2</label><figDesc>Jointly training the face alignment and super-resolution networks is necessary to obtain high performance: Super-FAN largely outperforms FAN-Ours-pixelfeature-heatmap-GAN (second best method). 3. The performance drop of Super-FAN for large poses (&gt; 60 o ) is almost twice as much as that of FAN-HR images. This indicates that facial pose is still an issue in face super-resolution. 4. Even a FAN trained exclusively to work with bilinearly upsampled images (Retrained FAN-Bilinear), clearly an unrealistic scenario, produces moderate results, and far inferior to the ones produced by Super-FAN. 5. FAN-Ours-pixel-feature outperforms both FAN-SR-GAN and FAN-SR-ResNet. This shows that the proposed super-resolution network of section 4.1 (which does not use heatmap or WGAN losses) already outperforms the state-of-the-art. 6. From FAN-Ours-pixel to Super-FAN, each of the losses added improves performance which is in accordance to the produced visual results of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Results produced by our system, SR-GAN<ref type="bibr" target="#b19">[20]</ref> and CBN<ref type="bibr" target="#b33">[34]</ref> on real-world low resolution faces from WiderFace.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results on the SCface dataset<ref type="bibr" target="#b7">[8]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Face size (defined as max(width, height)) distribution of the selected subset of low resolution images from WiderFace. (L2) 21.55 22.45 23.05 0.8001 0.8127 0.8240 Ours-pixel (L1) 21.47 22.40 23.00 0.7988 0.8120 0.8229</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Visual results on a subset of very low resolution images from the WiderFace dataset. The odd rows represent the input, while the even ones the output produced by Super-FAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Fitting examples produced by Super-FAN on a few images from LS3D-W. The predictions are plotted over the original low-resolution images. Notice that our method works well for faces found in challenging conditions such as large poses or extreme illumination conditions despite the poor image quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>PSNR-and SSIM-based super-resolution performance on LS3D-W balanced dataset across pose (higher is better). The results are not indicative of visual quality. See Fig. 4.</figDesc><table><row><cell>Method</cell><cell></cell><cell></cell><cell>30</cell><cell>PSNR 60</cell><cell>90</cell><cell>30</cell><cell>SSIM 60</cell><cell>90</cell></row><row><cell cols="8">bilinear upsample (baseline) 20.25 21.45 22.10 0.7248 0.7618 0.7829</cell></row><row><cell>SR-ResNet</cell><cell></cell><cell></cell><cell cols="5">21.21 22.23 22.83 0.7764 0.7962 0.8077</cell></row><row><cell>SR-GAN</cell><cell></cell><cell></cell><cell cols="5">20.01 20.94 21.48 0.7269 0.7465 0.7586</cell></row><row><cell>Ours-pixel</cell><cell></cell><cell></cell><cell cols="5">21.55 22.45 23.05 0.8001 0.8127 0.8240</cell></row><row><cell cols="2">Ours-pixel-feature</cell><cell></cell><cell cols="5">21.50 22.51 23.10 0.7950 0.7970 0.8205</cell></row><row><cell cols="3">Ours-pixel-feature-heatmap</cell><cell cols="5">21.55 22.55 23.17 0.7960 0.8105 0.8210</cell></row><row><cell cols="2">Ours-Super-FAN</cell><cell></cell><cell cols="5">20.85 21.67 22.24 0.7745 0.7921 0.8025</cell></row><row><cell>Method</cell><cell cols="4">[0-30] [30-60] [60-90]</cell><cell></cell><cell></cell></row><row><cell>FAN-bilinear</cell><cell>10.7%</cell><cell>6.9%</cell><cell>2.3%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FAN-SR-ResNet</cell><cell>48.9%</cell><cell>38.9%</cell><cell>21.4%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FAN-SR-GAN</cell><cell>47.1%</cell><cell>36.5%</cell><cell>19.6%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Retrained FAN-bilinear</cell><cell>55.9%</cell><cell>49.2%</cell><cell>37.8%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FAN-Ours-pixel</cell><cell>52.3%</cell><cell>45.3%</cell><cell>28.3%</cell><cell></cell><cell></cell><cell></cell></row><row><cell>FAN-Ours-pixel-feature</cell><cell>57.0%</cell><cell>50.2%</cell><cell>34.9%</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">FAN-Ours-pixel-feature-heatmap 61.0% Super-FAN 67.0% 63.0% 55.6%</cell><cell cols="2">42.3% 52.5%</cell><cell></cell><cell></cell></row><row><cell>FAN-HR images</cell><cell>75.3%</cell><cell>72.7%</cell><cell>68.2%</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>AUC across pose (calculated for a threshold of 10%; see<ref type="bibr" target="#b3">[4]</ref>) on our LS3D-W balanced test set. The results, in this case, are indicative of visual quality. See</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>PSNR and SSIM when training our generator with L2 and L1 pixel-losses. (no-skip) 21.55 22.45 23.05 0.8001 0.8127 0.8240 Ours-pixel (with skip) 21.56 22.45 23.04 0.8021 0.8132 0.8241</figDesc><table><row><cell>Method</cell><cell>30</cell><cell>PSNR 60</cell><cell>90</cell><cell>30</cell><cell>SSIM 60</cell><cell>90</cell></row><row><cell>Ours-pixel</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>AUC across pose (on our LS3D-W balanced test set) for L2 and L1 heatmap losses.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It is hard in general to compare with<ref type="bibr" target="#b33">[34]</ref> because the provided code pre-processes the facial images very differently to our method.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Binarized convolutional landmark localizers for human pose estimation and face alignment with limited resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">How far are we from solving the 2d &amp; 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attention-aware face hallucination via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Scface-surveillance cameras face database. Multimedia tools and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Delac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Grgic</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="863" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Training and investigating residual nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<ptr target="http://torch.ch/blog/2016/02/04/resnets.html.4" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wavelet-srnet: A wavelet-based cnn for multi-scale face super resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Annotated facial landmarks in the wild: A large-scale, realworld database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Köstinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Attribute and simile classifiers for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Nayar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="365" to="372" />
		</imprint>
	</monogr>
	<note>Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interactive facial feature localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="679" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="http://github.com/pytorch/pytorch.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">300 faces in-the-wild challenge: The first facial landmark localization challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sagonas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV-W</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Enhancenet: Single image super-resolution through automated texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition. arXiv</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structured face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1099" to="1106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Wider face: A face detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Ultra-resolving face images by discriminative generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Hallucinating very low-resolution unaligned and noisy face images by transformative discriminative autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep cascaded binetwork for face hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
