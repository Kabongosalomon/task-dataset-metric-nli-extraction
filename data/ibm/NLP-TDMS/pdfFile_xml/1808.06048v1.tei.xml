<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distractor-aware Siamese Networks for Visual Object Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Group Limited</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Group Limited</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">SenseTime Group Limited</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distractor-aware Siamese Networks for Visual Object Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Visual Tracking · Distractor-aware · Siamese Networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, Siamese networks have drawn great attention in visual tracking community because of their balanced accuracy and speed. However, features used in most Siamese tracking approaches can only discriminate foreground from the non-semantic backgrounds. The semantic backgrounds are always considered as distractors, which hinders the robustness of Siamese trackers. In this paper, we focus on learning distractor-aware Siamese networks for accurate and long-term tracking. To this end, features used in traditional Siamese trackers are analyzed at first. We observe that the imbalanced distribution of training data makes the learned features less discriminative. During the off-line training phase, an effective sampling strategy is introduced to control this distribution and make the model focus on the semantic distractors. During inference, a novel distractor-aware module is designed to perform incremental learning, which can effectively transfer the general embedding to the current video domain. In addition, we extend the proposed approach for long-term tracking by introducing a simple yet effective local-to-global search region strategy. Extensive experiments on benchmarks show that our approach significantly outperforms the state-of-thearts, yielding 9.6% relative gain in VOT2016 dataset and 35.9% relative gain in UAV20L dataset. The proposed tracker can perform at 160 FPS on short-term benchmarks and 110 FPS on long-term benchmarks. The code is available at https://github.com/foolwood/DaSiamRPN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Visual object tracking, which locates a specified target in a changing video sequence automatically, is a fundamental problem in many computer vision topics such as visual analysis, automatic driving and pose estimation. A core problem of tracking is how to detect and locate the object accurately and efficiently in challenging scenarios with occlusions, out-of-view, deformation, background cluttering and other variations <ref type="bibr">[38]</ref>.</p><p>Recently, Siamese networks, which follow a tracking by similarity comparison strategy, have drawn great attention in visual tracking community because of favorable performance [31, <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr">36,</ref><ref type="bibr">33,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr">37,</ref><ref type="bibr">16]</ref>. <ref type="bibr">SINT [31]</ref>, GOTURN <ref type="bibr" target="#b7">[8]</ref>, SiamFC <ref type="bibr" target="#b1">[2]</ref> and RASNet [36] learn a priori deep Siamese similarity function and use it in a run-time fixed way. CFNet <ref type="bibr">[33]</ref> and DSiam <ref type="bibr" target="#b6">[7]</ref> can online update the tracking model via a running average template and a fast transformation learning module, respectively. SiamRPN <ref type="bibr">[16]</ref> introduces a region proposal network after the Siamese network, thus formulating the tracking as a one-shot local detection task.</p><p>Although these tracking approaches obtain balanced accuracy and speed, there are 3 problems that should be addressed: firstly, features used in most Siamese tracking approaches can only discriminate foreground from the nonsemantic background. The semantic backgrounds are always considered as distractors, and the performance can not be guaranteed when the backgrounds are cluttered. Secondly, most Siamese trackers can not update the model <ref type="bibr">[31,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr">36,</ref><ref type="bibr">16]</ref>. Although their simplicity and fixed-model nature lead to high speed, these methods lose the ability to update the appearance model online which is often critical to account for drastic appearance changes in tracking scenarios. Thirdly, recent Siamese trackers employ a local search strategy, which can not handle the full occlusion and out-of-view challenges.</p><p>In this paper, we explore to learn Distractor-aware Siamese Region Proposal Networks (DaSiamRPN) for accurate and long-term tracking. SiamFC uses a weighted loss function to eliminate class imbalance of the positive and negative examples. However, it is inefficient as the training procedure is still dominated by easily classified background examples. In this paper, we identify that the imbalance of the non-semantic background and semantic distractor in the training data is the main obstacle for the representation learning. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, the response maps on the SiamFC can not distinguish the people, even the athlete in the white dress can get a high similarity with the target person. High quality training data is crucial for the success of end-to-end learning tracker. We conclude that the quality of the representation network heavily depends on the distribution of training data. In addition to introducing positive pairs from existing large-scale detection datasets, we explicitly generate diverse semantic negative pairs in the training process. To further encourage discrimination, an effective data augmentation strategy customizing for visual tracking are developed.</p><p>After the offline training, the representation networks can generalize well to most categories of objects, which makes it possible to track general targets. During inference, classic Siamese trackers only use nearest neighbour search to match the positive templates, which might perform poorly when the target undergoes significant appearance changes and background clutters. Particularly, the presence of similar looking objects (distractors) in the context makes the tracking task more arduous. To address this problem, the surrounding contextual and temporal information can provide additional cues about the targets and help to maximize the discrimination abilities. In this paper, a novel distractor-aware module is designed, which can effectively transfer the general embedding to the current video domain and incrementally catch the target appearance variations during inference.</p><p>Besides, most recent trackers are tailored to short-term scenario, where the target object is always present. These works have focused exclusively on short sequences of a few tens of seconds, which is poorly representative of practitioners' needs. Except the challenging situations in short-term tracking, severe out-ofview and full occlusion introduce extra challenges in long-term tracking. Since conventional Siamese trackers lack discriminative features and adopt local search region, they are unable to handle these challenges. Benefiting from the learned distractor-aware features in DaSiamRPN, we extend the proposed approach for long-term tracking by introducing a simple yet effective local-to-global search region strategy. This significantly improves the performance of our tracker in out-of-view and full occlusion challenges.</p><p>We validate the effectiveness of proposed DaSiamRPN framework on extensive short-term and long-term tracking benchmarks: VOT2016 <ref type="bibr" target="#b13">[14]</ref>, VOT2017 <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr">OTB2015 [38]</ref>, UAV20L and UAV123 <ref type="bibr">[22]</ref>. On short-term VOT2016 dataset, DaSiamRPN achieves a 9.6% relative gain in Expected Average Overlap compared to the top ranked method ECO <ref type="bibr" target="#b2">[3]</ref>. On long-term UAV20L dataset, DaSi-amRPN obtains 61.7% in Area Under Curve which outperforms the current best-performing tracker by relative 35.9%. Besides the favorable performance, our tracker can perform at far beyond real-time speed: 160 FPS on short-term datasets and 110 FPS on long-term datasets. All these consistent improvements demonstrate that the proposed approach establish a new state-of-the-art in visual tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Contributions</head><p>The contributions of this paper can be summarized in three folds as follows:</p><p>1, The features used in conventional Siamese trackers are analyzed in detail. And we find that the imbalance of the non-semantic background and semantic distractor in the training data is the main obstacle for the learning.</p><p>2, We propose a novel Distractor-aware Siamese Region Proposal Networks (DaSiamRPN) framework to learn distractor-aware features in the off-line training, and explicitly suppress distractors during the inference of online tracking.</p><p>3, We extend the DaSiamRPN to perform long-term tracking by introducing a simple yet effective local-to-global search region strategy, which significantly improves the performance of our tracker in out-of-view and full occlusion challenges. In comprehensive experiments of short-term and long-term visual tracking benchmarks, the proposed DaSiamRPN framework obtains state-of-the-art accuracy while performing at far beyond real-time speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Siamese Networks based Tracking. Siamese trackers follow a tracking by similarity comparison strategy. The pioneering work is SINT [31], which sim-ply searches for the candidate most similar to the exemplar given in the starting frame, using a run-time fixed but learns a priori deep Siamese similarity function. As a follow-up work, Bertinetto et.al <ref type="bibr" target="#b1">[2]</ref> propose a fully convolutional Siamese network (SiamFC) to estimate the feature similarity region-wise between two frames. RASNet [36] advances this similarity metric by learning the attention mechanism with a Residual Attentional Network. Different from SiamFC and RASNet, in GOTURN tracker <ref type="bibr" target="#b7">[8]</ref>, the motion between successive frames is predicted using a deep regression network. These threee trackers are able to perform at 86 FPS, 83FPS and 100 FPS respectively on GPU because no fine-tuning is performed online. CFNet <ref type="bibr">[33]</ref>  formulates the features learning and tracking process into a unified framework, enabling learned features are tightly coupled to tracking process.</p><p>Long-term Tracking. Traditional long-term tracking frameworks can be divided into two groups: earlier methods regard tracking as local key point descriptors matching with a geometrical model <ref type="bibr">[25,</ref><ref type="bibr">24,</ref><ref type="bibr">21]</ref>, and recent approaches perform long-term tracking by combining a short-term tracker with a detector. The seminal work of latter categories is TLD <ref type="bibr" target="#b9">[10]</ref>, which proposes a memory-less flock of flows as a short-term tracker and a template-based detector run in parallel. Ma et al. <ref type="bibr">[20]</ref>propose a combination of KCF tracker and a random ferns classifier as a detector that is used to correct the tracker. Similarly, MUSTer <ref type="bibr" target="#b8">[9]</ref> is a long-term tracking framework that combines KCF tracker with a SIFT-based detector that is also used to detect occlusions. Fan and Ling <ref type="bibr" target="#b5">[6]</ref> combines a DSST tracker <ref type="bibr" target="#b3">[4]</ref> with a CNN detector [31] that verifies and potentially corrects proposals of the short-term tracker. 3 Distractor-aware Siamese Networks</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Features and Drawbacks in Traditional Siamese Networks</head><p>Before the detailed discussion of our proposed framework, we first revisit the features of conventional Siamese network based tracking <ref type="bibr" target="#b1">[2,</ref><ref type="bibr">16]</ref>. Siamese trackers use metric learning at their core. The goal is to learn an embedding space that can maximize the interclass inertia between different objects and minimize the intraclass inertia for the same object. The key contribution leading to the popularity and success of Siamese trackers is their balanced accuracy and speed. <ref type="figure" target="#fig_0">Fig. 1</ref> visualizes of response maps of SiamFC and SiamRPN. It can be seen that for the targets, those with large differences in the background also achieve high scores, and even some extraneous objects get high scores. The representations obtained in SiamFC usually serve the discriminative learning of the categories in training data. In SiamFC and SiamRPN, pairs of training data come from different frames of the same video, and for each search area, the non-semantic background occupies the majority, while semantic entities and distractor occupy less. This imbalanced distribution makes the training model hard to learn instance-level representation, but tending to learn the differences between foreground and background.</p><p>During inference, nearest neighbor is used to search the most similar object in the search region, while the background information labelled in the first frame are omitted. The background information in the tracking sequences can be effectively utilized to increase the discriminative capability as shown in <ref type="figure" target="#fig_0">Fig. 1e</ref>.</p><p>To eliminate these issues, we propose to actively generate more semantics pairs in the offline training process and explicitly suppress the distractors in the online tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Distractor-aware Training</head><p>High quality training data is crucial for the success of end-to-end representation learning in visual tracking. We introduce series of strategies to improve the generalization of the learned features and eliminate the imbalanced distribution of the training data.   Semantic negative pairs can improve the discriminative ability. We attribute the less discriminative representation in SiamFC <ref type="bibr" target="#b1">[2]</ref> and SiamRPN <ref type="bibr">[16]</ref> to two level of imbalanced training data distribution. The first imbalance is the rare semantic negative pairs. Since the background occupies the majority in the training data of SiamFC and SiamRPN, most negative samples are non-semantic (not real object, just background), and they can be easily classified. That is to say, SiamFC and SiamRPN learn the differences between foreground and background, and the losses between semantic objects are overwhelmed by the vast number of easy negatives. Another imbalance comes from the intraclass distractors, which usually perform as hard negative samples in the tracking process. In this paper, semantic negative pairs are added into the training process. The constructed negative pairs consist of labelled targets both in the same categories and different categories. The negative pairs from different categories can help tracker to avoid drifting to arbitrary objects in challenges such as out-of-  Customizing effective data augmentation for visual tracking. To unleash the full potential of the Siamese network, we customize several data augmentation strategies for training. Except the common translation, scale variations and illumination changes, we observe that the motion pattern can be easily modeled by the shallow layers in the network. We explicitly introduce motion blur in the data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distractor-aware Incremental Learning</head><p>The training strategy in the last subsection can significantly improve the discrimination power on the offline training process. However, it is still hard to distinguish two objects with the similar attributes like <ref type="figure" target="#fig_4">Fig. 3a</ref>. SiamFC and SiamRPN use a cosine window to suppress the distractors. In this way, the performance is not guaranteed when the motion of objects are messy. Most existing Siamese network based approaches provide inferior performance when encountering with fast motion or background clutter. In summary, the potential flaw is mainly due to the misalignment of the general representation domain and the specifical target domains. In this section, we propose a distractor-aware module to effectively transfer the general representation to the video domain. The Siamese tracker learns a similarity metric f (z, x) to compare an exemplar image z to a candidate image x in the embedding space ϕ:</p><formula xml:id="formula_0">f (z, x) = ϕ(z) ϕ(x) + b · 1 (1)</formula><p>where denotes cross correlation between two feature maps, b · 1 denotes a bias which is equated in every location. The most similar object of the exemplar will be selected as the target.</p><p>To make full use of the label information, we integrate the hard negative samples (distractors) in context of the target into the similarity metric. In DaSiamRPN, the Non Maximum Suppression (NMS) is adopted to select the potential distractors d i in each frames, and then we collect a distractor set</p><formula xml:id="formula_1">D := {∀d i ∈ D, f (z, d i ) &gt; h ∩ d i = z t },</formula><p>where h is the predefined threshold, z t is the selected target in frame t and the number of this set |D| = n. Specifically, we get 17 * 17 * 5 proposals in each frame at first, and then we use NMS to reduce redundant candidates. The proposal with highest score will be selected as the target z t . For the remaining, the proposals with scores greater than a threshold are selected as distractors.</p><p>After that, we introduce a novel distractor-aware objective function to rerank the proposals P which have top-k similarities with the exemplar. The final selected object is denoted as q:</p><formula xml:id="formula_2">q = argmax p k ∈P f (z, p k ) −α n i=1 α i f (d i , p k ) n i=1 α i (2)</formula><p>the weight factorα control the influence of the distractor learning, the weight factor α i is used to control the influence for each distractor d i . It is worth noting that the computational complexity and memory usage increase n times by a direct calculation. Since cross correlation operation in the Equation <ref type="formula">(1)</ref> is a linear operator, we utilize this property to speed up the distractor-aware objective:</p><formula xml:id="formula_3">q = argmax p k ∈P (ϕ(z) −α n i=1 α i ϕ(d i ) n i=1 α i ) ϕ(p k )<label>(3)</label></formula><p>it enables the tracker run in the comparable speed in comparisons with SiamRPN. This associative law also inspires us to incrementally learn the target templates and distractor templates with a learning rate β t :</p><formula xml:id="formula_4">q T +1 = argmax p k ∈P ( T t=1 β t ϕ(z t ) T t=1 β t − T t=1 β tα n i=1 α i ϕ(d i,t ) T t=1 β t n i=1 α i ) ϕ(p k )<label>(4)</label></formula><p>This distractor-aware tracker can adapt the existing similarity metric (general) to a similarity metric for a new domain (specific). The weight factor α i can be viewed as the dual variables with sparse regularization, and the exemplars and distractors can be viewed as positive and negative samples in correlation filters. Actually, an online classifier is modeled in our framework. So the adopted classifier is expected to perform better than these only use general similarity metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">DaSiamRPN for Long-term Tracking</head><p>In this section, the DaSiamRPN framework is extended for long-term tracking.</p><p>Besides the challenging situations in short-term tracking, severe out-of-view and full occlusion introduce extra challenges in long-term tracking, which are shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. The search region in short-term tracking (SiamRPN) can not cover the target when it reappears, thus failing to track the following frames. We propose a simple yet effective switch method between short-term tracking phase and failure  cases. In failure cases, an iterative local-to-global search strategy is designed to re-detect the target. In order to perform switches, we need to identify the beginning and the end of failed tracking. Since the distractor-aware training and inference enable highquality detection score, it can be adopted to indicate the quality of tracking results. <ref type="figure" target="#fig_5">Fig. 4</ref> shows the detection scores and according tracking overlaps in SiamRPN and DaSiamRPN. The detection scores of SiamRPN are not indicative, which can be still high even in out-of-view and full occlusion. That is to say, SiamRPN tends to find an arbitrary objectness in these challenges which causes drift in tracking. In DaSiamRPN, detection scores successfully indicate status of the tracking phase.</p><p>During failure cases, we gradually increase the search region by local-toglobal strategy. Specifically, the size of search region is iteratively growing with a constant step when failed tracking is indicated. As shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, the localto-global search region covers the target to recover the normal tracking. It is worth noting that our tracker employs bounding box regression to detect the target, so the time-consuming image pyramids strategy can be discarded. In experiments, the proposed DaSiamRPN can perform at 110 FPS on long-term tracking benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Experiments are performed on extensive challenging tracking datasets, including VOT2015 <ref type="bibr" target="#b12">[13]</ref>, VOT2016 <ref type="bibr" target="#b13">[14]</ref> and VOT2017 <ref type="bibr" target="#b11">[12]</ref>  <ref type="formula">(2)</ref> is set to 0.5, α i is set to 1 for each distractor, and the incremental learning factor β t in Equation <ref type="formula" target="#formula_4">(4)</ref> is set to</p><formula xml:id="formula_5">t−1 i=0 ( η 1−η ) i , where η = 0.01.</formula><p>In the long-term tracking, we find that one step iteration of local-to-global is sufficient. Specifically, the sizes of the search region in short-term phase and defined failure cases are set to 255 and 767, respectively. The thresholds to enter and leave failure cases are set to 0.8 and 0.95. Our experiments are implemented using PyTorch on a PC with an Intel i7, 48G RAM, NVIDIA TITAN X. The proposed tracker can perform at 160 FPS on short-term benchmarks and 110 FPS on long-term benchmarks. The code and experimental results are available at https://github. com/foolwood/DaSiamRPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">State-of-the-art Comparisons on VOT Datasets</head><p>In this section the latest version of the Visual Object Tracking toolkit (vot2017challenge) is used. The toolkit applies a reset-based methodology. Whenever a failure (zero overlap with the ground truth) is detected, the tracker is reinitialized five frames after the failure. The performance is measured in terms of accuracy (A), robustness (R), and expected average overlap (EAO). In addition, VOT2017 also introduces a real-time experiment. We report all these metrics compared with a number of the latest state-of-the-art trackers on VOT2015, VOT2016 and VOT2017.</p><p>The EAO curve evaluated on VOT2016 is presented in <ref type="figure">Fig. 5a</ref> and 70 other state-of-the-art trackers are compared. The EAO of our baseline tracker SiamRPN on VOT2016 is 0.3441, which already outperforms most of state-of-the-arts. However, there is still a gap compared with the top-ranked tracker ECO (0.375), which improves continuous convolution operators on multi-level feature maps. Most remarkably, the proposed DaSiamRPN obtains a EAO of 0.411, outperforming state-of-the-arts by relative 9.6%. Furthermore, our tracker runs at stateof-the-art speed with 160FPS, which is 500× faster than C-COT and 20× faster than ECO.</p><p>For the evaluation on VOT2017, <ref type="figure">Fig. 5b</ref> reports the results of ours against 51 other state-of-the-art trackers with respect to the EAO score. DaSiamRPN ranks first with an EAO score of 0.326. Among the top 5 trackers, CFWCR, CFCF, ECO, and Gnet apply continuous convolution operator as the baseline approach. The top performer LSART [30] decomposes the target into patches and applies a weighted combination of patch-wise similarities into a kernelized ridge regression. While our method is conceptually much simpler, powerful and is also easy to follow. <ref type="figure">Fig. 5b</ref> also reveals the EAO values in the real-time experiment denoted by red points. Our tracker obviously is the top-performer with a real-time EAO of 0.326 and outperforms the latest state-of-the-art real-time tracker CSRDCF++ by relative 53.8%. <ref type="table">Table 1</ref> shows accuracy (A) and robustness (R), as well as expected average overlap (EAO) on VOT2015, VOT2016 and VOT2017. The baseline approach SiamRPN can process an astounding 200 frames per second while still getting an comparable performance with the state-of-the-arts. We find the performance gains of SiamRPN are mainly due to their accurate multi-anchors regression mechanism. We propose the distractor-aware module to improve the robustness, which can make our tracker much more harmonious. As a result, our approach, with the EAO of 0.446, 0.411 and 0.326 on three benchmarks, outperforms all the existing trackers by a large margin. We believe that the consistent improvements demonstrate that our approach makes real contributions by both the training process and online inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">State-of-the-art Comparisons on UAV Datasets</head><p>The UAV [22] videos are captured from low-altitude unmanned aerial vehicles. The dataset contains a long-term evaluation subset UAV20L and a short-term evaluation subset UAV123. The evaluation is based on two metrics: precision plot and success plot.</p><p>Results on UAV20L UAV20L is a long-term tracking benchmark that contains 20 sequences with average sequence length 2934 frames. Besides the challenging <ref type="table">Table 1</ref>: Performance comparisons on public short-term benchmarks. OP: mean overlap precision at the threshold of 0.5; DP: mean distance precision of 20 pixels; EAO: expected average overlap, and mean speed (FPS). The red bold fonts and blue italic fonts indicate the best and the second best performance.   The results including success plots and precision plots are illustrated in <ref type="figure" target="#fig_7">Fig. 6</ref>. It clearly illustrates that our algorithm, denoted by DaSiamRPN, outperforms the state-of-the-art trackers significantly in both measures. In the success plot, our approach obtains an AUC score of 0.617, significantly outperforming stateof-the-art short-term trackers SiamRPN [16] and ECO <ref type="bibr" target="#b2">[3]</ref>. The improvement   <ref type="bibr" target="#b8">[9]</ref> and TLD <ref type="bibr" target="#b9">[10]</ref> which are qualified to perform long-term tracking, the proposed DaSiamRPN outperforms these trackers by relative 45.8%, 87.5% and 213.2%. In the precision plot, our approach obtains a score of 0.838, outperforming state-of-the-art long-term tracker (PTAV <ref type="bibr" target="#b5">[6]</ref>) and short-term tracker (SiamRPN [16]) by relative 34.3% and 35.8%, respectively. The excellent performance of DaSiamRPN in this long-term tracking dataset can be attributed to the distractor-aware features and local-to-global search strategy.</p><p>For detailed performance analysis, we also report the results on various challenge attributes in UAV20L, i.e. full occlusion, out-of-view, background clutter and partial occlusion. <ref type="figure" target="#fig_0">Fig. 12</ref>   <ref type="figure" target="#fig_7">Fig. 6</ref> illustrates the precision and success plots of the compared trackers. The proposed DaSiamRPN approach outperforms all the other trackers in terms of success and precision scores. Specifically, our method achieves a success score of 0.586, which outperforms the SiamRPN (0.527) and ECO (0.525) method with a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">State-of-the-Art Comparisons on OTB Datasets</head><p>We evaluate the proposed algorithms with numerous fast and state-of-the-art trackers including SiamFC <ref type="bibr">[</ref>  <ref type="bibr" target="#b2">[3]</ref>, and the baseline tracker SiamRPN <ref type="bibr">[16]</ref>. All the trackers are initialized with the ground-truth object state in the first frame. Mean overlap precision (OP) and mean distance precision (DP) are reported in <ref type="table">Table 1</ref>.</p><p>Among the real-time trackers, SiamFC and CFNet are latest Siamese network based trackers while the accuracies is still left far behind the state-of-the-art BACF and ECO-HC with HOG features. The proposed DaSiamRPN tracker outperforms all these trackers by a large margin on both the accuracy and speed. For state-of-the-art comparisons on OTB, MDNet, trained on visual tracking datasets, performs the best against the other trackers at a speed of 1 FPS. C-COT and ECO achieve state-of-the-art performance, but their tracking speeds are not fast enough for real-time applications. The baseline tracker SiamRPN obtains an OP score of 81.9%, which is slightly less accurate than CCOT. The bottleneck of SiamRPN is its inferior robust performance. Since the distractoraware mechanisms in both training and inference focus on improving the robustness, the proposed DaSiamRPN tracker achieves 3.0% improvement on DP and performs best OP score of 86.5% on OTB2015.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Analyses</head><p>To verify the contributions of each component in our algorithm, we implement and evaluate four variations of our approach. Analyses results include EAO on VOT2016 <ref type="bibr" target="#b13">[14]</ref> and AUC on UAV20L <ref type="bibr">[22]</ref>.</p><p>As shown in <ref type="table" target="#tab_8">Table 2</ref>, SiamRPN is our baseline algorithm. In VOT2016, the EAO criterion increases to 0.368 from 0.344 when detection data is added in training. Similarly, when negative pairs and distractor-aware learning are adopted in training and inference, both the performance increases by near 2%. In UAV20L, detection data, negative pairs in training and distractor-aware inference gain the performance by 1%-2%. The AUC criterion increases to 61.7% from 49.8% when long-term tracking module is adopted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a distractor-aware Siamese framework for accurate and long-term tracking. During offline training, a distractor-aware feature learning scheme is proposed, which can significantly boost the discriminative power of the networks. During inference, a novel distractor-aware module is designed, effectively transferring the general embedding to the current video domain. In addition, we extend the proposed approach for long-term tracking by introducing a simple yet effective local-to-global search strategy. The proposed tracker obtains state-of-the-art accuracy in comprehensive experiments of short-term and long-term visual tracking benchmarks, while the overall system speed is still far from being real-time. 6 Supplementary Material</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Visualization of Siamese Network tracker</head><p>In this section, we provide further visualization of the response maps on VOT dataset. <ref type="figure" target="#fig_9">Fig. 8</ref> shows the heatmaps of SiamFC, SiamRPN, SiamRPN+, and DaSiamRPN. The proposed DaSiamRPN generates more discriminative response maps across all different videos. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Detailed results on VOT</head><p>In this section, detailed results on VOT2015, VOT2016 and VOT2017 are provided as shown in <ref type="figure" target="#fig_0">Fig. 9, Fig. 10</ref> and <ref type="figure" target="#fig_0">Fig. 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Additional results with attributes on UAV20L</head><p>In this section, we report additional results on the UAV20L with 12 different attributes, including out-of-view, background clutter, illumination variation, viewpoint change, camera motion, similar object, scale variation, aspect ratio change,      low resolution, fast motion, full occlusion, partial occlusion. The performance is ranked by area under curve (AUC) of success plot. As shown in <ref type="figure" target="#fig_0">Fig. 12</ref>, the proposed DaSiamRPN effectively handles these challenges and achieves leading performance in all attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Additional results with attributes on UAV123</head><p>Additional results on the UAV123 with 12 different attributes are reported in this section. As shown in <ref type="figure" target="#fig_0">Fig. 13</ref>, the performance is ranked by area under curve (AUC) of success plot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Detailed results on OTB</head><p>In this section, detailed results on OTB are provided. <ref type="figure" target="#fig_0">Fig. 14 and Fig. 15</ref> show the success plots for all 11 attributes on OTB2013 and OTB2015, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">Speed Analyses</head><p>In the main paper, we report the tracking speed of our proposed DaSiamRPN on Titan X (160 FPS). We now provide some tracking time analyses additionally in <ref type="table" target="#tab_11">Table 3</ref>. At the tracking stage, the computational bottleneck of our system is the cost of convolution operations existing in the forward process. However, our system is still more efficient than the traditional deep learning based trackers. The tracking speeds are reported on GTX 1060, GTX 1080, Titan X and Titan Xp.         </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Visualization of the response heatmaps of Siamese network trackers. (a) shows the search images. (b-e) show the heatmaps that produced by SiamFC, SiamRPN, SiamRPN+ (trained with distractors) and the DaSiamRPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>(a) positive pairs generated from detection datasets through augmenting still images. (b) negative pairs from the same category. (c) negative pairs from different categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Illustrations of our proposed Distractor-aware Siamese Region Proposal Networks (DaSiamRPN). The target and the background information are fully utilized in DaSiamRPN, which can suppress the influence of distractor during tracking. view and full occlusion, while negative pairs from the same categories make the tracker focused on fine-grained representation. The negative examples are shown in Fig. 2(b) and Fig. 2(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>The tracking results of video person7 in out-of-view challenge. First row: tracking snapshots of SiamRPN and DaSiamRPN. Second row: detection scores and according overlaps of the two methods. The overlaps are defined as intersection-over-union (IOU) between tracking results and ground truth. Red: ground truth. Green: tracking box. Blue: Search region box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>9 78.4 0.57 1.39 0.300 0.54 0.38 0.295 0.52 0.69 0.169 80 CSRDCF 70.7 78.7 0.56 0.86 0.320 0.51 0.24 0.338 0.49 0.36 0.256 13 BACF 76.7 81.5 0.59 14 90.9 0.60 0.69 0.378 0.54 0.34 0.257 ---1 C-COT 82.0 89.8 0.54 0.82 0.303 0.54 0.24 0.331 0.49 0.32 0.267 0.3 ECO 84.9 91.0 ---0.55 0.20 0.375 0.48 0.27 0.280 8 SiamRPN 81.9 85.0 0.58 1.13 0.349 0.56 0.26 0.344 0.49 0.46 0.244 200 Ours 86.5 88.0 0.63 0.66 0.446 0.61 0.22 0.411 0.56 0.34 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 :</head><label>6</label><figDesc>Success and precision plots on UAV [22] dataset. First and second subfigures are results of UAV20L, third and last sub-figures are results of UAV123. situations in short-term tracking, severe out-of-view and full occlusion introduce extra challenges. In this experiment, the proposed method is compared against recent trackers in [22]. Besides, ECO [3] (state-of-the-art short-term tracker), PTAV [6] (state-of-the-art long-term tracker), SiamRPN [16] (the baseline), SiamFC [2] and CFNet [33] (representative Siamese trackers) are added for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>plots of OPE − Full Occlusion<ref type="bibr" target="#b8">(9)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 :</head><label>8</label><figDesc>Comparisons with general Siamese network trackers. From left to right: original images; response maps from SiamFC; SiamRPN; SiamRPN+(our method); and DaSiamRPN(our method).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 9 :</head><label>9</label><figDesc>Expected overlap curves and ranking plot for DaSiamRPN on VOT2015.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 10 :</head><label>10</label><figDesc>Expected overlap curves and ranking plot for DaSiamRPN on VOT2016.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 11 :</head><label>11</label><figDesc>Expected overlap curves and ranking plot for DaSiamRPN on VOT2017.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>plots of OPE − Out−of−View<ref type="bibr" target="#b12">(13)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 13 :</head><label>13</label><figDesc>Success plots with attributes on UAV123. Best viewed on color display.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Fig. 14 :</head><label>14</label><figDesc>(k) MB on OTB-2013 The success plots on OTB-2013 for eleven challenge attributes: in-plain rotation, out-of-plane rotation, scale variation, out of view, occlusion, background clutter, deformation, illumination variation, low resolution, fast motion and motion blur.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>plots of OPE -in-plane rotation (51)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>(k) MB on OTB-2015Fig. 15: The success plots on OTB-2015 for eleven challenge attributes: in-plain rotation, out-of-plane rotation, scale variation, out of view, occlusion, background clutter, deformation, illumination variation, low resolution, fast motion and motion blur.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>6. 7 Fig. 16 :</head><label>716</label><figDesc>Qualitative ResultsTo visualize the superiority of the proposed framework, we show examples of the DaSiamRPN results compared to recent trackers (SiamRPN, SiamFC and PTAV) on challenging sample videos. As shown inFig. 16, the proposed DaSi-amRPN can handle the challenges while SiamRPN and SiamFC tend to drift to distractor. PTAV adopts long-term component, but fails to track in the second and last videos. The superiority performance of our algorithm can be attributed to the design of distractor-aware Siamese networks. The qualitative results of the DaSiamRPN and compared trackers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>interprets the correlation filters as a differentiable layer in a Siamese tracking framework, thus achieving an end-to-end representation learning. But the performance improvement is limited compared with SiamFC. FlowTrack [40] exploits motion information in Siamese architecture to improve the feature representation and the tracking accuracy. It is worth noting that CFNet and FlowTrack can efficiently online update the tracking model. Recently, SiamRPN [16] formulates the tracking as a one-shot local detection task by introducing a region proposal network after a Siamese network, which is end-to-end trained off-line with large-scale image pairs. Features for Tracking. Visual features play a significant role in computer vision tasks including visual tracking. Possegger et.al [26] propose a distractoraware model term to suppress visually distracting regions, while the color histograms features used in their framework are less robust than the deep features. DLT [35] is the seminal deep learning tracker which uses a multi-layer autoencoder network. The feature is pretrained on part of the 80M Tiny Image dataset [32] in an unsupervised fashion. Wang et al. [34] learn a two-layer neural network on a video repository, where temporally slowness constraints are imposed for feature learning. DeepTrack [17] learns two-layer CNN classifiers from binary samples and does not require a pre-training procedure. UCT [39]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, each with 60 videos, UAV20L[22]   with 20 long-term videos, UAV123 [22] with 123 videos and OTB2015[38]  with The parameters of the first three convolution layers are fixed and only the last two convolution layers are fine-tuned. There are totally 50 epoches performed and the learning rate is decreased in log space from 10 −2 to 10 −4 . We extract image pairs from VID [28] and Youtube-BB [27] by choosing frames with interval less than 100 and performing crop procedure as described in Section 3.2. In ImageNet Detection [28] and COCO Detection [18] datasets, image pairs are generated for training by augmenting still images. To handle the gray videos in benchmarks, 25% of the pairs are converted to grayscale during training. The translation is randomly performed within 12 pixels, and the range of random resize varies from 0.85 to 1.15.During inference phase, the distractor factorα in Equation</figDesc><table><row><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN</cell><cell></cell></row><row><cell>0.25 0.35 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamFC</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.15</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>66</cell><cell>61</cell><cell>56</cell><cell>51</cell><cell>46</cell><cell>41</cell><cell>36</cell><cell>31</cell><cell>26</cell><cell>21</cell><cell>16</cell><cell>11</cell><cell>6</cell><cell>1</cell><cell>49</cell><cell>45</cell><cell>41</cell><cell>37</cell><cell>33</cell><cell>29</cell><cell>25</cell><cell>21</cell><cell>17</cell><cell>13</cell><cell>9</cell><cell>5</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="11">(a) EAO on VOT2016</cell><cell></cell><cell></cell><cell></cell><cell cols="10">(b) EAO on VOT2017</cell></row><row><cell cols="27">Fig. 5: Expected average overlap plot for VOT2016 (a) and VOT2017 (b).</cell></row><row><cell cols="27">100 videos. All the tracking results are provided by official implementations to</cell></row><row><cell cols="12">ensure a fair comparison.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="14">4.1 Experimental Details</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="27">The modified AlexNet [15] pretrained using ImageNet [28] is used as described</cell></row><row><cell cols="7">in SiamRPN [16].</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>DaSiamRPN [0.617] SiamRPN [0.454] ECO [0.435] PTAV [0.423] SiamFC [0.399] CFNet [0.349] SRDCF [0.343] MUSTER [0.329] SAMF [0.317] MEEM [0.295] DSST [0.270] DCF [0.208] KCF [0.198] TLD [0.197] CSK [0.194].624] SiamRPN [0.617] SiamFC [0.613] ECO [0.604] CFNet [0.570] MUSTER [0.514] SRDCF [0.507] MEEM [0.482] DSST [0.459] SAMF [0.457] TLD [0.336] DCF [0.321] KCF [0.311] CSK [0.309]DaSiamRPN [0.586] SiamRPN [0.527] ECO [0.525] ECO−HC [0.506] SiamFC [0.498] SRDCF [0.464] CFNet [0.436] SAMF [0.396] MEEM [0.392] MUSTER [0.391] DSST [0.356] DCF [0.332] KCF [0.331] CSK [0.311] TLD [0.283]DaSiamRPN [0.796] SiamRPN [0.748] ECO [0.741] SiamFC [0.726] ECO−HC [0.725] SRDCF [0.676] CFNet [0.651] MEEM [0.627] SAMF [0.592] MUSTER [0.591] DSST [0.586] DCF [0.526] KCF [0.523] CSK [0.488] TLD [0.439]</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Precision plots of OPE</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Success plots of OPE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Precision plots of OPE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Precision</cell><cell>0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35 DaSiamRPN [0.838] 40 45 PTAV [00 50 0 0.1 0.2 0.3 0.6 0.7 0.4 0.5 Success rate</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>Precision</cell><cell>0 0 0.1 0.2 0.3 0.6 0.7 0.4 0.5</cell><cell>5</cell><cell>10</cell><cell>15</cell><cell>20</cell><cell>25</cell><cell>30</cell><cell>35</cell><cell>40</cell><cell>45</cell><cell>50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Location error threshold</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Location error threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>DaSiamRPN [0.486] PTAV [0.357] ECO [0.238] SiamFC [0.238] CFNet [0.220] MUSTER [0.200] SiamRPN [0.192] SAMF [0.174] SRDCF [0.170] MEEM [0.163] DSST [0.159] TLD [0.154] KCF [0.115] DCF [0.110] CSK [0.082].389] SiamFC [0.386] SRDCF [0.329] CFNet [0.322] MUSTER [0.309] SAMF [0.262] MEEM [0.253] DSST [0.241] TLD [0.212] CSK [0.209] KCF [0.191] DCF [0.188].409] SiamFC [0.366] CFNet [0.328] SRDCF [0.320] MUSTER [0.305] SAMF [0.288] MEEM [0.274] DSST [0.249] TLD [0.201] KCF [0.192] DCF [0.187] CSK [0.182]</head><label></label><figDesc>Fig. 7: Success plots with attributes on UAV20L. Best viewed on color display. ranges are relative 35.9% and 41.8%, respectively. Compared with PTAV<ref type="bibr" target="#b5">[6]</ref>, MUSTer</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="6">Success plots of OPE − Out−of−View (13)</cell><cell></cell><cell></cell><cell cols="6">Success plots of OPE − Background Clutter (5)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">Success plots of OPE − Partial Occlusion (18)</cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DaSiamRPN [0.592]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DaSiamRPN [0.513]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DaSiamRPN [0.595]</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SiamRPN [0.479] ECO [0.412]</cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>PTAV [0.435] ECO [0.244]</cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">SiamRPN [0.423] PTAV [0.415]</cell></row><row><cell>Success rate</cell><cell>0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell cols="2">0.7 PTAV [00 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 Success rate 0.6</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7 MUSTER [0.230] 0.8 0.9 MEEM [0.212] DSST [0.211] SAMF [0.201] SRDCF [0.156] KCF [0.148] DCF [0.140] TLD [0.111] SiamRPN [0.104] CSK [0.074] SiamFC [0.239] CFNet [0.243]</cell><cell>1</cell><cell>Success rate</cell><cell>0 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7 ECO [00.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>demonstrates that our tracker effectively handles these challenging situations while other trackers obtain lower scores. Specially, in full occlusion and background clutter attributes, the proposed DaSiamRPN outperforms SiamRPN [16] by relative 153.1% and 393.2%.</figDesc><table><row><cell>Results on UAV123 UAV123 dataset includes 123 sequences with average</cell></row><row><cell>sequence length of 915 frames. Besides the recent trackers in [22], ECO [3],</cell></row><row><cell>PTAV [6], SiamRPN [16], SiamFC [2], CFNet [33] are added for comparison.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2 :</head><label>2</label><figDesc>Ablation analyses of our algorithm on VOT2016<ref type="bibr" target="#b13">[14]</ref> and UAV20L[22]    </figDesc><table><row><cell>Component positive pairs in detection data? semantic negative pairs? distractor-aware updating? long-term tracking module?</cell><cell>SiamRPN</cell><cell cols="2">DaSiamRPN ! ! ! ! ! ! ! ! ! !</cell></row><row><cell>EAO in VOT2016</cell><cell>0.344</cell><cell>0.368 0.389 0.411</cell><cell>-</cell></row><row><cell>AUC in UAV20L(%)</cell><cell>45.4</cell><cell cols="2">47.2 48.6 49.8 61.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>15. Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with deep convolutional neural networks. In: Proceedings of theAdvances in Neural Information Processing Systems. pp. 1097-1105 (2012) 16. Li, B., Yan, J., Wu, W., Zhu, Z., Hu, X.: High performance visual tracking with siamese region proposal network. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 8971-8980 (2018) 17. Li, H., Li, Y., Porikli, F.: Deeptrack: Learning discriminative feature representations online for robust visual tracking. IEEE Transactions on Image Processing Mueller, M., Smith, N., Ghanem, B.: A benchmark and simulator for uav tracking. Lu, H., Yang, M.H.: Learning spatial-aware regressions for visual tracking. Torralba, A., Fergus, R., Freeman, W.T.: 80 million tiny images: A large data set for nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence 30(11), 1958-1970 (2008) 33. Valmadre, J., Bertinetto, L., Henriques, J.F., Vedaldi, A., Torr, P.H.S.: End-to-end representation learning for correlation filter based tracking. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2017) 34. Wang, L., Liu, T., Wang, G., Chan, K.L., Yang, Q.Zhu, Z., Huang, G., Zou, W., Du, D., Huang, C.: Uct: Learning unified convolutional networks for real-time visual tracking. In: Proceedings of the IEEE International Conference on Computer Vision Workshops (Oct 2017) 40. Zhu, Z., Wu, W., Zou, W., Yan, J.: End-to-end flow correlation tracking with spatial-temporal attention. arXiv preprint arXiv:1711.01124 (2017)</figDesc><table><row><cell>25(4), 1834-1848 (2016) 18. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Proceedings of the European Conference on Computer Vision. pp. 740-755. Springer (2014) 19. Lukezic, A., Vojir, T., Zajc, L.C., Matas, J., Kristan, M.: Discriminative correlation filter with channel and spatial reliability. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2017) 20. Ma, C., Yang, X., Zhang, C., Yang, M.H.: Long-term correlation tracking. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 5388-5396 (2015) 21. Maresca, M.E., Petrosino, A.: Matrioska: A multi-level approach to fast tracking by learning. In: Proceedings of the International Conference on Image Analysis and Processing. pp. 419-428. Springer (2013) 22. 445-461. Springer (2016) 23. Nam, H., Han, B.: Learning multi-domain convolutional neural networks for visual tracking. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (June 2016) 24. Nebehay, G., Pflugfelder, R.: Clustering of static-adaptive correspondences for de-formable object tracking. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2784-2791 (2015) 25. Pernici, F., Del Bimbo, A.: Object tracking by oversampling local features. IEEE Transactions on Pattern Analysis and Machine Intelligence 36(12), 2538-2551 (2014) 26. Possegger, H., Mauthner, T., Bischof, H.: In defense of color-based model-free tracking. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 2113-2120 (2015) 27. Real, E., Shlens, J., Mazzocchi, S., Pan, X., Vanhoucke, V.: Youtube-boundingboxes: A large high-precision human-annotated data set for object de-tection in video. arXiv preprint arXiv:1702.00824 (2017) 28. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., Fei-Fei, L.: ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) 115(3), 211-252 (2015) 29. Song, Y., Ma, C., Gong, L., Zhang, J., Lau, R., Yang, M.H.: Crest: Convolutional residual learning for visual tracking. In: Proceedings of the IEEE International Conference on Computer Vision (2017) hierarchical features. IEEE Transactions on Image Processing 24(4), 1424-1435 (2015) 35. Wang, N., Yeung, D.Y.: Learning a deep compact image representation for vi-sual tracking. In: Proceedings of the Advances in Neural Information Processing Systems. pp. 809-817 (2013) 36. Wang, Q., Teng, Z., Xing, J., Gao, J., Hu, W., Maybank, S.: Learning attentions: Residual attentional siamese network for high performance online visual tracking. In: The IEEE Conference on Computer Vision and Pattern Recognition (June 2018) 37. Wang, Q., Zhang, M., Xing, J., Gao, J., Hu, W., Maybank, S.: Do not lose the details: reinforced representation learning for high performance visual tracking. In: 27th International Joint Conference on Artificial Intelligence 38. Wu, Y., Lim, J., Yang, M.H.: Object tracking benchmark. IEEE Transactions on Pattern Analysis and Machine Intelligence 37(9), 1834-1848 (2015) 30. Sun, C., : Video tracking using learned 39.</cell></row></table><note>In: Proceedings of the European Conference on Computer Vision. pp.arXiv preprint arXiv:1706.07457 (2017) 31. Tao, R., Gavves, E., Smeulders, A.W.M.: Siamese instance search for tracking. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1420-1429 (2016) 32.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>DaSiamRPN [0.592] SiamRPN [0.479] ECO [0.412] PTAV [0.389] SiamFC [0.386] SRDCF [0.329] CFNet [0.322] MUSTER [0.309] SAMF [0.262] MEEM [0.253] DSST [0.241] TLD [0.212] CSK [0.209] KCF [0.191] DCF [0.188]DaSiamRPN [0.610] SiamRPN [0.574] ECO [0.506] SiamFC [0.503] ECO−HC [0.496] SRDCF [0.439] CFNet [0.401] MUSTER [0.385] MEEM [0.385] SAMF [0.379] DSST [0.322] DCF [0.314] KCF [0.311] CSK [0.289] TLD [0.272]DaSiamRPN [0.544] ECO [0.518] SiamRPN [0.516] ECO−HC [0.488] SiamFC [0.473] SRDCF [0.452] CFNet [0.409] SAMF [0.407] MUSTER [0.407] MEEM [0.397] DSST [0.362] KCF [0.342] DCF [0.341] TLD [0.300] CSK [0.294]DaSiamRPN [0.564] SiamRPN [0.531] ECO [0.496] ECO−HC [0.476] SiamFC [0.473] SRDCF [0.435] CFNet [0.403] MEEM [0.360] SAMF [0.359] MUSTER [0.358] DSST [0.312] DCF [0.293] KCF [0.291] CSK [0.269] TLD [0.264]DaSiamRPN [0.548] SiamRPN [0.500] ECO [0.445] ECO−HC [0.440] SiamFC [0.436] SRDCF [0.394] CFNet [0.368] MEEM [0.341] MUSTER [0.327] SAMF [0.325] DSST [0.286] DCF [0.269] KCF [0.267] TLD [0.248] CSK [0.243]</head><label></label><figDesc>Fig. 12: Success plots with attributes on UAV20L. Best viewed on color display.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell cols="7">Success plots of OPE − Background Clutter (5)</cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell cols="7">Success plots of OPE − Illumination Variation (8)</cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell>Success plots of OPE − Viewpoint Change (13)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN [0.513]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN [0.559]</cell><cell></cell><cell></cell><cell></cell><cell>DaSiamRPN [0.632]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PTAV [0.435] ECO [0.244]</cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PTAV [0.430] ECO [0.430]</cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell>SiamRPN [0.450] PTAV [0.418]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CFNet [0.243] SiamFC [0.239]</cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamFC [0.374] SiamRPN [0.343]</cell><cell></cell><cell>0.7</cell><cell></cell><cell>ECO [0.404] SiamFC [0.358]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MUSTER [0.230]</cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MEEM [0.334]</cell><cell></cell><cell>0.6</cell><cell></cell><cell>MUSTER [0.318]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Success rate</cell><cell>0.4 0.5 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">MEEM [0.212] DSST [0.211] SAMF [0.201] SRDCF [0.156] KCF [0.148] DCF [0.140]</cell><cell>Success rate</cell><cell>0.4 0.5 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CFNet [0.329] SAMF [0.323] SRDCF [0.295] DSST [0.288] MUSTER [0.242] DCF [0.216]</cell><cell>Success rate</cell><cell>0.4 0.5 0.3</cell><cell></cell><cell>SRDCF [0.303] CFNet [0.303] SAMF [0.276] MEEM [0.243] DSST [0.206] TLD [0.188]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TLD [0.111]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">KCF [0.184]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DCF [0.171]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamRPN [0.104] CSK [0.074]</cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TLD [0.167] CSK [0.153]</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell>CSK [0.157] KCF [0.151]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0 0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>0 0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>0 0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Overlap threshold</cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell cols="7">Success plots of OPE − Camera Motion (19)</cell><cell></cell><cell></cell><cell>1</cell><cell></cell><cell cols="7">Success plots of OPE − Similar Object (12)</cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell><cell cols="7">Success plots of OPE − Scale Variation (19)</cell><cell></cell><cell></cell><cell>0.9</cell><cell cols="2">Success plots of OPE − Aspect Ratio Change (16)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN [0.599]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN [0.648]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN [0.605]</cell><cell></cell><cell></cell><cell></cell><cell>DaSiamRPN [0.583]</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamRPN [0.436]</cell><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamRPN [0.532]</cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamRPN [0.443]</cell><cell></cell><cell>0.8</cell><cell></cell><cell>PTAV [0.410]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PTAV [0.420]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ECO [0.455]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ECO [0.423]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SiamRPN [0.388]</cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ECO [0.420]</cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamFC [0.438]</cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PTAV [0.416]</cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell>ECO [0.369]</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamFC [0.384] CFNet [0.331]</cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">PTAV [0.426] SRDCF [0.397]</cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamFC [0.385] CFNet [0.338]</cell><cell></cell><cell>0.6</cell><cell></cell><cell>SiamFC [0.330] CFNet [0.285]</cell></row><row><cell>Success rate</cell><cell>0.4 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SRDCF [0.327] MUSTER [0.307] SAMF [0.294] MEEM [0.283] DSST [0.257]</cell><cell>Success rate</cell><cell>0.4 0.5 0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CFNet [0.369] MUSTER [0.342] SAMF [0.333] MEEM [0.302] DSST [0.274]</cell><cell>Success rate</cell><cell>0.4 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SRDCF [0.332] MUSTER [0.314] SAMF [0.298] MEEM [0.277] DSST [0.251]</cell><cell>Success rate</cell><cell>0.4 0.5</cell><cell></cell><cell>MUSTER [0.275] SRDCF [0.270] SAMF [0.251] MEEM [0.231] DSST [0.203]</cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TLD [0.202] DCF [0.197]</cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TLD [0.225] CSK [0.215]</cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">TLD [0.193] DCF [0.188]</cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell>TLD [0.196] DCF [0.158]</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CSK [0.188] KCF [0.186]</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DCF [0.211] KCF [0.193]</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">KCF [0.177] CSK [0.173]</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell>KCF [0.144] CSK [0.134]</cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell></row><row><cell></cell><cell>0 0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell cols="3">0.4 Overlap threshold 0.5 0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>0 0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell cols="3">0.4 Overlap threshold 0.5 0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>0 0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell cols="3">0.4 Overlap threshold 0.5 0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>0 0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4 Overlap threshold 0.5 0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">Success plots of OPE − Low Resolution (11)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Success plots of OPE − Fast Motion (7)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Success plots of OPE − Full Occlusion (9)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Success plots of OPE − Partial Occlusion (18)</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN [0.527]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN [0.500]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN [0.486]</cell><cell></cell><cell></cell><cell></cell><cell>DaSiamRPN [0.595]</cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">PTAV [0.390] SiamRPN [0.335]</cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamRPN [0.422] PTAV [0.349]</cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">PTAV [0.357] ECO [0.238]</cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell>SiamRPN [0.423] PTAV [0.415]</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ECO [0.283] MUSTER [0.278] CFNet [0.244]</cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ECO [0.302] SiamFC [0.264] TLD [0.235]</cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamFC [0.238] CFNet [0.220] MUSTER [0.200]</cell><cell></cell><cell>0.7 0.6</cell><cell></cell><cell>ECO [0.409] SiamFC [0.366] CFNet [0.328]</cell></row><row><cell>Success rate</cell><cell>0.3 0.4 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamFC [0.244] SRDCF [0.228] SAMF [0.212] MEEM [0.195] TLD [0.159] DSST [0.159]</cell><cell>Success rate</cell><cell>0.4 0.5 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CFNet [0.214] MUSTER [0.206] SRDCF [0.197] MEEM [0.166] SAMF [0.140] DSST [0.123]</cell><cell>Success rate</cell><cell>0.3 0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamRPN [0.192] SAMF [0.174] SRDCF [0.170] MEEM [0.163] DSST [0.159] TLD [0.154]</cell><cell>Success rate</cell><cell>0.4 0.5 0.3</cell><cell></cell><cell>SRDCF [0.320] MUSTER [0.305] SAMF [0.288] MEEM [0.274] DSST [0.249] TLD [0.201]</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">KCF [0.119] CSK [0.117] DCF [0.113]</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DCF [0.108] CSK [0.102] KCF [0.064]</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">KCF [0.115] DCF [0.110] CSK [0.082]</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell>KCF [0.192] DCF [0.187] CSK [0.182]</cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell></row><row><cell></cell><cell>0 0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>0 0</cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.8</cell><cell></cell><cell>1</cell><cell>0 0</cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.8</cell><cell></cell><cell>1</cell><cell>0 0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Overlap threshold</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">Success plots of OPE − Out−of−View (30)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Success plots of OPE − Background Clutter (21)</cell><cell></cell><cell></cell><cell></cell><cell cols="9">Success plots of OPE − Illumination Variation (31)</cell><cell></cell><cell></cell><cell></cell><cell>Success plots of OPE − Viewpoint Change (60)</cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.9</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN [0.529]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN [0.460]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN [0.550]</cell><cell></cell><cell></cell><cell></cell><cell>DaSiamRPN [0.590]</cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamRPN [0.473] SiamFC [0.448]</cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamRPN [0.391] ECO [0.388]</cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamRPN [0.496] ECO [0.459]</cell><cell></cell><cell>0.8</cell><cell></cell><cell>SiamRPN [0.547] ECO [0.473]</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ECO−HC [0.434] ECO [0.425] SRDCF [0.399]</cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ECO−HC [0.378] MUSTER [0.352] CFNet [0.349]</cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ECO−HC [0.434] SiamFC [0.398] SRDCF [0.395]</cell><cell></cell><cell>0.7 0.6</cell><cell></cell><cell>SiamFC [0.461] ECO−HC [0.435] SRDCF [0.410]</cell></row><row><cell>Success rate</cell><cell>0.3 0.4 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CFNet [0.377] MEEM [0.323] SAMF [0.312] MUSTER [0.304] DSST [0.289] CSK [0.266]</cell><cell>Success rate</cell><cell>0.3 0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamFC [0.336] SRDCF [0.321] DSST [0.314] MEEM [0.307] SAMF [0.282] KCF [0.272]</cell><cell>Success rate</cell><cell>0.4 0.5 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CFNet [0.388] MEEM [0.320] MUSTER [0.317] SAMF [0.309] DSST [0.307] DCF [0.282]</cell><cell>Success rate</cell><cell>0.4 0.5 0.3</cell><cell></cell><cell>CFNet [0.401] MUSTER [0.358] MEEM [0.349] SAMF [0.326] DSST [0.304] DCF [0.282]</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">KCF [0.257] DCF [0.256] TLD [0.251]</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CSK [0.271] DCF [0.262] TLD [0.186]</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">KCF [0.270] CSK [0.251] TLD [0.210]</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell>TLD [0.277] CSK [0.275] KCF [0.274]</cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell></row><row><cell></cell><cell>0 0</cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.8</cell><cell></cell><cell>1</cell><cell>0 0</cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.8</cell><cell></cell><cell>1</cell><cell>0 0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>0 0</cell><cell></cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Overlap threshold</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">Success plots of OPE − Camera Motion (70)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Success plots of OPE − Similar Object (39)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Success plots of OPE − Scale Variation (109)</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Success plots of OPE − Aspect Ratio Change (68)</cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Success rate</cell><cell>0.4 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Success rate</cell><cell>0.3 0.4 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Success rate</cell><cell>0.3 0.4 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Success rate</cell><cell>0.3 0.4 0.5</cell><cell></cell></row><row><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell></row><row><cell></cell><cell>0 0</cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.8</cell><cell></cell><cell>1</cell><cell>0 0</cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.8</cell><cell></cell><cell>1</cell><cell>0 0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>0 0</cell><cell></cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Overlap threshold</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="7">Success plots of OPE − Low Resolution (48)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Success plots of OPE − Fast Motion (28)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="7">Success plots of OPE − Full Occlusion (33)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Success plots of OPE − Partial Occlusion (73)</cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN [0.437]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN [0.536]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">DaSiamRPN [0.380]</cell><cell></cell><cell></cell><cell></cell><cell>DaSiamRPN [0.512]</cell></row><row><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamRPN [0.398] ECO [0.396]</cell><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamRPN [0.491] ECO [0.415]</cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamRPN [0.331] ECO−HC [0.309]</cell><cell></cell><cell>0.7</cell><cell></cell><cell>SiamRPN [0.472] ECO [0.456]</cell></row><row><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamFC [0.365] ECO−HC [0.349] SRDCF [0.310]</cell><cell></cell><cell>0.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">SiamFC [0.401] ECO−HC [0.363] SRDCF [0.340]</cell><cell></cell><cell>0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">ECO [0.308] SiamFC [0.298] SRDCF [0.262]</cell><cell></cell><cell>0.6</cell><cell></cell><cell>ECO−HC [0.446] SiamFC [0.414] SRDCF [0.396]</cell></row><row><cell>Success rate</cell><cell>0.3 0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CFNet [0.300] MEEM [0.278] MUSTER [0.270] DSST [0.228] SAMF [0.224] TLD [0.206]</cell><cell>Success rate</cell><cell>0.3 0.4 0.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CFNet [0.277] MEEM [0.260] MUSTER [0.251] SAMF [0.250] DCF [0.202] TLD [0.194]</cell><cell>Success rate</cell><cell>0.3 0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">CFNet [0.228] MEEM [0.226] MUSTER [0.226] SAMF [0.211] DSST [0.200] DCF [0.186]</cell><cell>Success rate</cell><cell>0.4 0.5 0.3</cell><cell></cell><cell>CFNet [0.365] MEEM [0.331] SAMF [0.326] MUSTER [0.312] DSST [0.306] KCF [0.282]</cell></row><row><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">KCF [0.180] DCF [0.179]</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DSST [0.186] KCF [0.184]</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">KCF [0.185] TLD [0.184]</cell><cell></cell><cell></cell><cell>0.2</cell><cell></cell><cell>DCF [0.278] CSK [0.243]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CSK [0.173]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CSK [0.168]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CSK [0.158]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TLD [0.228]</cell></row><row><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell></cell></row><row><cell></cell><cell>0 0</cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.8</cell><cell></cell><cell>1</cell><cell>0 0</cell><cell>0.1</cell><cell>0.2</cell><cell>0.3</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell><cell>1</cell><cell>0 0</cell><cell></cell><cell>0.2</cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.8</cell><cell></cell><cell>1</cell><cell>0 0</cell><cell></cell><cell>0.2</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Overlap threshold</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Overlap threshold</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 3 :</head><label>3</label><figDesc>Speed of the proposed tracker on different platforms.</figDesc><table><row><cell>Trackers</cell><cell cols="2">GTX 1060 GTX 1080 Titan X Titan Xp</cell></row><row><cell>SiamRPN</cell><cell>160 FPS</cell><cell>200 FPS 200 FPS 240 FPS</cell></row><row><cell cols="2">DaSiamRPN 130 FPS</cell><cell>170 FPS 160 FPS 190 FPS</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fullyconvolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshop</title>
		<meeting>the European Conference on Computer Vision Workshop</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<idno>65.1-65.11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="472" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Parallel tracking and verifying: A framework for real-time and high accuracy visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="749" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-store tracker (muster): A cognitive psychology inspired approach to object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="749" to="758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tracking-learning-detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1409" to="1422" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning background-aware correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiani</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2017 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the The IEEE International Conference on Computer Vision Workshop</title>
		<meeting>the The IEEE International Conference on Computer Vision Workshop</meeting>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshop</title>
		<meeting>the IEEE International Conference on Computer Vision Workshop</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="564" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2016 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision Workshop</title>
		<meeting>the European Conference on Computer Vision Workshop</meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="217" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
