<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijie</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huazhu</forename><surname>Fu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Dai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Information Engineering</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Inception Institute of Artificial Intelligence</orgName>
								<address>
									<country key="AE">UAE</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ET-Net: A Generic Edge-aTtention Guidance Network for Medical Image Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Segmentation is a fundamental task in medical image analysis. However, most existing methods focus on primary region extraction and ignore edge information, which is useful for obtaining accurate segmentation. In this paper, we propose a generic medical segmentation method, called Edge-aTtention guidance Network (ET-Net), which embeds edge-attention representations to guide the segmentation network. Specifically, an edge guidance module is utilized to learn the edgeattention representations in the early encoding layers, which are then transferred to the multi-scale decoding layers, fused using a weighted aggregation module. The experimental results on four segmentation tasks (i.e., optic disc/cup and vessel segmentation in retinal images, and lung segmentation in chest X-Ray and CT images) demonstrate that preserving edge-attention representations contributes to the final segmentation accuracy, and our proposed method outperforms current state-of-theart segmentation methods. The source code of our method is available at https://github.com/ZzzJzzZ/ETNet.</p><p>Corresponding author (huazhu.fu@inceptioniai.org).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Medical image segmentation is an important procedure in medical image analysis. The shapes, size measurements and total areas of segmentation outcomes can provide significant insight into early manifestations of life-threatening diseases. As a result, designing an efficient general segmentation model deserves further attention. Existing medical image segmentation methods can mainly be divided into two categories: edge detection and object segmentation. The edge detection methods first identify object boundaries utilizing local gradient representations, and then separate the closed loop regions as the objects. These methods, which aim to obtain highly localized image information, can achieve high accuracy in boundary segmentation, and are adequate for simple structures. For example, the level-set technique is employed to minimize an objective function for estimating tumor segmentation based on shape priors <ref type="bibr" target="#b15">[16]</ref>. The template matching method is proposed to obtain optic disc boundary approximations with the Circular Hough Transform in retinal images <ref type="bibr" target="#b0">[1]</ref>. Other edge detection methods are employed to extract blood vessels in retinal images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11]</ref>. However, these edge detection methods depend on local edge representations and lack object-level information, which leads to trivial segmentation regions and discontinuous boundaries. By contrast, object segmentation methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b6">7]</ref> utilize global appearance models of foregrounds and backgrounds to identify the target regions, which preserves the homogeneity and semantic characteristics of objects, and reduces the uncertainties in detecting the boundary positions. A common way of doing this is to classify each pixel/patch in an image as foreground or background. For example, a superpixel classification method was proposed to segment the optic disc and cup regions for glaucoma screening <ref type="bibr" target="#b3">[4]</ref>. However, without utilizing edge information, several object segmentation methods need to refine the initial coarse segmentation results using additional post-processing technologies (e.g., Conditional Random Field and shape fitting), which is time-consuming and less related to previous segmentation representations.</p><p>Recently, the success of U-Net has significantly promoted widespread applications of segmentation on medical images, such as cell detection from 2D image <ref type="bibr" target="#b11">[12]</ref>, vessel segmentation from retinal images <ref type="bibr" target="#b5">[6]</ref> and lung region extraction from chest X-Ray and CT images <ref type="bibr" target="#b9">[10]</ref>. However, there are still several limitations when applying Deep Convolutional Neural Networks (DCNNs) based on a U-Net structure. In medical image segmentation, different targets sometimes have similar appearances, making it difficult to segment them using a U-Net based DCNN. Besides, inconspicuous objects are sometimes over-shadowed by irrelevant salient objects, which can confuse the DCNNs, since it cannot extract discriminative context features, leading to false predictions. In addition, target shapes and scale variations are difficult for DCNNs to predict. Although U-Net proposes to aggregate high-level and low-level features to address this problem, it only slightly alleviates it, since it aggregates features of different scale without considering their different contributes. Herein, we propose a novel method to extract discriminative context features and selectively aggregate multi-scale information for efficient medical image segmentation.</p><p>In this paper, we integrate both edge detection and object segmentation in one deep learning network. To do so, we propose a novel general medical segmentation method, called Edge-aTtention guidance Network (ET-Net), which embeds edge-attention representations to guide the process of segmentation. In our ET-Net, an edge guidance module (EGM) is provided to learn the edgeattention representations and preserve the local edge characteristics in the early encoding layers, while a weighted aggregation module (WAM) is designed to aggregate the multi-scale side-outputs from the decoding layers and transfer the edge-attention representations to high-level layers to improve the final results. We evaluate the proposed method on four segmentation tasks, including optic disc/cup segmentation and vessel detection in retinal images, and lung segmentation in Chest X-Ray and CT images. Results demonstrate that the proposed ET-Net outperforms the state-of-the-art methods in all tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Fig <ref type="figure" target="#fig_0">. 1</ref> illustrates the architecture of our ET-Net, which is primarily based on an encoder-decoder network, with the EGM and WAM modules appended on the end. The ResNet-50 <ref type="bibr" target="#b7">[8]</ref> is utilized as the encoder network, which comprises of four Encoding-Blocks (E-Blocks), one for each different feature map resolution.</p><p>For each E-Block, the inputs first go through a feature extraction stream, which consists of a stack of 1 × 1 − 3 × 3 − 1 × 1 convolutional layers, and are then summed with the shortcut of inputs to generate the final outputs. With this residual connection, the model can generate class-specific high-level features. The decoder path is formed from three cascaded Decoding-Blocks (D-Blocks), which are used to maintain the characteristics of the high-level features from the E-Blocks and enhance their representation ability. As shown in <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>, the D-Block first adopts a depth-wise convolution to enhance the representation of the fused low-level and high-level features. Then, a 1×1 convolution is used to unify the number of channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Edge Guidance Module</head><p>As stated in Sec. 1, edge information provides useful fine-grained constraints to guide feature extraction during segmentation. However, only low-level features preserve sufficient edge information. As such, we only apply the EGM at the top of early layers, i.e., E-Block 1 and 2, of the decoding path, as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. The EGM has two main fashions: 1) it provides an edge-attention representation to guide the process of segmentation in the decoding path; 2) it supervises the early convolutional layers using the edge detection loss. In our EGM, the outputs of E-Block 2 are upsampled to the same resolution as the outputs of E-Block 1, and then fed into the 1×1−3×3 convolutional layers and concatenated together. After that, the concatenated features go through one of two branches: a 1×1 convolutional layer to act as the edge guidance features in the decoding path, or another 1 × 1 convolutional layer to predict the edge detection results for early supervision. The Lovász-Softmax loss <ref type="bibr" target="#b1">[2]</ref> is used in our EGM, since it performs better than cross entropy loss for class imbalanced problems. It can be formulated as:</p><formula xml:id="formula_0">L = 1 C c∈C ∆ Jc (m(c)), and m i (c) = 1 − p i (c) if c = y i (c), p i (c) otherwise,<label>(1)</label></formula><p>where C denotes the class number, and y i (c) ∈ {−1, 1} and p i (c) ∈ [0, 1] are the ground truth label and predicted probability of pixel i for class c, respectively. ∆ Jc is the Lovász extension of the Jaccard index <ref type="bibr" target="#b1">[2]</ref>. With the edge supervision, the transmitted edge features are better able to guide the extraction of discriminative features in high-level layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Weighted Aggregation Module</head><p>In order to adapt to the shape and size variations of objects, existing methods tend to sum up multi-scale outputs along the channel dimension for final predictions (e.g., <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>). However, not all features in high-level layers are activated and benefit the recovery of objects. Aiming to address this, we develop the WAM to emphasize the valuable features, and aggregate multi-scale information and edge-attention representations to improve the segmentation performance. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, outputs of each D-Block are fed into the Weighted Blocks to highlight the valuable information. The structure of a Weighted Block is shown in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>. In this block, global average pooling is first employed to aggregate the global context information of inputs, and then two 1×1 convolutional layers with different non-linearity activation functions, i.e., ReLU and Sigmoid, are applied to estimate the layer relevance and generate the weights along the channel dimension. After that, the generated weights are multiplied with the outputs to yield more representative features.</p><p>Our WAM integrates the features of different scales via a bottom-up pathway, which generates a feature hierarchy consisting of feature maps of different sizes. Finally, the WAM also concatenates edge-attention representations from the EGM, and then applies a 1×1 convolution to extract features under edge-guided conditions. As with the edge detection in the EGM, our WAM also utilizes the Lovász-Softmax loss as the segmentation loss function. Thus, the total loss function of our ET-Net is defined as: L total = α · L seg + (1 − α) · L edge , where L edge and L seg denote the losses for edge detection in EGM and segmentation in WAM, respectively. In our experiments, weight α is empirically set to 0.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Implementation Details</head><p>For data augmentation, we apply a random mirror, random scale, which ranges from 0.5 to 2, and random rotation between -10 and 10 degrees, for all datasets. random color jitters with a probability of 0.5 are also applied to the data. All input images are randomly cropped to 512×512.</p><p>The initial weights of the encoder network come from ResNet-50 <ref type="bibr" target="#b7">[8]</ref> pretrained on ImageNet, and the parameters of the other layers are randomly initialized. A dilation strategy is used in E-Block 4, with an output stride of 1/16. During training, we set the batch size to 16 with synchronized batch normalization, and adopt poly learning rate scheduling lr = base lr × (1 − iters total iters ) power , in which the power is set to 0.9 and base lr is 0.005. The total iters is calculated by the num images × epochs/batch size, where epochs is set to 300 for all datasets. Our deep models are optimized using the Adam optimizer with a momentum of 0.9 and a weight decay of 0.0005. The whole ET-Net framework is implemented using PyTorch. Training (300 epochs) requires approximately 2.5 hours on one NVIDIA Titan Xp GPU. During testing, the segmentation results, including edge detection and object segmentation, are produced within 0.015 sec. per image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate our approach on three major types of medical images: retinal images, X-Ray and CT images. For convenience comparison, we select the evaluation metrics that are highly related to generic segmentation. Optic disc &amp; cup segmentation in retinal images: We evaluate our method on optic disc and cup segmentation in retinal images, which is a common task in glaucoma detection. Two public datasets are used in this experiment: the REFUGE 3 dataset, which consists of 400 training images and 400 validation images; the Drishti-GS <ref type="bibr" target="#b13">[14]</ref> dataset, which contains 50 training images and 51 validation images. Considering the negative influence of non-target areas in fundus images, we first localize the disc centers following the existing automatic disc detection method <ref type="bibr" target="#b4">[5]</ref>, and then transmit the localized images into our network.  The proposed approach is compared with the classic segmentation methods (i.e., FCN <ref type="bibr" target="#b12">[13]</ref> U-Net <ref type="bibr" target="#b11">[12]</ref>, M-Net <ref type="bibr" target="#b4">[5]</ref>, and Multi-task <ref type="bibr" target="#b2">[3]</ref> (it predicts edge and object predictions on the same features.), and the state-of-the-art segmentation method pOSAL <ref type="bibr" target="#b16">[17]</ref>, which achieved first place for the optic disc and cup segmentation tasks in the REFUGE challenge. The dice coefficients of optic disc (Dice OD ) and cup (Dice OD ), as well as mean intersection-over-union (mIoU), are employed as evaluation metrics. As shown in <ref type="table" target="#tab_1">Table 1</ref>, our ET-Net achieves the best performance on both the REFUGE and Drishti-GS datasets. Our model achieves particularly impressive results for optic cup segmentation, which is an especially difficult task, achieving 2% improvement of Dice OC over the next best method. Vessel segmentation in retinal images: We evaluate our method on vessel segmentation in retinal images. DRIVE <ref type="bibr" target="#b14">[15]</ref>, which contains 20 images for training and 20 for testing, is adopted in our experiments. The statistics in <ref type="table" target="#tab_2">Table 2</ref> show that our proposed method achieves the best performance, with 77.44% mIoU and 95.60% accuracy, when compared with classical methods (i.e., U-Net <ref type="bibr" target="#b11">[12]</ref>, M-Net <ref type="bibr" target="#b4">[5]</ref>, FCN <ref type="bibr" target="#b12">[13]</ref> and Multi-task <ref type="bibr" target="#b2">[3]</ref>). Lung segmentation in X-Ray images: We conduct lung segmentation experiments on Chest X-Rays, which is an important component for computer-aided diagnosis of lung health. We use the Montgomery County (MC) <ref type="bibr" target="#b8">[9]</ref> dataset, which contains 80 training images and 58 testing images. We compare our ET-Net with FCN <ref type="bibr" target="#b12">[13]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, M-Net <ref type="bibr" target="#b4">[5]</ref> and Multi-task <ref type="bibr" target="#b2">[3]</ref>, in terms of mIoU and accuracy (Acc.) scores. <ref type="table" target="#tab_2">Table 2</ref> shows the results, where our method achieves the state-of-the-art performance, with an Acc. of 98.65% and mIoU of 94.20%.  Lung segmentation in CT images: We evaluate our method on lung segmentation from CT images, which is fundamental for further lung nodule disease diagnosis. The Lung Nodule Analysis (LUNA) competition dataset 4 is employed, which is divided into 214 images for training and 53 images for testing. As with the lung segmentation from Chest X-Ray images, we compare our method with FCN <ref type="bibr" target="#b12">[13]</ref>, U-Net <ref type="bibr" target="#b11">[12]</ref>, M-Net <ref type="bibr" target="#b4">[5]</ref>, and Multi-task <ref type="bibr" target="#b2">[3]</ref>, in terms of mIoU and Acc. scores. The randomly cropped images are fed into the proposed network. As shown in <ref type="table" target="#tab_2">Table 2</ref>, our ET-Net outperforms previous state-of-the-art methods, obtaining an Acc. of 98.68% and mIoU of 96.23%. In addition to quantitative results, we provide qualitative segmentation results, shown in <ref type="figure" target="#fig_2">Fig. 3</ref>. As can be seen, our results are close to the ground truth. When compared with the predictions of other methods, it is clear that our results are better, especially, in the edge regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Ablation Study</head><p>To evaluate the contributions of each component of the proposed method, we conduct experiments with different settings on the Drishti-GS dataset. As shown in <ref type="table" target="#tab_3">Table 3</ref>, we choose the encoder-decoder network shown in <ref type="figure" target="#fig_0">Fig. 1</ref> as the base network, which achieves 90.11%, 95.77% and 84.41% in terms of Dice OC , Dice OD and mIoU, respectively. When we append the proposed EGM, it yields results of 91.24%/97.17%/86.49% (Dice OC /Dice OD /mIoU). This dramatically outperforms the base network, with only a small addition to the computational cost, proving that edge information is of vital importance for segmentation. To study the effect of the WAM, we append the WAM to the base network, without concatenating the edge features to base network. With the same training settings, this approach achieves performances of 91.49%/97.07%/86.62%, compared to the base network. The obvious performance gains for all three metrics illustrate the efficiency of the proposed the WAM. Finally, our whole ET-Net, with both EGM and WAM, obtains the best performance on the Drishti-GS test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this paper, we propose a novel Edge-aTtention Guidance network (ET-Net) for general medical image segmentation. By assuming that edge detection and region segmentation are mutually beneficial, we have proposed the Edge Guidance Module to detect object edges and generate edge-attention representations that contain sufficient edge information. Moreover, a Weighted Aggregation Module has been employed to highlight the valuable features of high-level layers, which are combined with the edge representations, to guide the final segmentation. Experiments on various medical imaging tasks have demonstrated the superiority of our proposed ET-Net compared to other state-of-the-art methods. In future work, we will extend our approach to 3D segmentation on CT and MRI volumes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of our ET-Net architecture, which includes the main encoderdecoder network with an edge guidance module and weighted aggregation module. 'Conv' denotes the convolutional layer, while 'U', 'C', and '+' denote the upsampling, concatenation, and addition layers, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Illustration of the E-Block and Weighted Block. 'U', '+' and '×' denote upsampling, addition, and multiplication layers, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Visualization of segmentation results. From left to right: optic disc/cup, and vessel segmentation in retinal fundus images, lung segmentation in Chest X-Ray and CT images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Optic disc/cup segmentation results on retinal fundus images.</figDesc><table><row><cell>Method</cell><cell></cell><cell>REFUGE</cell><cell></cell><cell></cell><cell>Drishti-GS</cell><cell></cell></row><row><cell></cell><cell cols="6">DiceOC (%) DiceOD (%) mIoU (%) DiceOC (%) DiceOD (%) mIoU (%)</cell></row><row><cell>FCN [13]</cell><cell>84.67</cell><cell>92.56</cell><cell>82.47</cell><cell>87.95</cell><cell>95.69</cell><cell>83.92</cell></row><row><cell>U-Net [12]</cell><cell>85.44</cell><cell>93.08</cell><cell>83.12</cell><cell>88.06</cell><cell>96.43</cell><cell>84.87</cell></row><row><cell>M-Net [5]</cell><cell>86.48</cell><cell>93.59</cell><cell>84.02</cell><cell>88.60</cell><cell>96.58</cell><cell>85.88</cell></row><row><cell>Multi-task [3]</cell><cell>86.74</cell><cell>94.01</cell><cell>84.36</cell><cell>88.96</cell><cell>96.55</cell><cell>85.94</cell></row><row><cell>pOSAL [17]</cell><cell>87.50</cell><cell>94.60</cell><cell>-</cell><cell>90.10</cell><cell>97.40</cell><cell>-</cell></row><row><cell>Our ET-Net</cell><cell>89.12</cell><cell>95.29</cell><cell>86.70</cell><cell>93.14</cell><cell>97.52</cell><cell>87.92</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Segmentation results on retinal fundus, X-Ray and CT images.</figDesc><table><row><cell>Method</cell><cell cols="2">DRIVE</cell><cell></cell><cell>MC</cell><cell cols="2">LUNA</cell></row><row><cell></cell><cell cols="6">Acc.(%) mIoU(%) Acc.(%) mIoU(%) Acc.(%) mIoU(%)</cell></row><row><cell>FCN [13]</cell><cell>94.13</cell><cell>74.55</cell><cell>97.35</cell><cell>90.53</cell><cell>96.18</cell><cell>93.82</cell></row><row><cell>U-Net [12]</cell><cell>94.45</cell><cell>75.46</cell><cell>97.82</cell><cell>91.64</cell><cell>96.63</cell><cell>94.79</cell></row><row><cell>M-Net [5]</cell><cell>94.58</cell><cell>75.81</cell><cell>97.96</cell><cell>91.95</cell><cell>97.27</cell><cell>94.92</cell></row><row><cell cols="2">Multi-task [3] 94.97</cell><cell>76.21</cell><cell>98.13</cell><cell>92.24</cell><cell>97.82</cell><cell>94.96</cell></row><row><cell>Our ET-Net</cell><cell>95.60</cell><cell>77.44</cell><cell>98.65</cell><cell>94.20</cell><cell>98.68</cell><cell>96.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of optic disc/cup segmentation on the Drishti-GS test set</figDesc><table><row><cell>Method</cell><cell cols="3">DiceOC (%) DiceOD(%) mIoU(%)</cell></row><row><cell>Base Network</cell><cell>90.11</cell><cell>95.77</cell><cell>84.41</cell></row><row><cell>Base Network + EGM</cell><cell>91.24</cell><cell>97.17</cell><cell>86.49</cell></row><row><cell>Base Network + WAM</cell><cell>91.49</cell><cell>97.07</cell><cell>86.62</cell></row><row><cell>Base Network + EGM + WAM</cell><cell>93.14</cell><cell>97.52</cell><cell>87.92</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://refuge.grand-challenge.org/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.kaggle.com/kmader/finding-lungs-in-ct-data/data</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting the optic disc boundary in digital fundus images using morphological, edge detection, and feature extraction techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aquino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Gegundez-Arias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The lovász-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rannen Triki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">DCAN: deep contour-aware networks for accurate gland segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Superpixel classification based optic disc and optic cup segmentation for glaucoma screening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint Optic Disc and Cup Segmentation Based on Multi-Label Deep Network and Polar Transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">DeepVessel: Retinal Vessel Segmentation via Deep Learning and Conditional Random Field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CE-Net: Context Encoder Network for 2D Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Two public chest x-ray datasets for computer-aided screening of pulmonary diseases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jaeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Candemir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>QIMS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Segmentation and Image Analysis of Abnormal Lungs at CT: Current Approaches, Challenges, and Future Trends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mansoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Bagci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Radiographics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Blood vessel segmentation algorithms review of methods, datasets and evaluation metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moccia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Momi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CMPB</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Drishti-gs: Retinal image dataset for optic nerve head(onh) segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Krishnadas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ISBI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ridge-based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A shape-based approach to the segmentation of medical imagery using level sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yezzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Patch-based output space adversarial learning for joint optic disc and cup segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09146</idno>
		<title level="m">Salient object detection in the deep learning era: An in-depth survey</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A deep network solution for attention and aesthetics aware photo cropping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
