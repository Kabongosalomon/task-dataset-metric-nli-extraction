<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Grid R-CNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
							<email>luxin@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
							<email>libuyu@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Yue</surname></persName>
							<email>yueyuxin@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
							<email>liquanquan@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
							<email>yanjunjie@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">SenseTime Group Limited</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Grid R-CNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper proposes a novel object detection framework named Grid R-CNN, which adopts a grid guided localization mechanism for accurate object detection. Different from the traditional regression based methods, the Grid R-CNN captures the spatial information explicitly and enjoys the position sensitive property of fully convolutional architecture. Instead of using only two independent points, we design a multi-point supervision formulation to encode more clues in order to reduce the impact of inaccurate prediction of specific points. To take the full advantage of the correlation of points in a grid, we propose a two-stage information fusion strategy to fuse feature maps of neighbor grid points. The grid guided localization approach is easy to be extended to different state-of-the-art detection frameworks. Grid R-CNN leads to high quality object localization, and experiments demonstrate that it achieves a 4.1% AP gain at IoU=0.8 and a 10.0% AP gain at IoU=0.9 on COCO benchmark compared to Faster R-CNN with Res50 backbone and FPN architecture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection task can be decomposed into object classification and localization. In recent years, many deep convolutional neural networks (CNN) based detection frameworks are proposed and achieve state-of-the-art results <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Although these methods improve the detection performance in many different aspects, their bounding box localization modules are similar. Typical bounding box localization module is a regression branch, which is designed as several fully connected layers and takes in high-level feature maps to predict the offset of the candidate box (proposal or predefined anchor).</p><p>In this paper we introduce Grid R-CNN, a novel object detection framework, where the traditional regression formulation is replaced by a grid point guided localization mechanism. And the explicit spatial representations are efficiently utilized for high quality localization. In contrast to regression approach where the feature map is collapsed into a vector by fully connected layers, Grid R-CNN divides the object bounding box region into grids and employs a fully convolutional network (FCN) <ref type="bibr" target="#b6">[7]</ref> to predict the locations of grid points. Owing to the position sensitive property of fully convolutional architecture, Grid R-CNN maintains the explicit spatial information and grid points locations can be obtained in pixel level. As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>.b, when a certain number of grid points at specified location are known, the corresponding bounding box is definitely determined. Guided by the grid points, Grid R-CNN can determine more accurate object bounding box than regression method which lacks the guidance of explicit spatial information.</p><p>Since a bounding box has four degrees of freedom, two independent points (e.g. the top left corner and bottom right corner) are enough for localization of a certain object. However the prediction is not easy because the location of the points are not directly corresponding to the local features. For example, the upper right corner point of the cat in <ref type="figure" target="#fig_0">Figure 1</ref>.b lies outside of the object body and its neighborhood region in the image only contains background, and it may share very similar local features with nearby pixels. To overcome this problem, we design a multi-point supervision formulation. By defining target points in a gird, we have more clues to reduce the impact of inaccurate prediction of some points. For instance, in a typical 3 × 3 grid points supervision case, the probably inaccurate y-axis coordinate of the top-right point can be calibrated by that of top-middle point which just locates on the boundary of the object. The grid points are effective designs to decrease the overall deviation.</p><p>Furthermore, to take the full advantage of the correlation of points in a gird, we propose an information fusion approach. Specifically, we design individual group of feature maps for each grid point. For one grid point, the feature maps of the neighbor grid points are collected and fused into an integrated feature map. The integrated feature map is utilized for the location prediction of the corresponding grid point. Thus complementary information from spatial related grid points is incorporated to make the prediction more accurate.</p><p>We showcase the effectiveness of our Grid R-CNN framework on the object detection track of the challenging COCO benchmark <ref type="bibr" target="#b9">[10]</ref>. Our approach outperforms traditional regression based state-of-the-art methods by a significant margin. For example, we surpass Faster R-CNN <ref type="bibr" target="#b2">[3]</ref> with a backbone of ResNet-50 <ref type="bibr" target="#b7">[8]</ref> and FPN <ref type="bibr" target="#b3">[4]</ref> architecture by 2.2% AP. Further comparison on different IoU threshold criteria shows that our approach has overwhelming strength in high quality object localization, with a 4.1% AP gain at IoU=0.8 and 10.0% AP gain at IoU=0.9.</p><p>The main contributions of our work are listed as follows:</p><p>1. We propose a novel localization framework called Grid R-CNN which substitute traditional regression network by fully convolutional network that preserves spatial information efficiently. To our best knowledge, Grid R-CNN is the first proposed region based (twostage) detection framework that locate object by predicting grid points on pixel level.</p><p>2. We design a multi-point supervision form that predicts points in grid to reduce the impact of some inaccurate points. We further propose a feature map level information fusion mechanism that enables the spatially related grid points to obtain incorporated features so that their locations can be well calibrated.</p><p>3. We perform extensive experiments and prove that Grid R-CNN framework is widely applicable across different detection frameworks and network architectures with consistent gains. The Grid R-CNN performs even better in more strict localization criterion (e.g. IoU threshold = 0.75). Thus we are confident that our grid guided localization mechanism is a better alternative for regression based localization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Since our new approach is based on two stage object detector, here we briefly review some related works. Twostage object detector was developed from the R-CNN architecture <ref type="bibr" target="#b0">[1]</ref>, a region-based deep learning framework that classify and locate every RoI (Region of Interest) generated by some low-level computer vision algorithms <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>. Then SPP-Net <ref type="bibr" target="#b10">[11]</ref> and Fast-RCNN <ref type="bibr" target="#b1">[2]</ref> introduced a new way to save redundant computation by extracting every region feature from the shared feature generated by entire image. Although SPP-Net and Fast-RCNN significantly improve the performance of object detection, the part of RoIs generating still cannot be trained end-to-end. Later, Faster-RCNN <ref type="bibr" target="#b2">[3]</ref> was proposed to solve this problem by utilizing a light region proposal network(RPN) to generate a sparse set of RoIs. This makes the whole detection pipeline an endto-end trainable network and further improve the accuracy and speed of the detector.</p><p>Recently, many works extend Faster R-CNN architecture in many aspects to achieve better performance. For example, R-FCN <ref type="bibr" target="#b11">[12]</ref> proposed to use region-based fully convolution network to replace the original fully connected network. FPN <ref type="bibr" target="#b3">[4]</ref> proposed a top-down architecture with lateral connections for building high-level semantic feature maps for variant scales. Mask R-CNN <ref type="bibr" target="#b4">[5]</ref> extended Faster R-CNN by adding a branch for predicting an pixel-wise object mask in parallel with the original bounding box recognition branch. Different from Mask R-CNN which extends Faster R-CNN by adding a mask branch, our method replaces the regression branch with a new grid branch to locate objects more accurately. Also, our methods need no extra annotation other than bounding box.</p><p>CornerNet <ref type="bibr" target="#b8">[9]</ref> is a single-stage object detector which uses paired key-points to locate the bounding box of the objects. It's a bottom-up detector that detects all the possible bounding box key-point(corner point) location through a hourglass <ref type="bibr" target="#b12">[13]</ref> network. In the meanwhile, an embedding network was designed to map the paired keypoints as close as possible. With above embedding mechanism, detected corners can be group as pairs and locate the bounding boxes.</p><p>It's worth noting that our approach is quite different from CornerNet. CornerNet is a one-stage bottom-up method, which means it directly generate keypoints from the entire image without defining instance. So the key step of the Cor-nerNet is to recognize which keypoints belong to the same instance and grouping them correctly. In contrast to that, our approach is a top-down two-stage detector which defines instance at first stage. What we focus on is how to locate the bounding box key-point more accurately. Furthermore, we designed grid points feature fusion module to exploit the features of related grid points and calibrate for more accurate grid points localization than two corner points only. In contrast to previous works with a box offset regression branch, we adopt a grid guided mechanism for high quality localization. The grid prediction branch adopts a FCN to output a probability heatmap from which we can locate the grid points in the bounding box aligned with the object. With the grid points, we finally determine the accurate object bounding box by a feature map level information fusion approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Grid R-CNN</head><p>An overview of Grid R-CNN framework is shown in <ref type="figure">Figure</ref> 2. Based on region proposals, features for each RoI are extracted individually from the feature maps obtained by a CNN backbone. The RoI features are then used to perform classification and localization for the corresponding proposals. In contrast to previous works, e.g. Faster R-CNN, we use a grid guided mechanism for localization instead of offset regression. The grid prediction branch adopts a fully convolutional network <ref type="bibr" target="#b6">[7]</ref>. It outputs a fine spatial layout (probability heatmap) from which we can locate the grid points of the bounding box aligned with the object. With the grid points, we finally determine the accurate object bounding box by a feature map level information fusion approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Grid Guided Localization</head><p>Most previous methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> use several fully connected layers as a regressor to predict the box offset for object localization. Whereas we adopt a fully convolutional network to predict the locations of predefined grid points and then utilize them to determine the accurate object bounding box.</p><p>We design an N × N grid form of target points aligned in the bounding box of object. An example of 3 × 3 case is shown in <ref type="figure" target="#fig_0">Figure 1</ref>.b, the gird points here are the four corner points, midpoints of four edges and the center point respectively. Features of each proposal are extracted by RoIAlign <ref type="bibr" target="#b4">[5]</ref> operation with a fixed spatial size of 14 × 14, followed by eight 3×3 dilated(for large receptive field) convolutional layers. After that, two 2× deconvolution layers are adopted to achieve a resolution of 56 × 56. The grid prediction branch outputs N × N heatmaps with 56 × 56 resolution, and a pixel-wise sigmoid function is applied on each heatmap to obtain the probability map. And each heatmap has a corresponding supervision map, where 5 pixels in a cross shape are labeled as positive locations of the target grid point. Binary cross-entropy loss is utilized for optimization.</p><p>During inference, on each heatmap we select the pixel with highest confidence and calculate the corresponding location on the original image as the grid point. Formally, a point (H x , H y ) in heatmap will be mapped to the point (I x , I y ) in origin image by the following equation:</p><formula xml:id="formula_0">I x = P x + H x w o w p I y = P y + H y h o h p<label>(1)</label></formula><p>where (P x , P y ) is the position of upper left corner of the proposal in input image, w p and h p are width and height of proposal, w o and h o are width and height of output heatmap. Then we determine the four boundaries of the box of object with the predicted grid points. Specifically, we denote the four boundary coordinates as B = (x l , y u , x r , y b ) representing the left, upper, right and bottom edge respectively. Let g j represent the j-th grid point with coordinate (x j , y j ) and predicted probability p j ,. Then we define E i as the set of indices of grid points that are located on the i-th edge, i.e., j ∈ E i if g j lies on the i-th edge of the bounding box. We have the following equation to calculate B with the set of g:</p><formula xml:id="formula_1">x l = 1 N j∈E1 x j p j , y u = 1 N j∈E2 y j p j x r = 1 N j∈E3 x j p j , y b = 1 N j∈E4 y j p j<label>(2)</label></formula><p>Taking the upper boundary y u as an example, it is the probability weighted average of y axis coordinates of the three upper grid points. </p><formula xml:id="formula_2">(b) (a)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Grid Points Feature Fusion</head><p>The grid points have inner spatial correlation, and their locations can be calibrated by each other to reduce overall deviation. Thus a spatial information fusion module is designed.</p><p>An intuitive implementation is a coordinate level average, but the rich information in the feature maps are discarded. A further idea is to extract the local features corresponding to the grid points on each feature map for a fusion operation. However this also discards potential effective information in different feature maps. Taking the 3 × 3 gird as an example, for the calibration of top left point, the features in the top left region of other neighbor points' feature maps (e.g. the top middle point) may provide effective information but not used. Therefore we design a feature map level information fusion mechanism to take full advantage of feature maps of each grid point.</p><p>To distinguish the feature maps of different points, we use N × N group of filters to extract the features for them individually (from the last feature map) and give them intermediate supervision of their corresponding grid points. Thus each feature map has specified relationship with a certain grid point and we denote the feature map corresponding to the i-th point as F i .</p><p>For each grid point, the points that have a L 1 distance of 1 (unit grid length) will contribute to the fusion, which are called source points. We define the set of source points w.r.t the i-th grid point as S i . For the j-th source point in S i , F j will be processed by three consecutive 5 × 5 convolution layers for information transfer and this process is denoted as a function T j→i . The processed features of all source points are then fused with F i to obtain an fusion feature map F i . An illustration of the top left grid point in 3 × 3 case is in <ref type="figure" target="#fig_2">Figure 3</ref>.a. We adopt a simple sum operation for the fusion in implementation and the information fusion is formulated as the following equation:</p><formula xml:id="formula_3">F i = F i + j∈Si T j→i (F j )<label>(3)</label></formula><p>Based on F i for each grid point, a second order of fusion is then performed with new conv layers T + j→i that don't share parameters with those in first order of fusion. And the second order fused feature map F i is utilized to output the final heatmap for the grid point location prediction. The second order fusion enables an information transfer in the range of 2 (L 1 distance). Taking the upper left grid point in 3 × 3 grids as an example (shown in <ref type="figure" target="#fig_2">Figure 3</ref>.b), it synthesizes the information from five other grid points for reliable calibration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Extended Region Mapping</head><p>Grid prediction module outputs heatmaps with a fixed spatial size representing the confidence distribution of the locations of grid points. Since the fully convolutional network architecture is adopted and spatial information is preserved all along, an output heatmap naturally corresponds to the spatial region of the input proposal in original image. However, a region proposal may not cover the entire object, which means some of the ground truth grid points may lie outside of the region of proposal and can't be labeled on the supervision map or predicted during inference.</p><p>During training, the lack of some grid points labels leads to inefficient utilization of training samples. While in inference stage, by simply choosing the maximum pixel on the heatmap, we may obtain a completely incorrect location for the grid points whose ground truth location is outside the corresponding region. In many cases over half of the grid points are not covered, e.g. in <ref type="figure" target="#fig_3">Figure 4</ref> the proposal (the small white box) is smaller than ground truth bounding box and 7 of the 9 grid points cannot be covered by output heatmap.</p><p>A natural idea is to enlarge the proposal area. This approach can make sure that most of the grid points will be included in proposal area, but it will also introduce redundant features of background or even other objects. Experiments show that simply enlarging the proposal area brings no gain but harms the accuracy of small objects detection.</p><p>To address this problem, we modify the relationship of output heatmaps and regions in the original image by a extended region mapping approach. Specifically, when the proposals are obtained, the RoI features are still extracted from the same region on the feature map without enlarging proposal area. While we re-define the representation area of the output heatmap as a twice larger corresponding region in the image, so that all grid points are covered in most cases as shown in <ref type="figure" target="#fig_3">Figure 4 (the dashed box)</ref>.</p><p>The extended region mapping is formulated as a modification of Equation 1:</p><formula xml:id="formula_4">I x = P x + 4H x − w o 2w o w p I y = P y + 4H y − h o 2h o h p<label>(4)</label></formula><p>After the new mapping, all the target grid points of the positive proposals (which have an overlap larger than 0.5 with ground truth box) will be covered by the corresponding region of the heatmap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>Network Configuration: We adopt the depth 50 or 101 ResNets <ref type="bibr" target="#b7">[8]</ref> w/o FPN <ref type="bibr" target="#b3">[4]</ref> constructed on top as backbone of the model. RPN <ref type="bibr" target="#b2">[3]</ref> is used to propose candidate regions. By convention, we set the shorter edge of the input image to 800 pixels in COCO dataset <ref type="bibr" target="#b9">[10]</ref> and 600 pixels in Pascal VOC dataset <ref type="bibr" target="#b26">[27]</ref>. In RPN, 256 anchors are sampled per image with 1:1 ratio of positive to negative anchors. The RPN anchors span 5 scales and 3 aspect ratios, and the IoU threshold of positive and negative anchors are 0.7 and 0.3 respectively. In classification branch, RoIs that have an overlap with ground truth greater than 0.5 are regarded as positive samples. We sample 128 RoIs per image in Faster R-CNN <ref type="bibr" target="#b2">[3]</ref> based model and 512 RoIs per image in FPN <ref type="bibr" target="#b3">[4]</ref> based model, with the 1:3 ratio of positive to negative. RoIAlign <ref type="bibr" target="#b4">[5]</ref> is adopted in all experiments, and the pooling size is 7 in category classification branch and 14 in grid branch. The grid prediction branch samples at most 96 RoIs per image and only positive RoIs are sampled for training.</p><p>Optimization: We use SGD to optimize the training loss with 0.9 momentum and 0.0001 weight decay. The backbone parameter are initialized by image classification task on ImageNet dataset <ref type="bibr" target="#b28">[29]</ref>, other new parameters are initialized by He (MSRA) initialization <ref type="bibr" target="#b29">[30]</ref>. No data augmentations except standard horizontal flipping are used. Our model is trained on 32 Nvidia TITAN Xp GPUs with one image on each for 20 epochs with an initial learning rate of 0.02, which decreases by 10 in the 13 and 18 epochs. We also use learning rate warming up and Synchronized BatchNorm machanism <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> to make multi-GPU training more stable.</p><p>Inference: During the inference stage, the RPN generates 300/1000 (Faster R-CNN/FPN) RoIs per image. Then the features of these RoIs will be processed by RoIAlgin <ref type="bibr" target="#b4">[5]</ref> layer and the classification branch to generate category score, followed by non-maximum suppression (NMS) with 0.5 IOU threshold. After that we select top 125 highest scoring RoIs and put their RoIAlign features into grid branch for further location prediction. Finally, NMS with 0.5 IoU threshold will be applied to remove duplicate detection boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We perform experiments on two object detection datasets, Pascal VOC <ref type="bibr" target="#b26">[27]</ref> and COCO <ref type="bibr" target="#b9">[10]</ref>. On Pascal VOC dataset, we train our model on VOC07+12 trainval set and evaluate on VOC2007 test set. On COCO <ref type="bibr" target="#b9">[10]</ref> dataset which contains 80 object categories, we train our model on the union of 80k train images and 35k subset of val images and test on a 5k subset of val (minival) and 20k test-dev.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Study</head><p>Multi-point Supervision: <ref type="table" target="#tab_0">Table 1</ref> shows how grid point selection affects the accuracy of detection. We perform experiments of variant grid formulations. The experiment of 2 points uses the supervision of upper left and bottom right corner of the ground truth box. In 4-point grid we add supervision of two other corner grid points. 9-point grid is a typical 3x3 grid formulation that has been described in section 3.1. All experiments in <ref type="table" target="#tab_0">Table 1</ref> are trained without feature fusion to avoid the extra gain from using more points for feature fusion. It can be observed that as the number of supervised grid points increases, the accuracy of the detection also increases. Grid Points Feature Fusion: Results in <ref type="table">Table 2</ref> shows the effectiveness of feature fusion. We perform experiments on several typical feature fusion methods and achieve different levels of improvement on AP performance. The bi-  <ref type="table">Table 3</ref>. Comparison of enlarging the proposal directly and extended region mapping strategy.</p><p>directional fusion method, as mentioned in <ref type="bibr" target="#b25">[26]</ref>, models the information flow as a bi-directional tree. For fair comparison, we directly use the feature maps from the first order feature fusion stage for grid point location prediction, and see a same gain of 0.3% AP as bi-directional fusion. And we also perform experiment of the complete two stage feature fusion. As can be seen in <ref type="table">Table 2</ref>, the second order fusion further improves the AP by 0.4%, with a 0.7% gain from the non-fusion baseline. Especially, the improvement of AP 0.75 is more significant than that of AP 0.5 , which indicates that feature fusion mechanism helps to improve the localization accuracy of the bounding box. Extended Region Mapping: <ref type="table">Table 3</ref> shows the results of our extended region mapping strategy compared with the original region representation and the method of directly enlarging the proposal box. Directly enlarging the region of proposal box for RoI feature extraction helps to cover more grid points of big objects but also brings in redundant information for small objects. Thus we can see that with this enlargement method there is a increase in AP large but a decrease in AP small , and finally a decline compared with the baseline. Whereas the extended region mapping strategy improves AP large performance as well as producing no negative influences on AP small , which leads to 1.2% improvement on AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-art Methods</head><p>On minival set, we mainly compare Grid R-CNN with two widely used two-stage detectors, Faster-RCNN and FPN. We replace the original regression based localization method by the grid guided localization mechanism in the two frameworks for fair comparison.</p><p>Experiments reduced by 10 at 15 and 17 epochs. The origianl evaluation criterion of PASCAL VOC is to calculate the mAP at 0.5 IoU threshold. We extend that to the COCO-style criterion which calculates the average AP across IoU thresholds from 0.5 to 0.95 with an interval of 0.05. We compare Grid R-CNN with R-FCN <ref type="bibr" target="#b11">[12]</ref> and FPN <ref type="bibr" target="#b3">[4]</ref>. Results in <ref type="table" target="#tab_2">Table 4</ref> show that our Grid R-CNN significantly improve AP over FPN and R-FCN by 3.6% and 9.7% respectively.</p><p>Experiments on COCO: To further demonstrate the generalization capacity of our approach, we conduct experiments on challenging COCO dataset. <ref type="table">Table 5</ref> shows that our approach brings consistently and substantially improvement across multiple backbones and frameworks. Compared with Faster R-CNN framework, Grid R-CNN improves AP over baseline by 2.1% with ResNet-50 backbone. The significant improvements are also shown on FPN framework based on both ResNet-50 and ResNet-101 backbones. Experiments in <ref type="table">Table 5</ref> demonstrate that Grid R-CNN significantly improve the performance of middle and large objects by about 3 points.</p><p>Results on COCO test-dev Set: For complete comparison, we also evaluate Grid R-CNN on the COCO test-dev set. We adopt ResNet-101 and ResNeXt-101 <ref type="bibr" target="#b22">[23]</ref> with FPN <ref type="bibr" target="#b3">[4]</ref> constructed on the top. Without bells and whistles, Grid R-CNN based on ResNet-101-FPN and ResNeXt-101-FPN could achieve 41.5 and 43.2 AP respectively. As shown in <ref type="table">Table 6</ref>, Grid R-CNN achieves very competitive performance comparing with other state-of-the-art detectors. It outperforms Mask R-CNN by a large margin without using any extra annotations. Note that since the techniques such as scaling used in SNIP <ref type="bibr" target="#b27">[28]</ref> and cascading in Cascade R-CNN <ref type="bibr" target="#b5">[6]</ref> are not applied in current framework of Grid R-CNN, there is still room for large improvement on performance (e.g. combined with scaling and cascading methods).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis and Discussion</head><p>Accuracy in Different IoU Criteria: In addition to the overview of mAP, in this part we focus on the localization quality of the Grid R-CNN. <ref type="figure" target="#fig_4">Figure 5</ref> shows the comparison between FPN based Grid R-CNN and baseline FPN with the same ResNet-50 backbone across IoU thresholds from 0.5 to 0.9. Grid R-CNN outperforms regression at higher IoU thresholds (greater than 0.7  Varying Degrees of Improvement in Different Categories: We have analyzed the specific improvement of Grid R-CNN on each category and discovered a meaningful and interesting phenomenon. As shown in <ref type="table">Table 7</ref>, the categories with the most gains usually have a rectangular or bar like shape (e.g. keyboard, laptop, fork, train, and refrigerator), while the categories suffering declines or having least gains usually have a round shape without structural edges (e.g. sports ball, frisbee, bowl, clock and cup). This phenomenon is reasonable since grid points are distributed in a rectangular shape. Thus the rectangular objects tend to have more grid points on the body but round objects can never cover all the grid points (especially the corners) with its body. Moreover, we are inspired to design points in circle shapes for better localization of objects with a round shape in future works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results Comparison:</head><p>We showcase the illustrations of our high quality object localization results in this part. As shown in <ref type="figure" target="#fig_5">Figure 6</ref>, Grid R-CNN (in the 1st and 3rd row) has an outstanding performance in accurate localization compared with the widely used Faster R-CNN (in the 2nd and 4th row). First and second row in <ref type="figure" target="#fig_5">figure 6</ref> show that Grid R-CNN outperforms Faster R-CNN in high quality object detection. Third and 4th row show that Grid R-CNN performs better in large object detection tasks.   <ref type="table">Table 7</ref>. The top 15 categories with most gains and most declines respectively, in the results of Grid R-CNN compared to Faster R-CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we propose a novel object detection framework, Grid R-CNN, which replaces the traditional box offset regression strategy in object detection by a grid guided mechanism for high quality localization. The grid branch locates the object by predicting grid points with the position sensitive merits of FCN and then determining the bounding box guided by the grid. Further more, we design a feature fusion module to calibrate the locations of grid points by transferring the spatial information in feature map level. Additionally, an extended region mapping mechanism is proposed to help RoIs get a larger represent-ing area to cover as many grid points as possible, which significantly improves the performance. Extensive experiments show that Grid R-CNN brings solid and consistent improvement and achieves state-of-the-art performance, especially on strict evaluation metrics such as AP at IoU=0.8 and IoU=0.9. Since the grid guided localization approach is easy to be extended to other frameworks, we will try to combine the scale selection and cascade techniques with Grid R-CNN and we believe a further gain can be obtained.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>(a) Traditional offset regression based bounding box localization. (b) Our proposed grid guided localization in Grid R-CNN. The bounding box is located by a fully convolutional network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the pipeline of Grid R-CNN. Region proposals are obtained from RPN and used for RoI feature extraction from the output feature maps of a CNN backbone. The RoI features are then used to perform classification and localization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>An illustration of the 3 × 3 case of grid points feature fusion mechanism acting on the top left grid point. The arrows represent the spatial information transfer direction. (a) First order feature fusion, feature of the point can be enhanced by fusing features from its adjacent points. (b) The second order feature fusion design in Grid R-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of the extended region mapping strategy. The small white box is the original region of the RoI and we extend the representation region of the feature map to the dashed white box for higher coverage rate of the grid points in the the ground truth box which is in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>AP results across IoU thresholds from 0.5 to 0.9 with an interval of 0.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results comparison. The results of Grid R-CNN are listed in the first and third row, while those of Faster R-CNN are in the second and fourth row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of different grid points strategies in Grid R-CNN. Experiments show that more grid points bring performance gains.</figDesc><table><row><cell>method</cell><cell cols="2">AP AP .5 AP .75</cell></row><row><cell>regression</cell><cell>37.4 59.3</cell><cell>40.3</cell></row><row><cell>2 points</cell><cell>38.3 57.3</cell><cell>40.5</cell></row><row><cell cols="2">4-point grid 38.5 57.5</cell><cell>40.8</cell></row><row><cell cols="2">9-point grid 38.9 58.2</cell><cell>41.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison with R-FCN and FPN on Pascal VOC dataset. Note that we evaluate the results with a COCO-style criterion which is the average AP across IoU thresholds range from 0.5 to [0.5:0.95].</figDesc><table><row><cell>on Pascal VOC: We train Grid R-CNN</cell></row><row><cell>on Pascal VOC dataset for 18 epochs with the learning rate</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>). The improvements over method backbone AP AP .5 AP .75 AP S AP M AP L Bounding box detection AP on COCO minival. Grid R-CNN outperforms both Faster R-CNN and FPN on ResNet-50 and ResNet-101 backbone. method backbone AP AP .5 AP .75 AP S AP M AP L Comparison with state-of-the-art detectors on COCO test-dev.baseline at AP 0.8 and AP 0.9 are 4.1% and 10% respectively, which means that Grid R-CNN achieves better performance mainly by improving the localization quality of the bounding box. In addition, the results of AP 0.5 indicates that grid branch may slightly affect the performance of the classification branch.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Faster R-CNN</cell><cell>ResNet-50 33.8 55.4</cell><cell>35.9</cell><cell>17.4 37.9 45.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Grid R-CNN</cell><cell>ResNet-50 35.9 54.0</cell><cell>38.0</cell><cell>18.6 40.2 47.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Faster R-CNN w FPN ResNet-50 37.4 59.3</cell><cell>40.3</cell><cell>21.8 40.9 47.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Grid R-CNN w FPN</cell><cell>ResNet-50 39.6 58.3</cell><cell>42.4</cell><cell>22.6 43.8 51.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Faster R-CNN w FPN ResNet-101 39.5 61.2</cell><cell>43.1</cell><cell>22.7 43.7 50.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Grid R-CNN w FPN</cell><cell>ResNet-101 41.3 60.3</cell><cell>44.4</cell><cell>23.4 45.8 54.1</cell></row><row><cell></cell><cell></cell><cell cols="4">YOLOv2 [14]</cell><cell>DarkNet-19</cell><cell>21.6 44.0</cell><cell>19.2</cell><cell>5.0</cell><cell>22.4 35.5</cell></row><row><cell></cell><cell></cell><cell cols="4">SSD-513 [15]</cell><cell>ResNet-101</cell><cell>31.2 50.4</cell><cell>33.3</cell><cell>10.2 34.5 49.8</cell></row><row><cell></cell><cell></cell><cell cols="4">DSSD-513 [16]</cell><cell>ResNet-101</cell><cell>33.2 53.3</cell><cell>35.2</cell><cell>13.0 35.4 51.1</cell></row><row><cell></cell><cell></cell><cell cols="4">RefineDet512 [17]</cell><cell>ResNet101</cell><cell>36.4 57.5</cell><cell>39.5</cell><cell>16.6 39.9 51.4</cell></row><row><cell></cell><cell></cell><cell cols="4">RetinaNet800 [18]</cell><cell>ResNet-101</cell><cell>39.1 59.1</cell><cell>42.3</cell><cell>21.8 42.7 50.2</cell></row><row><cell></cell><cell></cell><cell cols="3">CornerNet</cell><cell></cell><cell>Hourglass-104</cell><cell>40.5 56.5</cell><cell>43.1</cell><cell>19.4 42.7 53.9</cell></row><row><cell></cell><cell></cell><cell cols="5">Faster R-CNN+++ [8]</cell><cell>ResNet-101</cell><cell>34.9 55.7</cell><cell>37.4</cell><cell>15.6 38.7 50.9</cell></row><row><cell></cell><cell></cell><cell cols="5">Faster R-CNN w FPN [4]</cell><cell>ResNet-101</cell><cell>36.2 59.1</cell><cell>39.0</cell><cell>18.2 39.0 48.2</cell></row><row><cell></cell><cell></cell><cell cols="5">Faster R-CNN w TDM [19] Inception-ResNet-v2 [22]</cell><cell>36.8 57.7</cell><cell>39.2</cell><cell>16.2 39.8 52.1</cell></row><row><cell></cell><cell></cell><cell cols="3">D-FCN [20]</cell><cell></cell><cell>Aligned-Inception-ResNet 37.5 58.0</cell><cell>-</cell><cell>19.4 40.1 52.5</cell></row><row><cell></cell><cell></cell><cell cols="4">Regionlets [21]</cell><cell>ResNet-101</cell><cell>39.3 59.8</cell><cell>-</cell><cell>21.7 43.7 50.9</cell></row><row><cell></cell><cell></cell><cell cols="4">Mask R-CNN [5]</cell><cell>ResNeXt-101</cell><cell>39.8 62.3</cell><cell>43.4</cell><cell>22.1 43.2 51.2</cell></row><row><cell></cell><cell></cell><cell cols="5">Grid R-CNN w FPN (ours)</cell><cell>ResNet-101</cell><cell>41.5 60.9</cell><cell>44.5</cell><cell>23.3 44.9 53.1</cell></row><row><cell></cell><cell></cell><cell cols="5">Grid R-CNN w FPN (ours)</cell><cell>ResNeXt-101</cell><cell>43.2 63.0</cell><cell>46.6</cell><cell>25.1 46.5 55.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Faster R-CNN with FPN</cell></row><row><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Grid R-CNN with FPN</cell></row><row><cell></cell><cell>60</cell><cell>59.3</cell><cell>58.3</cell><cell>54.7</cell><cell>53.9</cell></row><row><cell></cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.3</cell><cell>46.3</cell></row><row><cell>mAP</cell><cell>30 40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>32.2</cell><cell>36.3</cell></row><row><cell></cell><cell>20</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>19.6</cell></row><row><cell></cell><cell>10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9.6</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">0.5</cell><cell cols="2">0.6</cell><cell>0.7</cell><cell>0.8</cell><cell>0.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">IoU threshold</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00726</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cornernet</surname></persName>
		</author>
		<title level="m">Detecting objects as paired keypoints In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06409</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<title level="m">Yolo9000: better, faster, stronger. arXiv preprint 1612</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06659</idno>
		<title level="m">Dssd: Deconvolutional single shot detector</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06897</idno>
		<title level="m">Single-shot refinement neural network for object detection</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06851</idno>
		<title level="m">Beyond skip connections: Top-down modulation for object detection</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.024087</idno>
		<title level="m">Deep Regionlets for Object Detection</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">An analysis of scale invariance in object detection-snip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08189</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1502.01852</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch SGD: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07240</idno>
		<title level="m">Megdet: A large mini-batch object detector</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
