<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Schwarz</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Vision Group MPI for Intelligent Systems</orgName>
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Vision Group MPI for Intelligent Systems</orgName>
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Niemeyer</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Vision Group MPI for Intelligent Systems</orgName>
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Autonomous Vision Group MPI for Intelligent Systems</orgName>
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, e.g., the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxelbased representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional generative adversarial networks have demonstrated impressive results in synthesizing high-resolution images <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b52">53]</ref> from unstructured image collections. However, despite this success, state-of-the-art models struggle to properly disentangle the underlying generative factors including 3D shape and viewpoint. This is in stark contrast to humans who have the remarkable ability to reason about the 3D structure of the world and imagine objects from novel viewpoints.</p><p>As reasoning in 3D is fundamental for applications in robotics, virtual reality or data augmentation, several recent works consider the task of 3D-aware image synthesis <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b75">76]</ref>, aiming at photorealistic image generation with explicit control over the camera pose. In contrast to 2D generative adversarial networks, approaches for 3D-aware image synthesis learn a 3D scene representation which is explicitly mapped to an image using differentiable rendering techniques, hence providing control over both, scene content and viewpoint. Since 3D supervision or posed images are often hard to obtain in practice, recent works try to solve this task using 2D supervision only <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39]</ref>. Towards this goal, existing approaches generate discretized 3D representations, i.e., a voxel-grid representing either the full 3D object <ref type="bibr" target="#b18">[19]</ref> or intermediate 3D features <ref type="bibr" target="#b38">[39]</ref> as illustrated in <ref type="figure">Fig. 1</ref>. While modeling the 3D object in color space allows for exploiting differentiable rendering, the cubic memory growth of voxel-based representations limits <ref type="bibr" target="#b18">[19]</ref> to low resolution and results in visible artifacts. Intermediate 3D features <ref type="bibr" target="#b38">[39]</ref> are more compact and scale better with image resolution. <ref type="figure">Figure 1</ref>: Motivation. Voxel-based approaches for 3D-aware image synthesis either generate a voxelized 3D model (e.g., PlatonicGAN <ref type="bibr" target="#b18">[19]</ref>, top) or learn an abstract 3D feature representation (e.g., HoloGAN <ref type="bibr" target="#b38">[39]</ref>, middle). This leads to discretization artifacts or degrades view-consistency of the generated images due to the learned neural projection function. In this paper, we propose a generative model for neural radiance fields (bottom) which represent the scene as a continuous function g θ that maps a location x and viewing direction d to a color value c and a volume density σ. Our model allows for generating 3D consistent images at high spatial resolution. We visualize 3D consistency by running a multi-view stereo algorithm (COLMAP <ref type="bibr" target="#b59">[60]</ref>) on several outputs of each method (right). Note that all models have been trained using 2D supervision only (i.e., from unposed RGB images).</p><p>However, this requires to learn a 3D-to-2D mapping for decoding the abstract features to RGB values, resulting in entangled representations which are not consistent across views at high resolutions.</p><p>In this paper, we demonstrate that the dilemma between coarse outputs and entangled latents can be resolved using conditional radiance fields, a conditional variant of a recently proposed continuous representation for novel view synthesis <ref type="bibr" target="#b35">[36]</ref>. More specifically, we make the following contributions: i) We propose GRAF, a generative model for radiance fields for high-resolution 3D-aware image synthesis from unposed images. In addition to viewpoint manipulations, our approach allows to modify shape and appearance of the generated objects. ii) We introduce a patch-based discriminator that samples the image at multiple scales and which is key to learn high-resolution generative radiance fields efficiently. iii) We systematically evaluate our approach on synthetic and real datasets. Our approach compares favorably to state-of-the-art methods in terms of visual fidelity and 3D consistency while generalizing to high spatial resolutions. We release our code and datasets at https://github.com/autonomousvision/graf.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image Synthesis: Generative Adversarial Networks (GANs) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b50">51]</ref> have significantly advanced the state-of-the-art in photorealistic image synthesis. In order to make the image synthesis process more controllable, several recent works have proposed to disentangle the underlying factors of variation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b73">74]</ref>. However, all of these methods ignore the fact that 2D images are obtained as projections of the 3D world. While some of the methods demonstrate that the disentangled factors capture 3D properties to some extend <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41]</ref>, modeling the image manifold using 2D convolutional networks remains a difficult task, in particular when seeking representations that faithfully disentangle viewpoint variations from object appearance and identity. Instead of directly modeling the 2D image manifold, we therefore follow a recent line of works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39]</ref> which aims at generating 3D representations and explicitly models the image formation process.</p><p>3D-Aware Image Synthesis: Learning-based novel view synthesis has been intensively investigated in the literature <ref type="bibr">[14, 36, 56, 61-64, 69, 75]</ref>. These methods generate unseen views from the same object and typically require camera viewpoints as supervision. While some works <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b61">[62]</ref><ref type="bibr" target="#b62">[63]</ref><ref type="bibr" target="#b63">[64]</ref><ref type="bibr" target="#b68">69]</ref> generalize across different objects without requiring to train an individual network per object, they do not yield a full probabilistic generative model for drawing unconditional random samples. In contrast, we are interested in generating novel objects from multiple views by learning a 3D-aware generative model from unposed 2D images.</p><p>Several recent works exploit generative 3D models for 3D-aware image synthesis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b75">76]</ref>. Many methods require 3D supervision <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b75">76]</ref> or assume 3D information as input <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b44">45]</ref>. E.g. Texture Fields <ref type="bibr" target="#b44">[45]</ref> synthesize novel textures conditioned on a particular 3D shape. Consequently, they require a 3D shape as input and colored surface points as supervision. Instead, we learn a generative model for both shape and texture from 2D images alone. This is a difficult task which only few works have attempted so far: <ref type="bibr">PLATONICGAN</ref>  <ref type="bibr" target="#b18">[19]</ref> learns a textured 3D voxel representation from 2D images using differentiable rendering techniques. However, such voxel-based representations are memory intensive, precluding image synthesis at high image resolutions. In this work, we avoid these memory limitations by using a continuous representation which allows for rendering images at arbitrary resolution. HoloGAN <ref type="bibr" target="#b38">[39]</ref> and some related works <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b39">40]</ref> learn a low-dimensional 3D feature combined with a learnable 3D-to-2D projection. However, as evidenced by our experiments, learned projections can lead to entangled latents (e.g., object identity and viewpoint), particularly at high resolutions. While 3D consistency can be encouraged using additional constraints <ref type="bibr" target="#b43">[44]</ref>, we take advantage of differentiable volume rendering techniques which do not need to be learned and thus incorporate 3D consistency into the generative model by design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implicit Representations:</head><p>Recently, implicit representations of 3D geometry have gained popularity in learning-based 3D reconstruction <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b58">59]</ref>. Key advantages over voxel <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71]</ref> or mesh-based <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b66">67]</ref> methods are that they do not discretize space and are not restriced in topology. Recent hybrid continuous grid representations <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b49">50]</ref> extend implicit representations to complicated or large scale scenes but require 3D input and do not consider texture. Another line of works <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b71">72]</ref> propose to learn continuous shape and texture representations from posed multi-view images only, by making the rendering process differentiable. As these models are limited to single objects or scenes of small geometric complexity, Mildenhall et al. <ref type="bibr" target="#b35">[36]</ref> propose to represent scenes as neural radiance fields which allow for multi-view consistent novel-view synthesis of more complex, real-world scenes from posed 2D images. They demonstrate compelling results on this task, however, their method requires many posed views, needs to be retrained for each scene, and cannot generate novel scenes. Inspired by this work, we exploit a conditional variant of this representation and show how a rich generative model can be learned from a collection of unposed 2D images as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We consider the problem of 3D-aware image synthesis, i.e., the task of generating high-fidelity images while providing explicit control over camera rotation and translation. We argue for representing a scene by its radiance field as such a continuous representation scales well wrt. image resolution and memory consumption while allowing for a physically-based and parameter-free projective mapping. In the following, we first briefly review Neural Radiance Fields (NeRF) <ref type="bibr" target="#b35">[36]</ref> which forms the basis for the proposed Generative Radiance Field (GRAF) model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Neural Radiance Fields</head><p>Neural Radiance Fields: A radiance field is a continuous mapping from a 3D location and a 2D viewing direction to an RGB color value <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33]</ref>. Mildenhall et al. <ref type="bibr" target="#b35">[36]</ref> proposed to use neural networks for representing this mapping. More specifically, they first map a 3D location x ∈ R 3 and a viewing direction d ∈ S 2 to a higher-dimensional feature representation using a fixed positional encoding which is applied element-wise to all three components of x and d:</p><formula xml:id="formula_0">γ(p) = sin(2 0 πp), cos(2 0 πp), sin(2 1 πp), cos(2 1 πp), sin(2 2 πp), cos(2 2 πp), . . .<label>(1)</label></formula><p>Following recent implicit models <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b47">48]</ref>, they then apply a multi-layer perceptron f θ (·) with parameters θ for mapping the resulting features to a color value c ∈ R 3 and a volume density σ ∈ R + :</p><formula xml:id="formula_1">f θ : R Lx × R L d → R 3 × R + (γ(x), γ(d)) → (c, σ)</formula><p>(2) As demonstrated in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b51">52]</ref>, the positional encoding γ(·) enables better fitting of high-frequency signals compared to directly using x and d as input to the multi-layer perceptron f θ (·). We confirm this with an ablation study in our supp. material. As the volume color c varies more smoothly with the viewing direction than with the 3D location, the viewing direction is typically encoded using fewer components, i.e., L d &lt; L x .</p><p>Volume Rendering: For rendering a 2D image from the radiance field f θ (·), Mildenhall et al. <ref type="bibr" target="#b35">[36]</ref> approximate the intractable volumetric projection integral using numerical integration. More formally, let {(c i r , σ i r )} N i=1 denote the color and volume density values of N random samples along a camera ray r. The rendering operator π(·) maps these values to an RGB color value c r :</p><formula xml:id="formula_2">π : (R 3 × R + ) N → R 3 {(c i r , σ i r )} → c r (3)</formula><p>The RGB color value c r is obtained using alpha composition</p><formula xml:id="formula_3">c r = N i=1 T i r α i r c i r T i r = i−1 j=1 1 − α j r α i r = 1 − exp −σ i r δ i r<label>(4)</label></formula><p>where T i r and α i r denote the transmittance and alpha value of sample point i along ray r and</p><formula xml:id="formula_4">δ i r = x i+1</formula><p>r − x i r 2 is the distance between neighboring sample points. Given a set of posed 2D images of a single static scene, Mildenhall et al. <ref type="bibr" target="#b35">[36]</ref> optimize the parameters θ of the neural radiance field f θ (·) by minimizing a reconstruction loss (sum of squared differences) between the observations and the predictions. Given θ, novel views can be synthesized by invoking π(·) for each pixel/ray.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generative Radiance Fields</head><p>In this work, we are interested in radiance fields as a representation for 3D-aware image synthesis. In contrast to <ref type="bibr" target="#b35">[36]</ref>, we do not assume a large number of posed images of a single scene. Instead, we aim at learning a model for synthesizing novel scenes by training on unposed images. More specifically, we utilize an adversarial framework to train a generative model for radiance fields (GRAF). <ref type="figure">Fig. 2</ref> shows an overview over our model. The generator G θ takes camera matrix K, camera pose ξ, 2D sampling pattern ν and shape/appearance codes z s ∈ R m /z a ∈ R n as input and predicts an image patch P . The discriminator D φ compares the synthesized patch P to a patch P extracted from a real image I. At inference time, we predict one color value for every image pixel. However, at training time, this is too expensive. Therefore, we instead predict a fixed patch of size K × K pixels which is randomly scaled and rotated to provide gradients for the entire radiance field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Generator</head><p>We sample the camera pose ξ = [R|t] from a pose distribution p ξ . In our experiments, we use a uniform distribution on the upper hemisphere for the camera location with the camera facing towards the origin of the coordinate system. Depending on the dataset, we also vary the distance of the camera from the origin uniformly. We choose K such that the principle point is in the center of the image. ν = (u, s) determines the center u = (u, v) ∈ R 2 and scale s ∈ R + of the virtual K × K patch P(u, s) which we aim to generate. This enables us to use a convolutional discriminator independent of the image resolution. We randomly draw the patch center u ∼ U(Ω) from a uniform distribution over the image domain Ω and the patch scale s from a uniform distribution s ∼ U([1, S]) where S = min(W, H)/K with W and H denoting the width and height of the target image. Moreover, we ensure that the entire patch is within the image domain Ω. The shape and appearance variables z s and z a are drawn from shape and appearance distributions z s ∼ p s and z a ∼ p a , respectively. In our experiments we use a standard Gaussian distribution for both p s and p a .</p><formula xml:id="formula_5">Ray Sampling: The K × K patch P(u, s) is determined by a set of 2D image coordinates P(u, s) = (sx + u, sy + v) x, y ∈ − K 2 , . . . , K 2 − 1<label>(5)</label></formula><p>which describe the location of every pixel of the patch in the image domain Ω as illustrated in <ref type="figure" target="#fig_1">Fig. 3</ref>. Note that these coordinates are real numbers, not discrete integers which allows us to continuously evaluate the radiance field. The corresponding 3D rays are uniquely determined by P(u, s), the camera pose ξ and intrinsics K. We denote the pixel/ray index by r, the normalized 3D rays by d r and the number of rays by R where R = K 2 during training and R = W H during inference.</p><p>3D Point Sampling: For numerical integration of the radiance field, we sample N points {x i r } along each ray r. We use the stratified sampling approach of <ref type="bibr" target="#b35">[36]</ref>, see supp. material for details.  <ref type="figure">Figure 2</ref>: Generative Radiance Fields. The generator G θ takes camera matrix K, camera pose ξ, 2D sampling pattern ν and shape/appearance codes z s ∈ R m /z a ∈ R n as input and predicts an image patch P . We use plate notation to illustrate R rays and N samples per ray. Note that the conditional radiance field g θ is the only component with trainable parameters. The discriminator D φ compares the synthesized patch P to a real patch P extracted from a real image I. At training time, we use sparse 2D sampling patterns of size K × K pixels for computational and memory efficiency. At inference time, we predict one color value for every pixel in the target image.   <ref type="figure">Figure 4</ref>: Conditional Radiance Field. While the volume density σ depends solely on the 3D point x and the shape code z s , the predicted color value c additionally depends on the viewing direction d and the appearance code z a , modeling view-dependent appearance, e.g., specularities.</p><p>Conditional Radiance Field: The radiance field is represented by a deep fully-connected neural network with parameters θ that maps the positional encoding (cf. Eq. (1)) of 3D location x ∈ R 3 and viewing direction d ∈ S 2 to an RGB color value c and a volume density σ:</p><formula xml:id="formula_6">g θ : R Lx × R L d × R Ms × R Ma → R 3 × R + (γ(x), γ(d), z s , z a ) → (c, σ)<label>(6)</label></formula><p>Note that in contrast to <ref type="bibr" target="#b1">(2)</ref>, g θ is conditioned on two additional latent codes: a shape code z s ∈ R Ms which determines the shape of the object and an appearance code z a ∈ R Ma which determines its appearance. We thus call g θ a conditional radiance field.</p><p>The network architecture of our conditional radiance field g θ is illustrated in <ref type="figure">Fig. 4</ref>. We first compute a shape encoding h from the positional encoding of x and the shape code z s . A density head σ θ transforms this encoding to the volume density σ. For predicting the color c at 3D location x, we concatenate h with the positional encoding of d and the appearance code z a and pass the resulting vector to a color head c θ . We compute σ independently of the viewpoint d and the appearance code z a to encourage multi-view consistency while disentangling shape from appearance. This encourages the network to use the latent codes z s and z a to model shape and appearance, respectively, and allows for manipulating them separately during inference. More formally, we have:</p><formula xml:id="formula_7">h θ : R Lx × R Ms → R H (γ(x), z s ) → h (7) c θ : R H × R L d × R Ma → R 3 (h(x, z s ), γ(d), z a ) → c<label>(8)</label></formula><formula xml:id="formula_8">σ θ : R H → R + h(x, z s ) → σ<label>(9)</label></formula><p>All mappings (h θ , c θ and σ θ ) are implemented using fully connected networks with ReLU activations. To avoid notation clutter, we use the same symbol θ to denote the parameters of each network.</p><p>Volume Rendering: Given the color and volume density {(c i r , σ i r )} of all points along ray r, we obtain the color c r ∈ R 3 of the pixel corresponding to ray r using the volume rendering operator in Eq. (3). Combining the results of all R rays, we denote the predicted patch as P as shown in <ref type="figure">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Discriminator</head><p>The discriminator D φ is implemented as a convolutional neural network (see supp. material for details) which compares the predicted patch P to a patch P extracted from a real image I drawn from the data distribution p D . For extracting a K × K patch from real image I, we first draw ν = (u, s) from the same distribution p ν which we use for drawing the generator patch above. We then sample the real patch P by querying I at the 2D image coordinates P(u, s) using bilinear interpolation. In the following, we use Γ(I, ν) to denote this bilinear sampling operation. Note that our discriminator is similar to PatchGAN <ref type="bibr" target="#b20">[21]</ref>, except that we allow for continuous displacements u and scales s while PatchGAN uses s = 1. It is further important to note that we do not downsample the real image I based on s, but instead query I at sparse locations to retain high-frequency details, see <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>Experimentally, we found that a single discriminator with shared weights is sufficient for all patches, even though these are sampled at random locations with different scales. Note that the scale determines the receptive field of the patch. To facilitate training, we thus start with patches of larger receptive fields to capture the global context. We then progressively sample patches with smaller receptive fields to refine local details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Training and Inference</head><p>Let I denote an image from the data distribution p D and let p ν denote the distribution over random patches (see Section 3.2.1). We train our model using a non-saturating GAN objective with R1regularization <ref type="bibr" target="#b33">[34]</ref> </p><formula xml:id="formula_9">V (θ, φ) = E zs∼ps, za∼pa, ξ∼p ξ , ν∼pν [f (D φ (G θ (z s , z a , ξ, ν)))] + E I∼p D , ν∼pν f (−D φ (Γ(I, ν))) − λ ∇D φ (Γ(I, ν)) 2 (10)</formula><p>where f (t) = − log(1 + exp(−t)) and λ controls the strength of the regularizer. We use spectral normalization <ref type="bibr" target="#b36">[37]</ref> and instance normalization <ref type="bibr" target="#b64">[65]</ref> in our discriminator and train our approach using RMSprop <ref type="bibr" target="#b26">[27]</ref> with a batch size of 8 and a learning rate of 0.0005 and 0.0001 for generator and discriminator, respectively. At inference, we randomly sample z s , z a and ξ, and predict a color value for all pixels in the image. Details on the network architectures can be found in the supp. material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets: We consider two synthetic and three real-world datasets in our experiments. To analyze our approach in a controlled setting we render 150k Chairs from Photoshapes <ref type="bibr" target="#b48">[49]</ref> following the rendering protocol of <ref type="bibr" target="#b45">[46]</ref>. We further use the Carla Driving simulator <ref type="bibr" target="#b11">[12]</ref> to create 10k images of 18 car models with randomly sampled colors and realistic texture and reflectance properties (Cars). We also validate our approach on three real-world datasets. We use the Faces dataset which comprises celebA <ref type="bibr" target="#b30">[31]</ref> and celebA-HQ <ref type="bibr" target="#b23">[24]</ref> for image synthesis up to resolution 128 2 and 512 2 pixels, respectively. In addition, we consider the Cats dataset <ref type="bibr" target="#b72">[73]</ref> and the Caltech-UCSD Birds-200-2011 <ref type="bibr" target="#b65">[66]</ref> dataset. For the latter, we use the available instance masks to composite the birds onto a white background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baselines:</head><p>We compare our approach to two state-of-the-art models for 3D-aware image synthesis using the authors' implementations 12 : PLATONICGAN <ref type="bibr" target="#b18">[19]</ref> generates a voxel-grid of the 3D object which is projected to the image plane using differentiable volumetric rendering. HoloGAN <ref type="bibr" target="#b38">[39]</ref> instead generates an abstract voxelized feature representation and learns the mapping from 3D to 2D using a combination of 3D and 2D convolutions. To analyze the consequences of a learned projection we further consider a modified version of HoloGAN (HoloGAN w/o 3D Conv) in which we reduce the capacity of the learned mapping by removing the 3D convolutional layers. For reference, we also compare our results to a state-of-the-art 2D GAN model <ref type="bibr" target="#b33">[34]</ref> with a ResNet <ref type="bibr" target="#b17">[18]</ref> architecture.  <ref type="figure">Figure 5</ref>: Camera Pose Interpolations for Cars and Chairs at image resolution 64 2 pixels for PLATONICGAN <ref type="bibr" target="#b18">[19]</ref> (PGAN), HoloGAN <ref type="bibr" target="#b38">[39]</ref> (HGAN) and our approach (Ours).   Evaluation Metrics: We quantify image fidelity using the Frechet Inception Distance (FID) <ref type="bibr" target="#b19">[20]</ref> and additionally report the Kernel Inception Distance (KID) <ref type="bibr" target="#b4">[5]</ref> in the supp. material. To assess 3D consistency we perform 3D reconstruction for images of size 256 2 pixels using COLMAP <ref type="bibr" target="#b59">[60]</ref>. We adopt Minimum Matching Distance (MMD) <ref type="bibr" target="#b0">[1]</ref> to measure the chamfer distance (CD) between 100 reconstructed shapes and their closest shapes in the ground truth for quantitative comparison and show qualitative results for the reconstructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chairs Birds Cars Cats</head><p>We now study several key questions relevant to the proposed model. We first compare our model to several baselines in terms of their ability to generate high-fidelity and high-resolution outputs. We then analyze the implications of learned projections and the importance of our multi-scale discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How do Generative Radiance Fields compare to voxel-based approaches?</head><p>We first compare our model against the baselines using an image resolution of 64 2 pixels. As shown in <ref type="figure">Fig. 5</ref>, all methods are able to disentangle object identity and camera viewpoint. However, PLATONICGAN has difficulties in representing thin structures and both PLATONICGAN and HoloGAN lead to visible artifacts in comparison to the proposed model. This is also reflected by larger FID scores in <ref type="table" target="#tab_1">Table 1</ref>. On Faces and Cats, HoloGAN achieves FID scores similar to our approach as both datasets exhibit only little variation in the azimuth angle of the camera while the other datasets cover larger viewpoint variations. This suggests that it is harder for HoloGAN to accurately capture the appearance of objects from different viewpoints due to its low-dimensional 3D feature representation and the learnable  projection. In contrast, our continuous representation does not require a learned projection and renders high-fidelity images from arbitrary views.</p><p>Do 3D-aware generative models scale to high-resolution outputs? Due to the voxel-based representation, PLATONICGAN becomes very memory intensive when scaled to higher resolutions. Thus, we limit our experiments to HoloGAN and HoloGAN w/o 3D Conv for this analysis. Additionally, we provide results for our model trained at 128 2 pixels, but sampled at higher resolution during inference (Ours sampled). The results in <ref type="table" target="#tab_2">Table 2</ref> show that this significantly improves over naïve bilinear upsampling (Ours upsampled) which indicates that our learned representation generalizes well to higher resolutions. As expected, our method achieves the smallest FID value when trained at full resolution. While our approach outperforms HoloGAN and HoloGAN w/o 3D Conv significantly on the Car dataset, HoloGAN w/o 3D Conv achieves results onpar with us on Faces where viewpoint variations are smaller. Interestingly, we found that HoloGAN w/o 3D Conv achieves lower FID values than the full HoloGAN model originally proposed in <ref type="bibr" target="#b38">[39]</ref> despite reduced model capacity. This is due to training instabilities which we observe when training HoloGAN at high resolutions. We even observe mode collapse at a resolution of 512 2 for which we are hence not able to report results.</p><p>Should learned projections be avoided? As illustrated in <ref type="figure">Fig. 6</ref>, HoloGAN (top) fails in disentangling viewpoint from appearance at high resolution, varying different style aspects like facial expression or even completely ignoring the pose input. We identify the learnable projection as the underlying cause for this behavior. In particular, we find that removing the 3D convolutional layers enables HoloGAN to adhere to the input pose more closely, see <ref type="figure">Fig. 6</ref> (middle). However, images from HoloGAN w/o 3D Conv are still not entirely multi-view consistent. To better investigate this observation we generate multiple images of the same instance at random viewpoints for both HoloGAN w/o 3D Conv and our approach, and perform dense 3D reconstruction using COLMAP <ref type="bibr" target="#b59">[60]</ref>. As reconstruction depends on the consistency across views, reconstruction accuracy can be considered as a proxy for the multi-view consistency of the generated images. As evident from <ref type="table" target="#tab_3">Table 3</ref> and <ref type="figure">Fig. 7</ref>, multi-view stereo works significantly better when using images from our method as input. In contrast, fewer correspondences can be established for HoloGAN w/o 3D Conv which uses learned 2D layers for upsampling. For HoloGAN with 3D convolutions, performance degrades even further as shown in the supp. material. We thus conjecture that learned projections should generally be avoided.   <ref type="table">Table 5</ref>: Focal Length. Ablation on the choice of the focal length of the camera in terms of FID.</p><p>Are Generative Radiance Fields able to disentangle shape from appearance? <ref type="figure">Fig. 8</ref> shows that in addition to disentangling camera and scene properties, our approach learns to disentangle shape and appearance which can be controlled during inference via z s and z a . For Cars and Chairs the appearance code controls the color of the object while for Faces it encodes skin and hair color.</p><p>How important is the multi-scale patch discriminator? To investigate whether we sacrifice image quality by using the proposed multi-scale patch discriminator, we compare our multi-scale discriminator (Patch) to a discriminator that receives the entire image as input (Full). As this is very memory intensive, we only consider images of resolution 64 2 and use half the hidden dimensions for h θ and c θ (dim/2). <ref type="table" target="#tab_5">Table 4</ref> shows that our patch discriminator achieves similar performance to the full image discriminator on Cars and performs even better on CelebA. A possible explanation for this phenomenon is that random patch sampling acts as a data augmentation strategy which helps to stabilize GAN training. In contrast, when using only local patches (s = 1), we observe that our generator is not able to learn the correct shape, resulting in a high FID value in <ref type="table" target="#tab_5">Table 4</ref>. We conclude that sampling patches at random scales is crucial for robust performance.</p><p>How important are the camera intrinsics? We ablate the sensitivity of our model to the chosen focal length on Cars under a fixed radius of 10 in <ref type="table">Table 5</ref>. Our model performs very similar for changes within 0.7f data to 1.0f data where f data is the focal length we use to render the training images. Even for larger focal lengths up to 1.8f data and with an orthographic projection we observe good performance. Only for very small focal lengths the generated images show distortions at the image borders resulting in higher FID values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have introduced Generative Radiance Fields (GRAF) for high-resolution 3D-aware image synthesis. We showed that our framework is able to generate high resolution images with better multi-view consistency compared to voxel-based approaches. However, our results are limited to simple scenes with single objects. We believe that incorporating inductive biases, e.g., depth maps or symmetry, will allow for extending our model to even more challenging real-world scenarios in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>3D-aware image synthesis is a relatively novel research area <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b43">44]</ref> and does not yet scale to generating complex real-world scenes, preventing immediate applications for society. However, our work takes an important step towards this goal as it enables high-fidelty reconstruction at resolutions beyond 64 2 pixels while requiring no 3D supervision as input. In the long run, we hope that our resesarch will facilitate the use of 3D-aware generative models in applications such as virtual reality, data augmentation or robotics. For example, intelligent systems such as autonomous vehicles require large amounts of data for training and validation which will be impossible to collect using static offline datasets. We believe that building generative, photo-realistic and large-scale 3D models of our world will ultimately allow for cost-efficient data collection and simulation. While many use-cases are possible, we believe that these types of models can be particularly beneficial to close the existing domain gap between real-world and synthetic data. However, working with generative models also requires care. While generating photorealistic 3D-scenarios is very intriguing it also bears the risk of manipulation and the creation of misleading content. In particular, models that can create 3D-consistent fake images might increase credibility of fake contents and might potentially fool systems that rely on multi-view consistency, e.g., modern face recognition systems. Therefore, we believe that it is of equal importance for the community to develop methods which are able to clearly distinguish between synthetic and real-world content. We see encouraging progress in this area, e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b57">58]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Ray Sampling. Given camera pose ξ, we sample rays according to ν = (u, s) which determines the continuous 2D translation u ∈ R 2 and scale s ∈ R + of a K × K patch. This enables us to use a convolutional discriminator independent of the image resolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>Viewpoint Interpolations on Faces and Cars at image resolution 256 2 pixels for HoloGAN [39] (HGAN), HoloGAN w/o 3D Conv (HGANr r 3D) and our approach (Ours). 3D Reconstruction from synthesized images at resolution 256 2 . Each pair shows one of the generated images and the 3D reconstruction from COLMAP [</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>FID at image resolution 64 2 pixels.</figDesc><table><row><cell></cell><cell>Cars</cell><cell>Faces</cell></row><row><cell></cell><cell cols="2">128 256 512 128 256 512</cell></row><row><cell cols="3">HoloGAN [39] 211 230 -39 61 -</cell></row><row><cell cols="3">w/o 3D Conv 180 189 251 31 33 51</cell></row><row><cell>Ours</cell><cell cols="2">41 71 84 35 49 49</cell></row><row><cell>upsampled</cell><cell cols="2">-91 128 -63 77</cell></row><row><cell>sampled</cell><cell cols="2">-74 104 -50 56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>FID at image resolution 128 2 -512 2 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Reconstruction</figDesc><table><row><cell>Accuracy on</cell></row><row><cell>Cars for 100 COLMAP reconstructions</cell></row><row><cell>compared to their closest shapes in the</cell></row><row><cell>ground truth in terms of MMD [1] mea-</cell></row><row><cell>suring chamfer distance (CD).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Figure 8 :</head><label>8</label><figDesc>Disentangling Shape / Appearance. Results from our model on Cars, Chairs and Faces.</figDesc><table><row><cell></cell><cell>Cars</cell><cell>CelebA</cell></row><row><cell>Full (64), dim/2</cell><cell>29</cell><cell>77</cell></row><row><cell>Patch (64), dim/2</cell><cell>32</cell><cell>28</cell></row><row><cell>Patch (128)</cell><cell>41</cell><cell>35</cell></row><row><cell>Patch (128), s = 1</cell><cell>277</cell><cell>153</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Patch Sampling Strategies. Ablation study comparing patch sampling strategies in terms of FID score.</figDesc><table><row><cell cols="2">f /f data FID</cell><cell>f /f data</cell><cell>FID</cell></row><row><cell>0.3</cell><cell>84</cell><cell>1.1</cell><cell>48</cell></row><row><cell>0.5</cell><cell>61</cell><cell>1.2</cell><cell>57</cell></row><row><cell>0.7</cell><cell>40</cell><cell>1.3</cell><cell>47</cell></row><row><cell>0.8</cell><cell>36</cell><cell>1.5</cell><cell>52</cell></row><row><cell>0.9</cell><cell>38</cell><cell>1.8</cell><cell>51</cell></row><row><cell>1.0</cell><cell>41</cell><cell>orthographic</cell><cell>53</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">PLATONICGAN: https://github.com/henzler/platonicgan 2 HoloGAN: https://github.com/thunguyenphuoc/HoloGAN</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We acknowledge the financial support by the BMWi in the project KI Delta Learning (project number 19A19013O) and the support from the BMBF through the Tuebingen AI Center (FKZ: 01IS18039A). We thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting Katja Schwarz and Michael Niemeyer. This work was supported by an NVIDIA research gift.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning representations and generative models for 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Diamanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML)</title>
		<meeting>of the International Conf. on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Protecting world leaders against deep fakes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Farid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nagano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Source generator attribution via inversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Albright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccloskey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Geometric image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Asian Conf. on Computer Vision (ACCV)</title>
		<meeting>of the Asian Conf. on Computer Vision (ACCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Demystifying MMD gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Binkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large scale GAN training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno>1608.04236</idno>
		<title level="m">Generative and discriminative voxel modeling with convolutional neural networks. arXiv.org</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep local shapes: Learning local SDF priors for detailed 3d reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chabra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning implicit fields for generative shape modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d-r2n2: A unified approach for single and multi-view 3d object reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CARLA: An open urban driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Codevilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. on Robot Learning (CoRL)</title>
		<meeting>Conf. on Robot Learning (CoRL)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dzanic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">D</forename><surname>Witherden</surname></persName>
		</author>
		<title level="m">Fourier spectrum discrepancies in deep network generated images. arXiv.org</title>
		<imprint>
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deepview: View synthesis with learned gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Broxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Debevec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Duvall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Overbeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tucker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning shape templates with structured implicit functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">AtlasNet: A papier-mâché approach to learning 3d surface generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Escaping plato&apos;s cave: 3d shape from adversarial rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ritschel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local implicit grid representations for 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niebner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ray tracing volume densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kajiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Herzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Analyzing and improving the image quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">High-fidelity synthesis with disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep marching cubes: Learning explicit surface representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Donne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards unsupervised learning of generative models for 3d controllable image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detection of gan-generated fake images over social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gragnaniello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Multimedia Information Processing and Retrieval (MIPR)</title>
		<meeting>IEEE Conf. on Multimedia Information essing and Retrieval (MIPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optical models for direct volume rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Max</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphic (TVCG)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="108" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Which training methods for gans do actually converge?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML)</title>
		<meeting>of the International Conf. on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Occupancy networks: Learning 3d reconstruction in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Nerf: Representing scenes as neural radiance fields for view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tancik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Spectral normalization for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detecting GAN generated fake images using co-occurrence matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nataraj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mohammed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Flenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Bappy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Media Watermarking, Security, and Forensics</title>
		<editor>A. M. Alattar, N. D. Memon, and G. Sharma</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hologan: Unsupervised learning of 3d representations from natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen-Phuoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen-Phuoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Richardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blockgan</surname></persName>
		</author>
		<title level="m">Learning 3d object-aware scene representations from unlabelled images. arXiv.org</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semisupervised stylegan for disentanglement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Debhath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML)</title>
		<meeting>of the International Conf. on Machine learning (ICML)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Occupancy flow: 4d reconstruction by learning particle dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Differentiable volumetric rendering: Learning implicit 3d representations without 3d supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">RGBD-GAN: unsupervised 3d representation learning from natural image datasets via RGBD image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Learning Representations (ICLR)</title>
		<meeting>of the International Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Texture fields: Learning texture representations in function space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning implicit surface light fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oechsle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on 3D Vision (3DV)</title>
		<meeting>of the International Conf. on 3D Vision (3DV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep mesh reconstruction from single rgb images via topology modification networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deepsdf: Learning continuous signed distance functions for shape representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Florence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Straub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lovegrove</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Photoshape: Photorealistic materials for large-scale shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rematas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Convolutional occupancy networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno>1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On the spectral bias of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Draxler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML)</title>
		<meeting>of the International Conf. on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with VQ-VAE-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Learning to disentangle factors of variation with manifold interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conf. on Machine learning (ICML)</title>
		<meeting>of the International Conf. on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Unsupervised learning of 3d structure from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised geometry-aware representation for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Faceforensics++: Learning to detect manipulated facial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rössler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Natsume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morishima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deepvoxels: Learning persistent 3d feature embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Scene representation networks: Continuous 3d-structure-aware neural scene representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sitzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhöfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wetzstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Monocular neural image based rendering with continuous view control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of view extrapolation with multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Pixel2Mesh: Generating 3D mesh models from single RGB images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Generative image modeling using style and structure adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Interpretable transformations with encoder-decoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Pix2vox: Context-aware 3d reconstruction from single and multi-view images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conf. on Computer Vision (ICCV)</title>
		<meeting>of the IEEE International Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yariv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
		<title level="m">Universal differentiable renderer for implicit neural representations. arXiv.org</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Cat head detection -how to effectively exploit shape and texture features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Modular generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>of the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Stereo magnification: learning view synthesis using multiplane images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fyffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Visual object networks: Image generation with disentangled 3d representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
