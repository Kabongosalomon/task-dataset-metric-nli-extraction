<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards a Better Match in Siamese Network Based Visual Object Tracker</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anfeng</forename><surname>He</surname></persName>
							<email>heanfeng@mail.ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Luo</surname></persName>
							<email>cluo@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
							<email>xinmei@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System</orgName>
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
							<email>wezeng@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards a Better Match in Siamese Network Based Visual Object Tracker</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Realtime Tracking · Siamese Network · Deep Convolutional Neural Networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, Siamese network based trackers have received tremendous interest for their fast tracking speed and high performance. Despite the great success, this tracking framework still suffers from several limitations. First, it cannot properly handle large object rotation. Second, tracking gets easily distracted when the background contains salient objects. In this paper, we propose two simple yet effective mechanisms, namely angle estimation and spatial masking, to address these issues. The objective is to extract more representative features so that a better match can be obtained between the same object from different frames. The resulting tracker, named Siam-BM, not only significantly improves the tracking performance, but more importantly maintains the realtime capability. Evaluations on the VOT2017 dataset show that Siam-BM achieves an EAO of 0.335, which makes it the best-performing realtime tracker to date.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>this category is the SiamFC tracker <ref type="bibr" target="#b1">[2]</ref>. The basic idea is to use the same DCNN to extract features from the target image patch and the search region, and to generate a response map by correlating the two feature maps. The position with the highest response indicates the position of the target object in the search region. The DCNN is pre-trained and remains unchanged during testing time. This allows SiamFC to achieve high tracking performance in realtime. Follow-up work of SiamFC includes SA-Siam, SiamRPN, RASNet, EAST, DSiam, CFNET and SiamDCF <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25</ref>].</p><p>Despite the great success of siamese network-based trackers, there are still some limitations in this framework. First, as previous research <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b14">15]</ref> pointed out, the CNN features are not invariant to large image transformations such as scaling and rotation. Therefore, SiamFC does not perform well when the object has large scale change or in-plane rotation. This problem is exaggerated when the tracked object is non-square, because there is no mechanism in the SiamFC framework that can adjust the orientation or the aspect ratio of the tracked object bounding box. Second, it is hard to determine the spatial region from which DNN features should be extracted to represent the target object. Generally speaking, including a certain range of the surrounding context is helpful to tracking, but too many of them could be unprofitable especially when the background contains distracting objects. Recently, Wang et al. <ref type="bibr" target="#b25">[26]</ref> also observed this problem and they propose to train a feature mask to highlight the features of the target object.</p><p>In this paper, we revisit the SiamFC tracking framework and propose two simple yet effective mechanisms to address the above two issues. The computational overhead of these two mechanisms is kept low, such that the resulting tracker, named Siam-BM, can still run in real-time on GPU.</p><p>First, our tracker not only predicts the location and the scale of the target object, but also predicts the angle of the target object. This is simply achieved by enumerating several angle options and computing DCNN features for each option. However, in order to maintain the high speed of the tracker, it is necessary to trim the explosive number of (scale, angle) combinations without tampering the tracking performance. Second, we propose to selectively apply a spatial mask to CNN feature maps when the possibility of distracting background objects is high. We apply such a mask when the aspect ratio of the target bounding box is far apart from 1:1. This simple mechanism not only saves the efforts to train an object-specific mask, but allows the feature map to include a certain amount of information of the background, which is in general helpful to tracking. Last, we also adopt a simple template updating mechanism to cope with the gradual appearance change of the target object. All these mechanisms are toward the same goal to achieve a better match between the same object from different frames. Therefore, the resulting tracker is named Siam-BM.</p><p>We carry out extensive experiments for the proposed Siam-BM tracker, over both the OTB and the VOT benchmarks. Results show that Siam-BM achieves an EAO of 0.335 at the speed of 32 fps on VOT-2017 dataset. It is the bestperforming realtime tracker in literature.</p><p>The rest of the paper is organized as follows. We review related work in Section 2. In Section 3, we revisit the SiamFC tracking framework and explain the proposed two mechanisms in details. Section 4 provides implementation details of Siam-BM and presents the experimental results. We finally conclude with some discussions in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Visual object tracking is an important computer vision problem. It can be modeled as a similarity matching problem. In recent years, with the widespread use of deep neural networks, there emerge a bunch of Siamese network based trackers, which performs similarity matching based on extracted DCNN features. The pioneering work in this category is the fully convolutional Siamese network (SiamFC) <ref type="bibr" target="#b1">[2]</ref>. SiamFC extract DCNN features from the target patch and the search region using AlexNet. Then, a response map is generated by correlating the two feature maps. The object is tracked to the location where the highest response is obtained. A notable advantage of this method is that it needs no or little online training. Thus, real-time tracking can be easily achieved.</p><p>There are a large number of follow-up work <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b25">26]</ref> of SiamFC. EAST <ref type="bibr" target="#b13">[14]</ref> attempts to speed up the tracker by early stopping the feature extractor if low-level features are sufficient to track the target. CFNet <ref type="bibr" target="#b28">[29]</ref> introduces correlation filters for low level CNNs features to speed up tracking without accuracy drop. SINT <ref type="bibr" target="#b23">[24]</ref> incorporates the optical flow information and achieves better performance. However, since computing optical flow is computationally expensive, SINT only operates at 4 frames per second (fps). DSiam <ref type="bibr" target="#b9">[10]</ref> attempts to online update the embeddings of tracked target. Significantly better performance is achieved without much speed drop. HP <ref type="bibr" target="#b7">[8]</ref> tries to tune hyperparameters for each sequence in SiamFC <ref type="bibr" target="#b1">[2]</ref> by optimizing it with continuous Q-Learning. RASNet <ref type="bibr" target="#b25">[26]</ref> introduces three kinds of attention mechanisms for SiamFC <ref type="bibr" target="#b1">[2]</ref> tracker. The authors share the same vision with us to look for more precise feature representation for the tracked object. SiamRPN <ref type="bibr" target="#b17">[18]</ref> includes a region proposal subnetwork to estimate the aspect ratio of the target object. This network will generate a more compact bounding box when the target shape changes. SA-Siam <ref type="bibr" target="#b11">[12]</ref> utilizes complementary appearance and semantic features to represent the tracked object. A channel-wise attention mechanism is used for semantic feature selection. SA-Siam achieves a large performance gain at a small computational overhead.</p><p>Apparently we are not the first who concerns transformation estimation in visual object tracking. In correlation filter based trackers, DSST <ref type="bibr" target="#b3">[4]</ref> and SAMF <ref type="bibr" target="#b18">[19]</ref> are early work that estimates the scale change of the tracked object. DSST <ref type="bibr" target="#b3">[4]</ref> does so by learning separate discriminative correlation filters for translation and scale estimation. SAMF <ref type="bibr" target="#b18">[19]</ref> uses a scale pyramid to search corresponding target scale. Recently, RAJSSC <ref type="bibr" target="#b30">[31]</ref> proposes to perform both scale and angle estimation in a unified correlation tracking framework by using the Log-Polar transformation. In SiamFC-based trackers, while the scale estimation has been considered in the original SiamFC tracker, angle estimation has not been considered before.</p><p>There are also a couple of previous research efforts to suppress the background noise. SRDCF <ref type="bibr" target="#b5">[6]</ref> and DeepSRDCF <ref type="bibr" target="#b4">[5]</ref> reduce background noise by introducing the spatial regularization term in loss function during the online training of correlation filters. RASNet <ref type="bibr" target="#b25">[26]</ref> and SA-Siam <ref type="bibr" target="#b11">[12]</ref> are two SiamFC-based trackers. They adopt spatial attention or channel-wise attention in the feature extraction network. They both need careful training of the attention blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Siam-BM Tracker</head><p>Our tracker Siam-BM is built upon the recent SA-Siam tracker <ref type="bibr" target="#b11">[12]</ref>, which is in turn built upon the SiamFC tracker <ref type="bibr" target="#b1">[2]</ref>. The main difference between SA-Siam and SiamFC trackers is that the former extracts semantic features in addition to appearance features for similarity matching. In this section, we will first revisit the SiamFC tracking framework and then present the two proposed mechanisms in Siam-BM towards a better matching of object features. In the original SiamFC <ref type="bibr" target="#b1">[2]</ref> work, M is set to 3 or 5 to deal with 3 or 5 different scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">An Overview of the SiamFC Tracking Framework</head><p>Both the target patch and the candidate patches go through the same DCNN network, which is fixed during testing time. The process of extracting DCNN features can be described by a function φ(·). Then, φ(T ) is correlated with</p><formula xml:id="formula_0">φ(C 1 ) through φ(C M ) and M response maps {R 1 , R 2 , ...R M } are computed.</formula><p>The position with the highest value in the response maps is determined by:</p><formula xml:id="formula_1">(x i , y i , m i ) = arg max x,y,m R m , (m = 1...M ),<label>(1)</label></formula><p>where x i , y i are the coordinates of the highest-value position and m is the index of the response map from which the highest value is found. Then, the tracking result is given by</p><formula xml:id="formula_2">B i = (x i , y i , s mi · w, s mi · h),</formula><p>where s mi is the scale factor of the m th i candidate patch. In this process, SiamFC tracker only determines the center position and the scale of the target object, but keeps the orientation and aspect ratio unchanged. This becomes a severe limitation of SiamFC tracker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Angle Estimation</head><p>As previous research <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref> has pointed out, DCNN features are not invariant to large image transformations, such as scaling and rotation. While scaling has been handled in the original SiamFC tracker, the rotation of the target object is not considered. Ideally, the change of object angle, or object rotation, can be similarly addressed as object scaling. Specifically, one could enumerate several possible angle changes and increase the number of candidate patches for similarity matching. However, with M scale choices and N angle choices, the number of candidate patches becomes M × N . It is quite clear that the tracker speed is inversely proportional to the number of candidate patches. Using contemporary GPU hardware, a SiamFC tracker becomes non-realtime even when M = N = 3.</p><p>Knowing the importance of realtime tracking, we intend to find a mechanism to reduce the number of candidate patches without tampering the performance of the tracker. The solution turns out to be a simple one: the proposed Siam-BM tracker adjusts the properties (scale or angle) of the tracked object only one at a time. In other words, Siam-BM can adjust both scale and angle in two frames, if necessary. As such, the number of candidate patches is reduced from M × N to M + N − 1. In our implementation, M = N = 3, so only 5 candidate patches are involved in each tracking process. Mathematically, each candidate patch is associated with an (s, a) pair, where s is the scaling factor and a is the rotation angle. It is forced that s = 1 (no scale change) when a = 0 (angle change), and a = 0 when s = 1. Similarly, the tracked object is determined by:</p><formula xml:id="formula_3">(x i , y i , k i ) = arg max x,y,k R k , (k = 1, 2, ...K),<label>(2)</label></formula><p>where K = M + N − 1 is the number of candidate patches. (x i , y i ) gives the center location of the tracked object and k i is associated with an (s, a) pair,</p><p>giving an estimation of the scale and angle changes. Both types of changes are accumulated during the tracking process. <ref type="figure" target="#fig_2">Fig.3</ref> illustrates the scale and angle estimation in the proposed Siam-BM tracker. In the figure, each candidate patch and each response map are labeled with the corresponding (s, a) pair. We can find that, when the target object has the same orientation in the target patch as in the candidate patch, the response is dramatically increased. In this example, the highest response in the map with (1, −π/8) is significantly higher than the top values in other maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Spatial Mask</head><p>Context information is helpful during tracking. However, including too much context information could be distracting when the background has salient objects or prominent features. In the SiamFC framework, the target patch is always a square whose size is determined only by the area of the target object. <ref type="figure">Fig.4</ref> shows some examples of target patches containing objects with different aspect ratios. It can be observed that, when the target object is a square, the background is made up of narrow stripes surrounding the target object, so the chance of having an integral salient object in it is small. But when the aspect ratio of the target object is far apart from 1 (vertical or horizontal), it is more likely to have salient objects in the background area. <ref type="figure">Fig. 4</ref>. Some examples of target patches containing objects with different aspect ratios. Target patches tend to include salient background objects when the object aspect ratio is far apart from 1:1.</p><p>We propose to selectively apply spatial mask to the target feature map. In particular, when the aspect ratio of the target object exceeds a predefined threshold th r , a corresponding mask is applied. We have mentioned that the proposed Siam-BM tracker is built upon a recent tracker named SA-Siam <ref type="bibr" target="#b11">[12]</ref>. In SA-Siam, there is an attention module which serves a similar purpose. However, we find that the spatial mask performs better and is more stable than the channel-wise attention scheme. Therefore, we replace the channel attention model in SA-Siam with spatial masking in Siam-BM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">The Siam-BM Tracker</head><p>Siam-BM is built upon SA-Siam <ref type="bibr" target="#b11">[12]</ref>, which contains a semantic branch and an appearance branch for feature extraction. The target patch has a size of 127×127 as in SiamFC, and the candidate patches have a size of 255×255. We set M = N = 3, so that there are 5 candidate patches and their corresponding scale and angle settings are (s, a) = (1.0375, 0), (0.964, 0), (1, 0), (1, π/8), (1, −π/8). Correspondingly, five response maps are generated after combining semantic and appearance branches. Similar to SiamFC and SA-Siam, normalization and cosine window are applied to each of the five response maps. An angle penalty of 0.975 is applied when a = 0 and a scale penalty of 0.973 is applied when s = 1. Following SA-Siam, both conv4 and conv5 features are used, and the spatial resolutions are 8 × 8 and 6 × 6, respectively. Spatial mask is applied when the aspect ratio is greater than th r = 1.5. <ref type="figure" target="#fig_3">Fig.5</ref> shows the fixed design of spatial masks when max{ w h , h w } &gt; th r . The white grids indicate a coefficient of 1 and the black grids indicate a coefficient of 0.</p><p>In addition, we perform template updating in Siam-BM. The template for frame t, denoted by φ(T t ) is defined as followings:</p><formula xml:id="formula_4">φ(T t ) = λ S × φ(T 1 ) + (1 − λ S ) × φ(T u t ),<label>(3)</label></formula><formula xml:id="formula_5">φ(T u t ) = (1 − λ U ) × φ(T u t−1 ) + λ U ×φ(T t−1 ).<label>(4)</label></formula><p>whereφ(T t−1 ) is the feature of the tracked object in frame t − 1. It can be cropped from the feature map of candidate regions of frame t − 1. φ(T u t ) is the moving average of feature maps with updating rate λ U . λ S is the weight of the first frame. In our implementation, we set λ S = 0.5, λ U = 0.006.</p><p>Note that the spatial mask is only applied to the semantic branch. This is because semantic responses are more sparse and centered than appearance responses, and it is less likely to exclude important semantic responses with spatial masks. The attention module in SA-Siam is removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the performance of Siam-BM tracker against stateof-the-art realtime trackers and carry out ablation studies to validate the contribution of angle estimation and spatial masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>OTB: The object tracking benchmarks (OTB) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> consist of two major datasets, namely OTB-2013 and OTB-100, which contain 51 and 100 sequences respectively. The two standard evaluation metrics on OTB are success rate and precision. For each frame, we compute the IoU (intersection over union) between the tracked and the groundtruth bounding boxes, as well as the distance of their center locations. A success plot can be obtained by evaluating the success rate at different IoU thresholds. Conventionally, the area-under-curve (AUC) of the success plot is reported. The precision plot can be acquired in a similar way, but usually the representative precision at the threshold of 20 pixels is reported.</p><p>VOT: We use the recent version of the VOT benchmark, denoted by VOT2017 <ref type="bibr" target="#b16">[17]</ref>. The VOT benchmarks evaluate a tracker by applying a reset-based methodology. Whenever a tracker has no overlap with the ground truth, the tracker will be re-initialized after five frames. Major evaluation metrics of VOT benchmarks are accuracy (A), robustness (R) and expected average overlap (EAO). A good tracker has high A and EAO scores but low R scores.</p><p>In addition to the evaluation metrics, VOT differs from OTB in groundtruth labeling. In VOT, the groundtruth bounding boxes are not always upright. Therefore, we only evaluate the full version of Siam-BM on VOT. OTB is used to validate the effectiveness of spatial mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Siam-BM</head><p>Similar to SA-Siam, the appearance network and the fuse module in semantic branch are trained using the ILSVRC-2015 video dataset (only color images are used). The semantic network uses the pretrained model for image classification on ILSVRC. Among a total of more than 4,000 sequences, there are around 1.3 million frames and about 2 million tracked objects with groundtruth bounding boxes. We strictly follow the separate training strategy in SA-Siam and the two branches are not combined until testing time.</p><p>We implement our model in TensorFlow [1] 1.7.0 framework in Python 3.5.2 environment. Our experiments are performed on a PC with a Xeon E5-2690 2.60GHz CPU and a Tesla P100 GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Analysis</head><p>Angle estimation: We first evaluate whether angle estimation improves the performance on the VOT benchmark. Spatial masking is not added, so our method is denoted by Siam-BM (w/o mask). There are two baseline methods. In addition to vanilla SA-Siam, we implement a variation of SA-Siam, denoted by SA-Siam (free angle). Specifically, when the bounding box of the tracked object is not upright in the first frame, the reported tracking results are tilted by the same angle in all the subsequent frames. <ref type="table" target="#tab_0">Table 1</ref> shows the EAO as well as accuracy and robustness of the three comparing schemes. Note that the performance of SA-Siam is slightly better than that reported in their original paper, which might due to some implementation differences. We can find that angle estimation significantly improves the tracker performance even when it is compared with the free angle version of SA-Siam. Spatial mask: We use the OTB benchmark for this ablation study. Angle estimation is not added to the trackers evaluated in this part, therefore our method is denoted by Siam-BM (mask only). For all the 100 sequences in OTB benchmark, we compute the aspect ratio of the target object using r = max( h w , w h ), where w and h are the width and height of the groundtruth bounding box in the first frame. We set a threshold th r , and if r &gt; th r , the object is called an elongated object. Otherwise, we call the object a mediocre object. At the testing stage, Siam-BM (mask only) applies spatial mask to elongated objects. At the training stage, we could either use the full feature map or the masked feature map for elongated objects. For mediocre objects, mask is not applied in training or testing. The comparison between different training and testing choices are included in <ref type="table" target="#tab_1">Table 2</ref>. Comparing (3)(4) with (5)(6) in the <ref type="table">Table,</ref> we can conclude that applying spatial mask significantly improves the tracking performance for elongated objects. Comparison between (3) and (4) shows that training with spatial mask will further improve the performance for elongated objects, which agrees with the common practice to keep the consistency of training and testing. An interesting finding is obtained when we comparing (1) with (2). If we apply spatial mask to elongated objects during training, the Siamese network seems to be trained in a better shape and the tracking performance for mediocre objects is also improved even though no spatial mask is applied during testing time. We then compare the performance of Siam-BM (mask only) with the state-ofthe-art realtime trackers on OTB-2013 and OTB-100, and the results are shown in <ref type="table" target="#tab_4">Table 3</ref> and <ref type="figure" target="#fig_5">Fig.6</ref>. The improvement of Siam-BM (mask only) with respect to SA-Siam demonstrates that the simple spatial masking mechanism is indeed effective.    <ref type="figure" target="#fig_6">Fig. 7</ref> shows the relationship between the object aspect ratio and the performance gain of spatial masking. Consistent with our observation, when the aspect ratio is far apart from 1, doing spatial masking is helpful. However, when the object is a mediocre one, masking the features is harmful. In general, the performance gain of feature masking is positively correlated with the deviation of aspect ratio from 1.</p><p>Siam-BM Finally, we show in Tabel 4 how the performance of Siam-BM is gradually improved with our proposed mechanisms. The EAO of the full-fledged Siam-BM reaches 0.335 on VOT2017, which is a huge improvement from 0.287 achieved by SA-Siam. Of course, as we add more mechanisms in Siam-BM, the tracking speed also drops, but the full-fledged Siam-BM still runs in realtime.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with the State-of-the-Art Trackers</head><p>We evaluate our tracker in VOT2017 main challenge and realtime subchallenge. The final model in this paper combines all components mentioned in previous section. We do not evaluate the final model in OTB because the groundtruth labeling in OTB is always upright bounding boxes and applying rotation does not produce a higher IoU even when the tracked bounding box is more precise and tight. As shown in <ref type="figure">Fig.8</ref>, our Siam-BM tracker is among the best trackers even when non-realtime trackers are considered. From <ref type="figure">Fig.9</ref>, we can see that Siam-  <ref type="bibr" target="#b2">[3]</ref> 0.652 0.874 0.643 0.856 60 BACF <ref type="bibr" target="#b15">[16]</ref> 0.656 0.859 0.621 0.822 35 PTAV <ref type="bibr" target="#b8">[9]</ref> 0.663 0.895 0.635 0.849 25 SA-Siam <ref type="bibr" target="#b11">[12]</ref> (baseline) 0.677 0.896 0.657 0.865 50 Siam-BM (mask only) 0.686 0.898 0.662 0.864 48 BM outperforms all realtime trackers in VOT2017 challenge by a large margin. The Accuracy-Robustness plot in <ref type="figure" target="#fig_0">Fig.10</ref> also shows the superiority of our tracker. We also compare the EAO value of our tracker with some of the latest trackers. RASNet <ref type="bibr" target="#b25">[26]</ref> achieves an EAO number of 0.281 in the main challenge and 0.223 in the realtime subchallenge. SiamRPN <ref type="bibr" target="#b17">[18]</ref> achieves an EAO number of 0.243 in the realtime subchallenge. The EAO number achieved by Siam-BM is much higher.  <ref type="figure">Fig. 8</ref>. EAO curves and rank in VOT17 main challenge. FA represents Free Angle here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we have designed a SiamFC-based visual object tracker named Siam-BM. The design goal is to achieve a better match between feature maps of the same object from different frames. In order to keep the realtime capability of the tracker, we propose to use low-overhead mechanisms, including parallel scale and angle estimation, fixed spatial mask and moving average template updating. The proposed Siam-BM tracker outperforms state-of-the-art realtime trackers by a large margin on the VOT2017 benchmark. It is even comparable to the best non-realtime trackers. In the future, we will investigate the adaptation of object aspect ratio during tracking.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Ground Truth SA-Siam (Free Angle) Siam-BM(Ours) Comparison with our tracker and baseline tracker. Best view in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 Fig. 2 .</head><label>22</label><figDesc>shows the basic operations in the SiamFC tracking framework. The input of the tracker is the target object bounding box B 0 in the first frame F 1 . A The SiamFC Tracking Framework bounding box can be described by a four-tuple (x, y, w, h), where (x, y) is the center coordinates and w, h are the width and the height, respectively. SiamFC crops the target patch T from the first frame, which is a square region covering B 0 and a certain amount of surrounding context. When the tracker comes to the i th frame, several candidate patches {C 1 , C 2 , ...C M } are drawn, all of which are centered at the tracked location of the previous frame, but differ in scales.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>-π/8) Candidate patches from F6, each labeled with (s, a) Illustrating the scale and angle estimation in Siam-BM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Spatial feature mask when the aspect ratio of target object exceeds a predefined threshold. Left two masks: h/w &gt; thr; right two masks: w/h &gt; thr; middle two masks: max{w/h, h/w} &lt; thr.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Comparing SiamBM (Mask only) with other high performance and real-time trackers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Performance gain of feature masking is positively correlated with the deviation of aspect ratio from 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison between Siam-BM (w/o mask) and two baseline trackers shows the effectiveness of angle estimation.</figDesc><table><row><cell>Trackers</cell><cell cols="3">EAO Accuracy Robustness</cell></row><row><cell>SA-Siam (vanilla)</cell><cell>0.261</cell><cell>0.505</cell><cell>1.276</cell></row><row><cell cols="2">SA-Siam (free angle) 0.287</cell><cell>0.529</cell><cell>1.234</cell></row><row><cell cols="2">Siam-BM (w/o mask) 0.301</cell><cell>0.544</cell><cell>1.305</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison between training and testing choices with or without spatial mask.</figDesc><table><row><cell>Testing</cell><cell>mediocre object</cell><cell cols="2">elongated object</cell></row><row><cell>Training</cell><cell>no mask</cell><cell>w/ mask</cell><cell>w/o mask</cell></row><row><cell>w/ mask</cell><cell>0.681 (1)</cell><cell>0.654 (3)</cell><cell>0.581 (5)</cell></row><row><cell>w/o mask</cell><cell>0.665 (2)</cell><cell>0.644 (4)</cell><cell>0.609 (6)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparing SiamBM (Mask only) with other high performance and real-time trackers</figDesc><table><row><cell></cell><cell>OTB2013</cell><cell>OTB100</cell></row><row><cell>Trackers</cell><cell cols="2">AUC Prec. AUC Prec. FPS</cell></row><row><cell>ECOhc</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Analysis of our tracker Siam-BM on the VOT2017. The impact of progressively integrating one contribution at a time is depicted.</figDesc><table><row><cell></cell><cell>Baseline</cell><cell>Angle</cell><cell>Spatial</cell><cell>Template</cell></row><row><cell></cell><cell cols="4">SA-Siam =⇒ Estimation =⇒ Mask =⇒ Updating</cell></row><row><cell>EAO</cell><cell>0.287</cell><cell>0.301</cell><cell>0.322</cell><cell>0.335</cell></row><row><cell>Accuracy</cell><cell>0.529</cell><cell>0.544</cell><cell>0.551</cell><cell>0.563</cell></row><row><cell>Robustness</cell><cell>1.234</cell><cell>1.305</cell><cell>1.07</cell><cell>0.977</cell></row><row><cell>FPS</cell><cell>50</cell><cell>35</cell><cell>34</cell><cell>32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>EAO curves and rank in VOT17 realtime challenge. FA represents Free Angle here.Fig. 10. Accuracy and robustness plots in VOT17 main challenge. Best trackers are closer to the topright corner. FA represents Free Angle here.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">Expected overlap curves for realtime</cell><cell></cell><cell></cell><cell></cell><cell cols="5">Expected overlap scores for realtime</cell></row><row><cell></cell><cell>0.4</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Siam−BM SA−Siam (FA) SiamFC</cell><cell></cell><cell>0.35</cell><cell></cell><cell>Ours</cell><cell></cell><cell>Siam−BM SA−Siam (FA) SiamFC</cell></row><row><cell></cell><cell>0.35</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Staple LSART</cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell></cell><cell></cell><cell>ECOhc Staple</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CFWCR</cell><cell></cell><cell></cell><cell></cell><cell cols="2">VOT17 realtime</cell><cell>csrf</cell></row><row><cell>Expected overlap</cell><cell>0.15 0.2 0.25 0.3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">CFCF ECO Gnet MCCT CCOT csr SiamDCF MCPF CRT ECOhc</cell><cell>Average expected overlap</cell><cell>0.15 0.2 0.25</cell><cell></cell><cell cols="2">challenge winner</cell><cell>ATLAS csr RCPF ECO MEEM SPCT CRT CFWCR SiamDCF UCT</cell></row><row><cell></cell><cell>0.05 0.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DLST csrf RCPF UCT SPCT ATLAS MEEM</cell><cell></cell><cell></cell><cell>0.1 0.05 0</cell><cell></cell><cell></cell><cell></cell><cell>MCCT MCPF Gnet CFCF CCOT DLST LSART</cell></row><row><cell></cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>300</cell><cell>350</cell><cell>400</cell><cell>450</cell><cell>500</cell><cell></cell><cell>21</cell><cell>17</cell><cell>13</cell><cell>9</cell><cell>5</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Sequence length</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Order</cell><cell></cell></row><row><cell cols="2">0.45 0.5 0.55 0.6 0.65 Fig. 9. 0.78 Accuracy 0.4</cell><cell></cell><cell cols="2">0.8 Siam−BM SA−Siam (FA) 0.82 SiamFC Staple LSART CFWCR CFCF ECO Gnet MCCT CCOT csr SiamDCF MCPF CRT ECOhc DLST csrf RCPF UCT SPCT ATLAS MEEM</cell><cell></cell><cell cols="7">0.84 AR plot for experiment baseline (weighted_mean) 0.86 0.88 0.9 0.92</cell><cell></cell><cell>0.94</cell><cell>0.96</cell><cell>0.98</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Robustness (S = 30.00)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fullyconvolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="850" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Häger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<meeting><address><addrLine>Nottingham</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional features for correlation filter based visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hyperparameter optimization for tracking with continuous deep q-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parallel tracking and verifying: A framework for real-time and high accuracy visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Branchout: Regularization for online ensemble tracking with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A twofold siamese network for real-time object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning policies for adaptive tracking with deep feature cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Spatial transformer networks</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning background-aware correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiani</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A scale adaptive kernel correlation filter tracker with feature integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transform-invariant convolutional neural networks for image classification and search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04057</idno>
		<title level="m">Dcfnet: Discriminant correlation filters network for visual tracking</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning attentions: Residual attentional siamese network for high performance online visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end learning of driving models from large-scale video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Recurrent filter learning for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint scale-spatial correlation tracking with adaptive rotation estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Oriented response networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
