<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unbiased Scene Graph Generation from Biased Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
							<email>kaihua001@e.ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
							<email>niu@ruc.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
							<email>jianqiang.jqh@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Damo Academy</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unbiased Scene Graph Generation from Biased Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Today's scene graph generation (SGG) task is still far from practical, mainly due to the severe training bias, e.g., collapsing diverse human walk on/ sit on/lay on beach into human on beach. Given such SGG, the down-stream tasks such as VQA can hardly infer better scene structures than merely a bag of objects. However, debiasing in SGG is not trivial because traditional debiasing methods cannot distinguish between the good and bad bias, e.g., good context prior (e.g., person read book rather than eat) and bad long-tailed bias (e.g., near dominating behind/in front of). In this paper, we present a novel SGG framework based on causal inference but not the conventional likelihood. We first build a causal graph for SGG, and perform traditional biased training with the graph. Then, we propose to draw the counterfactual causality from the trained graph to infer the effect from the bad bias, which should be removed. In particular, we use Total Direct Effect as the proposed final predicate score for unbiased SGG. Note that our framework is agnostic to any SGG model and thus can be widely applied in the community who seeks unbiased predictions. By using the proposed Scene Graph Diagnosis toolkit 1 on the SGG benchmark Visual Genome and several prevailing models, we observed significant improvements over the previous state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene graph generation (SGG) <ref type="bibr" target="#b63">[64]</ref> -a visual detection task of objects and their relationships in an imageseems to have never fulfilled its promise: a comprehensive visual scene representation that supports graph reasoning for high-level tasks such as visual captioning <ref type="bibr" target="#b68">[69,</ref><ref type="bibr" target="#b66">67]</ref> and VQA <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b13">14]</ref>. Once equipped with SGG, these high-level tasks have to abandon the ambiguous visual relationships <ref type="bibr" target="#b0">1</ref> Our code is publicly available on GitHub: https://github. com/KaihuaTang/Scene-Graph-Benchmark.pytorch The distribution of sample fraction for the most frequent 20 predicates in Visual Genome <ref type="bibr" target="#b21">[22]</ref>. (c) SGG from re-implemented MOTIFS <ref type="bibr" target="#b70">[71]</ref>. (d) SGG by the proposed unbiased prediction from the same model.</p><p>-yet on which are our core efforts made <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b5">6]</ref>, then pretend that there is a graph -nothing but a sparse object layout with binary links, and finally shroud it into graph neural networks <ref type="bibr" target="#b64">[65]</ref> for merely more contextual object representations <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b55">56]</ref>. Although this is partly due to the research gap in graph reasoning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b14">15]</ref>, the crux lies in the biased relationship prediction. <ref type="figure" target="#fig_0">Figure 1</ref> visualizes the SGG results from a state-of-theart model <ref type="bibr" target="#b70">[71]</ref>. We can see a frustrating scene: among almost perfectly detected objects, most of their visual relationships are trivial and less informative. For example in <ref type="figure" target="#fig_0">Figure 1</ref>(c), except the trivial 2D spatial layouts, we know little about the image from near, on, and has. Such heavily biased generation comes from the biased training data, more specifically, as shown in <ref type="figure" target="#fig_0">Figure 1(b)</ref>, the highlyskewed long-tailed relationship annotations. For example, if a model is trained for predicting on 1,000 times more than standing on, then, during test, the former is more likely to prevail over the latter. Therefore, to perform a sensible graph reasoning, we need to distinguish more fine-grained relationships from the ostensibly probable but trivial ones, such as replacing near with behind/in front of, and on with parking on/driving on in <ref type="figure" target="#fig_0">Figure 1(d)</ref>.  <ref type="figure">Figure 2</ref>. (a) The biased generation that directly predicts labels from likelihood. (b) An intuitive example of the proposed total direct effect, which calculates the difference between the real scene and the counterfactual one. Note that the "wipe-out" is only for the illustrative purpose but not considered as visual processing.</p><p>However, we should not blame the biased training because both our visual world per se and the way we describe it are biased: there are indeed more person carry bag than dog carry bag (i.e., the long-tail theory); it is easier for us to label person beside table rather than eating on (i.e., bounded rationality <ref type="bibr" target="#b51">[52]</ref>); and we prefer to say person on bike rather than person ride on bike (i.e., language or reporting bias <ref type="bibr" target="#b34">[35]</ref>). In fact, most of the biased annotations can help the model learn good contextual prior <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b70">71]</ref> to filter out the unnecessary search candidates such as apple park on table and apple wear hat. A promising but embarrassing finding <ref type="bibr" target="#b70">[71]</ref> is that: by only using the statistical prior of detected object class in the Visual Genome benchmark <ref type="bibr" target="#b21">[22]</ref>, we can already achieved 30.1% on Recall@100 for Scene Graph Detection -rendering all the much more complex SGG models almost useless -that is only 1.1-1.5% lower than the state-of-the-art <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b73">74]</ref>. Not surprisingly, as we will show in Section 5, conventional debiasing methods who do not respect the "good bias" during training, e.g., resampling <ref type="bibr" target="#b10">[11]</ref> and re-weighting <ref type="bibr" target="#b28">[29]</ref>, fail to generalize to unseen relationships, i.e., zero-shot SGG <ref type="bibr" target="#b30">[31]</ref>.</p><p>For both machines and humans, decision making is a collaboration of content (endogenous reasons) and context (exogenous reasons) <ref type="bibr" target="#b57">[58]</ref>. Take SGG as an example, in most SGG models <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b73">74]</ref>, the content is the visual features of the subject and object, and the context is the visual features of the subject-object union regions and the pairwise object classes. We humans -born and raised in the biased nature -are ambidextrous in embracing the good while avoiding the bad context, and making unbiased decisions together with the content. The underlying mechanism is causality-based: the decision is made by pursuing the main causal effect caused by the content but not the side-effect by context. However, on the other hand, machines are usually likelihood-based: the prediction is analogous to look-up the content and its context in a huge likelihood table, interpolated by population training. We believe that the key is to teach machines how to distinguish between the "main effect" and "side-effect". In this paper, we propose to empower machines the ability of counterfactual causality <ref type="bibr" target="#b40">[41]</ref> to pursue the "main effect" in unbiased prediction:</p><p>If I had not seen the content, would I still make the same prediction? The counterfactual lies between the fact that "I see" and the imagination "I had not", and the comparison between the factual and counterfactual will naturally remove the effect from the context bias, because the context is the only thing unchanged between the two alternatives.</p><p>To better illustrate the profound yet subtle difference between likelihood and counterfactual causality, we present a dog standing on surfboard example in Figure 2(a). Due to the biased training, the model will eventually predict the on. Note that even though the rest choices are not all exactly correct, thanks to the bias, they still help to filter out a large amount of unreasonable ones. To take a closer look at what relationship it is in the context bias, we are essentially comparing the original scene with a counterfactual scene <ref type="figure">(Figure 2</ref>(b)): only the visual features of the dog and surfboard are wiped out, while keeping the rest -the scene and the object classes -untouched, as if the visual features had ever existed. By doing this, we can focus on the main visual effects of the relationship without losing the context. We propose a novel unbiased SGG method based on the Total Direct Effect (TDE) analysis framework in causal inference <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b59">60]</ref>. <ref type="figure" target="#fig_2">Figure 3</ref>(a) shows the underlying causal graphs <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> of the two alternate scenes: factual and counterfactual. Although a formal introduction of them is given in Section 3-4, now you can simply understand the nodes as data features and the directed links as (parametric) data flows. For example, X → Y , Z → Y , and I → Y indicate that the relationship Y is a combined effect caused by content: the pair of object visual features X, context: their object classes Z, and scene: the image I; the faded links denote that the wiped-outX is no longer caused by I or affects Z. These graphs offer an algorithmic formulation to calculate TDE, which exactly realizes the counterfactual thinking in <ref type="figure">Figure 2</ref>. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>(b), the proposed TDE significantly improves most of the predicates, and impressively, the distribution of the improved performances is no longer long-tailed, indicating the fact that our improvement is indeed from the proposed method, but NOT from the better exploitation of the context bias. A closer analysis in <ref type="figure">Figure 6</ref> further shows that the worse predictions like on -though very few -are due to turning to more fine-grained results such as stand on and park on. We highlight that TDE is a model-agnostic prediction strategy and thus applicable for a variety of models and fusion tricks <ref type="bibr" target="#b72">[73,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b54">55]</ref>.</p><p>Last but not least, we propose a new standard of SGG diagnosis toolkit (cf. Section 5.2) for more comprehensive SGG evaluations. Besides traditional evaluation tasks, it consists of the bias-sensitive metric: mean Recall <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b5">6]</ref> and a new Sentence-to-Graph Retrieval for a more comprehensive graph-level metric. By using this toolkit on SGG benchmark Visual Genome <ref type="bibr" target="#b21">[22]</ref> and several prevailing baselines, we verify the severe bias in existing models and demonstrate the effectiveness of the proposed unbiased prediction over other debiasing strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Scene Graph Generation. SGG <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b70">71]</ref> has received increasing attention in computer vision community, due to the potential revolution that would be brought to down-stream visual reasoning tasks <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b15">16]</ref>. Most of the existing methods <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b60">61]</ref> struggle for better feature extraction networks. Zellers et al. <ref type="bibr" target="#b70">[71]</ref> firstly brought the bias problem of SGG into attention and the followers <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b5">6]</ref> proposed the unbiased metric (mean Recall), yet, their approaches are still restricted to the feature extraction networks, leaving the biased SGG problem unsolved. The most related work <ref type="bibr" target="#b26">[27]</ref> just prunes those dominant and easy-to-predict relationships in the training set. Unbiased Training. The bias problem has long been investigated in machine learning <ref type="bibr" target="#b56">[57]</ref>. Existing debiasing methods can be roughly categorized into three types: 1) data  augmentation or re-sampling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b2">3]</ref>, 2) unbiased learning through elaborately designed training curriculums or learning losses <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b28">29]</ref>, 3) disentangling biased representations from the unbiased <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b3">4]</ref>. The proposed TDE analysis can be regarded as the third category, but the main difference is that TDE doesn't require to train additional layers like <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b3">4]</ref> to model the bias, it directly separates the bias from existing models through the counterfactual surgeries on causal graphs. Mediation Analysis. It is also known as effect analysis <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b40">41]</ref>, which is widely adopted in medical, political or psychological research <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20]</ref> as the tool of studying the effect of certain treatments or policies. However, it has been neglected in the community of computer vision for years. There are very few recent works <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b67">68]</ref> trying to endow the model with the capability of causal reasoning. More detailed background knowledge can be found in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b58">59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Biased Training Models in Causal Graph</head><p>As illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>, we summarize the SGG framework in the form of Causal Graph (a.k.a., structural causal model) <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>. It is a directed acyclic graph G = {N , E}, indicating how a set of variables N interact with each other through the causal links E. It provides a sketch of the causal relations behind the data and how variables obtain their values, e.g., (I, X, Z) → Y . Before we conduct counterfactual analysis that deliberately manipulates the values of nodes and prunes the causal graph, we first revisit the conventional biased SGG model training in the graphical view.</p><p>The causal graph in <ref type="figure" target="#fig_4">Figure 4</ref>(b) is applicable to a variety of SGG methods, since it is highly general, imposing no constraints on the detailed implementations. We case-study three representative model formulations: the classic VTransE <ref type="bibr" target="#b72">[73]</ref>, the state-of-the-art MOTIFS <ref type="bibr" target="#b70">[71]</ref> and VC-Tree <ref type="bibr" target="#b54">[55]</ref>, using the language of nodes and links. Node I (Input Image&amp;Backbone). A Faster R-CNN <ref type="bibr" target="#b43">[44]</ref> is pre-trained and frozen in this node, It outputs a set of bounding boxes B = {b i |i = 1...n} and the feature map M from image I. Link I → X (Object Feature Extractor). It firstly extracts RoIAlign features [12] R = {r i } and tentative object labels L = {l i } by the object classifier on Faster R-CNN. Then, like MOTIFS <ref type="bibr" target="#b70">[71]</ref> or VCTree <ref type="bibr" target="#b54">[55]</ref>, we can use the following module to encode visual contexts for each object:</p><formula xml:id="formula_0">Input : {(r i , b i , l i )} =⇒ Output : {x i },<label>(1)</label></formula><p>where MOTIFS implements it as bidirectional LSTMs (Bi-LSTMs) and VCTree <ref type="bibr" target="#b54">[55]</ref> adopts bidirectional TreeLSTMs (Bi-TreeLSTMs) <ref type="bibr" target="#b52">[53]</ref>, early works like VTransE <ref type="bibr" target="#b72">[73]</ref> simply use fully connected layers. Node X (Object Feature). The pairwise object feature X takes value from {(x i , x j )|i = j; i, j = 1...n}. We slightly abuse the notation hereinafter, denoting the combination of representations from i and j as subscript e: x e = (x i , x j ). Link X → Z (Object Classification). The fine-tuned label of each object is decoded from the corresponding x i by:</p><formula xml:id="formula_1">Input : {x i } =⇒ Output : {z i },<label>(2)</label></formula><p>where MOTIFS <ref type="bibr" target="#b70">[71]</ref> and VCTree <ref type="bibr" target="#b54">[55]</ref> utilizes LSTM and TreeLSTM as decoders to capture the co-occurrence among object labels, respectively. The input of each LSTM/ TreeL-STM cell is the concatenation of feature and the previous label [x i ; z i−1 ]. VTransE <ref type="bibr" target="#b72">[73]</ref> uses the conventional fully connected layer as the classifier. Node Z (Object Class). It contains a pair of one-hot vectors for object labels z e = (z i , z j ). Link X → Y (Object Feature Input for SGG). For relationship classification, pairwise feature X are merged into a joint representation by the module:</p><formula xml:id="formula_2">Input : {x e } =⇒ Output : {x e },<label>(3)</label></formula><p>where another Bi-LSTMs and Bi-TreeLSTMs layers are applied in MOTIFS <ref type="bibr" target="#b70">[71]</ref> and VCTree <ref type="bibr" target="#b54">[55]</ref>, respectively, before concatenating the pair of object features. VTransE <ref type="bibr" target="#b72">[73]</ref> uses fully connected layers and element-wise subtraction for feature merging. Link Z → Y (Object Class Input for SGG). The language prior is calculated in this link through a joint embed- Node Y (Predicate Classification). The final predicate logits Y that takes inputs from the three branches is then generated by using a fusion function. In Section 5, we test two general fusion functions: 1) SUM:</p><formula xml:id="formula_3">ding layer z e = W z [z i ⊗ z j ],</formula><formula xml:id="formula_4">y e = W x x e + W v v e + z e , 2) GATE: y e = W r x e · σ(W x x e + W v v e + z e ),</formula><p>where · is element-wise product, σ(·) is a sigmoid function.</p><p>Training Loss. All models are trained by using the conventional cross-entropy losses of object labels and predicate labels. To avoid any single link spontaneously dominating the generation of logits y e , especially Z → Y , we further add auxiliary cross-entropy losses that individually predict y e from each branch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Unbiased Prediction by Causal Effects</head><p>Once the above training has been done, the causal dependencies among the variables are learned, in terms of the model parameters. The conventional biased prediction can only see the output of the entire graph given an image I = u without any idea about how a specific pair of objects affect their predicate. However, causal inference <ref type="bibr" target="#b40">[41]</ref> encourages us to think out of the black box. From the graphical point of view, we are no longer required to run the entire graph as a whole. We can directly manipulate the values of several nodes and see what would be going on. For example, we can cut off the link I → X and assign a dummy value to X, then investigate what the predicate would be. The above operation is termed intervention in causal inference <ref type="bibr" target="#b39">[40]</ref>. Next, we will make unbiased predictions by intervention and its induced counterfactuals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Notations</head><p>Intervention. It can be denoted as do(·). It wipes out all the in-coming links of a variable and demands the variable to take a certain value, e.g. do(X =x) in <ref type="figure" target="#fig_5">Figure 5</ref>(b), meaning X is no longer affected by its causal parents. Counterfactual. It means "counter to the facts" <ref type="bibr" target="#b46">[47]</ref>, and takes one step further that assigns the "clash of worlds" combination of values to variables. Take <ref type="figure" target="#fig_5">Figure 5</ref>(c) as an example, if the intervention do(X =x) is conducted on X, the variable Z still takes the original z as if x had existed. Causal Effect. Throughout this section, we will use the pairwise object feature X as our control variable where the intervention is conducted, aiming to assess its effects, due to the fact that there wouldn't be any valid relationship if the pair of objects do not exist. The observed X is denoted as x while the intervened unseen value isx, which is set to either the mean feature of the training set or zero vector. The object label z on <ref type="figure" target="#fig_5">Figure 5</ref>(c) is calculated from Eq. <ref type="formula" target="#formula_1">(2)</ref>, taking x as input. We denote the output logits Y after the intervention X =x as follows ( <ref type="figure" target="#fig_5">Figure 5</ref>(b)):</p><formula xml:id="formula_5">Yx(u) = Y (do(X =x)|u),<label>(4)</label></formula><p>where u is the input image in SGG. Following the above notation, the original and counterfactual Y , i.e., <ref type="figure" target="#fig_5">Figure 5</ref>(a,c), can be re-written as Y x (u) and Yx ,z (u), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Total Direct Effect</head><p>As we discussed in Section 1, instead of the static likelihood that tends to be biased, the unbiased prediction lies in the difference between the observed outcome Y x (u) and its counterfactual alternate Yx ,z (u). The later one is a contextspecific bias that we want to remove from prediction. Intuitively, the unbiased prediction that we seek is the visual stimuli from blank to the observed real objects with specific attributes, states, and behaviors, but not merely from the surroundings and language priors. Those specific visual cues of objects are the key to the more fine-grained and informative unbiased predictions, because even if the overall prediction is biased towards the relationship like dog on surfboard, the "straight legs" would cause more effect on standing on rather than sitting on. In causal inference <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>, the above prediction process can be calculated as Total Direct Effect (TDE):</p><formula xml:id="formula_6">T DE = Y x (u) − Yx ,z (u),<label>(5)</label></formula><p>where the first term is from the original graph and the second one is from the counterfactual, as illustrated in <ref type="figure" target="#fig_5">Figure 5</ref>. Note that there is another type of effect <ref type="bibr" target="#b58">[59]</ref>, Total Effect (TE), which is easy to be mixed up with TDE. Instead of deriving counterfactual bias Yx ,z (u), TE lets all the descendant nodes of X change with intervention do(X =x) as shown in <ref type="figure" target="#fig_5">Figure 5</ref>(b). TE is therefore formulated as:</p><formula xml:id="formula_7">T E = Y x (u) − Yx(u).<label>(6)</label></formula><p>The main difference lies in the fact that Yx(u) is not conditioned on the original object labels (those caused by x), so TE only removes the general bias in the whole dataset (similar to the b in y = k · x + b), rather than the specific bias caused by the mediator we care about. The subtle difference between TE and TDE is further defined as Natural Indirect Effect (NIE) <ref type="bibr" target="#b58">[59]</ref> or Pure Indirect Effect (PIE) <ref type="bibr" target="#b59">[60]</ref>. More experimental analyses among these three types of effect are given in Section 5.</p><p>Overall SGG. At last, the proposed unbiased prediction y † e is obtained by replacing the conventional one-time prediction with TDE, which essentially "thinks" twice: one for observational Y xe (u) = y e , the other for imaginary Yx ,ze (u) = y e (x, z e ). The unbiased logits of Y is therefore defined as follows:</p><formula xml:id="formula_8">y † e = y e − y e (x, z e ).<label>(7)</label></formula><p>It is also worth mentioning that the proposed TDE doesn't introduce any additional parameters and is widely applicable to a variety of models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Settings and Models</head><p>Dataset. For SGG, we used Visual Genome (VG) <ref type="bibr" target="#b21">[22]</ref> dataset to train and evaluate our models, which is composed of 108k images across 75k object categories and 37k predicate categories. However, as 92% of the predicates have no more than 10 instances, we followed the widely adopted VG split <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b4">5]</ref> containing the most frequent 150 object categories and 50 predicate categories. The original split only has training set (70%) and test set (30%). We followed <ref type="bibr" target="#b70">[71]</ref> to sample a 5k validation set from training set for parameter tuning. For Sentence-to-Graph Retrieval (cf. Section 5.2), we selected the overlapped 41,859 images between VG and MS-COCO Caption dataset <ref type="bibr" target="#b29">[30]</ref> and divided them into train/test-1k/test-5k (35,859/1,000/5,000) sets. The later two only contain images from VG test set in case of exposing to grount-truth SGs. Each image has at least 5 captions serving as human queries, the same as how we use searching engines. Model Zoo. We evaluated three models: VTransE <ref type="bibr" target="#b72">[73]</ref>, MOTIFS <ref type="bibr" target="#b70">[71]</ref>, VTree <ref type="bibr" target="#b54">[55]</ref>, and two fusion functions: SUM and GATE. They were re-implemented using the same codebase as we proposed. All models shared the same hyper-parameters and the pre-trained detector backbone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Scene Graph Generation Diagnosis</head><p>Our proposed SGG diagnosis has the following three evaluations: 1. Relationship Retrieval (RR). It can be further divided into three sub-tasks: (1) Predicate Classification (PredCls): taking ground truth bounding boxes and labels as inputs, (2) Scene Graph Classification (SGCls): using ground truth bounding boxes without labels, (3) Scene Graph Detection (SGDet): detecting SGs from scratch. The conventional metric of RR is Recall@K (R@K), which was abandoned in this paper due to the reporting bias <ref type="bibr" target="#b34">[35]</ref>. As illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>(b), previous methods like <ref type="bibr" target="#b70">[71]</ref> with good performance on R@K unfairly cater to "head" predicates, Predicate Classification</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Graph Classification Scene Graph Detection Model Fusion</head><p>Method mR@20 mR@50 mR100 mR@20 mR50 mR100 mR@20 mR50 mR100 IMP+ <ref type="bibr">[</ref> e.g., on, while neglect the "tail" ones, e.g., predicates like parked on, laying on have embarrassingly 0.0 Re-call@100. To speak for the valuable "tail" rather than the trivial "head", we adopted a recent replacement, mean Re-call@K (mR@K), proposed by Tang et al. <ref type="bibr" target="#b54">[55]</ref> and Chen et al. <ref type="bibr" target="#b5">[6]</ref>. mR@K retrieves each predicate separately and then averages R@K for all predicates.</p><p>2. Zero-Shot Relationship Retrieval (ZSRR). It was introduced by Lu et al. <ref type="bibr" target="#b30">[31]</ref> as Zero-Shot Recall@K and was firstly evaluated on VG dataset in this paper, which only reports the R@K of those subject-predicate-object triplets that have never been observed in the training set. ZSRR also has three sub-tasks as RR.</p><p>3. Sentence-to-Graph Retrieval (S2GR). It uses the image caption sentence as the query to retrieve images represented as SGs. Both RR and ZSRR are triplet-level evaluations, ignoring the graph-level coherence. Therefore, we design S2GR, using human descriptions to retrieve detected SGs. We didn't use proxy vision-language tasks like captioning <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b68">69]</ref> and VQA <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b13">14]</ref> as the diagnosis, because their implementations have too many components unrelated to SGG and their datasets are challenged by their own biases <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b32">33]</ref>  from the previous image retrieval with scene graph <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b49">50]</ref>, because the latter still consider the images as visual features but not SGs. Recall@20/100 (R@20/100) and median ranking indexes of retrieved results (Med) on the gallery size of 1,000 and 5,000 were evaluated. Note that S2GR should have diverse implementations as long as its spirit: graph-level symbolic retrieval, is fulfilled. We provide our implementation in the next sub-section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation Details</head><p>Object Detector. Following the previous works <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b54">55]</ref>, we pre-trained a Faster R-CNN <ref type="bibr" target="#b43">[44]</ref> and froze it to be the underlying detector of our SGG models. We equipped the Faster R-CNN with a ResNeXt-101-FPN <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b62">63]</ref>   <ref type="table">Table 3</ref>. The results of Sentence-to-Graph Retrieval.</p><p>bone and scaled the longer side of input images to be 1k pixels. The detector was trained on the training set of VG using SGD as optimizer. We set the batch size to 8 and the initial learning rate to 8 × 10 −3 , which was decayed by the factor of 10 on the 30k th and 40k th iterations. The final detector achieved 28.14 mAP on VG test set (using 0.5 IoU threshold). 4 2080ti GPUs were used for the pre-training. Scene Graph Generation. On top of the frozen detector, we trained SGG models using SGD as optimizer. Batch size and initial learning rate were set to be 12 and 12 × 10 −2 for PredCls and SGCls; 8 and 8 × 10 −2 for SGDet. The learning rate would be decayed by 10 two times after the validation performance plateaus. For SGDet, 80 RoIs were sampled for each image and Per-Class NMS <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b70">71]</ref> with 0.5 IoU was applied in object prediction. We sampled up to 1,024 subject-object pairs containing 75% background pairs during training. Different from previous works <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b4">5]</ref>, we didn't assume that non-overlapping subject-object pairs are invalid in SGDet, making SGG more general. Sentence-to-Graph Retrieval. We handled S2GR as a graph-to-graph matching problem. The query captions of each image were stuck together and parsed to a text-SG using <ref type="bibr" target="#b49">[50]</ref>. We set all the subject/object and predicates that appear less than 5 times to "UNKNOWN" tokens, obtaining a dictionary of size 4,459 subject/object entities and 645 predicates, respectively. The original image SG generated from SGDet contains a fixed number of RoIs and forces all valid subject-object pairs to predict foreground relationships, to serve the K number in mR@K, which is inappropriate for S2GR. Therefore, we used a threshold of 0.1 to filter RoIs by the label probabilities and removed all background predicates from the graph. Recall that the vocabulary size of the entity and predicate for image SGs are 150 and 50 as we mentioned before. To match the two heterogeneous graphs: image SG and text SG, in a unified space, we used BAN <ref type="bibr" target="#b18">[19]</ref> to encode the two graph types into fixed-dimension vectors to facilitate the retrieval. More details can be found in supplementary material.  <ref type="figure">Figure 6</ref>. The pie chart summarizes all the relationships, that are correctly detected by the baseline model but considered "incorrect" by TDE. The right side of the pie chart shows the corresponding labels given by the TDE. Combining with our qualitative examples, we believe that the drop of Recall@K is caused by two reasons: 1) the annotators preference towards simple annotations caused by bounded rationality <ref type="bibr" target="#b51">[52]</ref>, 2) TDE tends to predict more action-like relationships rather than vague prepositions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Studies</head><p>Except for the models and fusion functions that we've discussed before, we also investigated three conventional debiasing methods, two intuitive causal graph surgeries, and other two types of causal effects: 1) Focal: focal loss <ref type="bibr" target="#b28">[29]</ref> automatically penalizes well-learned samples and focuses on the hard ones. We followed the hyper-parameters (γ = 2.0, α = 0.25) optimized in <ref type="bibr" target="#b28">[29]</ref>. 2) Reweight: weighted cross-entropy is widely used in the industry for biased data. The inversed sample fractions were assigned to each predicate category as weights. 3) Resample <ref type="bibr" target="#b2">[3]</ref>: rare categories were up-sampled by the inversed sample fraction during training. 4) X2Y: since we argued that the unbiased effect was under the effect of object features X, it directly generated SG by the outputs of X → Y branch after biased training. 5) X2Y-Tr: it even cut off other branches, using X → Y for both training and testing. 6) TE: as we introduced in Section 4, TE is the debiasing method that not conditioned on the contexts. 7) NIE: it is the marginal difference between TDE and TE, i.e., NIE = TE-TDE, which can be considered as the pure effect caused by introducing the bias Z → Y . NOTE: although zero vector can also be used as the wiped-out inputx, we chose the mean feature of training set for minor improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Quantitative Studies</head><p>RR &amp; ZSRR. The results are listed in <ref type="table">Table 1&amp;</ref> 2. Despite the fact that conventional debiasing methods: Reweight and Resample, directly hack the mR@K metric, they only gained limited advantages in RR but not in ZSRR. In contrast to the high mR@K of Reweight in RR SGDet, it got embarrassingly 0.0/0.0 in ZSRR SGDet, indicating that such debiased training methods ruin the useful context prior. Focal loss <ref type="bibr" target="#b28">[29]</ref> barely worked for both RR and ZSRR. Causal graph surgeries, X2Y and X2Y-Tr, both improved RR and ZSRR from the baseline, yet their increases were limited. TE had a very similar performance to TDE, but as we discussed, it removed the general bias rather than the subject-object specific bias. NIE is the marginal improvements from TE to TDE, which was even worse than baseline. Although R@K is not a qualified metric for RR as we discussed, we still reported the R@50/100 performance of MOTIFS † -SUM in <ref type="figure">Figure 6</ref>. We can observe a performance drop from baseline to TDE, but a further analysis shows that those considered as correct in baseline and "incorrect" in TDE were mainly the "head" predicates, and they are classified by TDE into more fine-grained "tail" classes. Among all three models and two fusion functions, even the worst TDE performance outperforms previous state-of-theart methods <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b5">6]</ref> by a large margin on RR mR@K. S2GR. In S2GR, Focal and Reweight are even worse than the baseline. Among all the three conventional debiasing methods, Resample was the most stable one based on our experiments. X2Y and X2Y-Tr have minor advantages over baseline. TE takes the 2nd place and was only a little bit worse than TDE. NIE is the worst as we expected because it is only based on the pure context bias. It is worth highlighting that all the three models and two fusion functions had significant improvements after we applied TDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Qualitative Studies</head><p>We visualized several SGCls examples that generated from MOTIFS † -SUM baseline and TDE in the top and mid rows of <ref type="figure" target="#fig_6">Figure 7</ref>, scene graphs generated by TDE are much more discriminative compared to the baseline model which prefers trivial predicates like on. The right half of the mid row shows that the baseline model would even generate holding due to the long-tail bias when the girl is not touching the kite, implying that the biased predictions are easy to be "blind", while TDE successfully predicted looking at. The bottom of <ref type="figure" target="#fig_6">Figure 7</ref> is an example of S2GR, where the SGs detected by baseline model lost the detailed actions of people, considering both person walking on street and person standing on street as person on street, which caused worse retrieval results. All the examples show a clear trend that TDE is much more sensitive to those semantically informative relationships instead of the trivially biased ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We presented a general framework for unbiased SGG from biased training, and this is the first work addressing the serious bias issue in SGG. With the power of counterfactual causality, we can remove the harmful bias from the good context bias, which cannot be easily identified by traditional debiasing methods such as data augmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref> and unbiased learning <ref type="bibr" target="#b28">[29]</ref>. We achieved the unbiasedness by calculating the Total Direct Effect (TDE) with the help of a causal graph, which is a roadmap for training any SGG model. By using the proposed Scene Graph Diagnosis toolkit, our unbiased SGG results are considerably better than their biased counterparts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>This supplementary document is organized as follows: 1) section A: a comprehensive review of causal effect analysis in causal inference; 2) section B: more details of the simplified network structures in the original paper; 3) section C: more quantitative studies; 4) section D: more qualitative studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Review of Causal Effect Analysis</head><p>In this section, a comprehensive review of causal effect analysis is given in the form of the causal graph we proposed in Section 3, and we still follow the notations from the original paper. More detailed background knowledge about causal inference can be found in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref> while the extension of effect analysis (a.k.a. mediation analysis) is given in <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b58">59]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Mediator</head><p>Since the exhaustive introduction of causal inference would beyond the scope of this paper, we simplified or skipped the definitions of several concepts in the original paper without affecting the understanding. One of the skipped concepts is the mediator. In a causal graph, when we care about the effect of a variable X to the output variable Y , the descendant node of X that is located in the path between them is the mediator. For example, in the study of carcinogenesis by smoke (Cigarette → Nicotine → Cancer), nicotine is the mediator. In our case, object labels Z is the mediator of X to Y , which can be considered as the side effect of X that also affects Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Total, Direct and Indirect Effects</head><p>As we discussed in Section 4.2, without further counterfactual intervention on the mediator Z, the overall effect of X towards Y is regarded as the Total Effect (TE) of X on Y , which can be calculated as:</p><formula xml:id="formula_9">T E = Y x (u) − Yx(u).<label>(8)</label></formula><p>As illustrated in <ref type="figure" target="#fig_7">Figure 8</ref>, other than the path I → X that is cut off by the intervention X =x, all the other variables will take their values through the links of causal graph. Especially, the mediator Z will get valuez, which is calculated from Eq. (2) givenx as input. However, by only using the TE, we are still not able to separate the mediator-specific "causal effect" from "side effect", which limits the value of causal effect analysis. Thanks to the development of causal inference, here comes the decomposition of TE <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b59">60]</ref>. Generally, the TE of X is composed of the Direct Effect path X → Y and Indirect Effect (IE) caused by the sideeffect path X → Z → Y . Depending on whose effect we want to obtain, two kinds of decomposition can be applied. Decomposition 1: The first kind of decomposition is what we used in the Section 4.2, which separates the TE into the Total Direct Effect (TDE) and the Natural/Pure Indirect Effect (NIE/PIE). The former one has already been defined in the original paper as:</p><formula xml:id="formula_10">T DE = Y x (u) − Yx ,z (u),<label>(9)</label></formula><p>which can be regarded as the effect of X in the real situation, i.e., Z always takes the value z as if it had seen the real x. Meanwhile, the NIE or PIE is the effect caused by the mediator Z under a pure/natural situation, i.e., X will not take the value x under the specific case and it's only assigned to the general unactivated valuex. Therefore, the NIE of Z is denoted as:</p><formula xml:id="formula_11">N IE = Yx ,z (u) − Yx(u) (10) = T E − T DE,<label>(11)</label></formula><p>where we can easily identify that NIE is the effect of Z when it changes fromz to z in a pure environment, i.e., X =x. The illustrations of TDE and NIE are given in <ref type="figure" target="#fig_8">Figure 9</ref>.  culation of NDE/PDE is following:</p><formula xml:id="formula_12">T IE = Y x (u) − Y x,z (u).<label>(12)</label></formula><formula xml:id="formula_13">N DE = Y x,z (u) − Yx(u) (13) = T E − T IE,<label>(14)</label></formula><p>where NDE is the effect of X changing fromx to x under the pure environment Z =z. In general, we should put the effect we care under the real environment, i.e. TDE or TIE, so we can get the results specific to each cases. The above two types of decomposition are both commonly used in medical, political or psychological research <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b19">20]</ref>, which depends on which effect we want to obtain, main effect or side effect. Note that, if the system is a pure linear system, both two types of decomposition would be exactly the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Network Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Scene Graph Generation</head><p>In the original paper, we simplified the feature extraction module in Link I → X and the visual context module in Link I → Y . Their details will be given in this subsection. Feature Extraction Module. Since we adopted ResNeXt-101-FPN <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b62">63]</ref> as the backbone, the extracted M contains feature maps from 4 scales: (1/4, 1/8, 1/16, 1/32) → (M 0 , M 1 , M 2 , M 3 ). Each bounding box will be assigned to the corresponding M k , (k = 0, 1, 2, 3) based on their areas <ref type="bibr" target="#b33">[34]</ref>. Given a bounding box b i with area a i , the corresponding index k of feature map is calculated as follows: k = max(2, min(5, 4 + log 2 (a i /224 + 1 × 10 −6 ) )) − 2.</p><p>(15) Then ROIAlign <ref type="bibr" target="#b11">[12]</ref> will be applied to the selected bounding box b i on the corresponding M k for the feature r i as we described in Section 3.   Visual Context Module. To extract the visual context feature v e for the union box b i ∪ b j , we consider all 4 feature maps will provide complementary contextual information from different levels. Therefore, we extract ROIAlign <ref type="bibr" target="#b11">[12]</ref> features on all 4 feature maps before we project the visual context feature into a feature space of R 4096 . The entire module is summarized in the <ref type="table" target="#tab_7">Table 4</ref>, where the dummy mask operation in <ref type="bibr" target="#b6">(7)</ref> generates two masks for b i and b j independently, assigning 1.0 to the pixels inside the bounding box and 0.0 for the rest. The Special Treatment for PredCls. In the original paper, we skipped a special case of causal graph, i.e., causal graph for Predicate Classification (PredCls), for simplification. In PredCls, the ground truth object labels are given, which means the link X → Z is blocked by assigning ground truth labels. It won't affect TDE calculation, where Z takes the real value z. However, it's involved in the ablation studies of TE and NIE, where Z could be assigned toz. In this case,z will directly use to the mean vector of training set rather than be calculated from Eq. <ref type="bibr" target="#b1">(2)</ref>. We also need to notice that, for MOTIFS <ref type="bibr" target="#b70">[71]</ref>, Eq.(3) will take z e as input too, which is simplified in the original paper, because z e itself is derived from x e and it can be considered as the interaction between link X → Y and Z → Y in the causal graph.</p><formula xml:id="formula_14">Index Input Operation Output (1) (M 0 , b i ∪ b j ) ROIAlign (7 × 7 × 256) (2) (M 1 , b i ∪ b j ) ROIAlign (7 × 7 × 256) (3) (M 2 , b i ∪ b j ) ROIAlign (7 × 7 × 256) (4) (M 3 , b i ∪ b j ) ROIAlign (7 × 7 × 256) (5) (1-4) Concatenation (7 × 7 × 1024) (6) (5) Conv (7 × 7 × 256) (7) b i , b j dummy mask (27 × 27 × 2) (8)<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Sentence-to-Graph Retrieval</head><p>As we mentioned in the original paper, we treated Sentence-to-Graph Retrieval (S2GR) as the graph-to-graph matching problem, parsing query captions to text-SGs by <ref type="bibr" target="#b49">[50]</ref>. Both detected image-SGs and parsed text-SGs are composed of entities E k = {e k i } and relationships R k = {r k ij = (s k i , p k ij , o k j )}, where k ∈ {text, image}, subject and object categories (s k i , o k j ) share the same dictionary with e k i for each k, p k ij denotes the onehot vector of the predicate category.</p><p>The image-SGs and text-SGs are equipped with different embedding layers, because they have different dictionaries. The entities and relationships are encoded as:</p><formula xml:id="formula_15">E k embed = W k e E k ,<label>(16)</label></formula><formula xml:id="formula_16">R k embed = [W k s S k ; W k p P k ; W k o O k ],<label>(17)</label></formula><p>where</p><formula xml:id="formula_17">E k embed ∈ R N d ×N k e , R k embed ∈ R 3N d ×N k r , N d = 512</formula><p>is the dimension of embedded feature, N k e , N k r are numbers of entities and relationships for each image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2.1 Bilinear Attention Scene Graph Encoding</head><p>Since entities and relationships are both important for SGs, we apply Bilinear Attention Network (BAN) <ref type="bibr" target="#b18">[19]</ref> to encode their multimodal interactions into the same representation space. The same BAN model is used for both text-SGs and image-SGs, hence we remove k hereinafter for simplification. The original BAN involves two steps: 1) attention map generation, and 2) bilinear attended feature calculation. Because scene graph has already provides connections between entities and relationships, we skipped the first step and used normalized scene graph connection as attention map A ij = M ij / j M ij , where A, M ∈ R Ne×Nr , the scene graph connection M is defined as follows:</p><formula xml:id="formula_18">M ij = 1, if E i in R j , 0, if E i not in R j .<label>(18)</label></formula><p>The bilinear attended scene graph encoding is calculated by  be 12 × 10 −2 , which was decayed at 10 th and 25 th epochs by the factor of 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative Studies</head><p>The full results of Relationship Retrieval, including both conventional Recall@K and the adopted mean Re-call@K <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b5">6]</ref>, are given in <ref type="table" target="#tab_10">Table 6</ref>. Although a performance drop on conventional Recall@k is observed on TDE, the detailed analysis of the "decreased" predicates in <ref type="figure">Figure  6</ref> of the original paper implies that it's caused by a more fine-grained predicate classification.</p><p>The detailed predicate-level Recall@100 on PredCls of all three models, two fusion functions and baseline vs. TDE are given in <ref type="figure" target="#fig_0">Figure 12 13 14</ref>. Impressively, the distribution of the improved performances is no longer long-tailed while those conventional debiasing methods illustrated in <ref type="figure" target="#fig_0">Figure 11</ref> can't surpass the dataset distribution anyway. For TDE, very few decreased predicates are mainly due to the more fine-grained classification and we can observe significant improvements on their subclass predicates. Note that, unlike Reweight, which blindly hurt all frequent predicates, the proposed TDE will even improve some of the top-10 frequent predicates, like behind and above, which themselves are the subclasses of near. It further proves that the improvement of the proposed TDE doesn't come from hacking the distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Qualitative Studies</head><p>More Relationship Retrieval (RR) and Zero-Shot Relationship Retrieval (ZSRR) results are given in <ref type="figure" target="#fig_0">Figure 15</ref>, where top 10 relationships under SGCls are selected for each image. As we can see, other than the trivial relationship problem, conventional baseline barely distinguishes different entities. For example, in the left bottom image, the same sign is almost on every pole in the baseline while the TDE results are more sensitive to different entities. However, one of the problem of TDE is that it over emphasizes the action predicates. It even uses holding for pole and sign while the predicate on used by the baseline is more natural in this case.  <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b5">6]</ref>. The SGG models reimplemented under our codebase are denoted by the superscript †. Another example of Sentence-to-Graph Retrieval (S2GR) is illustrated in <ref type="figure" target="#fig_0">Figure 16</ref>. Although we only reported sub-graphs of the original SGDet results, due to the limited space, we can still find that the conventional baseline model is not able to detect predicate like eating, which causes the detected SGs only provide the spatial re- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence-to-Graph Diagnosis Query:</head><p>A group of kids and adults eating cake at a  <ref type="figure" target="#fig_0">Figure 16</ref>. An example of Sentence-to-Graph Retrieval (S2GR) results for MOTIFS † +SUM baseline (yellow box) and corresponding TDE (green box). The red boxes indicate ground truth matching results. Note that we only draw sub-graphs containing important objects and predicates, because the original detected scene graphs from SGDet have too many trivial objects and predicates.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>An example of scene graph generation (SGG). (a) An input image with bounding boxes. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>(a) The example of total direct effect calculation and corresponding operations on the causal graph, whereX represents wiped-out X. (b) Recall@100 of Predicate Classification for selected predicates ranking by sampling fraction. The biased generation refers to re-implemented MOTIFS<ref type="bibr" target="#b70">[71]</ref> and the proposed unbiased generation is the result from the same model using TDE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>(a) The framework used in our biased training. (b) The causal graph of the SGG framework. (c) An illustration of the proposed TDE inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>where ⊗ generates the one-hot unique vector R N ×N for the pair of N -way object labels. Link I → Y (Visual Context Input for SGG). This link extracts the contextual union region features v e = Convs(RoIAlign(M, b i ∪ b j )) where b i ∪ b j indicates the union box of two RoIs. The original causal graph of SGG together with two interventional and counterfactual alternates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Results of scene graphs generated from MOTIF † -SUM baseline (yellow) and corresponding TDE (green). Top: relationship retrieval results. Mid: zero shot relationship retrieval results. Red boxes indicate the zero shot triplets. Bottom: results of S2GR. Red boxes mean the correctly retrieved SGs. Part of the trivial detected objects are removed from the graphs, due to space limitation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>(DE) caused by the causal The illustration of Total Effect on causal graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>At the same time, since direct effect is not the target, their pure/natural effect should be removed from the TE. The cal-The illustration of Total Direct Effect and Pure/Natural Indirect Effect on causal graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 .</head><label>10</label><figDesc>The illustration of Total Indirect Effect and Pure/Natural Direct Effect on causal graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 .Figure 12 .</head><label>1112</label><figDesc>Conventional Debiasing Methods: Recall@100 on Predicate Classification for the most frequent 35 predicates. MOTIFS † [71]: Recall@100 on Predicate Classification for the most frequent 35 predicates.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 13 .Figure 14 .Figure 15 .</head><label>131415</label><figDesc>VCTree † [55]: Recall@100 on Predicate Classification for the most frequent 35 predicates. VTransE † [73]: Recall@100 on Predicate Classification for the most frequent 35 predicates.lationships, missing the most discriminative word eating in the query caption. Top 10 Relationship Retrieval (RR) and Zero-Shot Relationship Retrieval (ZSRR) results of SGCls for MOTIFS † +SUM baseline (yellow box) and corresponding TDE (green box). The red predicates indicate misclassified relationships, the purple predicates are those correctly classified relationships (in ground truth), the blue predicates are those not labeled in ground truth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>. In S2GR, the detected SGs (using SGDet) are regarded as the only representations of images, cut off all the dependencies on black-box visual features, so any bias on SGG would sensitively violate the coherence</figDesc><table><row><cell cols="3">Zero-Shot Relationship Retrieval</cell><cell>PredCls</cell><cell>SGCls</cell><cell>SGDet</cell></row><row><cell cols="2">Model Fusion</cell><cell cols="4">Method R@50/100 R@50/100 R@50/100</cell></row><row><cell></cell><cell></cell><cell cols="2">Baseline 10.9 / 14.5</cell><cell>2.2 / 3.0</cell><cell>0.1 / 0.2</cell></row><row><cell></cell><cell></cell><cell cols="2">Focal 10.9 / 14.4</cell><cell>2.2 / 3.1</cell><cell>0.1 / 0.3</cell></row><row><cell></cell><cell></cell><cell>Reweight</cell><cell>0.7 / 0.9</cell><cell>0.1 /0.1</cell><cell>0.0 / 0.0</cell></row><row><cell></cell><cell></cell><cell cols="2">Resample 11.1 / 14.3</cell><cell>2.3 / 3.1</cell><cell>0.1 / 0.3</cell></row><row><cell>MOTIFS  †</cell><cell>SUM</cell><cell cols="2">X2Y 11.8 / 17.6 X2Y-Tr 13.7 / 17.6 TE 14.2 / 18.1</cell><cell>2.3 / 3.7 3.1 / 4.2 1.4 / 2.0</cell><cell>1.6 / 2.7 1.8 / 2.8 1.4 / 1.8</cell></row><row><cell></cell><cell></cell><cell>NIE</cell><cell>2.4 / 3.2</cell><cell>0.2 / 0.4</cell><cell>0.3 / 0.6</cell></row><row><cell></cell><cell></cell><cell cols="2">TDE 14.4 / 18.2</cell><cell>3.4 / 4.5</cell><cell>2.3 / 2.9</cell></row><row><cell></cell><cell>GATE</cell><cell>Baseline TDE</cell><cell>7.4 / 10.6 7.7 / 11.0</cell><cell>0.9 / 1.3 1.9 / 2.6</cell><cell>0.2 / 0.4 1.9 / 2.5</cell></row><row><cell>VTransE  †</cell><cell>SUM GATE</cell><cell cols="2">Baseline 11.3 / 14.7 TDE 13.3 / 17.6 Baseline 4.2 / 5.9 TDE 5.3 / 7.9</cell><cell>2.5 / 3.3 2.9 / 3.8 1.9 / 2.6 2.1 / 3.0</cell><cell>0.8 / 1.5 2.0 / 2.7 1.9 / 2.6 1.9 / 2.7</cell></row><row><cell>VCTree  †</cell><cell>SUM GATE</cell><cell cols="2">Baseline 10.8 / 14.3 TDE 14.3 / 17.6 Baseline 4.4 / 6.8 TDE 5.9 / 8.1</cell><cell>1.9 / 2.6 3.2 / 4.0 2.5 / 3.3 3.0 / 3.7</cell><cell>0.2 / 0.7 2.6 / 3.2 1.8 / 2.7 2.2 / 2.8</cell></row></table><note>of SGs, resulting in worse retrieval results. For example, if walking on was detected as the biased alternative on, images would be mixed up with those have sitting on or laying on. Note that S2GR is fundamentally different</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc></figDesc><table /><note>The results of Zero-Shot Relationship Retrieval.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>in front of …… OTHERS in → covered in …… † †</head><label></label><figDesc></figDesc><table><row><cell>67.1</cell><cell>67.9</cell><cell>65.2</cell><cell>66</cell><cell cols="4">near OTHERS has</cell><cell></cell><cell></cell><cell>on has near</cell><cell>sitting on walking on …… using …… under</cell><cell>standing on parked on carrying behind above</cell></row><row><cell></cell><cell>51.4</cell><cell></cell><cell>46.2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">on</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>36.5</cell><cell>39.9</cell><cell>29.9</cell><cell>35.8</cell><cell>39.1</cell><cell>27.7</cell><cell>30.3</cell><cell>36.9</cell><cell>27.2</cell><cell>32.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>20.3</cell><cell>16.9</cell></row><row><cell cols="2">PredCls</cell><cell cols="2">PredCls</cell><cell cols="3">SGCls</cell><cell cols="3">SGCls</cell><cell>SGDet</cell><cell>SGDet</cell></row><row><cell cols="2">Recall@100</cell><cell cols="2">Recall@50</cell><cell cols="3">Recall@100</cell><cell cols="3">Recall@50</cell><cell>Recall@100</cell><cell>Recall@50</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">MOTIFS</cell><cell></cell><cell cols="2">MOTIFS</cell><cell></cell><cell cols="2">MOTIFS -TDE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>The details of Visual Context Module.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 ,</head><label>5</label><figDesc>where steps (4-10) are calculated 2 times, and the final output E graph ∈ R 1024 is a feature vector representing the whole SG. The same BAN is used for both text-SG or image-SG, i.e., the parameters of the BAN are shared.The model was trained by the triplet loss<ref type="bibr" target="#b48">[49]</ref> with L1 distance. The model was trained in 30 epochs by SGD optimizer and set batch size to be 12. Learning rate was set to</figDesc><table><row><cell>Index</cell><cell>Input</cell><cell>Loop</cell><cell>Operation</cell><cell>Output</cell></row><row><cell>(1)</cell><cell>Eembed</cell><cell></cell><cell>Input Shape</cell><cell>(Ne × 512)</cell></row><row><cell>(2)</cell><cell>Rembed</cell><cell></cell><cell>Input Shape</cell><cell>(Nr × 512)</cell></row><row><cell>(3)</cell><cell>A</cell><cell></cell><cell>Input Shape</cell><cell>(Ne × Nr)</cell></row><row><cell>(4)</cell><cell>(1)</cell><cell>start</cell><cell>Transpose + Unsqueeze</cell><cell>(512 × 1 × Ne)</cell></row><row><cell>(5)</cell><cell>(2)</cell><cell>↓</cell><cell>Transpose + Unsqueeze</cell><cell>(512 × Nr × 1)</cell></row><row><cell>(6)</cell><cell>(3)</cell><cell>↓</cell><cell>Unsqueeze</cell><cell>(1 × Ne × Nr)</cell></row><row><cell>(7)</cell><cell>(4),(6)</cell><cell>↓</cell><cell>Matrix Multiplication</cell><cell>(512 × 1 × Nr)</cell></row><row><cell>(8)</cell><cell>(5),(7)</cell><cell>↓</cell><cell>Matrix Multiplication</cell><cell>(512 × 1 × 1)</cell></row><row><cell>(9)</cell><cell>(8)</cell><cell>↓</cell><cell>Squeeze + FC</cell><cell>(512)</cell></row><row><cell>(10)</cell><cell>(4),(9)</cell><cell>end</cell><cell cols="2">Unsqueeze + Element-wise Addition (512 × 1 × Ne)</cell></row><row><cell>(11)</cell><cell>(10)</cell><cell></cell><cell>Sum Over Ne</cell><cell>512</cell></row><row><cell>(12)</cell><cell>(11)</cell><cell></cell><cell>FC + ReLU + FC + ReLU</cell><cell>1024</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 .</head><label>5</label><figDesc>The details of Bilinear Attention Scene Graph Encoding Module.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 .</head><label>6</label><figDesc>The SGG performances of Relationship Retrieval on both conventional Recall@K and mean Recall@K</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>table . adults of cake table group eat at girl girl woman pizza pizza plate plate on on on on on on girl girl woman pizza pizza plate plate at at at under under on table table eating table woman woman woman man glass glass cup table woman woman woman man glass glass cup on on on on on on on at at at at under under under man man child woman girl table plate plate plate food food on on on on on on on on on on man man child woman table plate plate plate food food at at at at at under eating under under girl</head><label>.</label><figDesc></figDesc><table><row><cell>kids</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments We'd like to thank all reviewers for their constructive comments. This work was partially supported by the NTU-Alibaba JRI.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Don&apos;t just assume; look and answer: Overcoming priors for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Influence of resampling on accuracy of imbalanced classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papanov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICMV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rubi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10169</idno>
		<title level="m">Reducing unimodal biases in visual question answering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Counterfactual critic multi-agent training for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Knowledge-embedded routing network for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Evaluation and validation of social and psychological markers in randomised trials of complex interventions in mental health: a methodological research programme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dunn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Emsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Landau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pickles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Scene graph generation with external knowledge and image reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and data engineering</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Women also snowboard: Overcoming bias in captioning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Gqa: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning by abstraction: The neural state machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The statistics of causal inference: A view from political methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Keele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bilinear attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A political mediation model of corporate response to social movement activism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Administrative Science Quarterly</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Referring relationships</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Resound: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and caption regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Repair: Removing representation bias by dataset resampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Vrr-vg: Refocusing visually-relevant relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10403" to="10412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mediation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Mackinnon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Fairchild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Explicit bias discovery in visual question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">maskrcnn-benchmark: Fast, modular reference implementation of Instance Segmentation and Object Detection algorithms in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Seeing through the human reporting bias: Visual classifiers from noisy human-centric labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Causal induction from visual observations for goal directed tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01751</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-R</forename><surname>Wen</surname></persName>
		</author>
		<title level="m">Counterfactual vqa: A cause-effect look at language bias. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Causality: models, reasoning and inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Direct and indirect effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th conference on uncertainty in artificial intelligence</title>
		<meeting>the 17th conference on uncertainty in artificial intelligence</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mackenzie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Book</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Why</surname></persName>
		</author>
		<title level="m">THE NEW SCIENCE OF CAUSE AND EFFECT. Basic Books</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.10496</idno>
		<title level="m">Two causal principles for improving visual dialog</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attentive relational networks for mapping images to scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Mediation analysis in epidemiology: methods, interpretation and bias. International journal of epidemiology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Richiardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bellocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zugna</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Identifiability and exchangeability for direct and indirect effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Greenland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Counterfactual thinking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Roese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological bulletin</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Edge and curve detection for visual scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thurston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on computers</title>
		<imprint>
			<date type="published" when="1971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generating semantically precise scene graphs from textual descriptions for improved image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fourth workshop on vision and language</title>
		<meeting>the fourth workshop on vision and language</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Explainable and explicit visual reasoning over scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Bounded rationality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Utility and probability</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improved semantic representations from tree-structured long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Visual commonsense r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Z Q S</forename><surname>Tan Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning to compose dynamic tree structures for visual contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Cognitive neuroscience of human counterfactual reasoning. Frontiers in human neuroscience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Van Hoeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">D</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Barbey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Explanation in causal inference: methods for mediation and interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vanderweele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Oxford University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">A three-way decomposition of a total effect into direct, indirect, and interactive effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Vanderweele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Epidemiology</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Exploring context and visual pattern of relationship for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Linknet: Relational embedding for scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Scene graph generation by iterative message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Graph r-cnn for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Deconfounded image captioning: A causal retrospect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Zoom-net: Mining deep feature interactions for visual relationship recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C. Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Neural motifs: Scene graph parsing with global context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Learning fair representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dwork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Predicate Classification Scene Graph Classification Scene Graph Detection Model Fusion Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<idno>R@20 / 50 / 100 mR@20 / 50 / 100 R@20 / 50 / 100 mR@20 / 50 / 100 R@20 / 50 / 100 mR@20 / 50 / 100</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Graphical contrastive losses for scene graph parsing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
