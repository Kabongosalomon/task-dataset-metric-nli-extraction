<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PENELOPIE: Enabling Open Information Extraction for the Greek Language through Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Papadopoulos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Crete</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Hellenic Army Academy</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♣</forename><forename type="middle">♦</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Papadakis</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Hellenic Army Academy</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">♦</forename></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Matsatsinis</surname></persName>
							<email>nmatsatsinis@isc.tuc.grnpapadakis@sse.gr</email>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Crete</orgName>
								<address>
									<country key="GR">Greece</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PENELOPIE: Enabling Open Information Extraction for the Greek Language through Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>https://github.com/lighteternal/PENE LOPIE. Our study has the following objectives: Figure 1: Steps of the PENELOPIE pipeline</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:48+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we present a methodology that aims at bridging the gap between high and low-resource languages in the context of Open Information Extraction, showcasing it on the Greek language. The goals of this paper are twofold: First, we build Neural Machine Translation (NMT) models for English-to-Greek and Greek-to-English based on the Transformer architecture. Second, we leverage these NMT models to produce English translations of Greek text as input for our NLP pipeline, to which we apply a series of pre-processing and triple extraction tasks. Finally, we back-translate the extracted triples to Greek. We conduct an evaluation of both our NMT and OIE methods on benchmark datasets and demonstrate that our approach outperforms the current state-of-the-art for the Greek natural language.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Open Information Extraction (OIE) techniques generally shine in high-resource languages (e.g. English, German) for which either linguistic principles leading to triple extraction have been identified or large annotated corpora and pretrained language models can be used. For lowresource languages like Modern Greek however, there is a relative sparsity of raw textual resources and annotated corpora that could lead to the development of similar systems. On the bright side, the need for multilingual resources (e.g. movie subtitles, applications, web content) has fueled several projects of compiling parallel corpora (i.e. collections of texts translated into one or more other languages than the original) over the last years. In this work, we propose a methodology that aims at enabling OIE for low-resource languages, focusing on the Greek OIE use case. To achieve this, we rely on Neural Machine Translation (NMT) as an intermediate step to translate the texts to English, in order to exploit the plethora of methods that exist for transforming English text to its structured representation.</p><p>We present PENELOPIE (Parallel EN-EL Open Information Extraction), a pipeline for information extraction from Greek corpora. An overview of our methodology is given in <ref type="figure">Figure 1</ref>. The code and related resources can be found in 1. to release a series of Transformer-based NMT models for English-to-Greek (EN2EL) and Greek-to-English (EL2EN) translation, trained on a consolidated parallel corpus and compare the translation results to the current-state-ofthe-art, 2. to leverage the aforementioned NMT models for the translation of Greek texts to their English counterpart and feed them to our English-based NLP pipeline. This pipeline incorporates a series of preprocessing tasks including in-place coreference resolution and extractive summarization, as well as an OIE system comprising of three extractors based on different approaches for more robust results. The extracted triples are finally back-translated to Greek and their quality is evaluated to facilitate comparison with other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we provide background information on neural machine translation and open information extraction approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Neural Machine Translation</head><p>NMT aims at modelling a direct mapping between source and target languages with deep neural networks. It has become the dominant paradigm of machine translation, achieving promising results in recent years which are usually surpassing those of traditional Statistical Machine Translation (SMT) approaches, given enough training data <ref type="bibr" target="#b23">(Stahlberg, 2020)</ref>. The invention of novel encoder-decoder architectures, from recurrent <ref type="bibr" target="#b24">(Sutskever et al., 2014;</ref><ref type="bibr" target="#b0">Bahdanau et al., 2015)</ref> and convolutional neural networks <ref type="bibr" target="#b8">(Kalchbrenner et al., 2014;</ref><ref type="bibr" target="#b6">Gehring et al., 2017)</ref> to self-attention (Transformer) mechanisms <ref type="bibr" target="#b28">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b22">So et al., 2019)</ref> has significantly pushed ahead the state-of-the-art, in terms of quality and efficiency, especially for morphologically rich languages. Another parallel line of research towards improving translation quality is to devise effective token encoding methods that can handle out-ofvocabulary (OOV) words, targeting the lack of 1to-1 correspondence between source and target languages, due to differences in their morphological structure. <ref type="bibr">Sennrich et al. (2016)</ref> utilized variants of byte-pair encoding (BPE) methods for word segmentation to enable the representation of rare and unseen words as a sequence of subword units, showing that NMT methods are capable of open-vocabulary translation. The latest advances in the field also include pretraining cross language models on multilingual data <ref type="bibr" target="#b2">(Conneau and Lample, 2019)</ref> or exploiting monolingual corpora for semisupervised learning through back-translation <ref type="bibr" target="#b20">(Sennrich et al., 2016a</ref>). It appears that not much effort has been targeted towards the Greek language with the notable exception of the Helsinki NLP group which has released EN2EL and EL2EN translation models evaluated on the Tatoeba dataset <ref type="bibr" target="#b27">(Tiedemann and Thottingal, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Information Extraction</head><p>Open information extraction (OIE) systems aim at distilling structured representations of information from natural language text, usually in the form of {subject, predicate, object} triples or n-ary propositions. Since OIE follows a relationindependent extraction paradigm, it can play a key role in many NLP applications including natural understanding and knowledge base construction, by extracting phrases that indicate semantic relationships between entities. In order to extract triples, most approaches try to identify linguistic extraction patterns, either hand-crafted or automatically learned from the data. An abundance of such systems exists, relying on concepts ranging from rule-based paradigms that focus on the grammatical and syntactic properties of the language <ref type="bibr" target="#b4">(Fader et al., 2011;</ref><ref type="bibr" target="#b3">Del Corro and Gemulla, 2013)</ref>, to supervised learning-based ones that leverage annotated data sources to train classifiers, with more recent implementations making use of language models <ref type="bibr" target="#b11">(Kolluru et al., 2020;</ref><ref type="bibr" target="#b17">Ro et al., 2020)</ref>. Despite the existence of so many approaches however, the majority of them just focuses on evaluating the efficiency of different triple extraction tools on raw data, without incorporating any preprocessing strategies to limit the number of potentially uninformative triples <ref type="bibr" target="#b14">(Niklaus et al., 2018)</ref>. Some more recent methods go beyond the triple extraction task by encompassing more thorough preprocessing and postprocessing strategies, including discourse analysis, coreference resolution or summarization to improve the quality of the extracted triples <ref type="bibr" target="#b9">(Kertkeidkachorn and Ichise, 2017;</ref><ref type="bibr" target="#b16">Papadopoulos et al., 2020)</ref>. There is currently no OIE system for the Greek language, although latest approaches that leverage pretrained language models allow for multilingual extractions through zero-shot learning <ref type="bibr" target="#b17">(Ro et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>The NMT architecture used in this paper for English-to-Greek (EN2EL) translation and Greekto-English (EL2EN) back-translation is a variant of the Transformer model <ref type="bibr" target="#b28">(Vaswani et al., 2017)</ref>, driven by the fact that self-attentional networks tend to perform distinctly better than other architectures on translation tasks <ref type="bibr" target="#b25">(Tang et al., 2020)</ref>. Both the encoder and the decoder are composed of stacked, multi-head, self-attention and fully-connected layers. One key difference between the two implementations is that ours includes a fully connected feed-forward network with an inner-layer dimensionality of = 1024, as opposed to the original one that uses a hidden layer with = 2048, in an effort to reduce computational cost, as our training testbed had limited memory capabilities. With regard to vocabulary construction, we relied on subword units extracted with BPE, experimenting with two different configurations of merge operations</p><p>Our approach for efficient open information extraction on translated texts combines a series of distinct modules for in-place coreference resolution, extractive summarization and parallel triple extraction with the following specifications:</p><p>Coreference Resolution: We rely on a variant of the pretrained end-to-end coreference resolution model from <ref type="bibr">Lee et al. (2017)</ref> using Span-BERT embeddings <ref type="bibr" target="#b7">(Joshi et al., 2020)</ref>, trained on the OntoNotes 5.0 dataset. Each translated sequence is pre-processed by the in-place coreference resolution component, where all noun phrases (mentions) referring to the same entity are substituted with that entity.</p><p>Summarization: Extractive text summarization is used on the coreference-resolved text to reduce the original documents' length by omitting peripheral information while highlighting key features that are appropriate for triple extraction. We use the transformer-based implementation from <ref type="bibr" target="#b12">Miller (2019)</ref> where all sentences are embedded into the multi-dimensional space using 1 https://github.com/moses-smt/mosesdecoder BERT embeddings. K-means clustering is then used on the sentence representations to identify those closest to the cluster's centroids for summary selection.</p><p>Parallel Triple Extraction: Here we combine three popular OIE systems, relying both on rulebased (handcrafted extraction heuristics and clauses) and learning-based (semantic role labelling and sequence BIO tagging) systems, relying on the complementarity between the different approaches to ensure maximum recall.</p><p>We provide additional information regarding the technical implementation of the described information extraction pipeline in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NMT Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset:</head><p>We exploited most of the EN-EL resources available in the OPUS repository <ref type="bibr" target="#b26">(Tiedemann, 2012)</ref>, with main ones being the ParaCrawl, OpenSubtitles, EUBookshop, DGT and Europarl datasets. We combined these with the available parallel corpora of CCMatrix <ref type="bibr" target="#b19">(Schwenk et al., 2019)</ref> mined in the textual content of Wikipedia, to create a dataset comprising 50451352 sentences (~6.3GB).</p><p>Preprocessing: We applied a cleaning script on the corpus that discarded any segment with a word exceeding 1000 characters, leading to a corpus of 36251157 sentences. We tokenized these using the Moses 1 tokenizer and split the dataset so that 1 every 23 sentences were assigned to the validation set and the rest to the training set. For the construction of the model dictionaries, we worked towards the creation of two different preprocessing setups leading in two training configurations:</p><p>a. For the first setup, we lower-cased all tokens in the train and test set, in an effort to reduce the dictionary size without losing translation quality. We then applied BPE segmentation 2 with an encoding size of 10000 to speed up training and inference. This resulted in dictionaries of 12892 and 9932 tokens for Greek and English accordingly. b. For the second setup, we applied BPE segmentation directly to the mixed-case text with an encoding size of 20000, resulting in dictionaries of size 23220 and 15284 for Greek and English respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NMT Model Settings and Training:</head><p>We utilized Fairseq <ref type="bibr" target="#b15">(Ott et al., 2019)</ref>, a popular sequence-to-sequence toolkit maintained by Facebook AI Research to train our models with data from both setups and ran our experiments on a machine with a single NVIDIA GeForce RTX-2080 SUPER (8GB of VRAM). We implemented a shallower variant of the Transformer architecture with 4 attention heads, 6 encoder and 6 decoder layers, both with an embedding size of 512 and a feed-forward hidden layer dimension of 1024. During training, regularization was done with a dropout of 0.3 and label smoothing of 0.1. We used the Adam optimizer (Kingma and Ba, 2015) with 4000 warm-up steps and a maximum learning rate of 0.0005. The model was trained for 5 epochs and the best checkpoint was selected based on the perplexity of the validation set. We used mixed precision during training <ref type="bibr" target="#b13">(Narang et al., 2018)</ref>, using FP16 precision to address our hardware limitations by reducing the memory consumption and time spent in memory. The produced models (4 in total) are as follows: i. a lower-case EL2EN and a lower-case EN2EL model from the first setup based on shorter dictionaries, ii. a mixed-case EL2EN and EN2EL model from the second setup on larger dictionaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Information Extraction Setup</head><p>Coreference Resolution Framework: Each EL2EN translated sequence was processed by the pretrained neural model from AllenNLP which relies on Lee et al. (2017) but has the original GloVe embeddings substituted with Span-BERT embeddings. This approach considers all possible spans in a document as potential mentions and learns distributions over possible antecedents for each span. Its ability to solve challenging pronoun disambiguation problems facilitated the creation of more informative triples.</p><p>Summarization Framework: In order to reduce the size of the ingested text, we relied on the pretrained extractive summarizer from <ref type="bibr" target="#b12">Miller (2019)</ref> made available by HuggingFace, that utilizes the BERT model for text embeddings and k-means clustering to identify sentences close to the centroid for summary selection.</p><p>3 https://tatoeba.org/ Triple Extraction Engines: We integrated 3 OIE engines based on different extraction strategies: a. Open IE 5.1 from UW and IIT Delhi which is based on the combination of four different rule-based and learning-based OIE tools, b. ClausIE from MPI that follows a clause-based approach, and c. AllenNLP OIE that formulates the triple extraction problem as a sequence BIO tagging problem and applies a bi-LSTM transducer to produce OIE tuples. We further employed a deduplication process to keep only the unique triples and eliminate all redundant extractions. Since the goal of our work was to provide triples in the Greek language and the produced triples were in English, we used our EN2EL NMT model to translate them back to Greek.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results and Discussion</head><p>We provide results both for the EL-EN NMT tasks and for the OIE task on Greek corpora, since the former can be evaluated independently. For both EN-EL and EL-EN directions we compare with the current state-of-the-art models produced by the Helsinki NLP group, evaluated on the Tatoeba dataset <ref type="bibr" target="#b27">(Tiedemann and Thottingal, 2020)</ref>. Another relevant implementation from the Facebook AI team provides results of their XLM-R model on the XNLI dataset <ref type="bibr" target="#b18">(Ruder et al., 2019)</ref>; however -given the different scope of that paperresults are presented in terms of cross-lingual classification accuracy and not in terms of NMT translation quality (e.g. BLEU), hindering direct comparisons. Nevertheless, we also provide BLEU and chrF scores on the parallel EN-EL corpus of the XNLI dataset hoping that it will facilitate comparisons with future models. The results on the Tatoeba test set showcase a significant performance gain of our models in terms of BLEU (+10.9 BLEU for EN2EL and +10.5 BLEU for EL2EN translations) over the Helsinki ones, while all models have very close chrF scores. The apparent difference in performance gains between the two different metrics can ascribed to the idiosyncratic morphological and syntactic properties of the Greek language (accent, inflation, declension etc.) that may result in the produced translations being slightly different from the original sequences. Since chrF incorporates character matches while BLEU does not, it is possible to produce translations that achieve low BLEU but acceptable chrF scores. Therefore, given that BLEU is an ngram-based metric and chrF is a character-based one, we consider the good results on both metrics as a positive characteristic towards producing quality estimates that are as close as possible to human judgements. The results also seem promising on the more challenging XNLI test set, although a direct comparison with other models would have been more useful. While the lowercase models seem to perform slightly better on every test, the richer vocabulary and correct casing of the mixed-case ones compensates for the slightly worse metrics scores. It should be noted that in order to ensure a fair comparison, the mixed-cased models were evaluated on the original reference translations, while the lower-case models were evaluated on a lower-case version of the same translations. Another aspect that adds to the reason why lower-case NMT models were able to showcase slightly better scores is that the former reduce the expansion of the vocabulary by neglecting some morphology information, while mixed-case models will increase the vocabulary to 5 https://huggingface.co/lighteternal keep the original morphological form and as a result may lose connections with the lowercase forms of some words. Finally, while our models were trained using the Fairseq framework, we also ported them to HuggingFace Transformers format and made them publicly available 5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">NMT performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">OIE performance</head><p>We evaluate the performance of PENELOPIE using the CaRB benchmark which is widely used for the comparison of OIE systems <ref type="bibr" target="#b1">(Bhardwaj et al., 2020)</ref>. Given the lack of a gold standard of Greek annotated triples, we created a translated version of the original CaRB test set for our experiments, consisting of 2715 sentences and their extracted semantic triples. The test set was automatically translated using our EN2EL mixed case model. We compare our extraction results with Multi2OIE from <ref type="bibr" target="#b17">Ro et al. (2020)</ref>, an OIE engine with state-of-the-art performance on English corpora. Multi2OIE relies on the pretrained multilingual BERT model and can perform multilingual extractions through zero-shot learning (it is trained on English data); thus it can be leveraged to produce results on the Greek CaRB test set. For PENELOPIE, results are only provided using the mixed-case NMT model (similar results to the lower-case one). It should be noted that the summarization module was not utilized during the benchmark, as the gold dataset consisted of single sentences. This is a general shortcoming in the assessment of OIE systems that leverage preprocessing features (such as summarization or coreference resolution); the gold triples and the metrics involved in the evaluation process favour exact matches of the processed sentences, rather than focusing on the usability of the extracted results. As a result, some of the gold triples in benchmark datasets -although valid-may have low contextual value. The scores are presented in terms of precision, recall and F1-score in  Our pipeline outperforms the state-of-the-art Multi2OIE on the Greek OIE task, on all metrics. The most remarkable difference in performance is shown in terms of recall, which can be partially attributed to the fact that PENELOPIE leverages a number of different extraction tools leading to a recall-oriented approach. In addition, given that all triples are individually back-translated to Greek, it is not guaranteed that the translation output of each element will match the span of the derived sentence, especially in languages with rich morphology (e.g. conjugation, declension). This justifies the relatively low scores of PENELOPIE compared to English OIE systems, whose F1scores may exceed 0.50 for state-of-the-art approaches (although a direct comparison between different languages is not straightforward). To this end, a source-target word alignment approach inspired by the work of <ref type="bibr" target="#b5">Garg et al. (2020)</ref> was explored, but current implementations seem to have difficulties in aligning tokens with accents 6 (e.g. Greek ones).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>We have presented the use of NMT models integrated in an OIE pipeline to achieve triple extraction for low-resource languages, showcasing our approach on the Greek language. To this end, we trained 4 models (2 EN2EL and 2 EL2EN) that outperform the state-of-the-art by a significant margin (&gt;10 BLEU) and made them publicly available. We leveraged these along with a set of preprocessing and triple extraction tools to construct the PENELOPIE pipeline aiming at information extraction from Greek texts. We demonstrated the efficiency of our methodology via a benchmark framework and obtained significantly better results (+116% in F1-score) compared to the best multilingual OIE system currently available.</p><p>For future work, we will focus more on wordlevel alignment to improve the quality of our extractions. We would also like to explore transfer learning approaches to create an end-to-end OIE system for Greek without relying on annotated datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>shows the evaluation of our models (lowercase and mixed-case) on the Tatoeba 3 and XNLI 4 test sets.</figDesc><table><row><cell cols="3">Evaluation on Tatoeba test set (EN-EL)</cell></row><row><cell>Model</cell><cell cols="2">BLEU chrF</cell></row><row><cell>Helsinki-2019-12-04-EN2EL</cell><cell>52.7</cell><cell>0.721</cell></row><row><cell>Helsinki-2019-12-18-EN2EL</cell><cell>56.4</cell><cell>0.745</cell></row><row><cell>OURS-lower-case-EN2EL</cell><cell>77.3</cell><cell>0.739</cell></row><row><cell>OURS-mixed-case-EN2EL</cell><cell>76.9</cell><cell>0.733</cell></row><row><cell>Helsinki-2019-12-04-EL2EN</cell><cell>69.4</cell><cell>0.801</cell></row><row><cell>OURS-lower-case-EL2EN</cell><cell>79.9</cell><cell>0.802</cell></row><row><cell>OURS-mixed-case-EL2EN</cell><cell>79.3</cell><cell>0.795</cell></row><row><cell cols="3">Evaluation on XNLI test set (EN-EL)</cell></row><row><cell>Model</cell><cell cols="2">BLEU chrF</cell></row><row><cell>OURS-lower-case-EN2EL</cell><cell>66.1</cell><cell>0.606</cell></row><row><cell>OURS-mixed-case-EN2EL</cell><cell>65.4</cell><cell>0.624</cell></row><row><cell>OURS-lower-case-EL2EN</cell><cell>67.4</cell><cell>0.633</cell></row><row><cell>OURS-mixed-case-EL2EN</cell><cell>66.2</cell><cell>0.623</cell></row><row><cell cols="3">Table 1: EN2EL and EL2EN NMT evaluation results</cell></row><row><cell cols="2">&amp; comparison with other models.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell>Model</cell><cell>Prec. Rec.</cell><cell>F1</cell></row><row><cell>Multi2OIE</cell><cell cols="2">0.200 0.084 0.118</cell></row><row><cell cols="3">PENELOPIE 0.231 0.284 0.255</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>PENELOPIE evaluation results on the translated CaRB testset &amp; comparison with Multi2OIE.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/rsennrich/subword-nmt</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/facebookresearch/XNLI</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The research work of D.P. was supported by the Hellenic Foundation for Research and Innovation </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyung</forename><forename type="middle">Hyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CARB: A crowdsourced benchmark for open IE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangnie</forename><surname>Bhardwaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Crosslingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ClausIE: Clause-based open information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luciano</forename><surname>Del Corro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Gemulla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW 2013 -Proceedings of the 22nd International Conference on World Wide Web</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identifying relations for Open Information Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2011 -Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Jointly learning to align and translate with transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarthak</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Peitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udhyakumar</forename><surname>Nallasamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Paulik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP 2019 -2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">34th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving Pre-training by Representing and Predicting Spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A convolutional neural network for modelling sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">52nd Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ACL 2014 -Proceedings of the Conference</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">T2KG: An End-to-End System for Creating Knowledge Graph from Unstructured Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natthawut</forename><surname>Kertkeidkachorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryutaro</forename><surname>Ichise</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">IMoJIE: Iterative Memory-Based Joint Open Information Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keshav</forename><surname>Kolluru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samarth</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vipul</forename><surname>Rathore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumen</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP 2017 -Conference on Empirical Methods in Natural Language Processing, Proceedings</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ACL 2020 -Proceedings of the Conference Kenton Lee, Luheng He, Mike Lewis,</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Miller</surname></persName>
		</author>
		<title level="m">Leveraging BERT for Extractive Text Summarization on Lectures</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mixed precision training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erich</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paulius</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksii</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018 -Conference Track Proceedings</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A Survey on Open Information Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Niklaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Cetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">André</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegfried</forename><surname>Handschuh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Computational Linguistics</title>
		<meeting>the 27th International Conference on Computational Linguistics<address><addrLine>Santa Fe, New Mexico, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-08" />
			<biblScope unit="page" from="3866" to="3878" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL HLT 2019 -2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies -Proceedings of the Demonstrations Session</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A methodology for open information extraction and representation from large scientific corpora: The CORD-19 data exploration use case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Papadakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonis</forename><surname>Litke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi2OIE: Multilingual Open Information Extraction based on Multi-Head Attention with BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngbin</forename><surname>Ro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukyung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pilsung</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1107" to="1117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised cross-lingual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL 2019 -57th Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<publisher>Tutorial Abstracts</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<title level="m">CCMatrix: Mining billions of high-quality parallel sentences on the WEB</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving neural machine translation models with monolingual data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 -Long Papers</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 -Long Papers</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The evolved transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">36th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural Machine Translation: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Stahlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Why self-attention? A targeted evaluation of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongbo</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annette</forename><surname>Rios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Parallel data, tools and interfaces in OPUS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Language Resources and Evaluation, LREC</title>
		<meeting>the 8th International Conference on Language Resources and Evaluation, LREC</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">OPUS-MT -Building open translation services for the World</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jörg</forename><surname>Tiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santhosh</forename><surname>Thottingal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)</title>
		<meeting>the 22nd Annual Conferenec of the European Association for Machine Translation (EAMT)<address><addrLine>Lisbon, Portugal</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
