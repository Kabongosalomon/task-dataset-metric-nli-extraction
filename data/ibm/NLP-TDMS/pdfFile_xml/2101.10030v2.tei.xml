<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly-supervised Video Anomaly Detection with Robust Temporal Feature Magnitude Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">South Australian Health and Medical Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanhong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajvinder</forename><surname>Singh</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">South Australian Health and Medical Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><forename type="middle">W</forename><surname>Verjans</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Faculty of Health and Medical Sciences</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">South Australian Health and Medical Research Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Australian Institute for Machine Learning</orgName>
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly-supervised Video Anomaly Detection with Robust Temporal Feature Magnitude Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Anomaly detection with weakly supervised video-level labels is typically formulated as a multiple instance learning (MIL) problem, in which we aim to identify snippets containing abnormal events, with each video represented as a bag of video snippets. Although current methods show effective detection performance, their recognition of the positive instances, i.e., rare abnormal snippets in the abnormal videos, is largely biased by the dominant negative instances, especially when the abnormal events are subtle anomalies that exhibit only small differences compared with normal events. This issue is exacerbated in many methods that ignore important video temporal dependencies. To address this issue, we introduce a novel and theoretically sound method, named Robust Temporal Feature Magnitude learning (RTFM), which trains a feature magnitude learning function to effectively recognise the positive instances, substantially improving the robustness of the MIL approach to the negative instances from abnormal videos. RTFM also adapts dilated convolutions and self-attention mechanisms to capture long-and shortrange temporal dependencies to learn the feature magnitude more faithfully. Extensive experiments show that the RTFM-enabled MIL model (i) outperforms several stateof-the-art methods by a large margin on three benchmark data sets (ShanghaiTech, UCF-Crime and XD-Violence) and (ii) achieves significantly improved subtle anomaly discriminability and sample efficiency. Code is available at https://github.com/tianyu0207/RTFM .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video anomaly detection has been intensively studied because of its potential to be used in autonomous surveillance systems <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b67">68]</ref>. The goal of video anomaly detection is to identify the time window when an anomalous event happened -in the context of surveillance, ex- <ref type="bibr">Figure 1</ref>. RTFM trains a feature magnitude learning function to improve the robustness of MIL approaches to normal snippets from abnormal videos, and detect abnormal snippets more effectively. Left: temporal feature magnitudes of abnormal and normal snippets ( x + and x − ), from abnormal and normal videos (X + and X − ). Assuming that µ = 3 denotes the number of abnormal snippets in the anomaly video, we can maximise the ∆score(X + , X − ), which measures the difference between the scores of abnormal and normal videos, by selecting the top k ≤ µ snippets with the largest temporal feature magnitude (the scores are computed with the mean of magnitudes of the top k snippets). Right: the ∆score(X + , X − ) increases with k ∈ [1, µ] and then decreases for k &gt; µ, showing evidence that our proposed RTFMenabled MIL model provides a better separation between abnormal and normal videos when k ≈ µ, even if there are a few normal snippets with large feature magnitudes. amples of anomaly are bullying, shoplifting, violence, etc. Although one-class classifiers (OCCs, also called unsupervised anomaly detection) trained exclusively with normal videos have been explored in this context <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b65">66]</ref>, the best performing approaches explore a weaklysupervised setup using training samples with video-level label annotations of normal or abnormal <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b67">68]</ref>. This weakly-supervised setup targets a better anomaly classification accuracy at the expense of a relatively small human annotation effort, compared with OCC approaches.</p><p>One of the major challenges of weakly supervised anomaly detection is how to identify anomalous snippets from a whole video labelled as abnormal. This is due to two reasons, namely: 1) the majority of snippets from an abnor-mal video consist of normal events, which can overwhelm the training process and challenge the fitting of the few abnormal snippets; and 2) abnormal snippets may not be sufficiently different from normal ones, making a clear separation between normal and abnormal snippets challenging. Anomaly detection trained with multiple-instance learning (MIL) approaches <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b69">70]</ref> mitigates the issues above by balancing the training set with the same number of abnormal and normal snippets, where normal snippets are randomly selected from the normal videos and abnormal snippets are the ones with the top anomaly scores from abnormal videos. Although partly addressing the issues above, MIL introduces four problems: 1) the top anomaly score in an abnormal video may not be from an abnormal snippet; 2) normal snippets randomly selected from normal videos may be relatively easy to fit, which challenges training convergence; 3) if the video has more than one abnormal snippet, we miss the chance of having a more effective training process containing more abnormal snippets per video; and 4) the use of classification score provides a weak training signal that does not necessarily enable a good separation between normal and abnormal snippets. These issues are exacerbated even more in methods that ignore important temporal dependencies <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b67">68]</ref>.</p><p>To address the MIL problems above, we propose a novel method, named Robust Temporal Feature Magnitude (RTFM) learning. In RTFM, we rely on the temporal feature magnitude of video snippets, where features with low magnitude represent normal (i.e., negative) snippets and high magnitude features denote abnormal (i.e., positive)) snippets. RTFM is theoretically motivated by the top-k instance MIL <ref type="bibr" target="#b20">[21]</ref> that trains a classifier using k instances with top classification scores from the abnormal and normal videos, but in our formulation, we assume that the mean feature magnitude of abnormal snippets is larger than that of normal snippets, instead of assuming separability between the classification scores of abnormal and normal snippets <ref type="bibr" target="#b20">[21]</ref>. RTFM solves the MIL issues above, as follows: 1) the probability of selecting abnormal snippets from abnormal videos increases; 2) the hard negative normal snippets selected from the normal videos will be harder to fit, improving training convergence; 3) it is possible to include more abnormal snippets per abnormal video; and 4) using feature magnitude to recognise positive instances is advantageous compared to MIL methods that use classification scores <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b50">51]</ref>, because it enables a stronger learning signal, particularly for the abnormal snippets that have a magnitude that can increase for the whole training process, and the feature magnitude learning can be jointly optimised with the MIL anomaly classification to enforce large margins between abnormal and normal snippets at both the feature representation space and the anomaly classification output space. <ref type="figure" target="#fig_5">Fig. 1</ref> motivates RTFM, showing that the selection of the top-k features (based on their magnitude) can provide a better separation between abnormal and normal videos, when we have more than one abnormal snippet per abnormal video and the mean snippet feature magnitude of abnormal videos is larger than that of normal videos.</p><p>In practice, RTFM enforces large margins between the top k snippet features with largest magnitudes from abnormal and normal videos, which has theoretical guarantees to maximally separate abnormal and normal video representations. These top k snippet features from normal and abnormal videos are then selected to train a snippet classifier. To seamlessly incorporate long and short-range temporal dependencies within each video, we combine the learning of long and short-range temporal dependencies with a pyramid of dilated convolutions (PDC) <ref type="bibr" target="#b61">[62]</ref> and a temporal self-attention module (TSA) <ref type="bibr" target="#b57">[58]</ref>. We validate our RTFM on three multi-scene anomaly detection benchmark data sets, namely ShanghaiTech <ref type="bibr" target="#b23">[24]</ref>, UCF-Crime <ref type="bibr" target="#b50">[51]</ref>, and XD-Violence <ref type="bibr" target="#b58">[59]</ref>. We show that our method outperforms the current SOTAs by a large margin on ShanghaiTech, UCF-Crime and XD-Violence using different pre-trained features (i.e., C3D and I3D). We also show that our method achieves substantially better sample efficiency and subtle anomaly discriminability than popular MIL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised Anomaly Detection.</p><p>Traditional anomaly detection methods assume the availability of normal training data only and address the problem with one-class classification using handcrafted features <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b64">65]</ref>. With the advent of deep learning, more recent approaches use the features from pre-trained deep neural networks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b66">67]</ref>. Others apply constraints on the latent space of normal manifold to learn compact normality representations <ref type="bibr">[1, 3-5, 8, 9, 11, 27, 29, 36, 38, 44, 47, 56, 69]</ref>. Alternatively, some approaches depend on data reconstruction using generative models to learn the representations of normal samples by (adversarially) minimising the reconstruction error <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b70">71]</ref>. These approaches assume that unseen anomalous videos/images often cannot be reconstructed well and consider samples of high reconstruction errors to be anomalies. However, due to the lack of prior knowledge of abnormality, these approaches can overfit the training data and fail to distinguish abnormal from normal events.</p><p>Weakly Supervised Anomaly Detection. Leveraging some labelled abnormal samples has shown substantially improved performance over the unsupervised approaches <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b58">59]</ref>. However, large-scale frame-level label annotation is too expensive to obtain. Hence, current SOTA video anomaly detection approaches rely on weakly supervised training that uses cheaper videolevel annotations. Sultani et al. <ref type="bibr" target="#b50">[51]</ref> proposed the use of video-level labels and introduced the large-scale weakly-supervised video anomaly detection data set, UCF-Crime. Since then, this direction has attracted the attention of the research community <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>Weakly-supervised video anomaly detection methods are mainly based on the MIL framework <ref type="bibr" target="#b50">[51]</ref>. However, most MIL-based methods <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b69">70]</ref> fail to leverage abnormal video labels as they can be affected by the label noise in the positive bag caused by a normal snippet mistakenly selected as the top abnormal event in an anomaly video. To deal with this problem, Zhong et al. <ref type="bibr" target="#b67">[68]</ref> reformulated this problem as a binary classification under noisy label problem and used a graph convolution neural (GCN) network to clear the label noise. Although this paper shows more accurate results than <ref type="bibr" target="#b50">[51]</ref>, the training of GCN and MIL is computationally costly, and it can lead to unconstrained latent space (i.e., normal and abnormal features can lie at any place of the feature space) that can cause unstable performance. By contrast, our method has trivial computational overheads compared to the original MIL formulation. Moreover, our method unifies the representation learning and anomaly score learning by an 2 -norm-based temporal feature ranking loss, enabling better separation between normal and abnormal feature representations, improving the exploration of weak labels compared to previous MIL methods <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b69">70]</ref>.</p><p>Temporal Dependency has been explored in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b67">68]</ref>. In anomaly detection, traditional methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b60">61]</ref> convert consecutive frames into handcrafted motion trajectories to capture the local consistency between neighbouring frames. Diverse temporal dependency modelling methods have been used in deep anomaly detection approaches, such as stacked RNN <ref type="bibr" target="#b25">[26]</ref>, temporal consistency in future frame prediction <ref type="bibr" target="#b23">[24]</ref>, and convolution LSTM <ref type="bibr" target="#b22">[23]</ref>. However, these methods capture short-range fixed-order temporal correlations only with single temporal scale, ignoring the long-range dependency from all possible temporal locations and the events with varying temporal length. GCN-based methods are explored in <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b67">68]</ref> to capture the long-range dependency from snippets features, but they are inefficient and hard to train. By contrast, our proposed module combines PDC <ref type="bibr" target="#b61">[62]</ref> and TSA <ref type="bibr" target="#b57">[58]</ref> on the temporal dimension to seamlessly and efficiently incorporate both the long and short-range temporal dependencies into our temporal feature ranking loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method: RTFM</head><p>Our proposed robust temporal feature magnitude (RTFM) approach aims to differentiate between abnormal and normal snippets using weakly labelled videos for training. Given a set of weakly-labelled training videos D =</p><formula xml:id="formula_0">{(F i , y i )} |D| i=1 , where F ∈ F ⊂ R T ×D are pre-computed</formula><p>features (e.g., I3D <ref type="bibr" target="#b6">[7]</ref> or C3D <ref type="bibr" target="#b52">[53]</ref>) of dimension D from the T video snippets, and y ∈ Y = {0, 1} denotes the video-level annotation (y i = 0 if F i is a normal video and y i = 1 otherwise). The model used by RTFM is denoted by r θ,φ (F) = f φ (s θ (F)) and returns a T -dimensional feature [0, 1] T representing the classification of the T video snippets into abnormal or normal, with the parameters θ, φ defined below. The training of this model comprises a joint optimisation of an end-to-end multi-scale temporal feature learning, and feature magnitude learning and an RTFM-enabled MIL classifier training, with the loss</p><formula xml:id="formula_1">min θ,φ |D| i,j=1 s (s θ (F i ), (s θ (F j )), y i , y j )+ f (f φ (s θ (F i )), y i ), (1) where s θ : F → X is the temporal feature extractor (with X ⊂ R T ×D ), f φ : X → [0, 1] T is the snippet classifier, s (.)</formula><p>denotes a loss function that maximises the separability between the top-k snippet features from normal and abnormal videos, and f (.) is a loss function to train the snippet classifier f φ (.) also using the top-k snippet features from normal and abnormal videos. Next, we discuss the theoretical motivation for our proposed RTFM, followed by a detailed description of the approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Theoretical Motivation of RTFM</head><p>Top-k MIL in <ref type="bibr" target="#b20">[21]</ref> extends MIL to an environment where positive bags contain a minimum number of positive samples and negative bags also contain positive samples, but to a lesser extent, and it assumes that a classifier can separate positive and negative samples. Our problem is different because negative bags do not contain positive samples, and we do not make the classification separability assumption. Following the nomenclature introduced above, a temporal feature extracted from a video is denoted by X = s θ (F) in (1), where snippet features are represented by the rows x t of X. An abnormal snippet is denoted by x + ∼ P +</p><p>x (x), and a normal snippet, x − ∼ P −</p><p>x (x). An abnormal video X + contains µ snippets drawn from P + x (x) and (T − µ) drawn from P −</p><p>x (x), and a normal video X − has all T snippets sampled from P −</p><p>x (x). To learn a function that can classify videos and snippets as normal or abnormal, we define a function that classifies a snippet using its magnitude (i.e., we use 2 norm to compute the feature magnitude), where instead of assuming classification separability between normal and abnormal snippets (as assumed in <ref type="bibr" target="#b20">[21]</ref>), we make a milder assumption that E[ x + dency on s θ (.) to produce x t , Ω k (X) contains a subset of k snippets from {x t } T t=1 and |Ω k (X)| = k. The separability between abnormal and normal videos is denoted by</p><formula xml:id="formula_2">d θ,k (X + , X − ) = g θ,k (X + ) − g θ,k (X − ).<label>(3)</label></formula><p>For the theorem below, we define the probability that a snip-</p><formula xml:id="formula_3">pet from Ω k (X + ) is abnormal with p + k (X + ) = min(µ,k) k+ , with &gt; 0 and from normal Ω k (X − ), p + k (X − ) = 0.</formula><p>This definition means that it is likely to find an abnormal snippet within the top k snippets in Ω k (X + ), as long as k ≤ µ. </p><formula xml:id="formula_4">[ x + 2 ] ≥ E[ x − 2 ], where X + has µ abnormal samples and (T − µ) normal samples, where µ ∈ [1, T ]</formula><p>, and X − has T normal samples. Let D θ,k (.) be the random variable from which the separability scores d θ,k (.) of (3) are drawn <ref type="bibr" target="#b20">[21]</ref>.</p><formula xml:id="formula_5">1. If 0 &lt; k &lt; µ, then 0 ≤ E[D θ,k (X + , X − )] ≤ E[D θ,k+1 (X + , X − )]. 2. For a finite µ, then lim k→∞ E[D θ,k (X + , X − )] = 0.</formula><p>Proof. Please see proof in the supplementary material.</p><p>Therefore, the first part of this theorem means that as we include more samples in the top k snippets of the abnormal video, the separability between abnormal and normal video tends to increase (even if it includes a few normal samples) as long as k ≤ µ. The second part of the theorem means that as we include more than µ top instances, the abnormal and normal video scores become indistinguishable because of the overwhelming number of negative samples both in the positive and negative bags. Both points are shown in <ref type="figure" target="#fig_5">Fig. 1</ref>, where score(X)=g θ,k (X), ∆score(X + , X − ) = d θ,k (X + , X − ), and = 0.4 to compute p + k (X + ). This theorem suggests that by maximising the separability of the top-k temporal feature snippets from abnormal and normal videos (for k ≤ µ), we can facilitate the classification of anomaly videos and snippets. It also suggests that the use of the top-k features to train the snippet classifier allows for a more effective training given that the majority of the top-k samples in the abnormal video will be abnormal and that we will have a balanced training using the top-k hardest normal snippets. The final consideration is that because we use just the top-k samples per video, our method is efficiently optimised with a relatively small amount of training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-scale Temporal Feature Learning</head><p>Inspired by the attention techniques used in video understanding <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b57">58]</ref>, our proposed multi-scale temporal network (MTN) captures the multi-resolution local temporal dependencies and the global temporal dependencies between video snippets (we depict MTN in <ref type="figure" target="#fig_5">Fig.1</ref> of the supplementary material). MTN uses a pyramid of dilated convolutions over the time domain to learn multi-scale representations for video snippets. Dilated convolution is usually applied in the spatial domain with the goal of expanding the receptive field without losing resolution <ref type="bibr" target="#b61">[62]</ref>. Here we propose to use dilated convolutions over the temporal dimension as it is important to capture the multi-scale temporal dependencies of neighbouring video snippets for anomaly detection.</p><p>MTN learns the multi-scale temporal features from the pre-computed fetures F = [f d ] D d=1 . Then given the feature f d ∈ R T , the 1-D dilated convolution operation with kernel W (l) k,d ∈ R W with k ∈ {1, ..., D/4}, d ∈ {1, ..., D}, l ∈ {PDC 1 , PDC 2 , PDC 3 }, and W denoting the filter size, is defined by</p><formula xml:id="formula_6">f (l) k = D d=1 W (l) k,d * (l) f d ,<label>(4)</label></formula><p>where * (l) represents the dilated convolution operator indexed by l, f (l) k ∈ R T represents the output features after applying the dilated convolution over the temporal dimension. The dilation factors for {PDC 1 , PDC 2 , PDC 3 } are {1, 2, 4}, respectively (this is shown in <ref type="figure" target="#fig_5">Fig.1</ref> of the supplementary material).</p><p>The global temporal dependencies between video snippets is achieved with a self-attention module, which has shown promising performance on capturing the long-range spatial dependency on video understanding <ref type="bibr" target="#b57">[58]</ref>, image classification <ref type="bibr" target="#b66">[67]</ref> and object detection <ref type="bibr" target="#b38">[39]</ref>. Motivated by the previous works using GCN to model global temporal information <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b67">68]</ref>, we re-formulate the spatial self-attention technique to work on the time dimension and capture global temporal context modelling. In detail, we aim to produce an attention map M ∈ R T ×T that estimates the pairwise correlation between snippets. Our temporal self-attention (TSA) module first uses a 1 × 1 convolution to reduce the spatial dimension from F ∈ R T ×D to F (c) ∈ R T ×D/4 with F (c) = Conv 1×1 (F). We then apply three separate 1 × 1 convolution layers to F (c) to produce</p><formula xml:id="formula_7">F (c1) , F (c2) , F (c3) ∈ R T ×D/4 , as in F (ci) = Conv 1×1 (F (c) ) for i ∈ {1, 2, 3}. The atten- tion map is then built with M = F (c1) F (c2) , which produces F (c4) = Conv 1×1 (MF (c3) ).</formula><p>A skip connection is added after this final 1 × 1 convolutional layer, as in</p><formula xml:id="formula_8">F (TSA) = F (c4) + F (c) .<label>(5)</label></formula><p>The output from the MTN is formed with a concatenation of the outputs from the PDC and MTN modulesF =</p><formula xml:id="formula_9">[F (l) ] l∈L ∈ R T ×D , with L = {PDC 1 , PDC 2 , PDC 3 , TSA}.</formula><p>A skip connection using the original features F produces the final temporal feature representation X = s θ (F) =F + F, where the parameter θ comprises the weights for all convolutions described in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Feature Magnitude Learning</head><p>Using the theory introduced in Sec. 3.1, we propose a loss function to model s θ (F) in <ref type="formula">(1)</ref>, where the top k largest snippet feature magnitudes from normal videos are minimised and the top k largest snippet feature magnitudes from abnormal videos are maximised. More specifically, we propose the following loss s (.) from (1) that maximises the separability between normal and abnormal videos:</p><formula xml:id="formula_10">s (s θ (F i ), s θ (F j ), y i , y j ) = max 0, m − d θ,k (X i , X j )</formula><p>, if y i = 1, y j = 0 0</p><p>, otherwise</p><p>where m is a pre-defined margin, X i = s θ (F i ) is the abnormal video feature (similarly for X j for a normal video), and d θ,k (.) represents separability function defined in (3) that computes the difference between the score of the top k instances, from g θ,k (.) in <ref type="formula">(2)</ref>, of the abnormal and normal videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">RTFM-enabled Snippet Classifier Learning</head><p>To learn the snippet classifier, we train a binary crossentropy-based classification loss function using the set Ω k (X) that contains the k snippets with the largest 2 -norm features from s θ (F) in (1). In particular, the loss f (.) from <ref type="formula">(1)</ref> is defined as</p><formula xml:id="formula_12">f (f φ (s θ (F)), y) = x∈Ω k (X) −(y log(f φ (x)) + (1 − y) log(1 − f φ (x))),<label>(7)</label></formula><p>where x = s θ (f ). Note that following <ref type="bibr" target="#b50">[51]</ref>, f (.) is accompanied by the temporal smoothness and sparsity regularisation, with the temporal smoothness defined as f φ (s θ (f t ))− f φ (s θ (f t−1 )) 2 to enforce similar anomaly score for neighbouring snippets, while the sparsity regularisation defined as T t=1 |f φ (s θ (f t ))| to impose a prior that abnormal events are rare in each abnormal video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Data Sets and Evaluation Measure</head><p>Our model is evaluated on three multi-scene benchmark datasets, created for the weakly supervised video anomaly detection task: ShanghaiTech <ref type="bibr" target="#b23">[24]</ref>, UCF-Crime <ref type="bibr" target="#b50">[51]</ref>, and XD-Violence <ref type="bibr" target="#b58">[59]</ref>. Specifically, UCF-Crime is a largescale anomaly detection data set <ref type="bibr" target="#b50">[51]</ref> that contains 1900 untrimmed videos with a total duration of 128 hours from real-world street and indoor surveillance cameras. Unlike the static backgrounds in ShanghaiTech, UCF-Crime consists of complicated and diverse backgrounds. Both training and testing sets contain the same number of normal and abnormal videos. The data set covers 13 classes of anomalies in 1,610 training videos with video-level labels and 290 test videos with frame-level labels.</p><p>XD-Violence is a recently proposed large-scale multiscene anomaly detection data set, collected from real life movies, online videos, sport streaming, surveillance cameras and CCTVs <ref type="bibr" target="#b58">[59]</ref>. The total duration of this data set is over 217 hours, containing 4754 untrimmed videos with video-level labels in the training set and frame-level labels in the testing set. It is currently the largest publicly available video anomaly detection data set.</p><p>ShanghaiTech is a medium-scale data set from fixedangle street video surveillance. It has 13 different background scenes and 437 videos, including 307 normal videos and 130 anomaly videos. The original data set <ref type="bibr" target="#b23">[24]</ref> is a popular benchmark for the anomaly detection task that assumes the availability of normal training data. Zhong et al. <ref type="bibr" target="#b67">[68]</ref> reorganised the data set by selecting a subset of anomalous testing videos into training data to build a weakly supervised training set, so that both training and testing sets cover all 13 background scenes. We use exactly the same procedure as in <ref type="bibr" target="#b67">[68]</ref> to convert ShanghaiTech for the weakly supervised setting.</p><p>Evaluation Measure. Similarly to previous papers <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b63">64]</ref>, we use the frame-level area under the ROC curve (AUC) as the evaluation measure for all data sets. Moreover, following <ref type="bibr" target="#b58">[59]</ref>, we also use average precision (AP) as the evaluation measure for the XD-Violence data set. Larger AUC and AP values indicate better performance. Some recent studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b39">40]</ref> recommend using the regionbased detection criterion (RBDC) and the track-based detection criterion (TBDC) to complement the AUC measure, but these two measures are inapplicable in the weakly-supervised setting. Thus, we focus on the AUC and AP measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>Following <ref type="bibr" target="#b50">[51]</ref>, each video is divided into 32 video snippets, i.e., T = 32. For all experiments, we set the margin m = 100, k = 3 in (6). The three FC layers described in the model (Sec. 3) have 512, 128 and 1 nodes, where each of those FC layers is followed by a ReLU activation function and a dropout function with a dropout rate of 0.7. The 2048D and 4096D features are extracted from the 'mix 5c' and 'f c 6' layer of the pre-trained I3D <ref type="bibr" target="#b17">[18]</ref> or C3D <ref type="bibr" target="#b16">[17]</ref> network, respectively. In MTN, we set the pyramid dilate rate as 1, 2 and 4, and we use the 3 × 1 Conv1D for each dilated convolution branch. For the self-attention block, we use a 1 × 1 Conv1D.</p><p>Our RTFM method is trained in an end-to-end manner using the Adam optimiser <ref type="bibr" target="#b18">[19]</ref> with a weight decay of 0.0005 and a batch size of 64 for 50 epochs. The learning rate is set to 0.001 for ShanghaiTech and UCF-Crime, and 0.0001 for XD-Violence. Each mini-batch consists of samples from 32 randomly selected normal and abnormal videos. The method is implemented using PyTorch <ref type="bibr" target="#b36">[37]</ref>. For all baselines, we use the published results with the same backbone as ours. For a fair comparison, we use the same benchmark setup as in <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b67">68</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on ShanghaiTech</head><p>The frame-level AUC results on ShanghaiTech are shown in Tab. 1. Our method RTFM achieves superior performance when compared with previous SOTA unsupervised learning methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b62">63]</ref> and weaklysupervised approaches <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b67">68]</ref>. With I3D-RGB features, our model obtains the best AUC result on this data set: 97.21%. Using the same I3D-RGB features, our RTFM-enabled MIL method outperforms current SOTA MIL-based methods <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b63">64]</ref> by 10% to 14%. Our model outperforms <ref type="bibr" target="#b54">[55]</ref> by more than 5% even though they rely on a more advanced feature extractor (i.e., I3D-RGB and I3D Flow). These results demonstrate the gains achieved from our proposed feature magnitude learning.</p><p>Our method also outperforms the GCN-based weaklysupervised method [68] by 11.7%, which indicates that our MTN module is more effective at capturing temporal dependencies than GCN. Additionally, considering the C3D-RGB features, our model achieves the SOTA AUC of 91.51%, significantly surpassing the previous methods with C3D-RGB by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on UCF-Crime</head><p>The AUC results on UCF-Crime are shown in Tab. 2. Our method outperforms all previous unsupervised learning approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b55">56]</ref>. Remarkably, using the same I3D-RGB features, our method also outperforms current  <ref type="bibr" target="#b67">[68]</ref> use a computationally costly alternating training scheme to achieve an AUC of 82.12%, while our method utilises an efficient end-to-end training scheme and outperforms their approach by 1.91%. Our method also surpasses the current SOTA unsupervised methods, BODS and GODS <ref type="bibr" target="#b55">[56]</ref>, by at least 13%. Considering the C3D features, our method surpasses the previous weakly supervised methods by a minimum 2.95% and a maximum 7.87%, indicating the effectiveness of our RTFM approach regardless of the backbone structure.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results on XD-Violence</head><p>XD-Violence is a recently released data set, on which few results have been reported, as displayed in Tab. 3. Our approach surpasses all unsupervised learning approaches by a minimum of 27.03% in AP. Comparing with SOTA weakly-supervised methods <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b58">59]</ref>, our method is 2.4% and 2.13% better than Wu et al. <ref type="bibr" target="#b58">[59]</ref> and Sultani et al. <ref type="bibr" target="#b50">[51]</ref>, using the same I3D features. With the C3D features, our RTFM achieves the best 75.89% AUC when compared with the MIL baseline by Sultani et al. <ref type="bibr" target="#b50">[51]</ref>. The consistent superiority of our method reinforces the effectiveness of our proposed feature magnitude learning method in enabling the MIL-based anomaly classification.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Sample Efficiency Analysis</head><p>We investigate the sample efficiency of our method by looking into its performance w.r.t. the number of abnormal videos used for training on ShanghaiTech. We reduce the number of abnormal training videos from the original 63 videos down to 25 videos, with the normal training videos and test data fixed. The MIL method in <ref type="bibr" target="#b50">[51]</ref> is used as a baseline. For a fair comparison, the same I3D features are used in both methods, and AUC results are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. As expected, the performance of both our method and Sultani et al. <ref type="bibr" target="#b50">[51]</ref> decreases with decreasing number of abnormal training videos, but the decreasing rate of our model is smaller that of than Sultani et al. <ref type="bibr" target="#b50">[51]</ref>, indicating the robustness of our RTFM. Remarkably, our method using only 25 abnormal training videos outperforms <ref type="bibr" target="#b50">[51]</ref> using all 63 abnormal videos by about 4%, i.e., although our method uses 60% less labelled abnormal training videos, it can still outperform Sultani et al. <ref type="bibr" target="#b50">[51]</ref>. This is because RTFM performs better recognition of the positive instances in the abnormal videos, and as a result, it can leverage the same training data more effectively than a MIL-based approach <ref type="bibr" target="#b50">[51]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Subtle Anomaly Discriminability</head><p>We also examine the ability of our method to detect subtle abnormal events on the UCF-Crime dataset, by studying the AUC performance on each individual anomaly class. The models are trained on the full training data and we use <ref type="bibr" target="#b50">[51]</ref> as baseline, and results are shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. Our model shows remarkable performance on human-centric abnormal events, even when the abnormality is very subtle. Particularly, our RTFM method outperforms Sultani et al. <ref type="bibr" target="#b50">[51]</ref> in 8 human-centric anomaly classes (i.e., arson, assault, burglary, robbery, shooting, shoplifting, stealing, vandalism), significantly lifting the AUC performance by 10% to 15% in subtle anomaly classes such as burglary, shoplifting, vandalism. This superiority is supported the theoretical results of RTFM that guarantee a good separability of the positive and negative instances. For the arrest, fighting, road accidents and explosion classes, our method shows competitive performance to <ref type="bibr" target="#b50">[51]</ref>. Our model is less effective in the abuse class because this class contains overwhelming human-centric abuse events in the training data but its testing videos contain animal abuse events only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Ablation Studies</head><p>We perform the ablation study on ShanghaiTech and UCF Crime with I3D features, as shown in Tab. 4, where the temporal feature mapping function s θ is decomposed into PDC and TSA, and FM represents the feature magnitude learning from Sec. 3.3. The baseline model replaces PDC and TSA with a 1×1 convolutional layer and is trained with the original MIL approach as in <ref type="bibr" target="#b50">[51]</ref>. The resulting baseline achieves only 85.96% AUC on ShanghaiTech and 77.32% AUC on UCF Crime (a result similar to the one in <ref type="bibr" target="#b50">[51]</ref>). By adding PDC or TSA, the AUC performance is boosted to 89.21% and 91.73% on ShanghaiTech and 79.32% and 78.96% on UCF, respectively. When both PDC and TSA are added, the AUC result increases to 92.32% and 82.12% for the two datasets, respectively. This indicates that PDC and TSA contributes to the overall performance, and they also complement each other in capturing both long and shortrange temporal relations. When adding only the FM module to the baseline, the AUC substantially increases by over 7% and 4% on ShanghaiTech and UCF Crime, respectively, indicating that our feature magnitude learning considerably improves over the original MIL method as it enables better exploitation of the labelled abnormal video data. Additionally, combining either PDC or TSA with FM helps further improve the performance. Then, the full model RTFM can achieve the best performance of 97.21% and 84.03% on the two datasets. An assumption made in theoretical motivation for RTFM is that the mean feature magnitudes for the top-k abnormal feature snippets is larger than the ones for normal snippets. We measure that on the testing videos of UCF-Crime and the mean magnitude of the top-k snippets from abnormal videos is 53.4 and for normal, it is 7.7. This shows empirically that our our assumption for Theorem A.1 is valid and that RTFM can effectively maximise the separability between normal and abnormal video snippets. This is further evidenced by the mean classification scores of 0.85 for the abnormal snippets and 0.13 for the normal snippets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Qualitative Analysis</head><p>In <ref type="figure" target="#fig_2">Fig. 3</ref>, we show the anomaly scores produced by our MIL anomaly classifier for diverse test videos from UCF-Crime and ShanghaiTech. Three anomalous videos and one normal video from UCF-Crime are used (stealing079, shoplifting028, robbery050 and normal876). As illustrated by the 2 -norm value curve (i.e., orange curves), our FM     <ref type="table">Table 4</ref>. Ablation studies of our method on ShanghaiTech and UCF-Crime.</p><p>module can effectively produce a small feature magnitude for normal snippets and a large magnitude for abnormal snippets. Furthermore, our model can successfully ensure large margins between the anomaly scores of the normal and abnormal snippets (i.e., blank and pink shadowed ar-eas, respectively). Our model is also able to detect multiple anomalous events in one video (e.g., stealing079), which makes the problem more difficult. Also, for the anomalous events stealing and shoplif ting, the abnormality is subtle and barely seen through the videos, but our model can still detect it. We also show the anomaly scores and feature magnitudes produced by our model for 01 0052 and 01 0053 from ShanghaiTech (last two figures in <ref type="figure" target="#fig_2">Fig. 3</ref>). Our model can effectively yield large anomaly scores for the anomalous event of vehicle entering in these two scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.">Computational Efficiency</head><p>Lastly, we investigate if our system can run in real time. During inference, our method processes a 16-frame clip in 0.76 seconds on a Nvidia 2080Ti-this time includes the I3D extraction time. This indicates that our system can achieve good real-time detection in real-world applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We introduced a novel method, named RTFM, that enables top-k MIL approaches for weakly supervised video anomaly detection. RTFM learns a temporal feature magnitude mapping function that 1) detects the rare abnormal snippets from abnormal videos containing many normal snippets, and 2) guarantees a large margin between normal and abnormal snippets. This improves the subsequent MILbased anomaly classification in two major aspects: 1) our RTFM-enabled model learns more discriminative features that improve its ability in distinguishing complex anomalies (e.g., subtle anomalies) from hard negative examples; and 2) it also enables the MIL classifier to achieve significantly improved exploitation of the abnormal data. These two capabilities respectively result in better subtle anomaly discriminability and sample efficiency than current SOTA MIL methods. They are also the two main drivers for our model to achieve SOTA performance on all three large benchmarks. Proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Supplementary Material</head><formula xml:id="formula_13">E[D θ,k (X + , X − )] = E[g θ,k (X + )] − E[g θ,k (X − )] = p + k (X + )E[ x + 2 ] + p − k (X + )E[ x − 2 ] − E[ x − 2 ] (S1) 1. Trivial given that E[ x + 2 ] ≥ E[ x − 2 ]</formula><p>and that p + k+1 (X + ) &gt; p + k (X + ) for 0 &lt; k &lt; µ 2. Trivial given that as µ is finite, lim k→∞ p + k (X + ) = 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Multi-scale Temporal Feature Learning</head><p>Our proposed multi-scale temporal network (MTN) captures the multi-resolution local temporal dependencies and the global temporal dependencies between video snippets, as displayed in <ref type="figure" target="#fig_5">Fig. S1</ref>. <ref type="figure" target="#fig_5">Figure S1</ref>. Our proposed MTN consists of two modules. The module on the left uses the pyramid dilated convolutions to capture the local consecutive snippets dependency over different temporal scales. The module on the right relies on a self-attention network to compute the global temporal correlations. The features from the two modules are concatenated to produce the MTN output.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem 3 . 1 (</head><label>31</label><figDesc>Expected Separability Between Abnormal and Normal Videos). Assuming that E</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Our proposed RTFM receives a T × D feature matrix F extracted from a video containing T snippets. Then, MTN captures the long and short-range temporal dependencies between snippet features to produce X = s θ (F). Next, we maximise the separability between abnormal and normal video features and train a snippet classifier using the top-k largest magnitude feature snippets from abnormal and normal videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Anomaly scores and feature magnitude values of our method on UCF-Crime (stealing079,shoplifting028, robbery050 nor-mal876), and ShanghaiTech (01 0052, 01 0053) test videos. Pink areas indicate the manually labelled abnormal events.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>AUC w.r.t. the number of abnormal training videos.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>AUC results w.r.t. individual classes on UCF-Crime.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>A. 1 .</head><label>1</label><figDesc>Theoretical Motivation of RTFM Theorem A.1 (Expected Separability Between Abnormal and Normal Videos). Assuming that E[ x + 2 ] ≥ E[ x − 2 ], where X + has µ abnormal samples and (T − µ) normal samples, where µ ∈ [1, T ], and X − has T normal samples. Let D θ,k (.) be the random variable from which the separability scores d θ,k (.) of Eq.3 in the main paper are drawn<ref type="bibr" target="#b20">[21]</ref>.1. If 0 &lt; k &lt; µ, then 0 ≤ E[D θ,k (X + , X − )] ≤ E[D θ,k+1 (X + , X − )].2. For a finite µ, then lim k→∞ E[D θ,k (X + , X − )] = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Frame-level AUC performance on UCF-Crime. * indicates we retrain the method in<ref type="bibr" target="#b50">[51]</ref> using I3D features. Best result in red and second best in blue.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Comparison of AP performance with other SOTA un/weakly-supervised methods on XD-Violence. * indicates we retrain the method in<ref type="bibr" target="#b50">[51]</ref> using I3D features. Best result in red and second best in blue.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">] ≥ E[ x − 2 ]. This means that by learning the snippet feature from s θ (F), such that normal ones have smaller feature magnitude than abnormal ones, we can satisfy this assumption. To enable such learning, we rely on an optimisation based on the mean feature magnitude of the top k snippets from a video<ref type="bibr" target="#b20">[21]</ref>, defined byg θ,k (X) = max Ω k (X)⊆{xt} T t=1 1 k xt∈Ω k (X) x t 2 ,(2)where g θ,k (.) is parameterised by θ to indicate its depen-</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Latent space autoregression for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Abati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning object motion patterns for anomaly detection and improved object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Basharat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Gritai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Classification-based anomaly detection for general data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liron</forename><surname>Bergman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yedid</forename><surname>Hoshen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.02359</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mvtec ad -a comprehensive real-world dataset for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bergmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sattlegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carsten</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Where&apos;s wally now? deep generative and discriminative embeddings for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Burlina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Jeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Video anomaly detection and localization using hierarchical feature representation and gaussian process regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yie-Tarng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsien</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A discriminative framework for anomaly detection in large videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allison</forename><surname>Del Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Anomaly detection in video via self-supervised and multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Barbalau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.07491,2020.5</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep anomaly detection using geometric transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Izhak</forename><surname>Golan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>El-Yaniv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9758" to="9769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmudul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghyun</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Amit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint detection and recounting of abnormal events by learning deep generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin&amp;apos;ichi</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3619" to="3627" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Object-centric auto-encoders and dummy anomalies for abnormal event detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariana-Iuliana</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7842" to="7851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorina</forename><surname>Radu Tudor Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2895" to="2903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anomaly detection in extremely crowded scenes using spatio-temporal motion pattern models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis</forename><surname>Kratz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ko</forename><surname>Nishino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1446" to="1453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multiple instance learning for soft bags via top instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="4277" to="4285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Temporal attention network for action proposal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2281" to="2285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Margin learning embedded prediction for video anomaly detection with a few anomalies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Future frame prediction for anomaly detection-a new baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="6536" to="6545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 fps in matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked rnn framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph embedded pose clustering for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Markovitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilad</forename><surname>Sharir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihi</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Somboon Hongeng, and Ramakant Nevatia. Event detection and analysis from video streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gérard</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Brémond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="873" to="889" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning regularity in skeleton trajectories for anomaly detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romero</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moussa</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning regularity in skeleton trajectories for anomaly detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romero</forename><surname>Morais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Budhaditya</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moussa</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11996" to="12004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Anomaly detection with multiple-hypotheses predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyu</forename><surname>Duc Tam Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Klar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4800" to="4809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Anomaly detection in video sequence with appearance-motion correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Trong-Nguyen Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Meunier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning representations of ultrahigh-dimensional data for random distance-based outlier detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longbing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2041" to="2050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep anomaly detection with deviation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 25th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-trained deep ordinal regression for end-to-end video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="12173" to="12182" />
		</imprint>
	</monogr>
	<note>Anton van den Hengel, and Xiao Bai</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning memory-guided normality for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoun</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bumsub</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. d Alché-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ocgan: One-class novelty detection using gans with constrained latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pramuditha</forename><surname>Perera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spotnet: Self-attention multi-task network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hughes</forename><surname>Perreault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume-Alexandre</forename><surname>Bilodeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Saunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maguelonne</forename><surname>Héritier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 17th Conference on Computer and Robot Vision (CRV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A survey of single-scene video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharathkumar</forename><surname>Ramachandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranga Raju</forename><surname>Vatsavai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Plug-and-play cnn for crowd motion analysis: An application in abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1689" to="1698" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdyar</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moin</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucio</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised behaviorspecific dictionary learning for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Huamin Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Søren Ingvor Olsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">B</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moeslund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="28" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep one-class classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Goernitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Deecke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Shoaib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Siddiqui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4393" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep semi-supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Ruff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nico</forename><surname>Vandermeulen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Görnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kloft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02694</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep-cascade: Cascading 3d deep neural networks for fast anomaly detection and localization in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reinhard</forename><surname>Klette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarially learned one-class classifier for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Sabokrou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Khalooei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmood</forename><surname>Fathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Support vector method for novelty detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John C</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="582" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep appearance features for abnormal behavior detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorina</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Radu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="779" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Moncef Gabbouj, and Alexandros Iosifidis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Sohrab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jenni</forename><surname>Raitoharju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="722" to="727" />
		</imprint>
	</monogr>
	<note>Subspace support vector data description</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Real-world anomaly detection in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waqas</forename><surname>Sultani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Few-shot anomaly detection for polyp frames from colonoscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Maicas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo Zorron Cheng Tao</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajvinder</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johan</forename><forename type="middle">W</forename><surname>Verjans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer Assisted Intervention-MICCAI 2020: 23rd International Conference</title>
		<meeting><address><addrLine>Lima, Peru</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="274" to="284" />
		</imprint>
	</monogr>
	<note>Proceedings, Part VI 23</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashanka</forename><surname>Venkataramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kuan-Chuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08616</idno>
		<title level="m">Rajat Vikram Singh, and Abhijit Mahalanobis. Attention guided anomaly detection and localization in images</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Weakly supervised video anomaly detection via center-guided discriminative learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Gods: Generalized one-class discriminative subspaces for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuck</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Not only look, but also listen: Learning multimodal violence detection under weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangtao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkuan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.01553</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on a hierarchical activity discovery within spatio-temporal contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huihuan</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="144" to="152" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Cloze test helps: Effective video anomaly detection via learning to complete video events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiping</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">En</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanfu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Kloft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.11988</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Temporal convolutional network with complementary inner bag loss for weakly supervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning semantic scene models by object classification and trajectory clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianzhu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1940" to="1947" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Video anomaly detection based on locality sensitive hashing filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shun</forename><surname>Sakai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="302" to="311" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1237" to="1246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiwang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03632</idno>
		<title level="m">Encoding structure-texture relation with p-net for anomaly detection in retinal images</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Motion-aware feature for improved video anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Newsam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.10211</idno>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Deep autoencoding gaussian mixture model for unsupervised anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Martin Renqiang Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daeki</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
