<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Lane Marker Detection via Row-wise Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungwoo</forename><surname>Yoo</surname></persName>
							<email>yoos@qti.qualcomm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><surname>Seok</surname></persName>
							<email>heeseokl@qti.qualcomm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Heesoo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myeong</forename><surname>Sungrack</surname></persName>
							<email>sungrack@qti.qualcomm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Hyoungwoo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Park</forename><surname>Janghoon</surname></persName>
							<email>janghoon@qti.qualcomm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho</forename><surname>Duck</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoon</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qualcomm</forename><surname>Korea</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename></persName>
						</author>
						<title level="a" type="main">End-to-End Lane Marker Detection via Row-wise Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In autonomous driving, detecting reliable and accurate lane marker positions is a crucial yet chalprolenging task. The conventional approaches for the lane marker detection problem perform a pixel-level dense prediction task followed by sophisticated post-processing that is inevitable since lane markers are typically represented by a collection of line segments without thickness. In this paper, we propose a method performing direct lane marker vertex prediction in an end-to-end manner, i.e., without any post-processing step that is required in the pixel-level dense prediction task. Specifically, we translate the lane marker detection problem into a row-wise classification task, which takes advantage of the innate shape of lane markers but, surprisingly, has not been explored well. In order to compactly extract sufficient information about lane markers which spread from the left to the right in an image, we devise a novel layer, inspired by <ref type="bibr" target="#b7">[8]</ref>, which is utilized to successively compress horizontal components so enables an end-to-end lane marker detection system where the final lane marker positions are simply obtained via argmax operations in testing time. Experimental results demonstrate the effectiveness of the proposed method, which is on par or outperforms the state-of-the-art methods on two popular lane marker detection benchmarks, i.e., TuSimple and CULane.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the explosive growth of the researches and developments on the computer vision technologies with sensor fusion, localization and path planning, the advanced driver assistance system (ADAS) or high-level self-driving system (SDS) has been widely adopted in recent vehicles such as Waymo <ref type="bibr" target="#b31">[31]</ref>, Uber [1], Lyft [2], Mobileye [34], Google car <ref type="bibr" target="#b27">[27]</ref> and Tesla <ref type="bibr" target="#b5">[6]</ref>. Especially, recent researches and projects <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr">34]</ref> on the ADAS and SDS are focused more on cameras than other sensors, e.g. LiDAR, due to the cost, design, and also big accuracy improvements in the camera- A number of researches on lane marker detection have been proposed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b29">29]</ref>. Most conventional lane marker detection methods are based on two-stage semantic segmentation approaches <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b22">23]</ref>. In the first stage of these approaches, a network is designed to perform a pixel-level classification to assign each pixel in an image to the binary label, i.e., lane marker or not. However, in each pixel classification, the dependencies or structures between pixels are not specifically considered, and thus additional post-processing is performed in the second stage to explicitly impose the constraints such as uniqueness or straightness of the detected lane marker. The postprocessing can be implemented with conditional random field, additional networks, or sophisticated CV techniques like RANSAC, but its computational complexity is not negligible and it should be carefully combined with the first stage by hand-tuning. Therefore, there approaches are hard to scale up for various environments and datasets. Another lane marker detection methods are generative adversarial network (GAN)-based approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21]</ref> which considers additional loss to impose such structural constraints.</p><p>In this paper, we consider a simple end-to-end frame-work for recognizing lane marker, called E2E-LMD, which directly predicts the lane marker vertices without any sophisticated post-processing step ( <ref type="figure" target="#fig_0">Figure 1</ref>). Here, the lane marker recognition problem is considered as multiple rowwise classification tasks for each lane marker type where features for classification are expressed through a two-stage module, and the final lane marker positions are simply obtained by argmax operations in testing time. The firststage layers successively compress and model the horizontal components for the shared representation of all lane markers, and the second-stage layers separately model the each lane marker based on this shared representation to directly output the lane marker vertices. In summary, the contribution of this paper can be summarized as follows: 1) We present a novel and intuitive framework for detecting lane markers. 2) The proposed method is on par or outperforms the recent state-of-theart methods in both benchmark datasets, i.e., TuSimple and CULane, without complex post-processing. And, finally, <ref type="bibr" target="#b2">3)</ref> We show that the proposed method can effectively capture lane marker representation in an efficient manner with extensive experiments and visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most traditional lane marker detection methods are based on hand-crafted low-level features. In <ref type="bibr" target="#b2">[3]</ref>, they proposed the line segment detection using selective Gaussian spatial filters, which is followed by post-processing steps. Recently, deep learning-based methods are employed to learn to extract features at various scenes. There are mainly two approaches based on convolutional neural networks (CNN): 1) Segmentation-based approach and 2) GAN-based approach.</p><p>The first approaches consider lane marker detection as a semantic segmentation task <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b25">25]</ref>. In <ref type="bibr" target="#b21">[22]</ref>, the benefits of lane marker segmentation are combined with a clustering approach designed for instance segmentation. In <ref type="bibr" target="#b25">[25]</ref>, they train a spatial CNN (SCNN) with propagating message as residual for detecting long continuous structure. In <ref type="bibr" target="#b13">[14]</ref>, pixel-wise clustering is applied based on conventional segmentation network. In <ref type="bibr" target="#b6">[7]</ref>, authors proposed a deep neural network that predicts a weights map like a segmentation output for each lane marker and a differentiable least-squares fitting module for mapping parameters for curve fitting. In <ref type="bibr" target="#b12">[13]</ref>, self-attention distillation (SAD) is proposed to allow the network to exploit attention maps within the network itself and complements the segmentation-based supervised learning.</p><p>Second, some methods adopt GAN for lane marker detection tasks. In <ref type="bibr" target="#b9">[10]</ref>, authors take lane marker labels as extra inputs and use GAN so that the segmentation maps resemble labels to predict the better segmentation outcomes. In <ref type="bibr" target="#b18">[19]</ref>, they generate low light conditioned images using GAN to increase the environmental adaptability of the network.</p><p>Other deep learning-based methods make an effort to solve lane marker detection from different aspects. In <ref type="bibr" target="#b16">[17]</ref>, they use extra labels of vanishing point to train the network to output better structural information. In <ref type="bibr" target="#b4">[5]</ref>, they consider the lane marker detection and classification problems as regression problems.</p><p>One work close to the proposed method is <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref> where column-wise representation is used to recognize free space in road scenes. This horizontal representation for detecting obstacles has been easily utilized for autonomous driving tasks since it can be efficiently translated to an occupancy grid representation. Based on the representation, they used convolutional neural network with simple successive vertical pool layers to regress free space boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>As reviewed in Sec. 2, the lane marker detection problem has been tackled with various approaches and each of them has its own pros and cons. However, most of them are based on semantic segmentation with complex postprocessing which hinders end-to-end training for extracting lane marker positions. Inspired by recent works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>, we consider the above problem as finding the set of horizontal locations of each lane marker in an image. Specifically, we divide an image into rows and obtain a row-wise representation for each lane marker using a convolutional neural network. Then lane marker detection can be thought as row-wise classification. In other words, contrasted to the conventional segmentation-based lane marker detection, the proposed method can directly provide lane marker positions. More specifically, given an input image X ∈ R 3×h×w where h and w are the image height and width, respectively, the objective is to find a lane marker l i (i = 1, · · · , N ) represented by the set of vertices {vl ij } = {(x ij , y ij )} (j = 1, · · · K). Here, N is the number of lane markers in X which is generally pre-defined, and K is the total number of vertices that is limited to h due to the row-wise representation.</p><p>The details of the proposed architecture, which is conceptually simple and can be utilized to any segmentationbased approaches, and its training and inference will be described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>Architecture Design: We propose a novel architecture composed of successive shared and lane marker-wise horizontal reduction modules (HRMs), which leads to removing horizontal components spatially and setting the channel size as the target width resolution.</p><p>The proposed end-to-end lane marker detection (E2E-LMD) architecture consists of three stages (see <ref type="figure">Fig. 2(a)</ref>). The first stage is a general encoder-decoder segmentation network <ref type="bibr" target="#b30">[30]</ref> which encodes information of lane markers in an image and reconstructs spatial resolution. In contrast to standard semantic segmentation approaches, in our implementation, we only recover spatial resolution as the half of an input size to reduce computational complexity.</p><p>In the second stage, we successively squeeze the horizontal dimension of the shared representation using HRMs without changing the vertical dimension. With this squeeze operation, we can obtain the row-wise representation in a more natural way. After running shared HRMs, we squeeze the remaining width of representation by lane marker-wise HRMs to make single vector representation for each row. We found that it is required to assign dedicated HRMs on each lane marker after the shared HRMs for increasing accuracy numbers, since each lane marker has different innate spatial and shape characteristics. For computational efficiency, however, only the first few HRMs are shared across lane markers, followed by lane marker-wise HRMs. With more shared layers we can save computational cost but each lane marker accuracy might be degraded.</p><p>In the last third stage, we have two branches for a lane marker l i : a row-wise vertex location branch and a vertexwise confidence branch. These branches perform classification and confidence regression on the last HRMsfeatures where spatial resolution only has the vertical dimension while the channel size meets the target horizontal resolution h , i.e., h = h/2. The row-wise vertex location branch predicts the horizontal position x ij of l i per y ij (y ij = 0, · · · , h ).</p><p>The vertex-wise confidence branch predicts the existence confidence vc ij whether (x ij , y ij ) is valid or not. Following <ref type="bibr" target="#b25">[25]</ref>, we also add a semantic lane marker confidence branch which produces lane marker-wise existance confidence lc i after shared HRMs.</p><p>Horizontal Reduction Module: To effectively compress the horizontal representation, we utilize residual layers proposed in <ref type="bibr" target="#b10">[11]</ref> (see <ref type="figure">Fig. 2(b)</ref>). Specifically, in the skip connection, we add a horizontal average pooling layer with a 1×1 convolution to down-sample horizontal components. Although pooling operations let the deeper layers gather more spatial context (to improve classification) and reduce computational complexity, they still have the drawback of reducing the pixel precision. Therefore, to effectively keep and enhance the horizontal representation, inspired by the pixel shuffle layer of <ref type="bibr" target="#b24">[24,</ref><ref type="bibr" target="#b32">32]</ref>, we propose to rearrange the elements of C × H × W input tensor to make a tensor of shape rC ×H ×W/r in the residual branch, which is somewhat a reverse operation of the original pixel shuffle block in <ref type="bibr" target="#b32">[32]</ref> so called the horizontal pixel unshuffle layer. By rearranging the representation, we can efficiently move spatial information to channel. Then we apply a convolution operation to reduce the increased channel rC to C which not only reduces computational complexity but also helps to effectively compress lane marker spatial information from the pixel unshuffle operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>Learned representations in successive layers <ref type="figure">Figure 3</ref>. Learned representations on decoder and shared HRM layers1,2,3: We visualize how features are encoded in different depths of our shared HRM layers after decoder. For each layer (row), we visualize the first three principal components as RGB values at each spatial locations. We observe that the features become more distinctive, adapted to specific locations and disentangled in the later layers.</p><p>To further improve the discrimination between lane markers, we add an attention mechanism by adding Squeeze and Excitation (SE) block <ref type="bibr" target="#b14">[15]</ref>. The SE block helps to include global information in the decision process by aggregating the information in the entire receptive field and recalibrates channel-wise feature responses which have spatial information encoded by the horizontal pixel unshuffle layer (see <ref type="figure">Fig. 3</ref> and <ref type="figure">Fig. 4)</ref>.</p><p>To confirm the effectiveness of the proposed architecture, we visualize the learned representation using PCA (Principal Component Analysis) (see <ref type="figure">Fig. 3</ref>). The visualized results show that the proposed architecture successfully compress the spatial lane marker information even though we squeeze the horizontal components in representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>The training objective is to optimize total loss L given by</p><formula xml:id="formula_0">L = L vl + λ 1 L vc + λ 2 L lc ,<label>(1)</label></formula><p>where L vl , L vc , and L lc are losses for lane marker vertex location, lane marker vertex confidence, and lane markerwise confidence, respectively. And λ 1 and λ 2 are weights for the last two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lane Marker Vertex Location Loss: As we formulated lane marker detection as row-wise classification on lane marker's horizontal position, any loss function for classification can be used.</head><p>Specifically, we tested three loss functions, i.e., cross-entropy (CE), KL-divergence (KL), and PL-loss (P L) <ref type="bibr" target="#b17">[18]</ref>. The CE loss L CE ij for lane marker l i at a vertical Input Before SE After SE <ref type="figure">Figure 4</ref>. Learned representations at shared HRM layers2,3 before/after SE module: We visualize how encoded features are changed before/after SE block. We observe that after SE block, lane representations become more discernible to be easily separate from each other. position y ij is computed using the ground truth location x gt ij and the predicted logits f ij having W/2 channels. To train the lane marker vertex location branch using the KL loss L KL ij , we first make a sharply-peaked target distribution of lane marker positions as a Laplace distribution Laplace gt (µ, b) with µ = x gt ij and b = 1, and then compare it with an estimated distribution Laplace pred (µ, b) by</p><formula xml:id="formula_1">µ = E fij [x ji ] = sof targmax(x ji ) = W/2 sof tmax(f ij ) · x ij b = E fij [|x ji − E fij [x ji ]|]<label>(2)</label></formula><p>, similarly with the 2D facial landmark detection algorithm in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">28]</ref>. In case of the P L loss, we follow the original formulation of <ref type="bibr" target="#b17">[18]</ref> by modeling the probability of lane marker positions as piecewise linear probability distribution.</p><p>For an input image, the total lane marker vertex location loss is given by</p><formula xml:id="formula_2">L vl = 1 N N i 1 K j e ij K j L type ij × e ij<label>(3)</label></formula><p>, where type ∈ {CE, KL, P L}, e ij denotes whether ground truth exists or not, i.e., e ij = 1 if there is l i having a lane marker vertex at y ij and e ij = 0 if not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lane Marker Vertex Confidence Loss:</head><p>The lane marker vertex existence is a binary classification problem, thus it can be trained using a binary CE loss L BCE ij between single scalar-value prediction at each y ij location of lane marker l i and ground truth existence e ij . The loss for an entire image is then computed as L ve =  <ref type="figure">Figure 5</ref>. The examples of video frames of (a) TuSimple [33] and (b) CULane <ref type="bibr" target="#b25">[25]</ref>. Ground truth lane markers are shown in various colored lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Inference</head><p>In testing time, lane marker vertices can be simply estimated per loss as follows: the argmax operation is used for the CE or P L loss, and the softargmax operation is used for the KL loss. As mentioned above, there are three outputs from the proposed architecture, i.e., horizontal location of lane marker vertices x ij , vertex-wise existence confidence vc ij , and lane marker-wise existence confidence lc i . Then the final lane marker vl ij for l i can be obtained by</p><formula xml:id="formula_3">{vl ij } = {(x ij , y ij )|vc ij &gt; T vc } if lc i &gt; T lc , ∅ else,<label>(4)</label></formula><p>where T vc and T lc are the thresholds of vertex-wise existence confidence and lane marker-wise existence confidence, respectively. Specifically, the sigmoid output of the vertex-wise and lane marker-wise existence branches is utilized to reject low-confident lane marker vertices and lane markers, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets: We consider two lane marking datasets for evaluating our method. TuSimple <ref type="bibr">[33]</ref> and CULane <ref type="bibr" target="#b25">[25]</ref> are widely used in previous works. Some examples of these datasets with ground truth are shown in <ref type="figure">Fig. 5</ref>.</p><p>1) TuSimple. The TuSimple dataset consists of 6,408 road images on US highways. The resolution of image is 1280 × 720. The dataset is composed of 3,626 for training, 358 for validation, and 2,782 for testing called the TuSimple test set of which the images are under different weather conditions.</p><p>2) CULane. The CULane dataset consists of 55 hours of videos which comprise urban, rural and highway scenes, and 133,235 frames are extracted from videos. The dataset is divided into 88,880 frames for training, 9,675 for validation, and 34,680 for testing called the CULane test set. The images have a resolution of 1640 × 590. The test set contains 9 different challenging driving scenarios ("Normal", "Crowd", "Highlight", "Shadow", "Arrow", "Curve", "Cross", "Night" and "No line").</p><p>Evaluation Metrics: For comparing the proposed method with previous lane marker detection methods, we used the following evaluation metrics for each particular dataset: 1) TuSimple. We report the official metric used in [33] as the evaluation criterion. The accuracy is calculated as the average correct number of vertices per image: Accuracy = Ncorrect Ngt , where N correct is the number of correctly predicted lane marker vertices, and N gt is the number of ground truth lane marker vertices. Also, we report the false positive (F P ) and false negative (F N ) scores.</p><p>2) CULane. As in <ref type="bibr" target="#b25">[25]</ref>, for judging whether the proposed method detects lane markers correctly, we consider each lane marking as a line with 30 pixel width and compute the intersection-over-union (IoU) between ground truths and predictions. Predictions whose IoUs are larger than 0.5 are considered as true positives (T P ). Then, we used F 1 -measure as the evaluation metric, which is defined as: F 1 = 2×P recision×Recall P recision+Recall , where P recision = T P T P +F P and Recall = T P T P +F N . Implementation Details: We resized the image of TuSimple and CULane to 256 × 512 and set N as 6 and 4 for each dataset. To assign an unique class ID to each lane marker l i , we set labels for each lane marker by ordering the relative distance from an image center. For example, we set the host left lane marker in TuSimple to label 0, the host right lane marker to label 1, and the remaining lane markers similarly to cover all N lane markers. For optimization, we used AdamW <ref type="bibr" target="#b19">[20]</ref> with gradual warmup and cosine annealing learning rate schedule with initial learning rate as 8e −4 . The weights λ 1 and λ 2 for loss function in Eq. 1 were set as 10 and 1, respectively. The number of shared HRM was fixed to 3 for all experiments and the number of channel C was set to 96. Each mini-batch has 14 images per GPU and we trained using 8 GPUs for 80 epochs on CULane and 140 epochs on Tusimple. Since we only recover the spatial resolution as the half size of an image, we resampled the result vertices to meet the original scale. To reduce overfitting, we applied Dropout with 0.1 probability after every HRM. Furthermore, we also applied data augmentation like random cropping, horizontal flipping, and photometric augmentations. In testing time, we set T vc , i.e., the threshold of vertex-wise existence confidence, as 0.6 and T lc , i.e., the threshold of lane marker-wise existence, as 0.5 for every experiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results</head><p>Quantitative analysis: To verify the effectiveness of our method, we performed extensive comparisons with several state-of-the-art methods. Following <ref type="bibr" target="#b12">[13]</ref>, we evaluated multiple backbones, i.e., ResNet-18 (R-18-E2E), ResNet-34 (R-34-E2E), ResNet-50 (R-50-E2E), ERF (ERF-E2E) <ref type="bibr" target="#b29">[29]</ref>. As illustrated in <ref type="table" target="#tab_0">Table 1</ref>, the proposed method attained  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm</head><p>Accuracy FP FN ResNet-18 <ref type="bibr" target="#b12">[13]</ref> 92.69% 0.0948 0.0822 ResNet-34 <ref type="bibr" target="#b12">[13]</ref> 92.84% 0.0918 0.0796 LaneNet <ref type="bibr" target="#b21">[22]</ref> 96.38% 0.0780 0.0244 EL-GAN <ref type="bibr" target="#b9">[10]</ref> 96.39% 0.0412 0.0336 FCN-Instance <ref type="bibr" target="#b13">[14]</ref> 96.5% 0.0851 0.0269 SCNN <ref type="bibr" target="#b25">[25]</ref> 96.53 % 0.0617 0.0180 R-18-SAD <ref type="bibr" target="#b12">[13]</ref> 96.02% 0.0786 0.0451 R-34-SAD <ref type="bibr" target="#b12">[13]</ref> 96.24% 0.0712 0.0344 The reason would be that the number of the TuSimple training images is not much enough to avoid the overfitting of the network. In <ref type="table">Table 3</ref>, the proposed method consistently outperforms the state-of-the-art methods in various scenarios of CULane dataset. Especially, the proposed method attained a better performance when comparing <ref type="bibr" target="#b18">[19]</ref>, which utilizes CycleGAN <ref type="bibr" target="#b34">[35]</ref> to augment insufficient scenario data.</p><p>Qualitative analysis: <ref type="figure" target="#fig_4">Fig. 7</ref> shows the localization of lane markers is successful at night, in the shadows, and when passing under the tunnel. <ref type="figure" target="#fig_3">Fig. 6</ref> shows a few failure cases. The proposed method often fails when there exists reflection over the bonnet that makes it try to find a lane marker and when there are severe curves or occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Experiments</head><p>We investigated the effects of different choices of our proposed method, e.g., the SE block existence and position, number of shared HRM layers and loss functions.</p><p>Architecture: First, to confirm the pros of including SE block, we evaluated the effect of SE block position and existence on HRM layer in Table 2(a). Following the ex-  <ref type="bibr" target="#b14">[15]</ref>, Post-SE performs much better than other configurations. It seems that the SE block at the end of the residual branch helps to recover the distinctiveness of lane markers whose information could be lost when squeezing the channel in the residual ConvBN layer (see <ref type="figure">Fig. 4</ref>).</p><p>The number of shared HRM: As discussed in Section 3.1, the number of shared HRMs is an important factor for the speed-accuracy trade-off. We changed the number of shared HRMs from 0 to 4 (accordingly the number of lane marker-wise HRMs varies from 6 to 2), and the results are summarized in Table 2(b). Note that the batch size of 8 is used in this experiment since the large number of shared HRMs requires much memory. As shown in <ref type="table" target="#tab_2">Table 2</ref>(b), we can tune the number of shared HRM according to the speedaccuracy trade-off.</p><p>Loss function: To compare loss functions in terms of effectiveness, accuracy numbers per loss function are summarized in <ref type="table" target="#tab_2">Table 2(c)</ref>. Surprisingly, in our experiments, simple CE loss is preferable than others. The reason would be that the proposed horizontal reduction module helps The results of to effectively incorporate spatial information between the ground truth position and the proximity between neighbors into a network, which leads to helping general CE loss to outperform other specially designed loss functions, i.e., KL and P L losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a new lane marker detection method to classify each lane marker and obtain its vertex in an end-to-end manner. A novel module for effective horizontal reduction has been devised, and with the module, the state-of-the-art performance is achieved without any complex post-processing. Although we designed the proposed architecture for the lane marker detection problem, it can be also used for other tasks, such as general polygon prediction and semantic/instance segmentation.In order to improve the proposed architecture in a better way, we plan to search the reduction module in an automatic manner.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The E2E-LMD framework for lane marker detection. based perception systems. Although there are a number of components related to the ADAS or SDS, such as lane marker detection, vehicle detection &amp; tracking, obstacle detection, scene understanding, and semantic segmentation, lane marker detection is one of the key components in camera perception and positioning for several applications, e.g., lane keeping/change assist.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) The schema of the E2E-LMD (b) The horizontal reduction module (HRM) Figure 2. The E2E-LMD architecture for lane marker detection. We extend general encoder-decoder architectures by adding successive horizontal reduction modules for end-to-end lane marker detection. Numbers under each block denote spatial resolution and channels. (a) Arrows with HRM denote a horizontal reduction module of (b). Arrows with Conv are output convolution with 1 × 1. Dashed arrows denote the global average pooling with a fully connected layer. (b) HRM is utilized to compress the horizontal representation. r denotes the pooling ratio for width part. Conv kernel size k is set as 3 except the last HRM layer which set as 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Loss: Following<ref type="bibr" target="#b25">[25]</ref>, we add a binary CE loss L BCE i to train the lane marker-wise existence prediction. The loss is computed using the predicted N -dimensional vector lc i and existence of each lane l i in the ground truth. The total loss is then computed asL le = 1 N N i L BCE i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Failed examples from the CULane and TuSimple test sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>E2E-LMD using ERFNet as a backbone network on the CULane and TuSimple test images. All rows except the last one show the CULane test images. Green dots are appropriately sampled for visualization purpose. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of different algorithms on the TuSimple test set.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation study on different settings Loss function: Results on the TuSimple dataset by changing the loss function.periments in the original SE paper<ref type="bibr" target="#b14">[15]</ref>, we consider three variants: (1) Pre-SE block, in which the SE block is moved before the horizontal pixel unshuffle layer (seeFig. 2);(2) Standard-SE block, in which the SE block is after the residual operation, i.e., after ConvBN in the residual branch; (3) Post-SE block, in which the SE block is moved after the summation of identity connection. Interestingly, in contrast to observations in the original SE paper</figDesc><table><row><cell>ERFNet-E2E</cell><cell></cell><cell></cell><cell cols="2">CULane</cell></row><row><cell>Architecture</cell><cell></cell><cell>Prec.</cell><cell>Recall</cell><cell cols="2">F-measure</cell></row><row><cell>Without SE</cell><cell></cell><cell>75.8</cell><cell>71.1</cell><cell></cell><cell>73.4</cell></row><row><cell>Pre-SE</cell><cell></cell><cell>75.7</cell><cell>71.5</cell><cell></cell><cell>73.5</cell></row><row><cell>Standard-SE</cell><cell></cell><cell>75.0</cell><cell>71.6</cell><cell></cell><cell>73.3</cell></row><row><cell>Post-SE</cell><cell></cell><cell>76.5</cell><cell>71.8</cell><cell></cell><cell>74.0</cell></row><row><cell cols="6">(a) SE Position: Results on the CULane dataset by changing the</cell></row><row><cell cols="3">position of SE block in HRM.</cell><cell></cell><cell></cell></row><row><cell>R-18-E2E</cell><cell>Flops</cell><cell></cell><cell cols="2">TuSimple</cell></row><row><cell># shared</cell><cell>ratio</cell><cell cols="2">Accuracy</cell><cell>FP</cell><cell>FN</cell></row><row><cell>1</cell><cell>1.00</cell><cell></cell><cell>96.06%</cell><cell>0.0316</cell><cell>0.0436</cell></row><row><cell>2</cell><cell>0.56</cell><cell></cell><cell>96.05%</cell><cell>0.0325</cell><cell>0.0419</cell></row><row><cell>3</cell><cell>0.34</cell><cell></cell><cell>96.04%</cell><cell>0.0311</cell><cell>0.0410</cell></row><row><cell>4</cell><cell>0.23</cell><cell></cell><cell>95.99%</cell><cell>0.0337</cell><cell>0.0443</cell></row><row><cell cols="6">(b) Number of sharing pooling layers: Results on the TuSimple</cell></row><row><cell cols="5">dataset by changing the number of shared HRMs.</cell></row><row><cell>R-18-E2E</cell><cell></cell><cell></cell><cell cols="2">TuSimple</cell></row><row><cell cols="2">Loss function</cell><cell cols="2">Accuracy</cell><cell>FP</cell><cell>FN</cell></row><row><cell cols="2">KL-divergence (KL)</cell><cell></cell><cell>95.49%</cell><cell>0.0376</cell><cell>0.0551</cell></row><row><cell cols="2">PL-Loss (P L)</cell><cell></cell><cell>95.69%</cell><cell>0.0455</cell><cell>0.0482</cell></row><row><cell cols="2">Cross-Entropy (CE)</cell><cell></cell><cell>96.04%</cell><cell>0.0311</cell><cell>0.0410</cell></row><row><cell>(c)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Comparison of different algorithms on the CULane test set. F1-measure is displayed except &quot;Cross&quot; for which only FP is shown</title>
		<imprint/>
	</monogr>
	<note>Table 3</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Cross</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real time detection of lane markers in urban streets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Gradient descent optimization of smoothed information retrieval metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingrui</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf. Retr</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Reliable multilane detection and classification by utilizing CNN as a regression network: Munich</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shriyash</forename><surname>Chougule</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nora</forename><surname>Koznek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asad</forename><surname>Ismail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganesh</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikram</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Schulze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Autonomous driving in the real world: Experiences with tesla autopilot and summon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Dikmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><forename type="middle">M</forename><surname>Burns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th international conference on automotive user interfaces and interactive vehicular applications</title>
		<meeting>the 8th international conference on automotive user interfaces and interactive vehicular applications</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="225" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">End-to-end lane detection through differentiable least-squares fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Realtime category-based and general obstacle detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Verner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ayash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goldner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time category-based and general obstacle detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Silberstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaul</forename><surname>Oron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Verner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ayash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Goldner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafi</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kobi</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Levi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="198" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Embedding loss driven generative adversarial networks for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cedric</forename><surname>Nugteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nra</forename><surname>Baka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>El-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Recent progress in road and lane detection: a survey. Machine vision and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aharon</forename><surname>Bar Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Raz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="727" to="745" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection CNNs by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to cluster for proposal-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Tae-Hee Lee, Hyun Seok Hong, Seung-Hoon Han, and In So Kweon. Vpgnet: Vanishing point guided network for lane and road marking detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Shin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Bailo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namil</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stixelnet: A deep convolutional network for obstacle detection and road segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Garnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Lane detection in low-light conditions using an efficient data enhancement : Light conditions style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haowei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.01177</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08408</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davy</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><forename type="middle">De</forename><surname>Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE intelligent vehicles symposium (IV)</title>
		<imprint>
			<biblScope unit="page" from="286" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tiled convolutional neural networks. In NIPS</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial CNN for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">ENet: A deep neural network architecture for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Chaurasia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangpil</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugenio</forename><surname>Culurciello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02147</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The google car: driving toward a better future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poczter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jankovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Business Case Studies (JBCS)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="14" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Laplace landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuncheng</forename><surname>Joseph P Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tulyakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ERFNet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inside waymo&apos;s self-driving car: My favorite transistors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel L Rosenband</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 Symposium on VLSI Circuits</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Mobileye: The future of driverless cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David B Yoffie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
