<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-Grained Visual Classification with Efficient End-to-end Localization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Hanselmann</surname></persName>
							<email>hanselmann@cs.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">AppTek GmbH</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
							<email>ney@cs.rwth-aachen.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Human Language Technology and Pattern Recognition Group</orgName>
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">AppTek GmbH</orgName>
								<address>
									<settlement>Aachen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fine-Grained Visual Classification with Efficient End-to-end Localization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>H.HANSELMANN AND H.NEY: EFFICIENT END-TO-END LOCALIZATION 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The term fine-grained visual classification (FGVC) refers to classification tasks where the classes are very similar and the classification model needs to be able to find subtle differences to make the correct prediction. State-of-the-art approaches often include a localization step designed to help a classification network by localizing the relevant parts of the input images. However, this usually requires multiple iterations or passes through a full classification network or complex training schedules. In this work we present an efficient localization module that can be fused with a classification network in an endto-end setup. On the one hand the module is trained by the gradient flowing back from the classification network. On the other hand, two self-supervised loss functions are introduced to increase the localization accuracy. We evaluate the new model on the three benchmark datasets CUB200-2011, Stanford Cars and FGVC-Aircraft and are able to achieve competitive recognition performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The research area of fine-grained visual classification (FGVC) addresses classification tasks where the different categories are quite similar in appearance and the differences can be very subtle. Such tasks include the categorization of different animal species or car models. State-of-the-art approaches typically rely on a strong convolutional neural network (CNN) as classification or backbone network. This model is then improved with methods that try to make it easier to find the subtle differences between the classes. One such method is localization where the discriminative region of the image to classify is localized and distracting background is discarded. Another advantage of localization is that the scale of the objects is normalized. However, existing approaches for localization usually require inefficient methods to obtain the discriminative regions such as multiple passes through a full classification network or complicated training schedules that prevent end-to-end integration. For this reason we aim to define an efficient localization module that can be integrated and trained in an end-to-end setup. To this end we define a novel, lightweight network to obtain the parameters defining the discriminative region of the image to classify. This module first generates an attention map which is then used to predict the bounding box of the discriminative region. c 2020. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.</p><p>It is trained by the gradients propagated backwards through the end-to-end training, as well as additional self-supervised loss functions. Overall, only the class labels for the training images are needed to train the model. We evaluate our model on the three standard benchmark datasets CUB200-2011, Stanford Cars and FGVC-Aircraft and are able to report very competitive recognition accuracies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related work</head><p>There are several approaches that focus on the localization component of a FGVC system. The spatial transformer network (STN) proposed in <ref type="bibr" target="#b10">[11]</ref> learns an affine transformation in an end-to-end setting. However, STNs can be difficult to train and in order to be able to estimate the affine parameters a complex network is necessary (in <ref type="bibr" target="#b10">[11]</ref> the same network architecture as for the classification network itself is used). This limits the choice of classification networks if GPU memory is limited. In the recurrent attention CNN (RA-CNN) <ref type="bibr" target="#b3">[4]</ref> an end-to-end model is presented that recurrently zooms into discriminative regions of the image. Again, complex networks are used to make the decisions about which patches to extract. Additionally, RA-CNN uses a complex training schedule with pre-training and iteratively fixing the weights of sub-networks. A similar idea has been proposed in <ref type="bibr" target="#b16">[17]</ref>. Here multiple classification networks are trained consecutively, where one network uses the output of the previous network to attend to discriminative regions. The final classification decision is found by building an ensemble of the different classification networks. The trilinear attention sampling network (TASN) <ref type="bibr" target="#b22">[23]</ref> uses attention to sample multiple interesting regions in the input image, which are then forwarded through the network again. In <ref type="bibr" target="#b6">[7]</ref> a method was proposed that aims to avoid having to use a full pass through a classification network to be able to localize the relevant image regions. To achieve this, a separate lightweight localization module is trained that learns to predict attention maps that can be used for localization. However, this requires multiple separate training runs instead of a unified end-to-end model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Efficient end-to-end localization</head><p>Our full end-to-end model consists of three main components, AttNet, AffNet and the actual classification network. An overview of the system and how the components interact is given in <ref type="figure">Figure 1</ref>. The two components AttNet and AffNet together perform the localization and are designed to be efficient and lightweight such that they do not increase the computation time or memory footprint significantly. Specifically, AttNet generates an attention map of the input image which is then used by AffNet to estimate the affine parameters that define the bounding box of the object in the image. These parameters are then used to return a cropped image which is processed by the classification network. As in the design of STN <ref type="bibr" target="#b10">[11]</ref>, the cropping operation is implemented using bilinear sampling. This results in a differentiable cropping procedure which is necessary to train all three components jointly in an end-to-end fashion using gradient descent.</p><p>The full model is trained using four different loss functions, but only a single class label for each training image is needed as annotation. The final output layer of the classification network is trained with the standard cross-entropy loss L CE , while a penultimate fully connected layer is trained with the embedding loss L emb as defined in <ref type="bibr" target="#b6">[7]</ref>. The latter minimizes the distance of each training sample to its respective class center while maximizing the distances between the class centers. In addition to L CE and L emb we define two weakly supervised losses L att and L a f f designed to help train AttNet and AffNet. As shown in <ref type="bibr" target="#b23">[24]</ref>, the mean M over the feature maps of the final convolutional layer in a classification network defines an attention map of the object in the image. In our setup we use this mean to guide the localization process in two ways. On the one hand we define L att to minimize the distance between the output of AttNet and M. This trains AttNet to predict the M for an input image without having to forward the image through the full classification network. On the other hand we calculate bounding box parameters based on M which then serve as target output for AffNet. The difference between this target output and the output of AffNet is then optimized by L a f f . The exact definition of L att and L a f f will be given in Section 2.1 and 2.2.</p><p>The total overall loss is then given by</p><formula xml:id="formula_0">L = L CE + λ L emb + L att + L a f f<label>(1)</label></formula><p>where the hyper-parameter λ is used as a weight for the embedding loss L emb .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">AttNet</head><p>Given an input image X, AttNet predicts an attention map A of dimension 1 × I × J. Since we do not want to sacrifice computational complexity to obtain A, AttNet needs to be lightweight and efficient, but still be able to predict accurate attention maps. It has been shown in <ref type="bibr" target="#b6">[7]</ref> that this can be achieved by using the first few layers until after the first residual block of a ResNet-50 <ref type="bibr" target="#b7">[8]</ref> and down-sizing the input to 64 × 64. For this reason we chose the same architecture for AttNet. As part of an end-to-end system AttNet learns from the gradient pushed back from the final layers of the full model (defined by L CE and the embedding loss L emb ). In addition, we apply local supervision with the loss</p><formula xml:id="formula_1">N N v N N h θ v θ h M a x P o o l M a x P o o l</formula><formula xml:id="formula_2">L att = λ att L SL1 (A, M)<label>(2)</label></formula><p>where M is defined as the mean over the feature maps of the last convolutional layer and L SL1 is the smooth L1 loss <ref type="bibr" target="#b5">[6]</ref>.</p><p>The influence of L att on the training of AttNet is regulated by the hyper-parameter λ att . To be able to regulate the influence of the gradient flowing back from the latter part of the model as well we introduce the hyper-parameter β att . The gradient with respect to A is then computed as</p><formula xml:id="formula_3">∂ L ∂ A = β att · ∂ L 1 ∂ A + ∂ L att ∂ A (3) = β att · ∂ L 1 ∂ A + λ att · ∂ L SL1 (A, M) ∂ A<label>(4)</label></formula><p>where L 1 is the overall loss without L att :</p><formula xml:id="formula_4">L 1 = L CE + λ L emb + L a f f<label>(5)</label></formula><p>By setting β att to zero we have the option to cut the gradient flowing back from the classification network and instead train only with the local loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">AffNet</head><p>Once AttNet has produced an attention map, the task of AffNet is to estimate the affine transformation parameters θ needed to perform the localization. It operates on single channel input with dimension 1 × I × J which represents an attention map. AffNet is composed of two parts, a pre-processing module and the affine parameter estimation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Pre-processing module</head><p>The bounding box can be obtained from an attention map by applying min-max normalization and binarization based on a threshold τ and computing the smallest rectangle containing all positions with value one <ref type="bibr" target="#b6">[7]</ref>. In order to incorporate this into AffNet a pre-processing module is defined. However, this module needs to be differentiable in order to be integrated in the end-to-end structure. This can be achieved by defining a network that applies minmax normalization, thresholding and binarization with differentiable layers. Specifically, min-max normalization is achieved by layers computing the minimum and maximum, a subtraction layer and a division layer. The thresholding is realized using a subtraction and a ReLU activation layer. Finally, the binarization is done with a multiplication and Sigmoid activation layer as in suggested <ref type="bibr" target="#b3">[4]</ref>.</p><p>Note that the pre-processing module only has one learnable parameter. This is the parameter w τ that defines the thresholding. It is initialized with the value τ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Affine parameter estimation</head><p>The second component of AffNet is the actual estimation of the affine parameters θ needed for the localization. Specifically, we define four parameters</p><formula xml:id="formula_5">θ = [s x , s y ,t x ,t y ]<label>(6)</label></formula><p>where s x and s y are the horizontal and vertical scale, while t x and t y are the horizontal and vertical translation.</p><p>A straightforward way to implement the estimation is to apply some neural network to the attention map and predict the parameters (e.g. a simple feed-forward neural network (FFNN) <ref type="bibr" target="#b3">[4]</ref>). However, we have observed that these networks often have difficulty to estimate very accurate parameters. For this reason we split up the problem into a horizontal and vertical part (illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>). Given an input attention map of size I × J, we apply max pooling with kernels of size I × 1 and 1 × J, respectively. Each of the two pooling operations generates a vector containing all necessary information to estimate the scaling and translation parameter for the respective dimension. Given these two vectors two small subnetworks NN h and NN v estimate the horizontal and the vertical transformation parameters for translation and scale, respectively. We use the same configuration for both sub-networks. They consist of one linear layer of size 128 with ReLU activation functions followed by a second linear layer of size two that returns the predicted affine parameters for the respective dimension.</p><p>Just as with AttNet we also define weak local supervision for AffNet. We generate target transformation parameters θ τ from the mean M obtained from the last convolutional layer of the classification network. To achieve this we first compute a bounding box as in <ref type="bibr" target="#b6">[7]</ref>. The bounding box coordinates are then converted into the scaling and translation parameters. The loss for AffNet is defined as</p><formula xml:id="formula_6">L a f f = λ a f f L SL1 (θ , θ τ )<label>(7)</label></formula><p>Analogously to Formula 3 we introduce the hyper-parameter β a f f to weight the gradient flowing back from the classification and embedding loss. The gradient with respect to θ is then given by</p><formula xml:id="formula_7">∂ L ∂ θ = β a f f · ∂ L 2 ∂ θ + ∂ L a f f ∂ A (8) = β a f f · ∂ L 2 ∂ θ + λ a f f · ∂ L SL1 (θ , θ τ ) ∂ θ (9)</formula><p>where L 2 is the overall loss without L a f f :</p><formula xml:id="formula_8">L 2 = L CE + λ L emb + L att<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Initialization</head><p>AttNet is created from a pre-trained ResNet-50 and we keep the weights as initialization. AffNet on the other hand is initialized randomly with the exception of the output layer. Here the weights are initialized with zero and the bias is set to one for s x and s y and to zero for t x and t y . As a result, the initial affine transformation is the identity transformation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental evaluation</head><p>First we evaluate AffNet and compare the model configuration with other possible choices. This is followed by an evaluation of the full end-to-end model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">AffNet</head><p>We evaluate the design of AffNet with a set of attention maps generated from the CUB200-2011 dataset using a trained classification network without localization. Some examples of the attention maps as well as the target bounding boxes are shown in the first two rows of <ref type="figure" target="#fig_2">Figure 3</ref>. The generated attention maps are split into a training and a test set. The performance is measured by the smooth L1 error achieved on the test set as well as the mean IoU for two different thresholds. Additionally, we report the runtime in milliseconds. We compare AffNet with three other architectures which are defined as follows:</p><p>• FFNN: This model is a small feed-forward network designed to have a similar number of parameters as AffNet. It has one fully connected layer of size 32 with batch normalization and ReLU activation functions.</p><p>• ResNet-S: In the definition of AttNet we use the first few layers of a ResNet-50 including the first residual module. The same can be done for AffNet and we refer to this shortened model as ResNet-S.</p><p>• ResNet-50: We also evaluate using a full ResNet-50. To fit the input dimension of 224 × 224 × 3 we re-scale the attention maps.</p><p>All three architectures use an output layer of size four to predict the four affine parameters for scale and translation. We test all architectures with and without applying the pre-processing module first.</p><p>The results are given in Tables 1. Comparing the models with and without the preprocessing module it becomes evident that the pre-processing module helps to improve the predictions significantly. For all network configurations the results for the IoU metric are  better by including the pre-processing module. Additionally, we can observe that AffNet achieves the best result with the least parameters and close to the fastest runtime. The second best result with respect to IoU is achieved by the shortened ResNet that was also used as architecture for AttNet. It is on par with AffNet for IoU &gt; 0.8, but if the threshold is set to 0.95 AffNet still achieves decent accuracy, while for ResNet-short the accuracy drops below 10%. Additionally, AffNet is much faster and needs much fewer parameters. The accuracy of AffNet is further illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. The third row contains predictions made by AffNet and we can observe that they almost match the target bounding boxes. For these reasons we believe AffNet is the ideal choice to estimate the affine parameters in the localization module for the end-to-end classification model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Full system</head><p>We evaluate the full end-to-end model on the three standard benchmark datasets CUB200-2011 <ref type="bibr" target="#b18">[19]</ref>, Stanford cars <ref type="bibr" target="#b12">[13]</ref> and FGVC-Aircraft <ref type="bibr" target="#b14">[15]</ref>.  <ref type="table">Table 2</ref>: Ablation study using the CUB200-2011 dataset and setting 1.</p><p>Our training setup is very similar to <ref type="bibr" target="#b6">[7]</ref>. We use ResNet-101 <ref type="bibr" target="#b7">[8]</ref> as classification network which has been pre-trained on ImageNet <ref type="bibr" target="#b7">[8]</ref> with overlapping test images removed. The input resolution is 448 × 448, and we train for 90 epochs with a starting learning rate of 0.003. The learning rate gets reduced every 30 epochs by multiplying with 0.1. Apart from the ablation study we set the hyper-parameters β att and β a f f to one and λ att and λ a f f to 16. The threshold τ that also initializes w τ is set to 0.3.</p><p>We evaluate with two settings. Setting 1 is similar to <ref type="bibr" target="#b6">[7]</ref> and we use ResNet-101 with an additional embedding layer with dimension 512. In setting 2 we modify the stem of ResNet-101 as in <ref type="bibr" target="#b8">[9]</ref> (ResNet-C) and use a feature dimension of 1024 for the embedding layer.</p><p>Due to the lightweight configuration of AttNet and AffNet, the full model can be trained on a single GPU with 11 GB memory with a batch-size of 14. While a forward pass of the classification network has a runtime of 162 ms, adding the localization module increases this only to 164 ms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Ablation study</head><p>In <ref type="table">Table 2</ref> we analyze the importance of the loss L att and L a f f as well as the two hyperparameters β att and β a f f . As baseline, we set the latter to zero and disable L att and L a f f (by setting λ att and λ a f f to zero as well). This means no local self-supervision is used and no gradient is flowing back through AffNet and AttNet. As a result, the initial parameters are never updated and the localization module is fixed to return the identity transformation. The baseline result of 87.0% accuracy therefore corresponds to using no localization at all. If we keep the two losses disabled but set β att and β a f f to one we end up with a setup similar to STNs <ref type="bibr" target="#b10">[11]</ref> where the affine parameters are learned only through the gradient flowing back from the classification loss. However, we can observe that this does not lead to an improvement over the baseline. Using only the local self-supervision and cutting off the gradients by setting β att and β a f f to zero does lead to an improvement over the baseline. This indicates that the local self-supervision is very important to achieve a good recognition performance. Setting β att and β a f f to one then results in a true end-to-end system and yields another improvement with a classification accuracy of 88.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Comparison to state-of-the-art</head><p>In <ref type="table" target="#tab_3">Table 3</ref> we compare the end-to-end system presented in this work with the best state-ofthe-art results on the CUB200-2011, Stanford Cars and FGVC Aircraft benchmarks. Compared with other methods focusing on localization (e.g. <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b16">17]</ref>) we can achieve the best accuracies while also offering a unified and efficient training process on top of efficient test-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Accuracy[%] CUB200-2011 Stanford Cars FGVC Aircraft STN <ref type="bibr" target="#b10">[11]</ref> 84.1 --RA-CNN <ref type="bibr" target="#b3">[4]</ref> 85.3 92.5 -ISE <ref type="bibr" target="#b16">[17]</ref> 87.2 94.1 90.9 NTS-Net <ref type="bibr" target="#b20">[21]</ref> 87.5 93.9 91.4 DCL <ref type="bibr" target="#b0">[1]</ref> 87.8 94.5 93.0 OSME-MAMC <ref type="bibr" target="#b17">[18]</ref> 86.5 93.0 -iSQRT-COV <ref type="bibr" target="#b13">[14]</ref> 88.7 93.3 91.4 CS Parts <ref type="bibr" target="#b11">[12]</ref> 89.5 92.5 -Spatial RNN <ref type="bibr" target="#b19">[20]</ref> 89  ing. On Stanford Cars only TResNet <ref type="bibr" target="#b15">[16]</ref> achieves a better result with respect to accuracy. TResNet is a very recently proposed new classification network that could also well be incorporated into our approach by replacing ResNet-101. Also on FGVC Aircraft there is with TBMSL-Net <ref type="bibr" target="#b21">[22]</ref> only one method with a better accuracy. However, TBMSL-Net is a multi-scale and multi-patch approach, while our model only uses a single pass.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>In this work we introduced an efficient localization module that can be integrated into an FGVC model in an end-to-end setup. We presented a new network to estimate affine parameters. This network is composed of two parts, AttNet and AffNet. We showed that our choice for AffNet is able to estimate very accurate parameters given an attention map as input. The localization module achieves the best performance boost when it is trained end-to-end with additional self-supervised loss functions. The latter derive their supervision signals from the mean over the feature maps of the last convolutional layer of the classification network. Especially on Stanford Cars and FGVC Aircraft very competitive recognition accuracies were obtained.</p><p>For future work it would be interesting to test other backbone classification networks such as the recently proposed TResNet. This could lead to another boost in classification accuracy.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>AttNet 1 ×Figure 1 :</head><label>11</label><figDesc>End-to-end model with localization. The dashed arrows indicate that here no gradient flows backwards since these computations are only used for generating the supervision signals for the self-supervised losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of AffNet. Vertical and horizontal max pooling is used to simplify the problem and generate two vectors containing the necessary information to estimate the vertical and horizontal transformation parameters, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Example of data used for AffNet evaluation. The upper row contains the input images and the target bounding boxes are shown in the middle row. The lower row shows the actual predictions estimated by AffNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different model architectures to estimate the affine parameters.</figDesc><table><row><cell>Network</cell><cell>Prep. module</cell><cell>Parameters</cell><cell cols="4">Runtime SL1 error IoU &gt; 0.8 IoU &gt; 0.95 (ms) (×e −3 )</cell></row><row><cell>FFNN</cell><cell></cell><cell>6500</cell><cell>0.1</cell><cell>6.4</cell><cell>0.46</cell><cell>0.00</cell></row><row><cell>FFNN</cell><cell></cell><cell>6501</cell><cell>0.3</cell><cell>4.3</cell><cell>0.61</cell><cell>0.01</cell></row><row><cell>ResNet-S</cell><cell></cell><cell>241732</cell><cell>0.9</cell><cell>2.7</cell><cell>0.80</cell><cell>0.01</cell></row><row><cell>ResNet-S</cell><cell></cell><cell>241733</cell><cell>1.0</cell><cell>1.1</cell><cell>0.97</cell><cell>0.09</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>23516228</cell><cell>28.6</cell><cell>5.4</cell><cell>0.47</cell><cell>0.01</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>23516229</cell><cell>30.4</cell><cell>1.9</cell><cell>0.88</cell><cell>0.02</cell></row><row><cell>AffNet</cell><cell></cell><cell>4868</cell><cell>0.2</cell><cell>2.7</cell><cell>0.81</cell><cell>0.02</cell></row><row><cell>AffNet</cell><cell></cell><cell>4869</cell><cell>0.4</cell><cell>0.5</cell><cell>0.98</cell><cell>0.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with state-of-the-art on the CUB200-2011, Stanford Cars and FGVC Aircraft benchmarks.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalong</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clément</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS workshop</title>
		<imprint>
			<publisher>Granada</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised complementary parts models for fine-grained image classification from the bottom up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weifeng</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Elope: Fine-grained visual classification with efficient localization, pooling and embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harald</forename><surname>Hanselmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bag of tricks for image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tong He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="558" to="567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gpipe: Efficient training of giant neural networks using pipeline parallelism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019-12" />
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<publisher>December</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classification-specific parts for improving fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Korsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bodesheim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joachim</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<meeting><address><addrLine>Dortmund, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-09" />
			<biblScope unit="page" from="62" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2013-12" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Towards faster training of global covariance pooling networks by iterative matrix square root normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Ridnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hussam</forename><surname>Lawen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asaf</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itamar</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.13630</idno>
		<title level="m">Tresnet: High performance gpu-dedicated architecture</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Increasingly specialized ensemble of convolutional neural networks for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Simonelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><forename type="middle">De</forename><surname>Natale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Messelodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel Rota</forename><surname>Bulo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2018-10" />
			<biblScope unit="page" from="594" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Errui</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
			<biblScope unit="page" from="805" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep attention-based spatially recursive networks for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbin</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1791" to="1802" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Three-branch and mutilscale learning for fine-grained image recognition (tbmsl-net)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guisheng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heliang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition<address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="page" from="5012" to="5021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning attentive pairwise interaction for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiqin</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.10191</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
