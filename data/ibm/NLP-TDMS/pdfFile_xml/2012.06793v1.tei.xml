<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fine-grained Classification via Categorical Memory Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Marsh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
						</author>
						<title level="a" type="main">Fine-grained Classification via Categorical Memory Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Fine-grained Classification</term>
					<term>Categorical Memory Module</term>
					<term>Inter-class Similarity</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivated by the desire to exploit patterns shared across classes, we present a simple yet effective class-specific memory module for fine-grained feature learning. The memory module stores the prototypical feature representation for each category as a moving average. We hypothesize that the combination of similarities with respect to each category is itself a useful discriminative cue. To detect these similarities, we use attention as a querying mechanism. The attention scores with respect to each class prototype are used as weights to combine prototypes via weighted sum, producing a uniquely tailored response feature representation for a given input. The original and response features are combined to produce an augmented feature for classification. We integrate our class-specific memory module into a standard convolutional neural network, yielding a Categorical Memory Network. Our memory module significantly improves accuracy over baseline CNNs, achieving competitive accuracy with state-of-the-art methods on four benchmarks, including CUB-200-2011, Stanford Cars, FGVC Aircraft, and NABirds.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>T HIS article studies fine-grained classification, one of the most inherently challenging visual recognition tasks. Standard classification techniques are able to rely upon large-scale global features to differentiate classes. Meanwhile, fine-grained datasets simultaneously exhibit both subtle inter-class variation, such as global structure and visual appearance, and large intraclass variation, such as pose. This has led most methods to rely on local differences to provide discriminative cues <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>. We propose an orthogonal research direction that instead seeks to exploit inter-class similarities, which is an essential characteristic of the fine-grained classification.</p><p>Convolutional neural networks (CNNs) tend to learn features that discard some semantic information in favour of more discriminative visual patterns. In the case of fine-grained classification where discriminative cues are particularly subtle, the loss of semantic information may lead to a lack of generalization. Our insight is that encouraging the network to represent similarities in addition to unique discriminative cues is beneficial for robust fine-grained feature learning. Consider the case study of bird classification. Different species share both semantic and visual attributes such as eye color, throat color, and breast pattern, as illustrated in <ref type="figure">Figure 1</ref>. Instead of discarding these shared semantics, we pose they could be used to produce more informative and robust features. A W. <ref type="bibr">Deng</ref>  <ref type="figure">Fig. 1</ref>. Visual example of the correlation between fine-grained classes. Though the two birds belong to different subspecies, they are semantically and visually similar. Both share some common attributes (the first column), e.g., eye color, breast pattern, and throat color. This similarity indicates their representations will share some underlying semantic cues. natural question arises: how can we use these shared inter-class semantics to improve the fine-grained classification?</p><p>Motivated by this, we propose a class-specific memory module that gives a network direct access to the statistics of classspecific feature representations. Our module is interpretable, with each slot in memory corresponding to the average feature of a given category, which we refer to as a class prototype. The class-specific memory module can be directly integrated into popular CNN architectures without any additional modifications. It is trained in an end-to-end supervised manner, with the memory module updated while training along with the rest of the network parameters. Given a labelled feature, we update the corresponding memory slot via moving average. Thus, each memory slot is encouraged to represent the prototypical information of its corresponding class.</p><p>Intuitively, each class should exhibit a specific distribution of similarities with respect to the other classes. We hypothesize that this distribution is itself a useful identifier of class. The rich semantic correlation between classes provides beneficial guidance for fine-grained feature learning. Specifically, we want to give neural networks the ability to perceive the shared semantics across classes while preserving the unique and subtle cues of the given input sample. To this end, we propose an attention-based method of addressing memory, where each prototype is retrieved based on its relevance to the current input feature. We define relevance as the attention score based on cosine similarity in the learned feature space. We use the attention score to retrieve relevant class prototypes with respect to a given input, producing a response feature. Finally, the input feature and the response feature are combined, resulting in an augmented feature. This augmented feature is characterized by 1) expressing inter-class similarities of the input while 2) maintaining the individuality of input representations.</p><p>The proposed method is simple, effective, adds no additional parameters, and can be applied across non-rigid (e.g, birds) and rigid (e.g, cars) visual datasets. Furthermore, the entire memory module, in practice a matrix, is itself negligible in terms of computational overhead (including reading/writing) and adds no additional trainable parameters.</p><p>The key contributions of this paper include:</p><p>• We introduce a novel Categorical Memory Network (CMN) for fine-grain visual classification. Our method incorporates both feature learning and inter-class knowledge to yield more expressive features. • To leverage the knowledge of all categories, we introduce an attention-based memory addressing method to adaptively fuse prototypes. • CMN yields significant improvement over baselines, achieving competitive accuracy with the state-of-theart accuracy on four fine-grained visual classification benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Fine-grained Feature Learning. Fine-grained visual categorization is a special case of image classification wherein there exists significantly more inter-class similarities than standard classification tasks (e.g ImageNet). Fine-grained datasets share the internal opposing dynamics of subtle inter-class and large intra-class variation, which together represent the central challenge for this task.</p><p>Deep Convolutional Neural Networks (CNNs) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> are unable to sufficiently represent discriminative information required to classify fine-grained images. To obtain more expressive representations, several feature encoding methods have been proposed <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. Luo et al. <ref type="bibr" target="#b11">[12]</ref> introduce a cross-layer regularizer to use multi-scale features for classification. Lin et al. <ref type="bibr" target="#b7">[8]</ref> present a bi-linear pooling method to extract features with two Convolutional Networks. Gao et al. <ref type="bibr" target="#b8">[9]</ref> introduce a bi-linear pooling in a kernelized framework, improving the computational efficiency of bi-linear pooling. Kong et al. <ref type="bibr" target="#b9">[10]</ref> introduce a low-rank bilinear classifier to decrease the computation time. Furthermore, Yu et al. <ref type="bibr" target="#b10">[11]</ref> propose a hierarchical bi-linear pooling approach to fuse multi-layer features for fine-grained classification.</p><p>Another area of research focuses on finding discriminative regions in images. Early studies <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref> make use of bounding boxes and part annotations to localize discriminative regions. While such supervised annotations have proven beneficial to fine-grained feature learning, they are costly to obtain. More recently, weakly supervised methods have emerged as a promising research area <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b23">[24]</ref>. These methods use image category labels to localize object parts and extract part features for classification. Yang et al. <ref type="bibr" target="#b0">[1]</ref> use a navigator network to detect informative regions. Zheng et al. <ref type="bibr" target="#b1">[2]</ref> group ConvNet channels to detect various local patterns. Similarly, Fu et al. <ref type="bibr" target="#b2">[3]</ref> recursively localizes parts at multiple scales while Sermanet et al. <ref type="bibr" target="#b21">[22]</ref> uses an attention mechanism to select a series of regions. Sun et al. <ref type="bibr" target="#b22">[23]</ref> introduce a one-squeeze multi-excitation module to learn region features. Moreover, Recasens et al. <ref type="bibr" target="#b3">[4]</ref> recently proposed the use of saliency maps to localize informative regions.</p><p>In addition to the above approaches, some works focus on the small inter-class variations <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>. Zhou et al. <ref type="bibr" target="#b26">[27]</ref> exploit the rich relationships among categories through bipartitegraph labels. Dubey et al. <ref type="bibr" target="#b24">[25]</ref> introduce a pairwise confusion loss to prevent CNNs from over-fitting. The confusion loss attempts to bring class conditional probability distributions closer to each other. This enables the network to preserve some shared semantic among classes. Our CMN explicitly encourage the network to leverage shared prototypical information across classes for enhancing the input features. The class-specific memory module gives a network direct access to the statistics of class-specific feature representations. Thus, the network can adaptively retrieve relevant feature prototypes of different classes to augment input features. Moreover, our method focuses on feature combinations after obtaining image features, and as such is compatible with the existing approaches.</p><p>Memory Networks. Memory networks <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> are a unique variant of neural networks that allow a network to explicitly read and write information to an external memory module. Inputs are sequentially saved to memory and retrieved based upon the relevance (usually calculated via attention) to the current network input. Weston et al. <ref type="bibr" target="#b27">[28]</ref> introduced memoryaugmented networks for the task of question answering. Santoro et al. <ref type="bibr" target="#b30">[31]</ref> use an external memory to tackle meta-learning tasks. Li et al. <ref type="bibr" target="#b31">[32]</ref> and Kim et al. <ref type="bibr" target="#b32">[33]</ref> use memory networks for data generation. The closely related concept of Neural Turing Machines <ref type="bibr" target="#b33">[34]</ref> provides more complex functionality such as continuous memory representations and location and contentbased access. We study memory networks in the context of fine-grained visual classification and introduce a categorical memory module to enhance the fine-grained feature learning.</p><p>In the class-specific memory module, each row of the module corresponds to the prototypical information of a single class in the form of a moving average. This practice is relevant to prototypical networks <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref> in few-shot setting. However, our work is significantly different from them. These works use prototypes as class centers for the nearest-neighbor classifier. In comparison, we use relevant class prototypes to enhance the input features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CATEGORICAL MEMORY NETWORK</head><p>The proposed class-specific memory module can be directly integrated into an existing convolutional neural network without additional modifications. We refer to this network as a Categorical Memory Network (CMN). In practice, we insert the memory module after the last convolution layer of a convolutional neural network, e.g., ResNet-50 <ref type="bibr" target="#b5">[6]</ref>.</p><p>An overview of CMN is shown in <ref type="figure">Figure 2</ref>. We first pass the image through the convolutional layers and the final global average pooling, producing an original feature. We then use CNN FC 1) read 2) write augmented feat. class original feat.</p><formula xml:id="formula_0">1 2 3 … … … response feat. … memory 1 2 3 + Fig. 2.</formula><p>Architecture of Categorical Memory Network (CMN) for fine-grained classification. CMN integrates a class-specific memory module into an existing convolution neural network architectures (e.g., ResNet-50). The module contains memory slots to record category prototypes. Given an original feature produced by the CNN as a query, we retrieve and combine the relevant prototypes, and then output a response feature. We then update the corresponding memory slot based on the label of the original feature. Finally, we augment original feature with response feature. The resulting augmented feature is used for classification.</p><p>this original feature as the input to the class-specific memory module to retrieve the relevant prototypes, leading to a response feature. This response feature is then combined with the original feature via a weighted sum with learned coefficients, producing an inter-class similarity aware augmented feature. Finally, this augmented feature is fed through a single fully connected layer and a softmax to obtain class predictions. The network is trained in an end-to-end manner via classbased supervised learning, i.e., we use cross-entropy loss. In the following section, we introduce the categorical memory module and detail its reading and writing procedures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Class-specific Memory Module</head><p>Motivated by the intuition that giving a network access to leverage class-specific information is beneficial to fine-grain feature learning, we introduce a memory module to store the mean feature vector for each class. We refer to this as the feature prototype of a class. The memory module primarily consists of two operations, 1) writing the feature prototypes to memory module, and 2) computing response feature based on the current input feature.</p><p>During training, the memory slots, i.e rows, are read from based on the relevance to the given input. We then update memory slots based on the categories of given inputs in a mini-batch. During testing, the memory is not updated and used only for reading.</p><p>1) Memory Writing: We implement memory as a matrix M ∈ R C×D , where C is the number of classes and D is the dimension of the prototype. Each row m i of M represents the feature prototype of the i-th class. Instead of updating the prototypes with respect to the entire training set, we update by moving average per mini-batch <ref type="bibr" target="#b0">1</ref> . Given a feature f i from i-th category, the corresponding prototype f i is computed as,</p><formula xml:id="formula_1">m i ← m i + β(f i − m i ),<label>(1)</label></formula><p>where β ∈ (0, 1) is a coefficient that controls the update rate of prototype m i .</p><p>2) Memory Reading: Our goal is to borrow some prototypical knowledge from relevant classes to augment the input feature, yielding more informative and comprehensive representations. <ref type="bibr" target="#b0">1</ref> We perform the moving average update with the current mini-batch samples after memory reading. . Visualization of memory addressing. Our module is able to retrieve the relevant class prototypes for a given query image (the first column). In this figure, we show exemplar images from the three most relevant prototypes (the second to fourth columns) with respect to a given query image. We highlight red the prototype of the same class. In the last column, we show the attention scores between the input image and each retrieved prototype.</p><p>We use attention as our addressing mechanism. Given an input feature as a query, our module produces a response feature that is a uniquely tailored linear combination of all class prototypes in memory. Specifically, we calculate the attention score of the input feature f with respect to each prototype m i (for i ∈ {1, . . . , C}). These attention scores are used as weights to combine the prototypes, resulting in a response feature. We define the memory addressing process as,</p><formula xml:id="formula_2">m = C i=1 w i m i ,<label>(2)</label></formula><p>where w i is the attention score for the prototype m i , and m is the response feature. Attention Score. Intuitively, if a prototype is close to input feature in the representation space, the prototype is more likely to contain relevant information and is weighted accordingly. In practice, we use cosine similarity distance as our similarity metric. We normalize the similarity scores by a softmax function, giving the final attention scores. This process is defined as,</p><formula xml:id="formula_3">w i = exp(f ·m i /τ ) C j=1 exp(f ·m j /τ ) ,<label>(3)</label></formula><formula xml:id="formula_4">+ softmax × distance memory M × 1 × 1 × × 1 × 1 × 1 1 2 3 original feat. response feat.</formula><p>augmented feat. <ref type="figure">Fig. 4</ref>. Pipeline of memory reading process. Given an original feature, we calculate its Euclidean distance to each class prototype, which we then normalize to produce attention scores. After that, we use attention scores as weights to combine prototypes, resulting a response feature (response feat.). Finally, we combine the original feature and the response feature by using a weighted summation, yielding an augmented feature (augmented feat.) that is used for classification. In this figure, ⊗ is matrix multiplication, and ⊕ is broadcast element-wise addition.</p><p>where w i is the normalized attention score between input feature f and prototype m i of class i, the temperature τ is empirically set as 0.1. In our case, each prototype represents one fine-grained class. Classes with strong similarities to an input image should receive higher attention scores and hence be weighted higher in the memory response. An illustration of this is shown in <ref type="figure" target="#fig_1">Figure 3</ref>. We show exemplary images from the three most relevant prototypes for a given input image query. The query image and the images from the classes represented by the retrieved prototypes share visual and semantic similarities, e.g., color, wing pattern, bill shape, and breast pattern. Moreover, we also observe that the input image is more likely to select the prototype corresponding to its class. Based on this, we think that our attention-based memory addressing method is effective to detect relevant class prototypes for the given image.</p><p>Feature Augmentation. The original feature, which is instance-specific, and response feature, which is class-specific, are combined to produce an augmented feature representation that is used for classification. Specifically, we use the following summation:</p><p>f aug = f + m.</p><p>We also attempted more sophisticated methods to combine two features such as element-wise scaling and fully-connected layers, but these did not improve performance. An example of memory reading pipeline is illustrated in <ref type="figure">Figure 4</ref>. The reading process can be simply done by matrix operations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Discussion</head><p>Working mechanism. The learning mechanism of our method is that it makes the network adaptively exploit the knowledge of each class to generate more expressive representations. The advantage of the mechanism is two-fold, 1) it encourages neural networks to preserve shared semantic information, enabling a comprehensive understanding of fine-grained classes; 2) it allows networks to utilize external knowledge from relevant classes to better represent a given image, resulting in more robust and informative representation. The result is a final feature that captures both instance specific characteristics and the inter-class semantic relationships for the given input. Specifically, for the most indistinguishable classes, CMN will learn to focus more on instance specific characteristics. Thus, the subtle differences essential for classifying these classes can be better represented.</p><p>It is worth noting that our method does not require any side information such as structural labels <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> or shared attributes <ref type="bibr" target="#b26">[27]</ref>. Instead, our network can learn to adaptively select shared cues across all categories for a given input. Moreover, our method is compatible with several part-localization based methods such as <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b3">[4]</ref>, and hence has the potential to further improve recognition.</p><p>Interpretability. The physical meaning of the proposed class-specific memory module is very clear. The module is used to store and share representative characteristics between classes. Specifically, each row of the module corresponds to the prototypical information of a single class in the form of a moving average. In addition, the attention-based memory reading is based on the relevance between given inputs and prototypes and hence is also interpretable. Note that, in fewshot setting, class prototypes are used as class centers for constructing the nearest-neighbor classifier <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. Our work is significantly different from them: we use relevant class prototypes to enhance the input features.</p><p>Attention distribution. We visualize the cumulative sum of attention scores for top-k most similar prototypes in memory averaged over all samples in a dataset. The cumulative sum curve directly represents the distribution of attention scores on a specific dataset. We show the cumulative sum curves of four datasets in <ref type="figure" target="#fig_2">Figure 5</ref>. We observe that approximately 80% of the response feature is made up of only half the prototypes. This is consistent with our intuition that more relevant prototypes contribute more information to augment the input feature. Efficiency. Our method introduces a memory module in the form of a C × D matrix. It involves an additional matrix multiplication, which is computationally negligible. Thus, it has very similar inference time with baseline even when the number of classes C is large. CMN requires only a single forward pass, while the most localization-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b39">[40]</ref> require multiple forward passes. Comparing with them, the computational cost of CMN is much less.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>We comprehensively evaluate our method on four datasets, including CUB-200-2011 <ref type="bibr" target="#b41">[42]</ref>, Stanford Cars <ref type="bibr" target="#b42">[43]</ref>, FGVC Aircraft <ref type="bibr" target="#b40">[41]</ref>, and NABirds <ref type="bibr" target="#b43">[44]</ref>, respectively. In the experiment, we follow the same train/test splits in <ref type="table" target="#tab_0">Table I</ref>. Our method does not utilize any auxiliary supervision methods such as extra annotations and relies solely on image labels for the learning supervision. We report the top-1 accuracy in all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>Implementation Details. We implement our method in Pytorch <ref type="bibr" target="#b44">[45]</ref> and train all models on a single Tesla P-100 GPU. We adopt ResNet-50 <ref type="bibr" target="#b5">[6]</ref> pre-trained on ImageNet <ref type="bibr" target="#b45">[46]</ref> as our backbone network. The proposed method is trained using the SGD optimizer with momentum 0.9, weight decay 1e-4 and a mini-batch size of 16. We use a learning rate 4e-3 for the backbone and 5× multiplier for the newly added layer. We train the network for 60 epochs and decay the learning rate by 0.1 after 40 epochs. We preprocess images to 448 × 448 and use random horizontal flipping with a probability of 0.5 for data augmentation. Moreover, we set coefficient β in Eq. 1 as  <ref type="bibr" target="#b47">[48]</ref> VGG-16 86.9 DFL-CNN <ref type="bibr" target="#b23">[24]</ref> VGG-16 91.1 MA-CNN <ref type="bibr" target="#b1">[2]</ref> VGG-19 89.9 Kernel-Pooling <ref type="bibr" target="#b47">[48]</ref> ResNet-50 85.7 FT-ResNet [6]* ResNet-50 90.0 DFL-CNN <ref type="bibr" target="#b23">[24]</ref> ResNet-50 91.7 NTS-Net <ref type="bibr" target="#b0">[1]</ref> ResNet-50 91.4 DCL <ref type="bibr" target="#b48">[49]</ref> ResNet-50 93.0 Cross-X <ref type="bibr" target="#b11">[12]</ref> ResNet-50 92.6 LIO <ref type="bibr" target="#b49">[50]</ref> ResNet-50 92.7 ACNet <ref type="bibr" target="#b50">[51]</ref> ResNet-50</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="92.4">CMN (ours)</head><p>ResNet-50 93.8 0.9. The entire network is trained in an end-to-end manner via the supervision of image labels. The network does not require any special initialization, multiple training stages, and part or bounding box annotations. We report the top-1 classification accuracy from the last epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Performance Comparison</head><p>Compared methods. We compare the proposed method with ResNet-50 based methods and also include the best results of VGG based methods. For a fair comparison, we do not include approaches using additional data or annotations. The baseline is fine-tuning from ResNet-50 (FT-ResNet) <ref type="bibr" target="#b5">[6]</ref>, which is used in NTS-Net <ref type="bibr" target="#b0">[1]</ref> and TASN <ref type="bibr" target="#b39">[40]</ref>.</p><p>Comparison on FGVC-Aircraft. In <ref type="table" target="#tab_0">Table II</ref>, we compare our method (CMN) with the existing methods on Aircraft. CMN achieves 93.8% accuracy, which is +3.8% higher over baseline (FT-ResNet). The achieved accuracy is highest on the Aircraft. This indicates that the proposed class-specific memory module is beneficial for improving feature representations. Moreover, CMN outperforms LIO <ref type="bibr" target="#b49">[50]</ref> and DCL [49] by 1.1% and 0.8%, respectively. In addition, our method surpasses ACNet [51] by 1.4%. Compared with TASN <ref type="bibr" target="#b39">[40]</ref> and NTS-Net <ref type="bibr" target="#b0">[1]</ref>, CMN also gains competitive accuracy, showing its effectiveness for Aircraft classification.</p><p>Comparison on Stanford Cars. As shown in <ref type="table" target="#tab_0">Table III</ref>, CMN gains 2.4% improvement over baseline (FT-ResNet). Compared with the state-of-the-art methods, CMN can achieve competitive accuracy. For example, it is 0.3% and 0.4% higher than ACNet <ref type="bibr" target="#b50">[51]</ref> and DCL <ref type="bibr" target="#b23">[24]</ref>, respectively. Moreover, our methods is also higher than other VGG based approaches, such as MA-CNN <ref type="bibr" target="#b1">[2]</ref> and Kernel-Pooling <ref type="bibr" target="#b47">[48]</ref>. The above comparisons demonstrate our method is effective for the two rigid datasets with a significant structural variation. It is worthy of noting that CMN does not require multiple forward passes like the most localization-based methods <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Therefore, we think the proposed method is efficient and effective for fine-grained feature learning. ResNet-50 93.1 DFL-CNN <ref type="bibr" target="#b23">[24]</ref> ResNet-50 93.1 NTS-Net <ref type="bibr" target="#b0">[1]</ref> ResNet-50 93.9 TASN <ref type="bibr" target="#b39">[40]</ref> ResNet-50 93.8 DCL <ref type="bibr" target="#b48">[49]</ref> ResNet-50 94.5 Cross-X <ref type="bibr" target="#b11">[12]</ref> ResNet-50 94.6 LIO <ref type="bibr" target="#b49">[50]</ref> ResNet-50 94.5 ACNet <ref type="bibr" target="#b50">[51]</ref> ResNet-50 94.6 CMN (ours)</p><p>ResNet-50 94.9</p><p>Comparison on CUB-200-2011. We additionally report the results on non-rigid CUB-200-2011 shown in <ref type="table" target="#tab_0">Table III</ref>. We show that our CMN achieves 2.4% improvement over baseline (FT-ResNet) and arrives at 88.2%. This result is also comparable with the current state-of-the-art method ACNet <ref type="bibr" target="#b50">[51]</ref> (88.2% vs. 88.1%). It is worth noting that CMN outperforms ACNet by +1.4% and +0.3% on Aircraft and Car, respectively. Compared with TASN <ref type="bibr" target="#b39">[40]</ref>, our method is also competitive (88.2% vs. 87.9%). In addition, TASN and NTS-Net <ref type="bibr" target="#b0">[1]</ref> require multiple forward passes to extract region features, making them significantly more computationally prohibitive. CMN requires only a single forward pass through the network, and thus is relatively efficient. Moreover, we think CMN would be compatible with these part-localization based methods, and further achieve improvements. Our method also gains comparable accuracy with Cross-X <ref type="bibr" target="#b11">[12]</ref>. Cross-X introduces a cross-layer regularizer to leverage multi-scale features from different layers, which we believe if combined with our method could also result in an improvement in classification accuracy.</p><p>Comparison on NABirds. The architecture of the purposed method is simple and effective. It can easily be applied to large-scale datasets. We also conduct experiments on another non-rigid dataset NABirds and report results in <ref type="table" target="#tab_4">Table V</ref>. In this dataset, we observe that our method also achieves +3.8% improvement over baseline and arrives at 87.8 %. This improvement also validate the effectiveness of our method. Moreover, CMN is competitive with the recent state-of-theart method Cross-X <ref type="bibr" target="#b11">[12]</ref>. More specific, our method is 1.6% higher than Cross-X in the same settings. We think our method considers shared patterns across classes (inter-class similarities) that is a characteristic of the fine-grained classification, and thus achieves improvements over baselines. ResNet-50 87.4 NTS-Net <ref type="bibr" target="#b0">[1]</ref> ResNet-50 87.5 TASN <ref type="bibr" target="#b39">[40]</ref> ResNet-50 87.9 DCL <ref type="bibr" target="#b48">[49]</ref> ResNet-50 87.8 Cross-X <ref type="bibr" target="#b11">[12]</ref> ResNet-50 87.7 LIO <ref type="bibr" target="#b49">[50]</ref> ResNet-50 88.0 ACNet <ref type="bibr" target="#b50">[51]</ref> ResNet-50 88.1 CMN (ours)</p><p>ResNet-50 88.2 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Component Analysis</head><p>Effect of attention-based memory reading. Given an input feature, we adopt an attention-based method of retrieving relevant prototypes (as detailed in Section III-A2). To validate its effectiveness, we introduce two variants for comparison, 1) equal selection, where all attention scores are equal to 1/C (C is the number of categories); 2) predicted selection <ref type="bibr" target="#b31">[32]</ref>, which uses a fully connected layer followed by a softmax to predict attention scores for the given input feature. We compare these two variants and our method in <ref type="figure">Figure 8</ref>. They both improve the accuracy compared with our baseline. This demonstrates that leveraging class prototypes is beneficial for fine-grained feature learning. Meanwhile, equal selection delivers worse performance than predicted selection (87.1% vs. 87.5%), indicating that the retrieved prototypes should be dependent on the input feature. Our attention-based method outperforms equal section and predicted selection by 1.1% and 0.7%, respectively. This is because the attention-based method is based on Euclidean distance, which is a direct and effective way to represent the feature correlation.</p><p>Importance of learning in the class-specific module. Class-specific memory module records the moving average of each class and is updated during training. To validate the importance of learning in the module, we test a random module for comparative purposes, i.e., we randomly initialize the memory matrix and fix it during training. In <ref type="table" target="#tab_0">Table VI</ref>  <ref type="figure">6</ref>. Visualization of the learned visual cues on the rigid dataset Aircraft and the non-rigid dataset CUB. Based on the attention scores computed by our method, we select images from three visually similar classes (the first three columns) and one dissimilar class (the last column) for each dataset. The similar classes share some visual patterns, e.g., "breast" and "bill" for birds, and "under-wing fuselage", "tail", and "upperdeck" for aircraft. Each class contains unique and subtle visual cues. Best viewed in color.  compare the two modules on Aircraft and CUB. We observe that the random module is on par with our baseline. This indicates that 1) naively increasing network capacity is not beneficial and 2) that the module should record useful and meaningful information and be updated during network learning. Importance of prototype diversity. We leverage all prototypes to compute a response feature (Eq. 2). In <ref type="figure">Figure 8</ref>, we show the effect on accuracy of using only the prototypes with the top-K attention scores on CUB. We observe that allowing the response to rely on more than just the most </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Baseline Attention Selection Equal Selection</head><p>Predicted Selection <ref type="figure">Fig. 8</ref>. Comparison of three attention score mechanisms for memory reading. We test attention-based selection i.e. our technique, equal weighting for all prototypes, and predicting scores based on the given image via a fully-connected layer. The results are on the CUB-200-2011 dataset. The proposed attentionbased selection is the best way to combine class prototypes. similar prototypes improves accuracy. Moreover, we observe that the accuracy does improve using more than 100 prototypes. More specifically, the accuracy does not improve whrn using more than 100 prototypes. This is because the response feature is most formed by the half relevant prototypes, which is consistent with our observation in attention distribution ( <ref type="figure" target="#fig_2">Figure  5</ref>): the most relevant prototypes contribute more information to enhance the input feature. Feature visualization. We average the attention scores for the images of each class. For a particular class, a higher average attention score indicates that the corresponding class is semantically similar. In <ref type="figure">Figure 6</ref>, we visualize the feature maps of last convolutional layer. We select the three most relevant classes (the first three columns) as defined by our method and one irrelevant class (the last column). We observe that images of correlated classes tend to focus of similar patterns, e.g., Shiny Cowbird, Brewer Blackbird, and Rusty Blackbird have the same visual evidence of breast pattern. This shared pattern can effectively distinguish them from irrelevant Red-headed Woodpecker. Moreover, each class has it own distinctive cues, e.g., Boeing 747-200 has a discriminative evidence on "tail" part, and Cessna 208 has an unique pattern on "wheel" part.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Further Evaluation.</head><p>We also conduct experiments on more general classification dataset CIFAR-100 <ref type="bibr" target="#b52">[53]</ref>, where the inter-class similarity is relatively small. In <ref type="table" target="#tab_0">Table VII</ref>, we validate the effect of the memory module on several networks. We observe that our CMN does not bring any improvement on this dataset. This is because the classes in CIFAR-100 have low class-class similarity. Furthermore, our method could add some unnecessary classclass cues to the input feature, the learning difficulty may be increased, thus having slightly lower accuracy.</p><p>In fine-grained classification datasets, the number of training images per class is relatively small. In comparison, CIFAR-100 contains adequate training data for each class. In addition, the class-specific prototypes have shown its effectiveness for few-shot learning <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. This motivates us to further study the working mechanism of CMN under the few-example setting. Specifically, we study whether CMN mainly helps when training data per class is scarce on CIFAR-100. To validate this, we reduce the number of training images in CIFAR-100 and report the experimental results in <ref type="figure" target="#fig_6">Figure 9</ref>. In practice, we create three subsets, 1) CIFAR-100 (subset A) has 5,000 training images; 2) CIFAR-100 (subset B) has 10,000 training images; 3) CIFAR-100 (subset C) has 25,000 training images. As shown in <ref type="figure" target="#fig_6">Figure 9</ref>, we observe that both CMN and original ResNet-18 achieve almost the same top-1 accuracy under the different CIFAR-100 subsets. This also indicates that CMN might not suitable for general classification dataset that contains relatively small inter-class similarity. Thus, we believe the advantage of CMN lies in that it has ability to leverage the shared semantic information across classes to well represent the input image. In summary, CMN is specifically tailored for fine-grained classification, where images are both semantically and visually similar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>We introduce an efficient, effective, and interpretable classspecific memory module for fine-grained visual classification. By storing the prototypical information of all classes, the memory module allows the network to share semantic information between classes. We use an attention-based method to retrieve relevant class prototypes for a given input and generate a specially tailored response feature to augment the original feature. We integrate the memory module with a convolutional neural network, yielding a Categorical Memory Network (CMN). Our method leverages relevant prototypes to produce representations that express inter-class semantics while preserving instance-specific discriminative cues. Moreover, encouraging the network to represent inter-class similarities leads to more robust fine-grained representations.</p><p>Our method significantly increases accuracy at effectively no additional computational cost, achieving accuracy competitive with state-of-the-art methods on four fine-grained classification benchmarks. In the future, we plan to explore unsupervised categorical memory networks where the aggregation of prototypes may allow for the discovery of unsupervised sub-categories.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>, J. Marsh, S. Gould, and L. Zheng (corresponding author) are with Australian National University, CBR, Australia. Email: firstname.lastname@anu.edu.au</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3. Visualization of memory addressing. Our module is able to retrieve the relevant class prototypes for a given query image (the first column). In this figure, we show exemplar images from the three most relevant prototypes (the second to fourth columns) with respect to a given query image. We highlight red the prototype of the same class. In the last column, we show the attention scores between the input image and each retrieved prototype.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Cumulative sum of attention scores for top-k highest matching prototypes in memory averaged over all samples in the (a) Car, (b) CUB, (c) Aircraft, and (d) NABirds datasets. Notice that on the right end of each graph, the attention scores of all classes sum to one.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>CUB-200-2011 contains 200 birds categories with roughly 30 training images per category. It has 5994 training images and 5794 testing images. FGVC-Aircraft is comprised of 10000 images over 100 categories, with a ratio of the training set to testing set of roughly 2 : 1. Stanford Car is made up of 196 categories of cars, with 8144 examples in training set and 8041 examples in the testing set. NABirds is a large scale dataset, consisting of 555 classes. It is made up of 23, 929 training images and 24, 633 testing images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>Importance of prototype diversity. We show the accuracy (%) of using only the prototypes with top-K attention scores to form a memory response feature. When K=200, the response is given access to all prototypes. The results are on the CUB-200-2011 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9 .</head><label>9</label><figDesc>Comparison between ResNet-18 and CMN under different training sets. CIFAR-100 contains all 50,000 training images; CIFAR-100 (subset A) has 5,000 training images, CIFAR-100 (subset B) has 10,000 training images, CIFAR-100 (subset C) has 25,000 training images. We find that both methods are on par with each other under four training sets. This indicates CMN is not suitable for general datasets where the inter-class similarity is relatively small.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I STATISTICS</head><label>I</label><figDesc>OF THREE FINE-GRAINED CLASSIFICATION DATASETS.</figDesc><table><row><cell>Dataset</cell><cell cols="3">#Train Set #Test Set #Category</cell></row><row><cell>FGVC Aircraft [41]</cell><cell>6, 667</cell><cell>3, 333</cell><cell>100</cell></row><row><cell>CUB-200-2011 [42]</cell><cell>5, 994</cell><cell>5, 794</cell><cell>200</cell></row><row><cell>Stanford Cars [43]</cell><cell>8, 144</cell><cell>8, 041</cell><cell>196</cell></row><row><cell>NABirds [44]</cell><cell>23,929</cell><cell>24,633</cell><cell>555</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF VARIOUS METHODS IN TERMS OF TOP-1 ACCURACY (%) ON rigid FGVC-AIRCRAFT. * DENOTES OUR RE-IMPLEMENTATION.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>Accuracy (%)</cell></row><row><cell>BilinearCNN [47]</cell><cell>VGG-16</cell><cell>84.1</cell></row><row><cell>Low-rank B-CNN [10]</cell><cell>VGG-16</cell><cell>87.3</cell></row><row><cell>Kernel-Pooling</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III COMPARISON</head><label>III</label><figDesc>OF VARIOUS METHODS IN TERMS OF TOP-1 ACCURACY (%) ON rigid STANFORD CARS. * DENOTES OUR RE-IMPLEMENTATION.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>Accuracy (%)</cell></row><row><cell>BilinearCNN [47]</cell><cell>VGG-16</cell><cell>91.3</cell></row><row><cell>Low-rank B-CNN [10]</cell><cell>VGG-16</cell><cell>90.9</cell></row><row><cell>Kernel-Pooling [48]</cell><cell>VGG-16</cell><cell>92.4</cell></row><row><cell>DFL-CNN [24]</cell><cell>VGG-16</cell><cell>93.3</cell></row><row><cell>RA-CNN [3]</cell><cell>VGG-19</cell><cell>92.5</cell></row><row><cell>MA-CNN [2]</cell><cell>VGG-19</cell><cell>92.8</cell></row><row><cell>Kernel-Pooling [48]</cell><cell>ResNet-50</cell><cell>91.1</cell></row><row><cell>MAMC [23]</cell><cell>ResNet-50</cell><cell>92.8</cell></row><row><cell>FT-ResNet [6]*</cell><cell>ResNet-50</cell><cell>92.5</cell></row><row><cell>DT-RAM [52]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE IV COMPARISON</head><label>IV</label><figDesc>OF VARIOUS METHODS IN TERMS OF TOP-1 ACCURACY (%) ON non-rigid CUB-200-2011. * DENOTES OUR RE-IMPLEMENTATION.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell>Accuracy (%)</cell></row><row><cell>BilinearCNN [47]</cell><cell>VGG-16</cell><cell>84.1</cell></row><row><cell>Low-rank B-CNN [10]</cell><cell>VGG-16</cell><cell>84.2</cell></row><row><cell>Kernel-Pooling [48]</cell><cell>VGG-16</cell><cell>86.2</cell></row><row><cell>DFL-CNN [24]</cell><cell>VGG-16</cell><cell>85.8</cell></row><row><cell>RA-CNN [3]</cell><cell>VGG-19</cell><cell>85.3</cell></row><row><cell>MA-CNN [2]</cell><cell>VGG-19</cell><cell>86.5</cell></row><row><cell>FT-ResNet [6]*</cell><cell>ResNet-50</cell><cell>85.8</cell></row><row><cell>DFL-CNN [24]</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V COMPARISON</head><label>V</label><figDesc>OF VARIOUS METHODS ON non-rigid NABIRDS DATASET. HERE, * DENOTES OUR RE-IMPLEMENTATION. OUR METHOD ACHIEVES COMPETITIVE ACCURACY WITH THE-STATE-OF-THE-ART METHODS.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Accuracy (%)</cell></row><row><cell>BilinearCNN [47]</cell><cell>VGG-16</cell><cell>79.4</cell></row><row><cell>FT-ResNet [6]*</cell><cell>ResNet-50</cell><cell>84.0</cell></row><row><cell>MaxEnt [26]</cell><cell>ResNet-50</cell><cell>69.2</cell></row><row><cell>MaxEnt [26]</cell><cell>DenseNet-161</cell><cell>83.0</cell></row><row><cell>Cross-X [12]</cell><cell>ResNet-50</cell><cell>86.2</cell></row><row><cell>CMN (ours)</cell><cell>ResNet-50</cell><cell>87.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VI IMPORTANCE</head><label>VI</label><figDesc>OF LEARNING IN THE CLASS-SPECIFIC MODULE. THE CLASS-SPECIFIC MODULE DENOTES RECORDING THE MOVING AVERAGE OF EACH CLASS. THE RANDOM MODULE MEANS THAT MEMORY MATRIX IS RANDOMLY INITIALIZED AND FIXED DURING TRAINING.TABLE VII EFFECT OF MEMORY MODULE ON CIFAR-100. THE MEMORY MODULE DOES NOT BRING ANY IMPROVEMENT AND IN FACT, SEEMS TO HINDER PERFORMANCE. THIS SUGGESTS THAT OUR METHOD IS SPECIFICALLY TAILORED FOR FINE-GRAINED RECOGNITION, WHERE IMAGES ARE SEMANTICALLY AND VISUALLY SIMILAR.</figDesc><table><row><cell>Methods</cell><cell></cell><cell cols="2">FGVC Aircraft</cell><cell>CUB-200-2011</cell></row><row><cell>Baseline</cell><cell></cell><cell>90.0</cell><cell>85.8</cell></row><row><cell cols="2">Random module</cell><cell>90.0</cell><cell>85.8</cell></row><row><cell cols="2">Class-specific module</cell><cell>93.8</cell><cell>88.2</cell></row><row><cell>Networks</cell><cell cols="2">Memory module</cell><cell>Top-1 Accuracy (%)</cell></row><row><cell>ResNet-18</cell><cell></cell><cell></cell><cell>75.42</cell></row><row><cell>ResNet-18</cell><cell></cell><cell></cell><cell>75.12</cell></row><row><cell>ResNet-34</cell><cell></cell><cell></cell><cell>75.54</cell></row><row><cell>ResNet-34</cell><cell></cell><cell></cell><cell>75.51</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell>77.10</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell>76.70</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to navigate for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="420" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning multi-attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5209" to="5217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4438" to="4446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to zoom: a saliency-based sampling layer for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="51" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<editor>ICLR</editor>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="317" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Low-rank bilinear pooling for fine-grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="365" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hierarchical bilinear pooling for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="574" to="589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-x learning for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8242" to="8251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The devil is in the channels: Mutual-channel loss for finegrained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4683" to="4695" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Part-based r-cnns for fine-grained category detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="834" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep lac: Deep localization, alignment and classification for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1666" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Part-stacked cnn for finegrained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Spda-cnn: Unifying semantic part detection and abstraction for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1143" to="1152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonparametric part transfer for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Goring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Freytag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Denzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2489" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Which and how many regions to gaze: Focus discriminative regions for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Object-part attention model for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1487" to="1500" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Weakly supervised fine-grained categorization with part-based image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V.-A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1713" to="1725" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Attention for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7054</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-attention multi-class constraint for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="805" to="821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning a discriminative filter bank within a cnn for fine-grained recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4148" to="4157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pairwise confusion for fine-grained visual classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="70" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Maximum-entropy fine grained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="637" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fine-grained image classification by exploring bipartite-graph labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1124" to="1133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hybrid computing using a neural network with dynamic external memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwińska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Agapiou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">538</biblScope>
			<biblScope unit="issue">7626</biblScope>
			<biblScope unit="page">471</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Meta-learning with memory-augmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning to generate with memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1177" to="1186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Memorization precedes generation: Learning unsupervised gans with memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01500</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Neural turing machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised learning via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02334</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Few-shot learning with localization in realistic settings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wertheimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6558" to="6567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Hyper-class augmented and regularized deep learning for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2645" to="2654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiple granularity descriptors for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2399" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5012" to="5021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Finegrained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The caltech-ucsd birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ipeirotis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="595" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bilinear convolutional neural networks for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Roy</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1309" to="1322" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2921" to="2930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Destruction and construction learning for fine-grained image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Look-into-object: Self-supervised structure modeling for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention convolutional binary neural tree for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="468" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dynamic computational time for visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1199" to="1209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Citeseer, Tech. Rep</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
