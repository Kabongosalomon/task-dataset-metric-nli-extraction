<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient Piecewise Training of Deep Structured Models for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T05:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent advances in semantic image segmentation have mostly been achieved by training deep convolutional neural networks (CNNs). We show how to improve semantic segmentation through the use of contextual information; specifically, we explore 'patch-patch' context between image regions, and 'patch-background' context. For learning from the patch-patch context, we formulate Conditional Random Fields (CRFs) with CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Efficient piecewise training of the proposed deep structured model is then applied to avoid repeated expensive CRF inference for back propagation. For capturing the patch-background context, we show that a network design with traditional multi-scale image input and sliding pyramid pooling is effective for improving performance. Our experimental results set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-overunion score of 78.0 on the challenging PASCAL VOC 2012 dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic image segmentation aims to predict a category label for every image pixel, which is an important yet challenging task for image understanding. Recent approaches have applied convolutional neural network (CNNs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b2">3]</ref> to this pixel-level labeling task and achieved remarkable success. Among these CNN-based methods, fully convolutional neural networks (FCNNs) <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b2">3]</ref> have become a popular choice, because of their computational efficiency for dense prediction and end-to-end style learning.</p><p>Contextual relationships are ubiquitous and provide important cues for scene understanding tasks. Spatial context can be formulated in terms of semantic compatibility relations between one object and its neighboring objects or image patches (stuff), in which a compatibility relation is an indication of the co-occurrence of visual patterns. For  <ref type="figure">Figure 1</ref>. An illustration of the prediction process of our method. Both our unary and pairwise potentials are formulated as multiscale CNNs for capturing semantic relations between image regions. Our method outputs low-resolution prediction after CRF inference, then the prediction is up-sampled and refined in a standard post-processing stage to output the final prediction.</p><p>example, a car is likely to appear over a road, and a glass is likely to appear over a table. Context can also encode incompatibility relations. For example, a car is not likely to be surrounded by sky. These relations also exist at finer scales, for example, in object part-to-part relations, and part-toobject relations. In some cases, contextual information is the most important cue, particularly when a single object shows significant visual ambiguities. A more detailed discussion of the value of spatial context can be found in <ref type="bibr" target="#b20">[21]</ref>. We explore two types of spatial context to improve the segmentation performance: patch-patch context and patchbackground context. The patch-patch context is the semantic relation between the visual patterns of two image patches. Likewise, patch-background context is the semantic relation between a patch and a large background region.</p><p>Explicitly modeling the patch-patch contextual relations has not been well studied in recent CNN-based segmentation methods. In this work, we propose to explicitly model the contextual relations using conditional random fields (CRFs). We formulate CNN-based pairwise potential functions to capture semantic correlations between neighboring patches. Some recent methods combine CNNs and CRFs for semantic segmentation, e.g., the dense CRFs applied in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b4">5]</ref>. The purpose of applying the dense CRFs in these methods is to refine the upsampled low-resolution prediction to sharpen object/region boundaries. These methods consider Potts-model-based pairwise potentials for enforcing local smoothness. There the pairwise potentials are conventional log-linear functions. In contrast, we learn more general pairwise potentials using CNNs to model the semantic compatibility between image regions. Our CNN pairwise potentials aim to improve the coarse-level prediction rather than doing local smoothness, and thus have a different purpose compared to Potts-model-based pairwise potentials. Since these two types of potentials have different effects, they can be combined to improve the segmentation system. <ref type="figure">Fig. 1</ref> illustrates our prediction process.</p><p>In contrast to patch-patch context, patch-background context is widely explored in the literature. For CNNbased methods, background information can be effectively captured by combining features from a multi-scale image network input, and has shown good performance in some recent segmentation methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref>. A special case of capturing patch-background context is considering the whole image as the background region and incorporating the image-level label information into learning. In our approach, to encode rich background information, we construct multi-scale networks and apply sliding pyramid pooling on feature maps. The traditional pyramid pooling (in a sliding manner) on the feature map is able to capture information from background regions of different sizes.</p><p>Incorporating general pairwise (or high-order) potentials usually involves expensive inference, which brings challenges for CRF learning. To facilitate efficient learning we apply piecewise training of the CRF <ref type="bibr" target="#b42">[43]</ref> to avoid repeated inference during back propagation training.</p><p>Thus our main contributions are as follows. 1. We formulate CNN-based general pairwise potential functions in CRFs to explicitly model patch-patch semantic relations.</p><p>2. Deep CNN-based general pairwise potentials are challenging for efficient CNN-CRF joint learning. We perform approximate training, using piecewise training of CRFs <ref type="bibr" target="#b42">[43]</ref>, to avoid the repeated inference at every stochastic gradient descent iteration and thus achieve efficient learning.</p><p>3. We explore background context by applying a network architecture with traditional multi-scale image input <ref type="bibr" target="#b12">[13]</ref> and sliding pyramid pooling <ref type="bibr" target="#b25">[26]</ref>. We empirically demonstrate the effectiveness of this network architecture for semantic segmentation.</p><p>4. We set new state-of-the-art performance on a number of popular semantic segmentation datasets, including NYUDv2, PASCAL VOC 2012, PASCAL-Context, and SIFT-flow. In particular, we achieve an intersection-overunion score of 78.0 on the PASCAL VOC 2012 dataset, which is the best reported result to date.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>Exploiting contextual information has been widely studied in the literature (e.g., <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7]</ref>). For example, the early work "TAS" <ref type="bibr" target="#b20">[21]</ref> models different types of spatial context between Things and Stuff using a generative probabilistic graphical model.</p><p>The most successful recent methods for semantic image segmentation are based on CNNs. A number of these CNNbased methods for segmentation are region-proposal-based methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref>, which first generate region proposals and then assign category labels to each. Very recently, FCNNs <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref> have become a popular choice for semantic segmentation, because of their effective feature generation and end-to-end training. FCNNs have also been applied to a range of other dense-prediction tasks recently, such as image restoration <ref type="bibr" target="#b9">[10]</ref>, image super-resolution <ref type="bibr" target="#b7">[8]</ref> and depth estimation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">29]</ref>. The method we propose here is similarly built upon fully convolution-style networks.</p><p>The direct prediction of FCNN based methods usually are in low-resolution. To obtain high-resolution predictions, a number of recent methods focus on refining the low-resolution prediction to obtain high resolution prediction. DeepLab-CRF <ref type="bibr" target="#b2">[3]</ref> performs bilinear upsampling of the prediction score map to the input image size and apply the dense CRF method <ref type="bibr" target="#b23">[24]</ref> to refine the object boundary by leveraging the color contrast information. CRF-RNN <ref type="bibr" target="#b47">[48]</ref> extends this approach by implementing recurrent layers for end-to-end learning of the dense CRF and the FCNN network. The work in <ref type="bibr" target="#b34">[35]</ref> learns deconvolution layers to upsample the low-resolution predictions. The depth estimation method <ref type="bibr" target="#b29">[30]</ref> explores super-pixel pooling for building the gap between low-resolution feature map and highresolution final prediction. Eigen et al. <ref type="bibr" target="#b8">[9]</ref> perform coarseto-fine learning of multiple networks with different resolution outputs for refining the coarse prediction. The methods in <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref> explore middle layer features (skip connections) for high-resolution prediction. Unlike these methods, our method focuses on improving the coarse (low-resolution) prediction by learning general CNN pairwise potentials to capture semantic relations between patches. These refinement methods are complementary to our method.</p><p>Combining the strengths of CNNs and CRFs for segmentation has been the focus of several recently developed approaches. DeepLab-CRF in <ref type="bibr" target="#b2">[3]</ref> trains FCNNs and applies a dense CRF <ref type="bibr" target="#b23">[24]</ref> method as a post-processing step. CRF-RNN <ref type="bibr" target="#b47">[48]</ref> and the method in <ref type="bibr" target="#b39">[40]</ref> extend DeepLab and <ref type="bibr" target="#b24">[25]</ref> by jointly learning the dense CRFs and CNNs. They consider Potts-model based pairwise potential functions which enforce smoothness only. The CRF model in these methods is for refining the up-sampled prediction. Unlike these methods, our approach learns CNNbased pairwise potential functions for modeling semantic  relations between patches.</p><p>Jointly learning CNNs and CRFs has also been explored in other applications apart from segmentation. The recent work in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> proposes to jointly learn continuous CRFs and CNNs for depth estimation from single monocular images. The work in <ref type="bibr" target="#b44">[45]</ref> combines CRFs and CNNs for human pose estimation. The authors of <ref type="bibr" target="#b3">[4]</ref> explore joint training of Markov random fields and deep neural networks for predicting words from noisy images and image s classification. Different from these methods, we explore efficient piecewise training of CRFs with CNN pairwise potentials. Given an image, we first apply a convolutional network to generate a feature map. We refer to this network as 'FeatMap-Net'. The resulting feature map is at a lower resolution than the original image because of the downsampling operations in the pooling layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Modeling semantic pairwise relations</head><p>We then create the CRF graph as follows: for each location in the feature map (which corresponds to a rectangular region in the input image) we create one node in the CRF graph. Pairwise connections in the CRF graph are constructed by connecting one node to all other nodes which lie within a spatial range box (the dashed box in <ref type="figure" target="#fig_2">Fig. 2</ref>). We consider different spatial relations by defining different types of range box, and each type of spatial relation is modeled by a specific pairwise potential function. As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, our method models the "surrounding" and "above/below" spatial relations. In our experiments, the size of the range box (dash box in the figure) size is 0.4a × 0.4a. Here we denote by a the length of the short edge of the feature map.</p><p>Note that although 'FeatMap-Net' defines a common architecture, in fact we train three such networks: one for the unary potential and one each for the two types of pairwise potential.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Contextual Deep CRFs</head><p>Here we describe the details of our deep CRF model. We denote by x ∈ X one input image and y ∈ Y the labeling mask which describes the label configuration of Edge feature vector Feature map</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>One connection</head><p>In CRF graph</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FeatMap-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unary-Net</head><p>Node feature vector Unary potential output One node in CRF graph</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRF graph</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pairwise-Net</head><p>Pairiwise potential output</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generate features (low resolution)</head><p>Construct CRF graph <ref type="figure" target="#fig_3">Figure 3</ref>. An illustration of generating unary or pairwise potential function outputs. First a feature map is generated by a FeatMap-Net, and a CRF graph is constructed based on the spatial resolution of the feature map. Finally the Unary-Net (or Pairwise-Net) produces potential function outputs. each node in the CRF graph. The energy function is denoted by E(y, x; θ) which models the compatibility of the input-output pair, with a small output value indicating high confidence in the prediction y. All network parameters are denoted by θ which we need to learn. The conditional likelihood for one image is formulated as follows:</p><formula xml:id="formula_0">P (y|x) = 1 Z(x) exp[−E(y, x)].<label>(1)</label></formula><p>Here</p><formula xml:id="formula_1">Z(x) = y exp[−E(y, x)] is the partition function.</formula><p>The energy function is typically formulated by a set of unary and pairwise potentials:</p><formula xml:id="formula_2">E(y, x) = U ∈U p∈N U U (y p , x p ) + V ∈V (p,q)∈S V V (y p , y q , x pq ).</formula><p>Here U is a unary potential function, and to make the exposition more general, we consider multiple types of unary potentials with U the set of all such unary potentials. N U is a set of nodes for the potential U . Likewise, V is a pairwise potential function with V the set of all types of pairwise potential. S V is the set of edges for the potential V . x p and x pq indicates the corresponding image regions which associate to the specified node and edge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Unary potential functions</head><p>We formulate the unary potential function by stacking the FeatMap-Net for generating feature maps and a shallow fully connected network (referred to as Unary-Net) to generate the final output of the unary potential function. The unary potential function is written as follows:</p><formula xml:id="formula_3">U (y p , x p ; θ U ) = −z p,yp (x; θ U ).<label>(2)</label></formula><p>Here z p,yp is the output value of Unary-Net, which corresponds to the p-th node and the y p -th class. <ref type="figure" target="#fig_3">Fig. 3</ref>   it integrates with FeatMap-Net. The unary potential at each CRF node is simply the K-dimensional output (where K is the number of classes) of Unary-Net applied to the node feature vector from the correpsonding location in the feature map (i.e. the output of FeatMap-Net).  <ref type="bibr" target="#b22">[23]</ref>). The feature vector for each node in the pair is from the feature map output by FeatMap-Net. The edge features of one pair are then fed to a shallow fully connected network (referred to as Pairwise-Net) to generate the final output that is the pairwise potential. The size of this is K × K to match the number of possible label combinations for a pair of nodes. The pairwise potential function is written as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Pairwise potential functions</head><formula xml:id="formula_4">V (y p , y q , x pq ; θ V ) = −z p,q,yp,yq (x; θ V ).<label>(3)</label></formula><p>Here z p,q,yp,yq is the output value of Pairwise-Net. It is the confidence value for the node pair (p, q) when they are labeled with the class value (y p , y q ), which measures the compatibility of the label pair (y p , y q ) given the input image x. θ V is the corresponding set of CNN parameters for the potential V , which we need to learn. Our formulation of pairwise potentials is different from the Potts-model-based formulation in the existing methods of <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b47">48]</ref>. The Potts-model-based pairwise potentials are a log-linear functions and employ a special formulation for enforcing neighborhood smoothness. In contrast, our pairwise potentials model the semantic compatibility between two nodes with the output for every possible value of the label pair (y p , y q ) individually parameterized by CNNs.</p><p>In our system, after obtaining the coarse level prediction, we still need to perform a refinement step to obtain the final high-resolution prediction (as shown in <ref type="figure">Fig. 1</ref>). Hence we also apply the dense CRF method <ref type="bibr" target="#b23">[24]</ref>, as in many other re-cent methods, in the prediction refinement step. Therefore, our system takes advantage of both contextual CNN potentials and the traditional smoothness potentials to improve the final system. More details are described in Sec. 5.</p><p>As in <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b19">20]</ref>, modeling asymmetric relations requires the potential function is capable of modeling input orders, since we have: V (y p , y q , x pq ) = V (y q , y p , x qp ). Take the asymmetric relation "above/below" as an example; we take advantage of the input pair order to indicate the spatial configuration of two nodes, thus the input (y p , y q , x pq ) indicates the configuration that the node p is spatially lies above the node q.</p><p>The asymmetric property is readily achieved with our general formulation of pairwise potentials. The potential output for every possible pairwise label combination for (p, q) is individually parameterized by the pairwise CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Exploiting background context</head><p>To encode rich background information, we use multiscale CNNs and sliding pyramid pooling <ref type="bibr" target="#b25">[26]</ref> for our FeatMap-Net. <ref type="figure" target="#fig_4">Fig. 4</ref> shows the details of the FeatMap-Net.</p><p>CNNs with multi-scale image network inputs have shown good performance in some recent segmentation methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref>. The traditional pyramid pooling (in a sliding manner) on the feature map is able to capture information from background regions of different sizes. We observe that these two techniques (multi-scale network design and pyramid pooling) for encoding background information are very effective for improving performance.</p><p>Applying CNNs on multi-scale images has shown good performance in some recent segmentation methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">33]</ref>. In our multi-scale network, an input image is first resized into 3 scales, then each resized image goes through 6 convolution blocks to output one feature map. In our experiment, the 3 scales for the input image are set to 1.2, 0.8 and 0.4. All scales share the same top 5 convolution blocks. In addition, each scale has an exclusive convolution block ("Conv  <ref type="figure">Figure 5</ref>. Details for sliding pyramid pooling. We perform 2-level sliding pyramid pooling on the feature map for capturing patchbackground context, which encode rich background information and increase the field-of-view for the feature map.</p><p>Block 6" in the figure) which captures scale-dependent information. The resulting 3 feature maps (corresponding to 3 scales) are of different resolutions, therefore we upscale the two smaller ones to the size of the largest feature map using bilinear interpolation. These feature maps are then concatenated to form one feature map. We perform spatial pyramid pooling <ref type="bibr" target="#b25">[26]</ref> (a modified version using sliding windows) on the feature map to capture information from background regions in multiple sizes. This increases the field-of-view for the feature map and thus it is able to capture the information from a large image region. Increasing the field-of-view generally helps to improve performance <ref type="bibr" target="#b2">[3]</ref>.</p><p>The details of spatial pyramid pooling are illustrated in <ref type="figure">Fig. 5</ref>. In our experiment, we perform 2-level pooling for each image scale. We define 5 × 5 and 9 × 9 sliding pooling windows (max-pooling) to generate 2 sets of pooled feature maps, which are then concatenated to the original feature map to construct the final feature map.</p><p>The detailed network layer configuration for all networks are described in <ref type="figure">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Prediction</head><p>In the prediction stage, our deep structured model will generate low-resolution prediction (as shown in <ref type="figure">Fig. 1)</ref>, which is 1/16 of the input image size. This is due to the stride setting of pooling or convolution layers for subsampling. Therefore, we apply two prediction stages for obtaining the final high-resolution prediction: the coarse-level prediction stage and the prediction refinement stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Coarse-level prediction stage</head><p>We perform CRF inference on our contextual structured model to obtain the coarse prediction of a test image. We consider the marginal inference over nodes for prediction: ∀p ∈ N : P (y p |x) = y\yp P (y|x).</p><p>The obtained marginal distribution can be further applied in the next prediction stage for boundary refinement. Fully-con 512 Fully-con K 2 <ref type="figure">Figure 6</ref>. The detailed configuration of the networks: FeatMap-Net, Unary-Net and Pairwise-Net. K is the number of classes. For FeatMap-Net, the top 5 convolution blocks share the same configuration as the convolution blocks in the VGG-16 network. The stride of the last max pooling layer is 1, and for the other max pooling layers we use the same stride setting as VGG-16.</p><p>Our CRF graph does not form a tree structure, nor are the potentials submodular, hence we need to an apply approximate inference. To address this we apply an efficient message passing algorithm which is based on the mean field approximation <ref type="bibr" target="#b35">[36]</ref>. The mean field algorithm constructs a simpler distribution Q(y), e.g., a product of independent marginals: Q(y) = p∈N Q p (y p ), which minimizes the KL-divergence between the distribution Q(y) and P (y). In our experiments, we perform 3 mean field iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Prediction refinement stage</head><p>We generate the score map for the coarse prediction from the marginal distribution which we obtain from the mean-field inference. We first bilinearly up-sample the score map of the coarse prediction to the size of the input image. Then we apply a common post-processing method <ref type="bibr" target="#b23">[24]</ref> (dense CRF) to sharpen the object boundary for generating the final high-resolution prediction. This postprocessing method leverages low-level pixel intensity information (color contrast) for boundary refinement. Note that most recent work on image segmentation similarly produces low-resolution prediction and have a upsampling and refinement process/model for the final prediction, e.g., <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>In summary, we simply perform bilinear upsampling of the coarse score map and apply the boundary refinement post-processing. We argue that this stage can be further improved by applying more sophisticated refinement methods, e.g., training deconvolution networks <ref type="bibr" target="#b34">[35]</ref>, training multiple coarse to fine learning networks <ref type="bibr" target="#b8">[9]</ref>, and exploring middle layer features for high-resolution prediction <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b31">32]</ref>. It is expected that applying better refinement approaches will gain further performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CRF training</head><p>A common approach for CRF learning is to maximize the likelihood, or equivalently minimize the negative loglikelihood, which can be written for one image as:</p><p>− log P (y|x; θ) = E(y, x; θ) + log Z(x; θ). (5) Adding regularization to the CNN parameter θ, the optimization problem for CRF learning is:</p><formula xml:id="formula_6">min θ λ 2 θ 2 2 + N i=1 E(y (i) , x (i) ; θ) + log Z(x (i) ; θ) . (6)</formula><p>Here x (i) , y (i) denote the i-th training image and its segmentation mask; N is the number of training images; λ is the weight decay parameter. We can apply stochastic gradient (SGD) based methods to optimize the above problem for learning θ. The energy function E(y, x; θ) is constructed from CNNs, and its gradient ∇ θ E(y, x; θ) easily computed by applying the chain rule as in conventional CNNs. However, the partition function Z brings difficulties for optimization. Its gradient is:</p><formula xml:id="formula_7">∇ θ logZ(x; θ) = y exp[−E(y, x; θ)] y exp[−E(y , x; θ)] ∇ θ [−E(y, x; θ)] = − E y∼P (y|x;θ) ∇ θ E(y, x; θ)<label>(7)</label></formula><p>Generally the size of the output space Y is exponential in the number of nodes, which prohibits the direct calculation of Z and its gradient. The CRF graph we considered for segmentation here is a loopy graph (not tree-structured), for which the inference is generally computationally expensive. More importantly, usually a large number of SGD iterations (tens or hundreds of thousands) are required for training CNNs. Thus performing inference at each SGD iteration is very computationally expensive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Piecewise training of CRFs</head><p>Instead of directly solving the optimization in (6), we propose to apply an approximate CRF learning method. In the literature, there are two popular types of learning methods which approximate the CRF objective : pseudolikelihood learning <ref type="bibr" target="#b0">[1]</ref> and piecewise learning <ref type="bibr" target="#b42">[43]</ref>. The main advantage of these methods in term of training deep CRF is that they do not involve marginal inference for gradient calculation, which significantly improves the efficiency of training. Decision tree fields <ref type="bibr" target="#b36">[37]</ref> and regression tree fields <ref type="bibr" target="#b21">[22]</ref> are based on pseudo-likelihood learning, while piecewise learning has been applied in the work <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Here we develop this idea for the case of training the CRF with the CNN potentials. In piecewise training, the conditional likelihood is formulated as a number of inde-pendent likelihoods defined on potentials, written as:</p><formula xml:id="formula_8">P (y|x) = U ∈U p∈N U P U (y p |x) V ∈V (p,q)∈S V P V (y p , y q |x).</formula><p>The likelihood P U (y p |x) is constructed from the unary potential U . Likewise, P V (y p , y q |x) is constructed from the pairwise potential V . P U and P V are written as:</p><formula xml:id="formula_9">P U (y p |x) = exp[−U (y p , x p )] y p exp[−U (y p , x p )] ,<label>(8)</label></formula><formula xml:id="formula_10">P V (y p , y q |x) = exp[−V (y p , y q , x pq )] y p ,y q exp[−V (y p , y q , x pq )] .<label>(9)</label></formula><p>Thus the optimization for piecewise training is to minimize the negative log likelihood with regularization:</p><formula xml:id="formula_11">min θ λ 2 θ 2 2 − N i=1 U ∈U p∈N (i) U log P U (y p |x (i) ; θ U ) + V ∈V (p,q)∈S (i) V log P V (y p , y q |x (i) ; θ V ) . (10)</formula><p>Compared to the objective in (6) for direct maximum likelihood learning, the above objective does not involve the global partition function Z(x; θ). To calculate the gradient of the above objective, we only need to calculate the gradient ∇ θ U log P U and ∇ θ V log P V . With the definition in <ref type="bibr" target="#b7">(8)</ref>, P U is a conventional Softmax normalization function over only K (the number of classes) elements. Similar analysis can also be applied to P V . Hence, we can easily calculate the gradient without involving expensive inference. Moreover, we are able to perform parallel training of potential functions, since the above objective is formulated as a summation of independent log-likelihoods. As previously discussed, CNN training usually involves a large number of gradient update iterations. However this means that expensive inference during every gradient iteration becomes impractical. Our piecewise approach here provides a practical solution for learning CRFs with CNN potentials on large-scale data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>We evaluate our method on 4 popular semantic segmentation datasets: PASCAL VOC 2012, NYUDv2, PASCAL-Context and SIFT-flow. The segmentation performance is measured by the intersection-over-union (IoU) score <ref type="bibr" target="#b11">[12]</ref>, the pixel accuracy and the mean accuracy <ref type="bibr" target="#b31">[32]</ref>.</p><p>The first 5 convolution blocks and the first convolution layer in the 6th convolution block are initialized from the VGG-16 network <ref type="bibr" target="#b41">[42]</ref>. All remaining layers are randomly initialized. All layers are trained using backpropagation/SGD. As illustrated in <ref type="figure" target="#fig_2">Fig. 2</ref>, we use 2 types of pairwise potential functions. In total, we have 1 type of unary potential function and 2 types of pairwise potential functions. We formulate one specific FeatMap-Net and po- tential network (Unary-Net or Pairwise-Net) for one type of potential function. We apply simple data augmentation in the training stage; specifically, we perform random scaling (from 0.7 to 1.2) and flipping of the images for training.</p><p>Our system is built on MatConvNet <ref type="bibr" target="#b45">[46]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Results on NYUDv2</head><p>We first evaluate our method on the dataset NYUDv2 <ref type="bibr" target="#b40">[41]</ref>. NYUDv2 dataset has 1449 RGB-D images. We use the segmentation labels provided in <ref type="bibr" target="#b14">[15]</ref> in which labels are processed into 40 classes. We use the standard training set which contains 795 images and the test set which contains 654 images. We train our models only on RGB images without using the depth information.</p><p>Results are shown in <ref type="table" target="#tab_1">Table 1</ref>. Unless otherwise specified, our models are initialized using the VGG-16 network. VGG-16 is also used in the competing method FCN <ref type="bibr" target="#b31">[32]</ref>. Our contextual model with CNN pairwise potentials achieves the best performance, which sets a new state-ofthe-art result on the NYUDv2 dataset. Note that we do not use any depth information in our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Component Evaluation</head><p>We evaluate the performance contribution of different components of the FeatMap-Net for capturing patch-background context on the NYUDv2 dataset. We present the results of adding different components of FeatMap-Net in <ref type="table" target="#tab_2">Table 2</ref>. We start from a baseline setting of our FeatMap-Net ("FullyConvNet Baseline" in the result table), for which multi-scale and sliding pooling is removed. This baseline setting is the conventional fully convolution network for segmentation, which can be considered as our implementation of the FCN method in <ref type="bibr" target="#b31">[32]</ref>. The result shows that our CNN baseline implementation ("FullyConvNet") achieves very similar performance (slightly better) than the FCN method. Applying multiscale network design and sliding pyramid pooling significantly improve the performance, which clearly shows the benefits of encoding rich background context in our approach. Applying the dense CRF method <ref type="bibr" target="#b23">[24]</ref> for boundary refinement gains further improvement. Finally, adding our contextual CNN pairwise potentials brings significant further improvement, for which we achieve the best performance in this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Results on PASCAL VOC 2012</head><p>PASCAL VOC 2012 <ref type="bibr" target="#b11">[12]</ref> is a well-known segmentation evaluation dataset which consists of 20 object categories  and one background category. This dataset is split into a training set, a validation set and a test set, which respectively contain 1464, 1449 and 1456 images. Following a conventional setting in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3]</ref>, the training set is augmented by extra annotated VOC images provided in <ref type="bibr" target="#b16">[17]</ref>, which results in 10582 training images. We verify our performance on the PASCAL VOC 2012 test set. We compare with a number of recent methods with competitive performance. Since the ground truth labels are not available for the test set, we report the result through the VOC evaluation server. The results of IoU scores are shown in the last column of <ref type="table" target="#tab_3">Table 3</ref>. We first train our model only using the VOC images. We achieve 75.3 IoU score which is the best result amongst methods that only use the VOC training data.</p><p>To improve the performance, following the setting in recent work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5]</ref>, we train our model with the extra images from the COCO dataset <ref type="bibr" target="#b26">[27]</ref>. With these extra training images, we achieve an IoU score of 77.2.</p><p>For further improvement, we also exploit the the middlelayer features as in the recent methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b17">18]</ref>. We learn extra refinement layers on the feature maps from middle layers to refine the coarse prediction. The feature maps from the middle layers encode lower level visual information which helps to predict details in the object boundaries. Specifically, we add 3 refinement convolution layers on top   of the feature maps from the first 5 max-pooling layers and the input image. The resulting feature maps and the coarse prediction score map are then concatenated and go through another 3 refinement convolution layers to output the refined prediction. The resolution of the prediction is increased from 1/16 (coarse prediction) to 1/4 of the input image. With this refined prediction, we further perform boundary refinement <ref type="bibr" target="#b23">[24]</ref> to generate the final prediction. Finally, we achieve an IoU score of 78.0, which is best reported result on this challenging dataset. <ref type="bibr" target="#b0">1</ref> The results for each category are shown in <ref type="table" target="#tab_3">Table 3</ref>. We outperform competing methods in most categories. For only using the VOC training set, our method outperforms the second best method, DPN <ref type="bibr" target="#b30">[31]</ref>, on 18 categories out of 20. Using VOC+COCO training set, our method outperforms DPN <ref type="bibr" target="#b30">[31]</ref> on 15 categories out of 20. Some prediction examples of our method are shown in <ref type="figure" target="#fig_8">Fig. 7.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Results on PASCAL-Context</head><p>The PASCAL-Context <ref type="bibr" target="#b33">[34]</ref> dataset provides the segmentation labels of the whole scene (including the "stuff" la- <ref type="bibr" target="#b0">1</ref> The result link at the VOC evaluation server: http://host. robots.ox.ac.uk:8080/anonymous/XTTRFF.html bels) for the PASCAL VOC images. We use the segmentation labels which contain 60 classes (59 classes plus the " background" class ) for evaluation. We use the provided training/test splits. The training set contains 4998 images and the test set has 5105 images.</p><p>Results are shown in <ref type="table" target="#tab_4">Table 4</ref>. Our method significantly outperforms the competing methods. To our knowledge, ours is the best reported result on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Results on SIFT-flow</head><p>We further evaluate our method on the SIFT-flow dataset. This dataset contains 2688 images and provide the segmentation labels for 33 classes. We use the standard split for training and evaluation. The training set has 2488 images and the rest 200 images are for testing. Since images are in small sizes, we upscale the image by a factor of 2 for training. Results are shown in <ref type="table" target="#tab_5">Table 5</ref>. We achieve the best performance for this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>We have proposed a method which combines CNNs and CRFs to exploit complex contextual information for semantic image segmentation. We formulate CNN based pairwise potentials for modeling semantic relations between image regions. Our method shows best performance on several popular datasets including the PASCAL VOC 2012 dataset. The proposed method is potentially widely applicable to other vision tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Deep structured model: contextual deep CRF Prediction refinement stage: up-sample &amp; boundary refine Coarse-level prediction stage: inference on contextual CRF Low-resolution prediction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>An illustration of constructing pairwise connections in a CRF graph. A node is connected to all other nodes which lie within the range box (dashed box in the figure). Two types of spatial relations are described in the figure, which correspond to two types of pairwise potential functions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3</head><label>3</label><figDesc>conceptualizes our architecture at a high level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>The details of our FeatMap-Net. An input image is first resized into 3 scales, then each resized image goes through 6 convolution blocks to output one feature map. Top 5 convolution blocks are shared for all scales. Every scale has a specific convolution block (Conv Block 6). We perform 2-level sliding pyramid pooling (seeFig. 5for details). d indicates the feature dimension.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3</head><label>3</label><figDesc>likewise illustrates how the pairwise potentials are generated. The edge features are formed by concatenating the corresponding feature vectors of two connected nodes (similar to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Some prediction examples of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Segmentation results on NYUDv2 dataset (40 classes). We compare to a number of recent methods. Our method significantly outperforms the existing methods.</figDesc><table><row><cell cols="5">method training data pixel accuracy mean accuracy IoU</cell></row><row><cell>Gupta et al. [16]</cell><cell>RGB-D</cell><cell>60.3</cell><cell>-</cell><cell>28.6</cell></row><row><cell>FCN-32s [32]</cell><cell>RGB</cell><cell>60.0</cell><cell>42.2</cell><cell>29.2</cell></row><row><cell>FCN-HHA [32]</cell><cell>RGB-D</cell><cell>65.4</cell><cell>46.1</cell><cell>34.0</cell></row><row><cell>ours</cell><cell>RGB</cell><cell>70.0</cell><cell>53.6</cell><cell>40.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation Experiments. The table shows the value added by the different system components of our method on the NYUDv2 dataset (40 classes).</figDesc><table><row><cell cols="4">method pixel accuracy mean accuracy IoU</cell></row><row><cell>FCN-32s [32]</cell><cell>60.0</cell><cell>42.2</cell><cell>29.2</cell></row><row><cell>FullyConvNet Baseline</cell><cell>61.5</cell><cell>43.2</cell><cell>30.5</cell></row><row><cell>+ sliding pyramid pooling</cell><cell>63.5</cell><cell>45.3</cell><cell>32.4</cell></row><row><cell>+ multi-scales</cell><cell>67.0</cell><cell>50.1</cell><cell>37.0</cell></row><row><cell>+ boundary refinement</cell><cell>68.5</cell><cell>50.9</cell><cell>38.3</cell></row><row><cell>+ CNN contextual pairwise</cell><cell>70.0</cell><cell>53.6</cell><cell>40.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Individual category results on the PASCAL VOC 2012 test set (IoU scores). Our method performs the best Only using VOC training data FCN-8s [32] 76.8 34.2 68.9 49.4 60.3 75.3 74.7 77.6 21.4 62.5 46.8 71.8 63.9 76.5 73.9 45.2 72.4 37.4 70.9 55.1 62.2 Zoom-out [33] 85.6 37.3 83.2 62.5 66.0 85.1 80.7 84.9 27.2 73.2 57.5 78.1 79.2 81.1 77.1 53.6 74.0 49.2 71.7 63.3 69.6 DeepLab [3] 84.4 54.5 81.5 63.6 65.9 85.1 79.1 83.4 30.7 74.1 59.8 79.0 76.1 83.2 80.8 59.7 82.2 50.4 73.1 63.7 71.6 CRF-RNN [48] 87.5 39.0 79.7 64.2 68.3 87.6 80.8 84.4 30.4 78.2 60.4 80.5 77.8 83.1 80.6 59.5 82.8 47.8 78.3 67.1 72.0 DeconvNet [35] 89.9 39.3 79.7 63.9 68.2 87.4 81.2 86.1 28.5 77.0 62.0 79.0 80.3 83.6 80.2 58.8 83.4 54.3 80.7 65.0 72.5 DPN [31] 87.7 59.4 78.4 64.9 70.3 89.3 83.5 86.1 31.7 79.9 62.6 81.9 80.0 83.5 82.3 60.5 83.2 53.4 77.9 65.0 74.1 ours 90.6 37.6 80.0 67.8 74.4 92.0 85.2 86.2 39.1 81.2 58.9 83.8 83.9 84.3 84.8 62.1 83.2 58.2 80.8 72.3 75.3 Using VOC+COCO training data DeepLab [3] 89.1 38.3 88.1 63.3 69.7 87.1 83.1 85.0 29.3 76.5 56.5 79.8 77.9 85.8 82.4 57.4 84.3 54.9 80.5 64.1 72.7 CRF-RNN [48] 90.4 55.3 88.7 68.4 69.8 88.3 82.4 85.1 32.6 78.5 64.4 79.6 81.9 86.4 81.8 58.6 82.4 53.5 77.4 70.1 74.7 BoxSup [5] 89.8 38.0 89.2 68.9 68.0 89.6 83.0 87.7 34.4 83.6 67.1 81.5 83.7 85.2 83.5 58.6 84.9 55.8 81.2 70.7 75.2 DPN [31] 89.0 61.6 87.7 66.8 74.7 91.2 84.3 87.6 36.5 86.3 66.1 84.4 87.8 85.6 85.4 63.6 87.3 61.3 79.4 66.4 77.5 ours+ 94.1 40.7 84.1 67.8 75.9 93.4 84.3 88.4 42.5 86.4 64.7 85.4 89.0 85.8 86.0 67.5 90.2 63.8 80.9 73.0 78.0</figDesc><table><row><cell>method</cell><cell>aero</cell><cell>bike</cell><cell>bird</cell><cell>boat</cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat</cell><cell>chair</cell><cell>cow</cell><cell>table</cell><cell>dog</cell><cell>horse</cell><cell>mbike</cell><cell>person</cell><cell>potted</cell><cell>sheep</cell><cell>sofa</cell><cell>train</cell><cell>tv</cell><cell>mean</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Segmentation results on PASCAL-Context dataset (60 classes). Our method performs the best.</figDesc><table><row><cell cols="4">method pixel accuracy mean accuracy IoU</cell></row><row><cell>O2P [2]</cell><cell>-</cell><cell>-</cell><cell>18.1</cell></row><row><cell>CFM [6]</cell><cell>-</cell><cell>-</cell><cell>34.4</cell></row><row><cell>FCN-8s [32]</cell><cell>65.9</cell><cell>46.5</cell><cell>35.1</cell></row><row><cell>BoxSup [5]</cell><cell>-</cell><cell>-</cell><cell>40.5</cell></row><row><cell>ours</cell><cell>71.5</cell><cell>53.9</cell><cell>43.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Segmentation results on SIFT-flow dataset (33 classes). Our method performs the best.</figDesc><table><row><cell cols="4">method pixel accuracy mean accuracy IoU</cell></row><row><cell>Liu et al. [28]</cell><cell>76.7</cell><cell>-</cell><cell>-</cell></row><row><cell>Tighe et al. [44]</cell><cell>75.6</cell><cell>41.1</cell><cell>-</cell></row><row><cell>Tighe et al. (MRF) [44]</cell><cell>78.6</cell><cell>39.2</cell><cell>-</cell></row><row><cell>Farabet et al. (balance) [13]</cell><cell>72.3</cell><cell>50.8</cell><cell>-</cell></row><row><cell>Farabet et al. [13]</cell><cell>78.5</cell><cell>29.6</cell><cell>-</cell></row><row><cell>Pinheiro et al. [38]</cell><cell>77.7</cell><cell>29.8</cell><cell>-</cell></row><row><cell>FCN-16s [32]</cell><cell>85.2</cell><cell>51.7</cell><cell>39.5</cell></row><row><cell>ours</cell><cell>88.1</cell><cell>53.4</cell><cell>44.9</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Published in Proc. IEEE Conf. Computer Vision and Pattern Recognition (CVPR) 2016.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments This research was supported by the Data to Decisions Cooperative Research Centre and by the Australian Research Council through the Australian Centre for Robotic Vision (CE140100016). C. Shen's participation was supported by an ARC Future Fellowship (FT120100969). I. Reid's participation was supported by an ARC Laureate Fellowship (FL130100102).</p><p>C. Shen is the corresponding author (e-mail: chunhua.shen@adelaide.edu.au).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficiency of pseudolikelihood estimation for simple Gaussian fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Besag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations</title>
		<meeting>Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning deep structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learn</title>
		<meeting>Int. Conf. Machine Learn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BoxSup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vis</title>
		<meeting>Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Context as supervisory signal: Discovering objects with predictable context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Restoring an image taken through a window covered with dirt or rain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vis</title>
		<meeting>Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Info. Process. Syst</title>
		<meeting>Adv. Neural Info. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. J. Comp. Vis</title>
		<meeting>Int. J. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning hierarchical features for scene labeling. IEEE T. Pattern Analysis &amp; Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual organization and recognition of indoor scenes from rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning rich features from rgb-d images for object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vis</title>
		<meeting>Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Markov random fields with asymmetric interactions for modelling spatial context in structured scene labelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Heesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Petrou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Signal Processing Systems</title>
		<imprint>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning spatial context: Using stuff to find things</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Regression tree fieldsan efficient, non-parametric approach to image labeling problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jancsary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Lampert. Closed-form training of conditional random fields for large scale image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected CRFs with Gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Info. Process. Syst</title>
		<meeting>Adv. Neural Info. ess. Syst</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Parameter learning and convergent inference for dense random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Sift flow: Dense correspondence across scenes and its applications. IEEE T. Pattern Analysis &amp; Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep convolutional neural fields for depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1502.07411" />
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vis</title>
		<meeting>Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Feedforward semantic segmentation with zoom-out features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mostajabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vis</title>
		<meeting>Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structured learning and prediction in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Trends. Comput. Graph. Vis</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Decision tree fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vis</title>
		<meeting>Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural networks for scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Machine Learn</title>
		<meeting>Int. Conf. Machine Learn</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vis</title>
		<meeting>Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fully connected deep structured networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1503.02351.2" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Learning Representations</title>
		<meeting>Int. Conf. Learning Representations</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Piecewise training for undirected models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Uncertainty Artificial Intelli</title>
		<meeting>Conf. Uncertainty Artificial Intelli</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Finding things: Image parsing with regions and per-exemplar detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tighe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Adv. Neural Info</title>
		<meeting>Adv. Neural Info</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">MatConvNet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The layout consistent random field for recognizing and segmenting partially occluded objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Pattern Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Pattern Recogn</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Conditional random fields as recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Comp. Vis</title>
		<meeting>Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
