<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /Users/yhou/git/grobid-0.5.3/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.3" ident="GROBID" when="2019-02-06T23:06+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Story Comprehension for Predicting What Happens Next</title>
			</titleStmt>
			<publicationStmt>
				<publisher>Association for Computational Linguistics</publisher>
				<availability status="unknown"><p>Copyright Association for Computational Linguistics</p>
				</availability>
				<date>September 7-11, 2017. 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
							<email>snigdha@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
							<email>hpeng7@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
							<email>danr@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois</orgName>
								<address>
									<settlement>Urbana-Champaign</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Story Comprehension for Predicting What Happens Next</title>
					</analytic>
					<monogr>
						<title level="m">Natural Language Processing</title>
						<meeting> <address><addrLine>Copenhagen, Denmark</addrLine></address>
						</meeting>
						<imprint>
							<publisher>Association for Computational Linguistics</publisher>
							<biblScope unit="page" from="1603" to="1614"/>
							<date type="published">September 7-11, 2017. 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
				<p>Automatic story comprehension is a fundamental challenge in Natural Language Understanding, and can enable computers to learn about social norms, human behavior and commonsense. In this paper, we present a story comprehension model that explores three distinct semantic aspects: (i) the sequence of events described in the story, (ii) its emotional trajectory, and (iii) its plot consistency. We judge the model&apos;s understanding of real-world stories by inquiring if, like humans, it can develop an expectation of what will happen next in a given story. Specifically, we use it to predict the correct ending of a given short story from possible alternatives. The model uses a hidden variable to weigh the semantic aspects in the context of the story. Our experiments demonstrate the potential of our approach to characterize these semantic aspects, and the strength of the hidden variable based approach. The model outperforms the state-of-the-art approaches and achieves best results on a publicly available dataset.</p>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Narratives are a fundamental part of human language and culture. They serve as vehicles to share experiences, information and goals. For these reasons, automatically understanding stories is an interesting but challenging task for Computational Linguists <ref type="bibr" target="#b32">(Mani, 2012)</ref>. Story comprehension involves not only an array of NLP capabilities, but also some common sense knowledge and an understanding of normative social behavior <ref type="bibr">(Charniak, 1972)</ref>. Past research has focused on various aspects of story understandContext: One day Wesley's auntie came over to visit. He was happy to see her, because he liked to play with her. When she started to give his little sister attention, he got jealous. He got angry at his auntie and bit her hand when she wasn't looking.</p><p>Incorrect Ending: She gave him a cookie for being so nice. Correct Ending: He was scolded.</p><p>Figure 1: Example from the story-cloze task: predict the correct ending to a given short story out of provided options.</p><p>ing such as identifying character personas <ref type="bibr">(Bam- man et al., 2014;</ref><ref type="bibr" target="#b55">Valls-Vargas et al., 2015)</ref>, interpersonal relationships , plotpatterns <ref type="bibr" target="#b21">(Jockers, 2013)</ref>, narrative structures <ref type="bibr">(Fin- layson, 2012</ref>). There has also been an interest in predicting what is expected to happen next in a piece of text <ref type="bibr">(Chambers and Jurafsky, 2008)</ref>. Human readers are good at filling-in-the-gaps or inferring information that is not explicitly stated in the text. However, computers are not yet able to match their performance on predicting what could be the likely next step in a given sequence of events described in a story.</p><p>Recently, <ref type="bibr" target="#b37">Mostafazadeh et al. (2016)</ref> introduced the story-cloze task for testing this ability, albeit without the aspect of language generation. This task requires choosing the correct ending to a given four sentences long story (also referred to as context) from two provided alternatives. <ref type="figure">Fig. 1</ref> shows an example story consisting of a short context, and two ending options.</p><p>In this work we address this story-cloze task. While the short nature and third person narrative style of these stories help us circumvent the problem of speaker identification and processing long dialogues, the crowdsourced dataset ensures that they reflect real-world and commonsense stories. Our approach emphasizes the joint contribution of multiple aspects to story understanding, which future research can build upon.</p><p>In this paper we explore three semantic aspects of story understanding: (i) the sequence of events described in the story, (ii) the evolution of sentiment and emotional trajectories, and (iii) topical consistency. The first aspect is motivated from approaches in semantic script induction, and evaluates if events described in an ending-alternative are likely to occur within the sequence of events described in the preceding context. For example, in the story in <ref type="figure">Fig. 1</ref>, Wesley gets angry and bites his sister's hand. So, a next likely step might suggest that he would be scolded. However, there are multiple semantic aspects to story understanding beyond analyzing events and scripts. Stories often describe characters (e.g. Wesley) who need to be viewed as social and emotional agents. They not only describe events involving these characters, but also reflect their social lives and emotional states. Our model captures this by evaluating if the sentiment described in an ending option makes sense considering the context of the story. For example, in the story in <ref type="figure">Fig. 1</ref>, the general sentiment of being scolded is better aligned with the sentiment of Wesley being angry and jealous, compared to that of being nice. Also, stories generally revolve around coherent themes and topics. Our model accounts for that by analyzing if the topic of an ending option is consistent with the preceding context. We present a log-linear model that is used to weigh the various aspects of the story using a hidden variable. It then uses this hidden variable to predict the correct ending for the given story.</p><p>We demonstrate the strength of our approach by comparing it with the existing state-of-the-art methods for this task. We first validate the predictive potential of the features that correspond to the three semantic aspects through a simple classifier trained using these features. We then demonstrate the benefit of using our hidden variable approach by showing that it significantly outperforms the above mentioned classifier and other baselines, and achieves an accuracy of 77.60% on the task. Our key contributions are:</p><p>• We model story understanding as a joint model over multiple semantic aspects, and utilize the idea for predicting a story's end.</p><p>• We design linguistic features that incorporate world knowledge and narrative awareness.</p><p>• We present a hidden variable approach to weigh these aspects in a story's context.</p><p>• We empirically demonstrate that our approach significantly outperforms state-ofthe-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Predicting Story Ending</head><p>Given an L sentences long context, c = c 1 , c 2 , c 3 . . . c L , and two ending-options, o 1 and o 2 , we aim to predict which ending option forms an inconsistent story. This is a binary classification task. We assume that the inconsistency can arise from one (or more) of certain semantic aspects. In this section, we first describe the intuition behind using these aspects and the features that we designed to capture them (Sec. 2.1). We then describe our model which uses a latent variable to weigh these aspects in light of the story, and then predicts its ending (Sec. 2.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Measuring Consistency</head><p>Our approach analyzes the following aspects of story understanding: Event-sequence, Sentimenttrajectory, and Topical Consistency.</p><p>Event-sequence: For a story, or any piece of text, to be coherent, it needs to describe a meaningful or 'mutually entailing' sequence of events <ref type="bibr">(Chatman, 1980)</ref>. For instance, in <ref type="figure">Figure 1</ref> Wesley got angry → Wesley bit her hand → Wesley was scolded describes a more coherent sequence of events, as compared to Wesley got angry → Wesley bit her hand → Wesley got a cookie Prior work in script-learning attempts to model such prototypical sequence of events (usually captured through verbs). For this task, we wanted to model events at an abstraction level that would be generalizable and yet semantically meaningful. <ref type="bibr" target="#b41">Peng and Roth (2016)</ref> recently proposed a neural SemLM approach, to model such sequence of events using a language model of FrameNet ( <ref type="bibr" target="#b2">Baker et al., 1998</ref>) frames that are evoked in the given text.</p><p>It represents an event using the corresponding predicate frame and its sense, obtained using a Sematic Role Labeler ( <ref type="bibr" target="#b47">Punyakanok et al., 2004</ref>). It also extends the frame definition to include explicit discourse markers (such as but, and) since they model relationships between frames. For example, in <ref type="figure">Fig-ure 1</ref>, the SemLM representation for the last sentence of the context is 'Get.01-and-bit.01'. Here, '01' indicates specific predicate senses for verbs 'get' and 'bit' with 'and' being a discourse marker. Also, it produces 'scold.01' and 'give.01' for the correct and incorrect endings respectively. We train this language model using a log bilinear language model <ref type="bibr" target="#b34">(Mnih and Hinton, 2007</ref>) on a collection of unannotated short stories (see Sec. 3.1) and also 20 years of New York Times data <ref type="bibr">1</ref> .</p><p>Given a sequence of frames evoked in the context, such a trained language model can then be used to get the conditional probabilities of the frame(s) evoked in each of the two ending-options. The option with more probable frame(s) is likely to be the appropriate ending. With this intuition in mind, for each of the two ending-option, o i , we design features whose values are probabilities of frames evoked in that option (f o i ), given the sequence of frames,</p><formula xml:id="formula_0">f 1 , f 2 , . . . f D , evoked in the context, c = c 1 , c 2 , c 3 . . . c L .</formula><p>We consider increasingly longer frame-contexts for conditional probability computation, i.e. for each option, o i , we extract the following features:</p><formula xml:id="formula_1">P (f o i |f D ), P (f o i |f D f D−1 ), . . . P (f o i |f D f D−1 . . . f 1 ).</formula><p>For each of these features, we additionally also include a comparative binary feature whose value is 1 if the conditional probability of one of the options (o 2 ) is greater than the corresponding conditional probability of the other option (o 1 ) (E.g. P (o 2 |f D ) &gt; P (o 1 |f D )), and −1 otherwise. Our preliminary experiments indicated that these features were helpful for supervised classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment-trajectory:</head><p>As mentioned before, stories are different from objective texts such as news articles, as they additionally describe sentiments or emotions. Some stories can be categorized as happy stories while others as sad. However, most stories depict evolving sentiments in their plots as they progress <ref type="bibr" target="#b56">(Vonnegut, 1981)</ref>.</p><p>With the goal of modeling such sentiment trajectories, we assumed that a story can be divided into the following narrative-segments: a beginning, a body, a climax, and an ending. While this narrative-segmentation process warrants deeper research, in this paper we adopt a simple methodology. We treat the first sentence of the L sentences long context as the beginning, the next L − 2 sentences are treated as the body, the last sentence of the context forms the climax, and the two options form the (possible) ending 2 . We then assigned a positive, negative, or neutral sentiment to each segment, represented as S(segment) = sign(number of positive words -number of negative words) in the segment. The sentiment polarity of a word was determined by a look-up from pre-trained sentiment lexica ( <ref type="bibr" target="#b30">Liu et al., 2005;</ref><ref type="bibr" target="#b58">Wilson et al., 2005)</ref> 3 . Thus, the L length context can now be viewed as a sequence of its segment's sentiments. Lastly, we learn sentiment trajectories in form of N-gram language models from an unannotated corpus of short stories (Sec. 3.1) that learn: (i) P (S(ending)|S(climax), S(body), S(beginning)); (ii) P (S(ending)|S(climax), S(body)); and (iii) P (S(ending)|S(climax)).</p><p>The process described above learns typical sentiment trajectories over narrative-segments. However, it does not model a story's overall sentiment (i.e. whether it is a happy or a sad story, in general). To capture this notion, we train another language model to learn P (S(ending)|S(context)), where S(context) is the sentiment of the full context (without segmentation).</p><p>Finally, for each ending option, we extract features whose values are the four conditional probabilities described above. As before we also consider four comparative binary features.</p><p>Topical Consistency: This aspect is motivated by the idea that stories are topically cohesive <ref type="bibr">(Bam- berg, 2012)</ref>, and in a typical story, new topics (concepts, entities or ideas) are not introduced towards the end because it does not allow the story-writer enough narrative space and time to develop and describe them <ref type="bibr" target="#b22">(Jovchelovitch and Bauer, 2000</ref>). We capture the notion of topic of a sentence using topic-words (the nouns and verbs appearing in it (Lapata and <ref type="bibr" target="#b27">Barzilay, 2005)</ref>). For each option, we first align each of its topic-words with the most similar topic-word in one of the context-sentences, while defining the alignment score as this similarity value. We measure similarity between two words using the cosine similarity of their vector space representations (using pretrained <ref type="bibr">GloVe (Pennington et al., 2014</ref>) vectors). We then quantify the topical-closeness of an ending option with the context using averaged alignment score of its topic-words <ref type="bibr">4</ref> . For each ending option, we extract one feature whose value is this topical-closeness with the context. As before, we also include a binary comparative feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hidden Coherence Model</head><p>Sec. 2.1 described the three semantic consistency aspects and the corresponding features. We now describe our model which uses these features (represented as f co in the rest of this paper) to identify the (in)coherent ending-option. The model is also dependent on another feature set, φ co , which will be discussed later in this section.</p><p>Formally, our model addresses the following binary classification problem: given the multisentence context, c, and two ending-options, o 1 and o 2 , predict the answer, a ∈ {0, 1}. The correct ending for the story is o 1 when a = 0 and o 2 otherwise. Our training data consists of instances (context and ending options) labeled with corresponding answers a. It does not contain any other annotation (like semantic consistency aspects).</p><p>The model proceeds by assuming that there are K different semantic consistency aspects and that an ending-option can lead to an incoherent story by violating any of these aspects (our implementation uses K = 3 corresponding to the three aspects described in Sec. 2.1). The model achieves this by assuming that each instance belongs to a latent category, z ∈ {1, 2, 3 . . . K}, which advises the model on the importance of these aspects for the given instance. Using these definitions and assumptions, the probability of an answer given the context and the ending-options can be modeled as:</p><formula xml:id="formula_2">P (a|c, o 1 , o 2 ) = K z P (z|c, o 1 , o 2 )P (a|z, c, o 1 , o 2 )</formula><p>We parameterize P (z|c, o 1 , o 2 ) as:</p><formula xml:id="formula_3">P (z|c, o 1 , o 2 ) = e −λz φco k e −λ k φco 4</formula><p>An alternative would be to compute similarity between averaged vector representations of the topic-words of the context and the ending-option(s). However, that assumes that a story is strictly about a single topic. Instead they reflect interplay of multiple related and 'narrow topics. E.g. a story describing a teacher walking in rain is about topics like 'teacher', 'walk', 'rain', etc. The correct ending option describes a passer-by helping the teacher. 'passer-by' was far from an average of all topics but close to the 'walk' topic. where, φ co is the feature vector used for assigning a value to the hidden variable for an instance, and λ z is the weight vector of the log-linear model for the z th aspect. There are K weight vectors, one corresponding to each of the K aspects.</p><p>For predicting the answer, a, we assume that each aspect has a separate logistic-regression based prediction model parameterized as:</p><formula xml:id="formula_4">P (a|z, c, o 1 , o 2 ) = (e − wz f z co ) 1−a 1 + e − wz f z co</formula><p>where f z co is the feature vector constructed from the context and ending-options for the z th aspect, and w z are the corresponding weights. Training: The model parameters, w z and λ z , are learned during the training process by maximizing the log-likelihood of the data. We use Expectation-Maximization ( <ref type="bibr">Dempster et al., 1977)</ref> for training. During the E-step we compute the expectations for latent variable assignments using parameter values from the previous iteration as:</p><formula xml:id="formula_5">&lt; z k n &gt;∝ e − λ k φco K k e − λ k φco P (a n |z k n , c n , o 1n , o 2n )</formula><p>where, a subscript of n represents the n th training instance out of a total of N instances. z k n represents n th instance getting assigned to the k th aspect, and &lt;&gt; denotes expected values.</p><p>In the M-step, given the expected assignments, we maximize the following expected log complete likelihood with respect to the model parameters using gradient ascent:</p><formula xml:id="formula_6">&lt; L &gt; = N n K k &lt; z k n &gt; log e − λ k φco K k e − λ k φco + log (e − w k f k co ) 1−an 1 + e − w k f k co</formula><p>Features: Our model uses two types of features: (i) for aspect-specific prediction model, f k co , and (ii) for hidden aspect assignment, φ co . The features extracted for each of the K = 3 aspects, f k co , were described in Sec. 2.1. For the hidden aspect assignment, we needed features that could analyze the two options in light of the given context, and characterize the importance of various aspects for the given instance. One way to measure an aspect's importance is by quantifying how different the two options are with respect to that aspect. The underlying assumption is that the option that leads to an inconsistent story, by compromising on one of the aspects, would differ significantly from the other option in that aspect. We quantify an aspect's importance using the normalized L1 distance between the corresponding features, f k co , extracted for the two options in Sec.</p><note type="other">2.1 (ignoring the comparative binary features, and normalizing by the number of features). Specifically, for an aspect k, lets represent the feature extracted for the two options by f k 1 and f k 2 (each of length n) then the corresponding 'importance feature' for this aspect = | f k 1 − f k 2 |/n. For example, for topicalconsistency, for each option, we extracted 1 feature measuring its topical-closeness to the context. For φ co computation we consider the absolute difference between this value for the two options. To summarize, for each instance, we define φ co as a set of K + 1 features: K of these measure the importance of each of the aspects, while the last one is an additional always-one feature which captures the context-insensitive bias in the data.</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Empirical Evaluation</head><p>In this section we describe our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Dataset</head><p>For our experiments, we have used a publicly available collection of commonsense short stories released by <ref type="bibr" target="#b37">Mostafazadeh et al. (2016)</ref>. It consists of about 100K unannotated five-sentences long stories. For collecting these stories, Amazon Mechanical Turk workers were asked to compose novel five-sentence long stories on everyday topics. They were prompted to write coherent stories with a specific beginning and ending, with something happening in between. This resulted in a wide variety in topics with causal and temporal links between the events described in the story. Also, the workers were asked to limit the length of individual sentences to 70 characters which yielded short and succinct sentences, and to not use informal language or quotations.</p><p>The dataset also contains an additional set of 3, 742 four-sentences long stories (context) with two ending options, only one of which is correct. Each instance is annotated with this correctness information. This set was collected by asking Amazon Mechanical Turk workers to write a coherent and an incoherent ending to a given short story. The workers were asked to ensure that both the options shared at least one character from the story, and that the options, in isolation, made sense. This resulted in non-trivial alternative endings, and was also validated by other human subjects for high quality. This set was divided by <ref type="bibr" target="#b37">Mostafazadeh et al. (2016)</ref> into validation and test sets of 1871 instances each for the Story-Cloze Task, and were used for training and evaluating our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Baselines</head><p>We use the following baselines in our experiments: DSSM: (Mostafazadeh et al., 2016) It trains two deep neural networks ( <ref type="bibr" target="#b18">Huang et al., 2013</ref>) to project the context and the ending-options into the same vector space. Based on these vector representations, it predicts the ending-option with the largest cosine similarity with the context. Msap: The task addressed in this paper was also a shared task for an EACL'17 workshop and this baseline ( <ref type="bibr" target="#b50">Schwartz et al., 2017)</ref> represents the best performance reported on its leaderboard ( <ref type="bibr" target="#b38">Mostafazadeh et al., 2017)</ref>. It trains a logistic regression based on stylistic and languagemodel based features. LR: Our next baseline is a simple logistic regression model which is agnostic to the fact that there are multiple types of aspects. Given a context and ending-options, it predicts the answer using the same features (Sec. 2.1) as the Hidden Coherence model but clubs them all into one feature-vector. Majority Vote: This ensemble method uses the features extracted for each of the K = 3 aspects, to train K separate logistic regression models. It then makes a prediction by taking a majority vote of these K classifiers. Soft Voting: This baseline also learns K different aspect-specific classifiers. However, instead of taking a majority vote, it computes a score for each option,</p><formula xml:id="formula_7">o i , as Π K k P k (ending = o i |c, o 1 , o 2 )</formula><p>. Here P k represents the probability obtained from the k th logistic regression. The final prediction corresponds to the option with greater score. Aspect-aware Ensemble: Like the voting methods, this baseline also trains K different aspectspecific classifiers. However, it makes the final prediction by training another logistic regression over their predictions. <ref type="table">Table 1</ref> shows accuracies of various models on the held-out test set. An always-one classifier would get 51.3% accuracy on the task and human performance is reported to be 100% (Mostafazadeh Model Accuracy DSSM ( <ref type="bibr" target="#b37">Mostafazadeh et al., 2016)</ref> 58.5% Msap ( <ref type="bibr" target="#b50">Schwartz et al., 2017)</ref> 75 Lastly, we can see that the proposed Hidden Coherence model, with an accuracy of 77.60%, outperforms all other models. The superior performance of our model indicates the benefit of the context-sensitive weighing of individual consistency aspects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Ablation Study</head><p>We now investigate the predictive value of the various aspect-specific features. <ref type="table">Table 2</ref> shows the performance of a logistic regression model trained using all the features (All) and then using individual feature-groups. We can see that the features extracted from the aspect analyzing the event-sequence have the strongest predictive power, followed by those characterizing Sentiment-trajectory. The features measuring top-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Features</head><p>Accuracy All 74.4% Event-sequence 71.6% Sentiment 64.5% Topic 55.2% <ref type="table">Table 2</ref>: Performance comparison of various aspect features. Our event-sequence based features are most helpful followed by Sentiment-trajectory and then Topical Consistency based features.</p><p>ical consistency result in lowest accuracy but they still perform better than random on the task. <ref type="table">Table 3</ref> shows example stories, and weights given to the three aspects. An aspect's weight is its contribution towards the predicted output, and is shown as a bar of vertically stacked blocks in the last column. A block's height is proportional to its aspect's weight. Light grey block represents Event-sequence, and dark grey and black blocks represent Sentiment-trajectory and Topical consistency respectively. The first row describes the story of a man hurting himself. A human reader can guess from commonsense knowledge that people usually recover (correct ending) after being hurt and do not repeat their mistake (incorrect ending). Accordingly, our model also primarily used the aspect analyzing events in this story, which is indicated by the long light grey block in its weight bar. Also, we can see that the topic of both the options is consistent with the story, and the model gave a very small weight to the Topical Consistency aspect indicated by the almost indiscernible black block in its weight bar. Similarly, the second row describes the story of Pam being proud of her yard work. There is a striking sentimental contrast between the two options (upset versus satisfied), and the model relies primarily on sentiments (dark grey). The last row, describes the story of Maria making candy apples. The incorrect ending introduces a new entity/idea, apple pie, resulting in topical incoherence of this option with the rest of the story. The model relies primarily on topic (black) and events (light grey). Reliance on events makes sense because it is likely for a person to enjoy what they fondly cook. The model gave a weight of 40% to the topical aspect, which is high as compared to its average weight across the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorrect</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ending Correct Ending Weights</head><p>He didn't know how the television worked. He tried to fix it, anyway. He climbed up on the roof and fiddled with the antenna. His foot slipped on the wet shingles and he went tumbling down.</p><p>He decided that was fun and to try tumbling again.</p><p>Thankfully, he recovered.</p><p>Pam thought her front yard looked boring. So she decided to buy several plants. And she placed them in her front yard. She was proud of her work.</p><p>Pam was upset at herself.</p><p>Pam was satisfied.</p><p>Maria smelled the fresh Autumn air and decided to celebrate. She wanted to make candy apples. She picked up the ingredients at a local market and headed home. She cooked the candy and prepared the apples.</p><p>Maria's apple pie was delicious.</p><p>She enjoyed the candy apples. <ref type="table">Table 3</ref>: Examples of stories, ending-options, and aspect weights learned by our model. Aspect weights are shown as bars of stacked blocks in the last column (light grey, dark grey and black represent Eventsequence, Sentiment-trajectory and Topical Consistency respectively). A block's height is proportional to its component's weight. Black blocks are sometimes not visible because there were too small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Discussion</head><p>Error Analysis: <ref type="table">Table 4</ref> shows examples of stories for which our model could not predict the correct ending. We believe that many of these stories require a deeper understanding of language and commonsense. For example, in the story described in the first row, the protagonist accepted an invitation from his friends to go to a club but danced terribly, and so he was asked to stay home the next time. To make the correct prediction in this story, the model not only needs to understand that if one does not dance well at a club they are likely to be not invited in the future, but also that staying home is the same as not getting invited. Similarly, the second row shows a story in which Johnny asks Anita out, but she makes an excuse. He later sees her with another guy and decides not to ask her out again. This example requires identifying that Anita's excuse was a lie indicating her disinterest in Johnny, which makes it unlikely for Johnny to invite her again. It also needs an understanding of inter-personal relationships, i.e. seeing a potential lover with another person leads to estrangement.</p><p>Social Analysis: To further explore the significance of social relations in stories, we consider the special case of romantic stories. We use a deterministic heuristic to identify romantic stories using lexical matches with a handcrafted list containing words like marry, proposal, girlfriend, ask out, etc. We then applied the following two rules:</p><p>(i) if a story contains two characters, then output the option whose sentiment matches that of the context, (ii) if a story contains three characters, then output the option with negative sentiment. Most stories in our dataset contained few characters. These rules are motivated by the intuition that a romantic story between two people can have a happy or sad ending depending on the context. However, a romantic story with three people is likely to describe a love triangle, and so not end well. Expectedly, these rules had low coverage (of about 60 stories), but a considerably high accuracy (70%) when active. Furthermore, a closer analysis revealed that most errors resulted from incorrect coreference resolutions (leading to incorrect count of characters). This indicates the utility of understanding semantics of social relationships for story comprehension and it could potentially be another aspect to consider while solving such tasks.</p><p>Sentiment Analysis: We now explore the insights obtained by modeling sentiments in stories. <ref type="bibr" target="#b37">Mostafazadeh et al. (2016)</ref> presented two baselines for this task whose outputs were simply the ending whose sentiment agreed with (i) the complete story, or (ii) the climax (last sentence of the story). While their performances were close to random, our sentiment based features yield a much higher accuracy of 64.5% (see <ref type="table">Table 2</ref>). This could possibly be attributed to our approach's ability to learn such rules from the data itself, rather</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Incorrect Ending</head><p>Correct Ending My friends all love to go to the club to dance. They think it's a lot of fun and always invite. I finally decided to tag along last Saturday. I danced terribly and broke a friend's toe.</p><p>The next weekend, I was asked to please stay home.</p><p>My friends decided to keep inviting me as I am so much fun. Johnny thought Anita was the girl for him, but he was wrong. He invited her out but she said she didn't feel well. Johnny decided to go to a club, just to drink and listen to music. At midnight, he looked back and saw Anita dancing with another guy.</p><p>Johnny did not ask Anita out again.</p><p>Johnny wanted to ask Anita out again. <ref type="table">Table 4</ref>: Examples of stories incorrectly predicted by our model. than making hard assumptions. For instance, our language model of overall narrative sentiments indicates that while happy stories mostly have happy endings (with a conditional probability of 74%), the reverse is not true. In particular, sad stories (with overall negative sentiments) end with a negative sentiment in only 52% of the cases. We made similar observations regarding sentimental conformity between endings and climaxes.</p><p>Our features' superior performance can also be attributed to their deeper understanding of not just overall sentiments but also their trajectories. Our language models indicate that stories that exhibit a positive sentiment in all three narrative segments (beginning, body, and climax) have very high chance of happy endings (83%). Similarly, stories with negative sentiments in the three segments also have a fair chance of having sad endings (60%). This is different from stories with an overall negative sentiment, in which case the sentiment may be exhibited in only certain narrative segments. The language models also identify a pattern of hopeful stories, in which the sentiment begins as negative but moves towards positive in the body and climax, resulting in mostly happy endings (∼ 70%). This was not true for the reverse case: pessimistic stories with positive beginning but negative body (and/or climax) were equally likely to have positive or negative endings (52%). Supplementary material contains sample stories for each of the above observations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>We now review previous work done in this field. Our work touches upon several research areas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Story understanding:</head><p>Our work is most closely related to the field of narrative understanding. Apart from event-centric understanding of narrative plots <ref type="bibr" target="#b28">(Lehnert, 1981;</ref><ref type="bibr" target="#b33">McIntyre and Lapata, 2010;</ref><ref type="bibr" target="#b15">Goyal et al., 2010;</ref><ref type="bibr">Elsner, 2012;</ref><ref type="bibr" target="#b12">Finlayson, 2012)</ref>, recent methods have focused on understanding narratives from the perspective of characters <ref type="bibr" target="#b57">(Wilensky, 1978)</ref> mentioned in them. These methods study character personas ( <ref type="bibr" target="#b5">Bamman et al., 2013</ref><ref type="bibr" target="#b6">Bamman et al., , 2014</ref>) or Proppian <ref type="bibr" target="#b46">(Propp, 1968)</ref> roles <ref type="bibr" target="#b54">(Valls-Vargas et al., 2014</ref>, inter-character relationships <ref type="bibr" target="#b19">(Iyyer et al., 2016;</ref><ref type="bibr">Chaturvedi et al., , 2017</ref>, and social networks of characters ( <ref type="bibr" target="#b10">Elson et al., 2010;</ref><ref type="bibr" target="#b9">Elson, 2012;</ref><ref type="bibr" target="#b1">Agarwal et al., 2013</ref><ref type="bibr" target="#b0">Agarwal et al., , 2014</ref><ref type="bibr" target="#b25">Krishnan and Eisenstein, 2015;</ref><ref type="bibr" target="#b52">Srivastava et al., 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Events-centered learning:</head><p>Our Entity-sequence component is closely related to semantic script learning. Script learning focuses on representing text using a prototypical sequences of events, their participants and causal relationships between them, called scripts ( <ref type="bibr" target="#b49">Schank and Abelson, 1977;</ref><ref type="bibr" target="#b36">Mooney and DeJong, 1985)</ref>. Several statistical methods have been proposed to automatically learn scripts or scripts-like structures from unstructured text <ref type="bibr">Ju- rafsky, 2008, 2009;</ref><ref type="bibr" target="#b20">Jans et al., 2012;</ref><ref type="bibr" target="#b40">Orr et al., 2014;</ref><ref type="bibr" target="#b43">Pichotta and Mooney, 2014</ref>). Such methods for script-learning also include Bayesian approaches <ref type="bibr">(Bejan, 2008;</ref><ref type="bibr" target="#b13">Frermann et al., 2014</ref>), sequence alignment algorithms ( <ref type="bibr" target="#b48">Regneri et al., 2010)</ref> and neural networks <ref type="bibr" target="#b35">(Modi and Titov, 2014;</ref><ref type="bibr" target="#b16">Granroth-Wilding and Clark, 2016;</ref><ref type="bibr">Pi- chotta and Mooney, 2016)</ref>. There has also been work on representing events in a structured manner using schemas, which are learned probabilistically <ref type="bibr">(Chambers, 2013;</ref><ref type="bibr">Cheung et al., 2013;</ref><ref type="bibr" target="#b39">Nguyen et al., 2015)</ref>, using graphs ( <ref type="bibr">Balasubrama- nian et al., 2013)</ref> or neural approaches <ref type="bibr" target="#b53">(Titov and Khoddam, 2015)</ref>. Recently, <ref type="bibr" target="#b11">Ferraro and Durme (2016)</ref> presented a unified Bayesian model for scripts and frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Textual Coherence:</head><p>Our work is also related to the study of coherence in discourse. A significant amount of prior work is primarily based on the Centering Theory Framework ( <ref type="bibr" target="#b17">Grosz et al., 1995)</ref> and focus on entities and their syntactic roles <ref type="bibr" target="#b23">(Karamanis, 2003;</ref><ref type="bibr" target="#b24">Karamanis et al., 2004;</ref><ref type="bibr" target="#b27">Lapata and Barzilay, 2005;</ref><ref type="bibr" target="#b7">Barzilay and Lapata, 2008;</ref><ref type="bibr" target="#b8">Elsner and Charniak, 2008)</ref>. Other approaches measure coherence using topic drift within a domain ( <ref type="bibr">Barzilay and Lee, 2004;</ref><ref type="bibr" target="#b14">Fung and Ngai, 2006</ref>), co-occurrence of words <ref type="bibr" target="#b26">(Lapata, 2003;</ref><ref type="bibr" target="#b51">Soricut and Marcu, 2006</ref>), syntactic patterns (Louis and Nenkova, 2012) and discourse relations <ref type="bibr" target="#b45">(Pitler and Nenkova, 2008;</ref><ref type="bibr" target="#b29">Lin et al., 2011)</ref>. The nature of the tasks addressed by these works (such as determining the correct arrangement order for a set of sentences) makes them focus on learning sequential order of the various discourse components (entities, ideas, etc.). Our goal, instead, is to choose between alternatives of discourse components themselves (and not just their order) to produce a consistent story.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Story comprehension is a complex Natural Language Understanding task involving linguistic intelligence as well as a semantic and social knowledge of the real world. This paper studies story comprehension from the perspective of learning what is likely to happen next in a story. We present a model that given a short story, predicts its correct ending. It incorporates three aspects of storyunderstanding, that are based on an analysis of the events, sentiments and topics described in the story. While this is the best-performing model till date on this task, our analysis indicates a need for even deeper analysis of human behavior and societal norms to further improve our understanding. This work emphasizes that there are multiple aspects to story understanding, which future research can build upon.</p><p>Regina Barzilay and Lillian <ref type="bibr">Lee. 2004</ref>. Catching the drift: Probabilistic content models, with applications to generation and summarization. In <ref type="table">Proceed- ings of the Human Language Technology Confer- ence of the North American Chapter of the Asso- ciation for Computational Linguistics, HLT-</ref> A. P. Dempster, N. M. <ref type="bibr">Laird, and D. B. Rubin. 1977.</ref> Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society: Series B, 39:1-38.</p><p>Micha Elsner. 2012. Character-based Kernels for Novelistic Plot Structure. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics EACL 2012, pages 634-644.</p></div>
			<note place="foot" n="1"> Owing to the large size of the training data and the fact that we abstract to the frame-semantic (and not verb) level, we cover most instances (76%) in our dataset.</note>

			<note place="foot" n="2"> The reported segmentation process made sense from qualitative analysis on a random sample, and also led to superior performance compared to alternate strategies. 3 Polarities of &apos;negated&apos; word were reversed (determined from neg dependency relation in the corresponding sentence).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work is partly supported by the IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) -a research collaboration as part of the IBM Cognitive Horizon Network; by the US Defense Advanced Research Projects Agency (DARPA) under contract FA8750-13-2-0008; and by the Army Research Laboratory (ARL) under agreement W911NF-09-2-0053. The views expressed are those of the authors and do not reflect the official policy or position of IBM, the Department of Defense or the U.S. Government.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frame semantic tree kernels for social network extraction from text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriramkumar</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anup</forename><surname>Kotalwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiehan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter</title>
		<meeting>the 14th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="211" to="219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic extraction of social networks from literary text: A case study on alice in wonderland</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anup</forename><surname>Kotalwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth International Joint Conference on Natural Language Processing</title>
		<meeting>the Sixth International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1202" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The berkeley framenet project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><forename type="middle">F</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Fillmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, COLING-ACL 1998</title>
		<meeting>the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, COLING-ACL 1998</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="86" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generating coherent event schemas at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niranjan</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Soderland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1721" to="1731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Narrative analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bamberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">APA handbook of research methods in psychology</title>
		<editor>D. L. Long A. T. Panter D. Rindskopf H. Cooper, P. M. Camic and K. Sher</editor>
		<imprint>
			<publisher>American Psychological Association</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="85" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Latent Personas of Film Characters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="352" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Bayesian Mixed Effects Model of Literary Character</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bamman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ted</forename><surname>Underwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="370" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Modeling local coherence: An entity-based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Coreference-inspired coherence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="41" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Modeling Narrative Discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Elson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extracting Social Networks from Literary Fiction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Dames</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathleen</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL 2010</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="138" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A unified bayesian model of scripts, frames and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Ferraro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2601" to="2607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning Narrative Structure from Annotated Folktales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark Alan Finlayson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A hierarchical bayesian model for unsupervised induction of script knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lea</forename><surname>Frermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">One story, one flow: Hidden markov story models for multilingual multidocument summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Ngai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TSLP</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatically Producing Plot Unit Representations for Narrative Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Riloff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What happens next? event prediction using a compositional neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Granroth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Wilding</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2727" to="2733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Centering: A framework for modeling the local coherence of discourse</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><forename type="middle">J</forename><surname>Grosz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><forename type="middle">K</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Weinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="203" to="225" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning deep structured semantic models for web search using clickthrough data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Acero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><forename type="middle">P</forename><surname>Heck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM International Conference on Information and Knowledge Management, CIKM 2013</title>
		<meeting>the 22nd ACM International Conference on Information and Knowledge Management, CIKM 2013</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">2333</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Feuding families and former friends: Unsupervised learning for dynamic fictional relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Guha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><forename type="middle">L</forename><surname>Boyd-Graber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1544" />
		</imprint>
	</monogr>
	<note>NAACL HLT 2016</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Skip n-grams and ranking functions for predicting script events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bram</forename><surname>Jans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Vulic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariefrancine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2012</title>
		<meeting>the 13th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2012</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="336" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">L</forename><surname>Jockers</surname></persName>
		</author>
		<title level="m">Macroanalysis: Digital Methods and Literary History. Topics in the Digital Humanities</title>
		<imprint>
			<publisher>University of Illinois Press</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Narrative interviewing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><surname>Jovchelovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">W</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Qualitative Researching With Text, IMAge and Sound : a Practical Handbook</title>
		<editor>Martin W. Bauer and G. Gaskell</editor>
		<meeting><address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="57" to="74" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Entity Coherence for Descriptive Text Structuring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikiforos</forename><surname>Karamanis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
		<respStmt>
			<orgName>Division of Informatics, University of Edinburgh</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Evaluating centeringbased metrics of coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikiforos</forename><surname>Karamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Poesio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Mellish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Oberlander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, ACL 2004</title>
		<meeting>the 42nd Annual Meeting of the Association for Computational Linguistics, ACL 2004</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="391" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">You&apos;re Mr. Lebowski, I&apos;m the Dude&quot;: Inducing Address Term Formality in Signed Social Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinodh</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2015</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1616" to="1626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Probabilistic text structuring: Experiments with sentence ordering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, ACL 2003</title>
		<meeting>the 41st Annual Meeting of the Association for Computational Linguistics, ACL 2003</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Automatic evaluation of text coherence: Models and representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth International Joint Conference on Artificial Intelligence, IJ-CAI 2005</title>
		<meeting>the Nineteenth International Joint Conference on Artificial Intelligence, IJ-CAI 2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1085" to="1090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Plot Units and Narrative Summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wendy</forename><forename type="middle">G</forename><surname>Lehnert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="293" to="331" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatically evaluating text coherence using discourse relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwee</forename><forename type="middle">Tou</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="997" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Opinion Observer: Analyzing and Comparing Opinions on the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsheng</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th international conference on World Wide Web, WWW 2005</title>
		<meeting>the 14th international conference on World Wide Web, WWW 2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="342" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A coherence model based on syntactic patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1157" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Computational Modeling of Narrative. Synthesis Lectures on Human Language Technologies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjeet</forename><surname>Mani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Plot Induction and Evolutionary Search for Story Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Mcintyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL 2010</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1562" to="1572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Three new graphical models for statistical language modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on Machine Learning</title>
		<meeting>the 24th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="641" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inducing neural models of script knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Modi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Eighteenth Conference on Computational Natural Language Learning<address><addrLine>CoNLL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="49" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning schemata for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dejong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Joint Conference on Artificial Intelligence, IJCAI 1985</title>
		<meeting>the 9th International Joint Conference on Artificial Intelligence, IJCAI 1985</meeting>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="page" from="681" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A corpus and cloze evaluation for deeper understanding of commonsense stories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies NAACL HLT 2016</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies NAACL HLT 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="839" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Lsdsem 2017 shared task: The story cloze test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annie</forename><surname>Louis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Allen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</title>
		<meeting>the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="46" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generative event schema induction with entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiem-Hieu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Tannier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Ferret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romaric</forename><surname>Besançon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning Scripts as Hidden Markov Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">Walker</forename><surname>Orr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Tadepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janardhan</forename><surname>Rao Doppa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoli</forename><surname>Fern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">G</forename><surname>Dietterich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1565" to="1571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Two discourse driven language models for semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="290" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Statistical script learning with multi-argument events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2014</title>
		<meeting>the 14th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2014</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="220" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning statistical scripts with LSTM recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2800" to="2806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Revisiting readability: A unified framework for predicting text quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Pitler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Nenkova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2008 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="186" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Propp</forename><surname>Vladimir Iakovlevich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
		<respStmt>
			<orgName>Morphology of the Folktale. University of Texas</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Semantic role labeling via integer linear programming inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasin</forename><surname>Punyakanok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dav</forename><surname>Zimak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Computational Linguistics</title>
		<meeting>the 20th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning Script Knowledge with Web Experiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaela</forename><surname>Regneri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manfred</forename><surname>Pinkal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL 2010</title>
		<meeting>the 48th Annual Meeting of the Association for Computational Linguistics, ACL 2010</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="979" to="988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Schank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abelson</surname></persName>
		</author>
		<title level="m">Scripts, Plans, Goals, and Understanding: An Inquiry Into Human Knowledge Structures (Artificial Intelligence Series</title>
		<imprint>
			<publisher>Psychology Press</publisher>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Story cloze task: UW NLP system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leila</forename><surname>Zilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of LSDSem</title>
		<meeting>LSDSem</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="52" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Discourse generation using utility-trained coherence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Marcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, ACL 2006</title>
		<meeting>the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, ACL 2006</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1105" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Inferring interpersonal relations in narrative summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shashank</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<idno>Febru- ary 12-17</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirtieth AAAI Conference on Artificial Intelligence<address><addrLine>Phoenix, Arizona, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2807" to="2813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Unsupervised induction of semantic roles within a reconstructionerror minimization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Khoddam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2015</title>
		<meeting>the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Toward automatic role identification in unannotated folk tales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Valls-Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontañón</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</title>
		<meeting>the Tenth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="188" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Narrative hermeneutic circle: Improving character role identification from natural language text via feedback loops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josep</forename><surname>Valls-Vargas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jichen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontañón</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015</title>
		<meeting>the Twenty-Fourth International Joint Conference on Artificial Intelligence, IJCAI 2015</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2517" to="2523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Vonnegut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Palm Sunday. RosettaBooks LLC</title>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Understanding Goal-Based Stories. Outstanding Dissertations in the Computer Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Wilensky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978" />
			<publisher>Garland Publishing</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Recognizing Contextual Polarity in PhraseLevel Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theresa</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janyce</forename><surname>Wiebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, HLT/EMNLP 2005</title>
		<meeting>the Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, HLT/EMNLP 2005</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="347" to="354" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
