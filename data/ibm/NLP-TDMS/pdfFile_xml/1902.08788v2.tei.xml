<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Facial Motion Prior Networks for Facial Expression Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuedong</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shikai</forename><surname>Chen</surname></persName>
							<email>skchen@seu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongchao</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
							<email>jianfei.cai@monash.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">Lab, Lenovo Research</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<settlement>Singapore</settlement>
									<region>â€  AI</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Facial Motion Prior Networks for Facial Expression Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-facial expression recognition</term>
					<term>deep learning</term>
					<term>prior knowledge</term>
					<term>facial-motion mask</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep learning based facial expression recognition (FER) has received a lot of attention in the past few years. Most of the existing deep learning based FER methods do not consider domain knowledge well, which thereby fail to extract representative features. In this work, we propose a novel FER framework, named Facial Motion Prior Networks (FMPN). Particularly, we introduce an addition branch to generate a facial mask so as to focus on facial muscle moving regions. To guide the facial mask learning, we propose to incorporate prior domain knowledge by using the average differences between neutral faces and the corresponding expressive faces as the training guidance. Extensive experiments on three facial expression benchmark datasets demonstrate the effectiveness of the proposed method, compared with the state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Facial expression is one of the most important components in daily communications of human beings. It is generated by movements of facial muscles. While different people have different kinds of facial expressions caused by their own expressive styles or personalities, many studies show that there are several types of basic expressions shared by different peoples with different cultural and ethnic background <ref type="bibr" target="#b0">[1]</ref>. Research on automatic recognition of such basic facial expressions has drawn great attention during the past decades.</p><p>Traditional approaches tend to conduct facial expression recognition (FER) by using Gabor Wavelets, sparse coding, etc., where many studies show that subtracting neutral faces from their corresponding expressive faces can help the algorithms to emphasize on the facial moving areas, and significantly improve the expression recognition rate <ref type="bibr" target="#b1">[2]</ref>.</p><p>Convolutional Neural Networks (CNNs) have been widely applied to FER in the recent years. CNNs can achieve good performance by learning powerful high-level features which are better than those conventional hand-crafted features. There are also some other methods proposing to combine global appearance features with local geometry features for FER. Specifically, they feed facial expression recognition networks with not only the original images, but also their related facial landmarks <ref type="bibr" target="#b2">[3]</ref>, or optical flow <ref type="bibr" target="#b3">[4]</ref>. <ref type="bibr" target="#b4">[5]</ref> extracts more precise facial features by focusing on some specific local parts, inspired by Action Units (AUs) <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Recently, a de-expression framework was proposed in <ref type="bibr" target="#b7">[8]</ref>. They used Generative Adversarial Networks (GANs) to generate neutral faces by learning and filtering the expressive information which is later used for facial expression classification.</p><p>Despite great progress, the existing deep learning based FER methods still have some limitations. Firstly, most methods do not incorporate domain knowledge well, so the global features they extract tend to be less discriminative and less representative for FER. Secondly, although some deep learning based methods consider domain knowledge to extract local geometry information by assuming the availability of the full set of landmarks, optical flows or AUs, their assumptions might not be valid since in many cases we might not have all the extra local geometry information and the AU detection task itself is a challenging one. Finally, GANs based method is also impractical since it requires the neutral and expressive faces of the same person are simultaneously available for all the training data. In addition, the image quality of the generated faces is hard to control, which directly affects the performance of the subsequent expression classifier.</p><p>Therefore, to address these problems, we propose a novel FER framework. Our contribution can be summarized as follows. Firstly, we design a novel end-to-end deep learning framework named Facial Motion Prior Networks (FMPN) for FER, where we introduce an addition stream to generate a mask to focus on facial muscle moving regions. Secondly, we incorporate prior domain knowledge by using the average differences between neutral faces and the corresponding expressive faces as the guidance for the facial motion mask learning. Finally, our method outperforms current state-of-theart results on two laboratory-controlled datasets and one inthe-wild dataset, which demonstrates the effectiveness of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PROPOSED APPROACH</head><p>An expressive face of a person is a deformation from the neutral face. In other words, the differences between a neutral face and its corresponding expressive face contain lots of information related to facial expressions. Since basic facial expressions, such as anger, fear, happiness and so on, share traits of uniformity across different people and races, it is reasonable to learn to emphasize specific facial moving parts when conducting expression recognition. On the other hand, emphasizing local facial parts (moving muscle) should not lead Classification Net (CN). An expressive face is converted to gray scale and fed to FMG to generate a facial-motion mask. Then the mask is applied to and fused with the original input expressive face in PFN. The output of PFN is further fed to CN to extract more powerful features and predict facial expression label. l G and l C are loss functions at FMG and CN, respectively, which are end-to-end jointly optimized during training. Note that the learning of FMG is guided by pseudo ground truth masks, which are the average differences between neutral faces and their corresponding expressive faces (see top right corner).</p><p>to totally ignoring holistic facial image (whole face), because attributes such as gender and age, provided by the whole face image, can also affect types of expressions significantly. Therefore, both local features and holistic features should be taken into consideration. Inspired by the above analysis, we propose a novel approach to recognize facial expressions. <ref type="figure" target="#fig_0">Fig. 1</ref> shows the architecture of the proposed FMPN framework, which consists of three networks: Facial-Motion Mask Generator (FMG), Prior Fusion Net (PFN) and Classification Net (CN). FMG is constructed to generate a mask, namely facial-motion mask, which highlights moving areas of the given gray scale expressive face. PFN aims to fuse the original input image with the facial-motion mask generated by FMG to introduce domain knowledge to the whole framework. CN is a typical Convolutional Neural Network (CNN) for feature extraction and classification, such as VGG, ResNet or Inception. We mainly discuss FMG and PFN in the following subsections.</p><p>Facial-Motion Mask Generator (FMG) is built to generate a facial-motion mask, which is used to highlight expressionrelated facial motion regions. Instead of making the network to learn active areas blindly, we choose to guide it via some pseudo ground-truth masks that are generated by modeling basic facial expressions.</p><p>In particular, as aforementioned, facial expressions are caused by the contraction of facial muscles, and different people with the same expression share a similar pattern. Therefore, for one specific type of facial expressions, we model muscle moving areas as the difference between an expressive face and its corresponding neutral face, while the characteristic of similarity is modeled by averaging the above differences of all the training instances in the same expression category. Specifically, for a k-th type of facial expressions, e.g., happiness, its ground truth mask I (k) m is constructed as</p><formula xml:id="formula_0">I (k) m = Ï•( 1 N k N k i Î¾(R (k) e,i ) âˆ’ Î¾(R (k) n,i ) ),<label>(1)</label></formula><p>where R (k) e is the unprocessed/raw face with the k-th type of facial expressions, R (k) n is the corresponding neutral face (faces of the same person shares one neutral face), N k is the number of faces in the k-th expression category, and Î¾(Â·) and Ï•(Â·) refer to the pre-processing and the post-processing, respectively.</p><p>Since facial moving areas are directly modeled as the absolute differences in Eq. (1), the expressive and neutral faces need to be well aligned, which is ensured by the pre-processing function Î¾(Â·). Specifically, a standard spatial transformation is performed by aligning the detected facial landmarks with the standard reference landmarks. In addition, considering that the average face difference is often with close-contrast values, we further introduce a post-processing function Ï•(Â·) which applies histogram equalization to adjust the difference values for a better distribution. The generated ground truth masks of CK+ <ref type="bibr" target="#b8">[9]</ref> for the seven basic expressions are shown in <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>Given an expressive face, FMG is designed to learn a facialmotion mask, trained with the guidance from the pre-computed ground truth mask related to that expression. Particularly, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the input expressive face is first downsampled and convoluted to extract feature related to geometric structure through two convolutional layers. Then the geometric feature is filtered and transferred to semantic feature related to the dynamic area through four residual blocks. Finally, two transposed convolutional layers are appended to project the learned semantic feature back to spatial domain as moving muscle focused mask. We use MSE (mean square error) for the training objective function of FMG:</p><formula xml:id="formula_1">l G (I e , k) = E(f G (I e ) âˆ’ I (k) m ) 2 ,<label>(2)</label></formula><p>where I e is the input expressive face, I (k) m is the ground truth mask corresponding to the expression class of I e , defined in (1), and f G (I e ) refers to the mask generated by FMG.</p><p>One might ask why we do not directly use the computed ground truth masks for facial expression recognition, instead of learning to generate a facial-motion mask. One main reason is that during testing we do not know which ground truth mask should be chosen since different expressions have different facial-motion masks. We would also like to point out that, considering expressive faces with the same expression in different datasets have similar moving muscles, the ground truth masks obtained from one dataset are mostly likely to be suitable for another dataset. This can help overcome the limitation that some dataset may not contain paired expressive and neutral faces to compute ground truth masks.</p><p>Prior Fusion Net (PFN) is designed to automatically fuse the original input face with the face that is masked by the facial-motion mask learned from FMG. The former is to provide holistic features, while the latter emphasizes the moving areas, which reflects the common expression definition and domain knowledge. Specifically, PFN produces a fused output I s by a weighted sum, which can be written as</p><formula xml:id="formula_2">I s = w 1 Â· I e + w 2 Â· (I e âŠ— f G (I e ))),<label>(3)</label></formula><p>where I e is the RGB version of the gray-scale image I e , I e âŠ— f G (I e ) refers to the masked face, which is obtained by element-wise multiplication of face I e and its corresponding mask f G (I e ), and w 1 and w 2 are weights of convolutional layers, which are updated during training. After PFN, the fused output I s will then be fed into a CNN based classification network, which can be VGG, ResNet or others. The classification network is trained with the cross entropy loss:</p><formula xml:id="formula_3">l C (I s , k) = âˆ’ log( exp(f C (I s ) (k) ) K i exp(f C (I s ) (i) ) ),<label>(4)</label></formula><p>where f C (I s ) (i) is the i-th output value of the classification network f C (Â·), k is the target expression, and K is the total number of facial expression classes in a given dataset. In this way, the total loss becomes</p><formula xml:id="formula_4">l total = Î» 1 Â· l G + Î» 2 Â· l C ,<label>(5)</label></formula><p>where l G and l C are defined in <ref type="formula" target="#formula_1">(2)</ref> and <ref type="formula" target="#formula_3">(4)</ref>, respectively, and Î» 1 and Î» 2 are hyperparameters, being empirically set in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXPERIMENTS A. Implementation Details</head><p>We use Inception V3 as the CN, while other CNN models such as VGG, ResNet can also be adopted. For all datasets, five landmarks are extracted, followed by face normalization. In training, input are randomly cropped from four corners or center. Random horizontal flip is also employed. Training list is shuffled at the beginning of each training epoch.</p><p>The CN is initialized using parameters pretrained on Im-ageNet, while others are randomly initialized. The training starts by tuning only FMG for 300 epochs, using Adam optimizer. The learning rate is initialized as 10 âˆ’4 and decayed linearly to 0 from epoch 150. After that, the whole framework is jointly trained, with Î» 1 = 10 and Î» 2 = 1 in (5). The learning rate for FMG is reset to 10 âˆ’5 , while the rest uses 10 âˆ’4 . We jointly train the entire framework for another 200 epochs, and linearly decay the learning rates from epoch 100. The proposed model is implemented using PyTorch, and code is available at https://github.com/donydchen/FMPN-FER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Expression Recognition Results</head><p>We consider two baseline methods: one is using only the classification network, referred as CNN (baseline), the other is using the entire framework but without the training guidance from the ground truth masks, referred as CNN (no l G ).</p><p>The Extended Cohn-Kanade Dataset (CK+) <ref type="bibr" target="#b8">[9]</ref> is a laboratory-controlled benchmark dataset labelled with seven basic expressions. Following the settings of <ref type="bibr" target="#b7">[8]</ref>, the last three frames of each labelled sequence are extracted, results in a dataset with 981 images. These images are grouped according to the person identity and resort in an ascending order. And 10-fold subject-independent cross-validation experiments are conducted. As shown in TABLE I, our proposed method outperforms all other state-of-the-art approaches.Compared with the baselines, CNN(baseline) and CNN(no l G ), our final model achieves a large gain, which indicates that the guidance from the moving muscle mask benefits FER. <ref type="figure" target="#fig_2">Fig. 3 (left)</ref> gives the details of the average accuracy in confusion matrix. We can see that happiness is the easiest one to be recognized, likely due to its unique feature of moving muscle around mouth (see <ref type="figure" target="#fig_1">Fig. 2</ref>), and anger has the relatively lowest recognition rate, which has some confusion with disgust and sadness. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Approach</head><p>Setting Accuracy (%) CK+ MMI AffectNet LBP-TOP <ref type="bibr" target="#b10">[11]</ref> sequence-based 88.99 59.51 -HOG 3D <ref type="bibr" target="#b11">[12]</ref> sequence-based 91.44 60.89 -DTAGN(Joint) <ref type="bibr" target="#b2">[3]</ref> sequence-based 97.25 70.24 -STM-Explet <ref type="bibr" target="#b12">[13]</ref> sequence-based 94.19 75.12 -IACNN <ref type="bibr" target="#b13">[14]</ref> image-based 95.37 71.55 -DeRF <ref type="bibr" target="#b7">[8]</ref> image  The MMI <ref type="bibr" target="#b9">[10]</ref> is another laboratory-controlled dataset labelled with six basic expressions (without contempt). As a typical procedure, three peak frames around the center of each labelled sequence are selected, results in a dataset with a total of 624 expressive faces. Similar to CK+, 10-fold personindependent cross-validation experiments are conducted. As illustrated in TABLE I, compared with the image-based approaches, our method outperforms them by over 9.51%. Compared with the sequence-based methods, which use temporal information, our approach still achieves over 7.62% accuracy improvement. In addition, the large gaps between the two baseline models and our final model further demonstrate the importance of introducing the facial-motion mask and the usefulness of the guidance from the pre-computed groundtruth masks. <ref type="figure" target="#fig_2">Fig. 3 (right)</ref> gives the confusion matrix. We can see that it is similar to those of CK+. This indicates that facial expressions share similar patterns across different datasets.</p><p>The AffectNet <ref type="bibr" target="#b14">[15]</ref> is a very large in-the-wild dataset. We conduct experiments on a subset of AffectNet. We randomly select around 3500 images for each of the seven basic expressions, resulting in a total of 24530 images. We randomly split it into training and testing set with a ratio of 9 to 1. Since the neutral faces corresponding to expressive faces are not available, we cannot generate the ground truth masks based on this dataset itself. Thus, we borrow the ground truth masks from CK+. Since AffectNet has enough data, we remove the pretraining for FMG. All other settings, including hyperparameters, learning rates, etc., remain unchanged.</p><p>As shown in TABLE I, transferring muscle moving masks from CK+ improves the recognition rate, which suggests that the information of facial moving muscles can be shared across different datasets, and it does help improve the performance of expression recognition. Note that it seems that the gain is not as significant as those in other datasets. This is mainly because of the large number of test images in AffectNet and the challenge of dealing with in-the-wild images. Other stateof-the-art methods did not report their results on AffectNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. CONCLUSION</head><p>We have proposed a novel FER framework, which incorporates prior knowledge. Particularly, for a given expressive face, we generate a facial mask to focus on facial muscle moving regions and we use the average differences between neutral faces and expressive faces as the guidance for the facial mask learning. Our method achieves the best results on CK+ and MMI datasets We have also reported our results on the largescale in-the-wild dataset, AffectNet. These outstanding results demonstrates the effectiveness of the proposed model in FER.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Architecture of the proposed method. The model is composed of three networks: Facial-Motion Mask Generator (FMG), Prior Fusion Net (PFN) and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Ground truth masks of the CK+. Corresponding expressions from left to right, top to bottom are: anger, contempt, disgust, fear, happiness, sadness and surprise. It can be seen that different facial expressions have different moving muscles, which can benefit the recognition of expressions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Confusion matrix on the CK+ (left) and MMI (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I AVERAGE</head><label>I</label><figDesc>ACCURACY ON THE CK+, MMI AND AFFECTNET.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Automatic facial expression analysis: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fasel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="259" to="275" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recognizing facial actions using gabor wavelets with neutral face average difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Bazzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">V</forename><surname>Lamar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="505" to="510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint fine-tuning in deep neural networks for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2983" to="2991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep spatial-temporal feature fusion for facial expression recognition in static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Au-inspired deep networks for facial expression feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">159</biblScope>
			<biblScope unit="page" from="126" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep adaptive attention for joint facial action unit detection and face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ECCV</title>
		<imprint>
			<biblScope unit="page" from="705" to="720" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Facial action unit detection using attention and relation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Facial expression recognition by deexpression residue learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Ciftci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in CVPR</title>
		<imprint>
			<biblScope unit="page" from="2168" to="2177" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">The extended cohn-kanade dataset (ck+): A complete dataset for action unit and emotion-specified expression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Saragih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ambadar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="94" to="101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Web-based database for facial expression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valstar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rademaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dynamic texture recognition using local binary patterns with an application to facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="915" to="928" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>MarszaÅ‚ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="275" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning expressionlets on spatio-temporal manifold for dynamic facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1749" to="1756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Identity-aware convolutional neural network for facial expression recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="558" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Affectnet: A database for facial expression, valence, and arousal computing in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mollahosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03985</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
