<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CodeTrans: Towards Cracking the Language of Silicone&apos;s Code Through Self-Supervised Deep Learning and High Performance Computing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2000">2000</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Elnaggar</surname></persName>
							<email>ahmed.elnaggar@tum.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ding</surname></persName>
							<email>wei.ding@tum.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Gibbs</surname></persName>
							<email>tgibbs@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Feher</surname></persName>
							<email>tfeher@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Angerer</surname></persName>
							<email>cangerer@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Severini</surname></persName>
							<email>silvia@cis.uni-muenchen.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Matthes</surname></persName>
							<email>matthes@tum.de</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burkhard</forename><surname>Rost</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer</roleName><forename type="first">Severini</forename><surname>©2000 Elnaggar</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Ding, Matthes, Rost, Jones, Gibbs, Feher, Angerer</roleName><forename type="first">Severini</forename><surname>Elnaggar</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics TUM (Technical</orgName>
								<orgName type="institution">University of Munich</orgName>
								<address>
									<addrLine>Boltzmannstrasse 3</addrLine>
									<postCode>85748</postCode>
									<settlement>Garching</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Llion Jones</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Google AI Google</orgName>
								<address>
									<addrLine>1600 Amphitheatre Parkway, Mountain View</addrLine>
									<postCode>94043</postCode>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<address>
									<addrLine>Vereinigte Staaten</addrLine>
									<postCode>2788, 95051</postCode>
									<settlement>Nvidia, San Tomas Expy, Santa Clara</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Center for Information and Language Processing Ludwig-Maximilians</orgName>
								<orgName type="institution">Universität München</orgName>
								<address>
									<addrLine>Oettingenstraße 67</addrLine>
									<postCode>D-80538</postCode>
									<settlement>München</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Department of Informatics TUM (Technical</orgName>
								<orgName type="institution">University of Munich)</orgName>
								<address>
									<addrLine>Boltzmannstrasse 3</addrLine>
									<postCode>85748</postCode>
									<settlement>Garching</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CodeTrans: Towards Cracking the Language of Silicone&apos;s Code Through Self-Supervised Deep Learning and High Performance Computing</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Journal of Machine Learning Research</title>
						<imprint>
							<biblScope unit="volume">1</biblScope>
							<biblScope unit="page" from="1" to="48"/>
							<date type="published" when="2000">2000</date>
						</imprint>
					</monogr>
					<note type="submission">Submitted 4/00;</note>
					<note>Editor: Leslie Pack Kaelbling</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Software Engineering</term>
					<term>Natural Language Processing</term>
					<term>Transformer</term>
					<term>Source Code Summarization</term>
					<term>Commit Message Generation</term>
					<term>API Generation</term>
					<term>Program Synthesis</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Currently, a growing number of mature natural language processing applications make people's life more convenient. Such applications are built by source code -the language in software engineering. However, the applications for understanding source code language to ease the software engineering process are under-researched. Simultaneously, the transformer model, especially its combination with transfer learning, has been proven to be a powerful technique for natural language processing tasks. These breakthroughs point out a promising direction for process source code and crack software engineering tasks. This paper describes CodeTrans -an encoder-decoder transformer model for tasks in the software engineering domain, that explores the effectiveness of encoder-decoder transformer models for six software engineering tasks, including thirteen sub-tasks. Moreover, we have investigated the effect of different training strategies, including single-task learning, transfer * . Equal contribution. Correspondence to ahmed.elnaggar@tum.de. learning, multi-task learning, and multi-task learning with fine-tuning. CodeTrans outperforms the state-of-the-art models on all the tasks. To expedite future works in the software engineering domain, we have published our pre-trained models of CodeTrans. 1 1. https://github.com/agemagician/CodeTrans language query is composed of attention-based encoder-decoder GRUs. Polosukhin and Skidanov (2018) also used GRU and RNN cells to build a model synthesizing LISP-inspired domain-specific language (DSL) code.</p><p>Nevertheless, the transformer models have also lately gained attention for software engineering tasks. Feng et al. <ref type="formula">(2020)</ref> proposed a transformer model called CodeBERT to support natural language and programming language tasks like natural language code search, code documentation generation, etc. Moreover, Lachaux et al. (2020) published a Transcoder model for translating functions between C++, Java, and Python based on the transformer architecture. These models all achieved state-of-the-art results for software engineering tasks.</p><p>This paper proposes our CodeTrans models for software engineering tasks based on the encoder-decoder transformer architecture. We applied CodeTrans to six different kinds of tasks, including Code Documentation Generation, Source Code Summarization, Code Comment Generation, Git Commit Message Generation, API Sequence Recommendation, and Program Synthesis. Furthermore, we trained CodeTrans using single-task learning, transfer learning, and multi-task learning on one NVIDIA GPU and Google Cloud TPUs. Also, we used both supervised tasks and self-supervised tasks for building a language model in the software engineering domain. Our models achieve state-of-the-art performance on all the tasks. We have also contributed our pre-trained checkpoints and published the models for each task in Hugging Face model Hub. 2   The structure of this paper is as follows: In Section 2, we introduce the tasks of our experiments, the datasets, and our preprocessing methods. We explain the model architecture and the vocabulary representation in Section 3. We list our experiment details during the training in Section 4. The models' performances are compared in Section 5. In Section 6 we offer our reflection and conclusion. Future work necessary for further research is also introduced in this section.</p><p>2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Software engineering can be considered a process of designing, implementing, testing, and maintaining information systems such as applications, frameworks, or other software components <ref type="bibr" target="#b46">(Royce, 1987</ref>) -this is a highly complex undertaking. So experienced specialists invent and use different tools and methods (for example, design patterns, code documentation, unit tests, version control tools, etc.) to control and improve the software quality and make the software engineering process more effective and convenient. In software engineering, works are done by using different programming languages. Programming language can be considered as a type of language to communicate with the computer systems to build the applications and achieve the requirements. Therefore, natural language processing techniques could also be applied to solve the programming languages' tasks to assist the software engineering process.</p><p>Recently, using models like BERT <ref type="bibr" target="#b22">(Devlin et al., 2018)</ref>, XL-NET <ref type="bibr" target="#b51">(Yang et al., 2019)</ref>, ALBERT <ref type="bibr" target="#b33">(Lan et al., 2019)</ref>, RoBERTa <ref type="bibr" target="#b37">(Liu et al., 2019b)</ref>, <ref type="bibr">GPT-3 (Brown et al., 2020)</ref>, and T5 <ref type="bibr" target="#b43">(Raffel et al., 2020)</ref> have become a trend in natural language processing. All these models have the Transformer architecture of <ref type="bibr" target="#b48">Vaswani et al. (2017)</ref> with attention mechanisms. Furthermore, their large pre-trained checkpoints are very suitable to be finetuned on downstream supervised tasks. Such a transfer learning technique helps transfer the knowledge gained from different large datasets to a small specific dataset, avoids overfitting, and saves computational cost. Among them, <ref type="bibr" target="#b43">Raffel et al. (2020)</ref> carried out a large number of experiments to explore transfer learning performance and proved its effectiveness in natural language processing tasks. In addition, the Multi-Task Deep Neural Network (MT-DNN) proposed by <ref type="bibr" target="#b36">Liu et al. (2019a)</ref> based on Bert also obtained excellent results on Natural Language Understanding tasks. MT-DNN involves the pre-training stage of Bert together with a multi-task learning approach and shows the latter's excellent performance.</p><p>In the software engineering domain, transformer models with transfer learning and multitask learning are under-explored. According to the recent research by Watson (2020), the Recurrent Neural Network <ref type="bibr" target="#b30">(Kombrink et al., 2011)</ref> (including long short-term memory (LSTM) <ref type="bibr" target="#b25">(Hochreiter and Schmidhuber, 1997)</ref> and Gated Recurrent Unit (GRU) <ref type="bibr" target="#b21">(Cho et al., 2014)</ref>) were the most popular model for software engineering tasks from 2000 to 2019. These kinds of models can understand a sequence of texts. <ref type="bibr" target="#b28">Iyer et al. (2016)</ref> proposed CODE-NN with the architecture consisting of LSTM guided by a global attention model <ref type="bibr" target="#b38">(Luong et al., 2015)</ref> to summarize SQL and CSharp code snippets. <ref type="bibr" target="#b26">Hu et al. (2018)</ref> published the DeepCom composed by encoder-decoder LSTMs to generate code comments for Java code functions automatically. <ref type="bibr" target="#b29">Jiang et al. (2017)</ref> developed the Neural Machine Translation (NMT) with encoder-decoder RNNs to generate commit messages from git change diffs. The model DeepAPI from <ref type="bibr" target="#b24">Gu et al. (2016)</ref> to generate API usage sequences for a given natural</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Tasks and Datasets</head><p>This section describes the supervised tasks and the corresponding datasets, and the unlabeled datasets for self-supervised tasks. Furthermore, we explain the data preprocessing methods for different languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Supervised Tasks and Datasets</head><p>In this work, we trained six supervised tasks in the software engineering domain as follows.</p><p>Code Documentation Generation: For the Code Documentation Generation supervised tasks, a code function was an input to the model, and the model generated the documentation for this function. CodeSearchNet Corpus Collection 3 <ref type="bibr" target="#b27">(Husain et al., 2019)</ref> was selected as the dataset for this task. It contains six programming languages' functions/methods, including Python, Java, Go, Php, Ruby, and Javascript. These functions with their documentation were downloaded from the Github 4 repository. We used the pre-processed version by CodeBERT 5 <ref type="bibr" target="#b23">(Feng et al., 2020)</ref>, which already provides a parsed and tokenized dataset.</p><p>Source Code Summarization: Given a short code snippet, the Source Code Summarization task generates a summary for this code. This task involves Python, SQL, and CSharp languages. We used the dataset 6 generated by <ref type="bibr" target="#b28">Iyer et al. (2016)</ref>. This dataset is extracted from StackOverflow. <ref type="bibr">7</ref> The code snippets are from accepted answers that contain exactly one code snippet, and the summarization is the corresponding title of the question. <ref type="bibr" target="#b28">Iyer et al. (2016)</ref> asked human annotators to provide two additional titles for 200 randomly chosen code snippets from the validation and test set for SQL and CSharp code. We followed their preprocessing methods and evaluation using the test dataset annotated by human annotators.</p><p>Code Comment Generation: Like Code Documentation Generation, this task focuses on generating the JavaDoc for the Java functions. We used the corpus 8 from <ref type="bibr" target="#b26">Hu et al. (2018)</ref> for this task. They focused on Javadoc comments from 9,714 Java open source projects from Github. The first sentence in the Javadoc description is extracted as the expected comment.</p><p>Git Commit Message Generation: The task Git Commit Message Generation aims to generate a commit message describing the git commit changes. The development-assisted tool Git is a version-control system for tracking changes in files and codes during software engineering. A well-structured code commit helps to overview the project development and control the code changes and the development quality. We used the dataset 9 from <ref type="bibr" target="#b29">Jiang et al. (2017)</ref> based on 1,000 Java repositories having the most Github stars. API Sequence Recommendation: We aimed to generate the API usage sequence, including the class and function names, by giving the model a natural language description as an input. It would be beneficial to suggest to developers the API sequence when searching and asking about the corresponding usages. The dataset 10 was extracted by <ref type="bibr" target="#b24">Gu et al. (2016)</ref> from Java projects with at least one star from Github.</p><p>Program Synthesis: Program synthesis is the task of synthesizing or generating programming codes based on natural language description. We used the AlgoLisp dataset 11 <ref type="bibr" target="#b41">(Polosukhin and Skidanov, 2018</ref>) for the Program Synthesis task. This dataset was extracted from homework assignments for introductory computer science courses, so each example in this dataset consisted of a question and an answer. We input the question into our model and expect the model to output the correct LISP-inspired DSL code answer. <ref type="table" target="#tab_1">Table 1</ref> compares the number of samples in training, validation, and testing datasets for each supervised learning task. We observe that the API Sequence Recommendation has the most significant number of samples. The second-largest dataset is the Code Comment Generation dataset. Four out of six tasks have the datasets extracted from GitHub. Furthermore, three out of six tasks used the function-level as an input rather than a complete program-level input.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Unlabeled Datasets</head><p>Unlabeled datasets are used in the transfer learning pre-training stage and the multi-task learning. They are helpful when building a language model for tasks in the software engineering domain and make the final model more generalized against overfitting <ref type="bibr" target="#b47">(Ruder, 2017)</ref>. We involved six different corpora for unlabeled datasets, covering nine programming languages and English. CodeSearchNet Corpus Collection: The CodeSearchNet Corpus Collection <ref type="bibr" target="#b27">(Husain et al., 2019)</ref> was used as one of the supervised tasks and as one of the self-supervised tasks. This dataset is divided into two parts -functions with the function documentation and functions without documentation. In the self-supervised training, we have used both of these parts together. For functions without documentation, we directly considered each function as a separate sentence sequence. For functions with documentation, we concatenated each pair of tokenized-function and its tokenized-documentation as one input sentence sequence. All the code samples in this corpus are function-level samples.</p><p>Public Git Archive: We used CSharp and an additional Java datasets from the Public Git Archive dataset 12 <ref type="bibr" target="#b39">(Markovtsev and Long, 2018)</ref>. The Public Git Archive has code in the file-level containing the import statements, multiple functions, and comments. Such file-level data could help the models to understand more information like API usage and the relationship between different functions.</p><p>150k Python Dataset: We included the 150k Python Dataset 13 <ref type="bibr" target="#b45">(Raychev et al., 2016)</ref> from the SRILAB at ETH Zurich. The Python programs in this data-set are collected from GitHub repositories with permissive and non-viral licenses by removing duplicate files,  <ref type="bibr">,189,162 133,191 122,602 30,913,716 38,403,607</ref>   <ref type="bibr" target="#b52">(Yao et al., 2018)</ref>. This dataset was extracted from StackOverflow. StaQC contained SQL question-code pairs of questions tagged by "SQL," "database," or "oracle" from StackOverflow. The SQL code in StaQC is code-snippet level and not the entire file-level code.</p><p>LISP: We created a new LISP dataset, with which we extracted 20 GitHub repositories 15 having the most stars from the Lisp Topic by applying the GitHub Rest API 16 and parsed the files into function-level LISP code.</p><p>One Billion Word Language Model Benchmark Corpus: Despite the programming languages, we used one Billion Word Language Model Benchmark corpus <ref type="bibr" target="#b20">(Chelba et al., 2013)</ref> as our self-supervised English data-set. Text data in one Billion Word Language Model Benchmark corpus is obtained from the WMT11 website. 17 Normalization and tokenization were applied to the data, and duplicated sentences were removed. The vocabulary was constructed by discarding all words with a count below three, and sentence order was randomized. Finally, the corpus contains almost one billion words in the training data. <ref type="table" target="#tab_3">Table 2</ref> shows the number of samples each dataset used in self-supervised learning. In total, we have around 40 million samples for this self-supervised training. One Billion Word Language Model Benchmark corpus has more than 30 million data samples and is 14. https://github.com/LittleYUYU/StackOverflow-Question-Code-Dataset 15. https://github.com/topics/lisp?o=desc&amp;s=stars 16. https://docs.github.com/en/rest/reference/repos#contents 17. http://statmt.org/wmt11/training-monolingual.tgz the corpus with the most number of samples. Among the programming language datasets, CodeSearchNet Corpus is the most extensive corpus. When comparing only the programming languages, the Java language has the most self-supervised samples with more than two million inputs. Following them, Javascript and Python, where each have more than one million samples. Ruby, SQL, and LISP have the least number of self-supervised inputs. They only have around 150,000 samples or fewer, each.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Dataset Preprocessing</head><p>We mainly followed the instructions to preprocess the datasets if these guides existed with their original publications. We used the same parsers and tokenizers as the original repositories. We first removed the programs' comments for those without instructions or guidance because they are not part of the code. We applied different parsers to parse the code and get the code structure and element for programming language samples. Also, we substituted characters, strings, and numbers (integer, real, hexadecimal) with specific tokens in the code. Furthermore, we replaced the newline characters (\n, \r, \r\n) as &lt; newline &gt;. We also tokenized each sample and concatenated tokens in the sample to a sentence by inserting a space character between every two tokens to make all the samples from different datasets consistent at the input-level. Lastly, as an important point, we added a unique prefix for each task to allow models, specifically multi-task models, to distinguish between training samples from different tasks. For example, in code documentation generation for Javascript, we added "function documentation generation javascript: " as a prefix for all samples on this task.</p><p>• English: We used the tokenize 18 package from the NLTK Natural Language Toolkit <ref type="bibr" target="#b18">(Bird, 2006)</ref>. Most of the tasks' datasets contain at least one part English sentence, so this tokenizer is used in preprocessing these English texts. However, we did not apply it to the English corpus because it was already tokenized. We have used the English tokenizer for commit messages and API sequences because we do not have a specific tokenizer for them.</p><p>• Python: For the Python functions from CodeSearchNet Corpus Collection, 19 treesitter library for Python 20 was applied in order to have fair comparison between our results and theirs. For other Python codes, we used the Python tokenize 21 library that contains functions to separate the tokens of a string and returns the token's value and the type (number, string, etc.). For the rest of the tasks, we used it because it is the official library from Python, which is well maintained and updated regulary.</p><p>• Java: For the Java functions from CodeSearchNet Corpus Collection, tree-sitter library to parse Java 22 functions was applied. In addition, for other tasks involving the Java language, we used the Python library called javalang 23 that provides a lexer and a parser targeting Java.</p><p>• Php, Go, Javascript, Ruby: These languages are all involved in the CodeSearchNet Corpus Collection. They were parsed by <ref type="bibr" target="#b27">Husain et al. (2019)</ref> using modified parsers based on the tree-sitter library for Php, 24 Go, 25 and Javascript. <ref type="bibr">26</ref> We followed their methods and didn't make any changes.</p><p>• SQL and CSharp: The Python library sqlparse 27 was used for SQL tasks. This library provides support for parsing, splitting, and formatting SQL statements. For CSharp code, ANTLR 28 (ANother Tool for Language Recognition) parser from Parr (2013) was applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CodeTrans</head><p>We explain our models, the vocabulary generation steps, and the hardware we applied in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Model</head><p>We adapted the encoder-decoder model proposed by <ref type="bibr" target="#b48">Vaswani et al. (2017)</ref> and the T5 29 framework implemented by <ref type="bibr" target="#b43">Raffel et al. (2020)</ref> to our tasks. The original T5 publication proposed five different model sizes -Small, Base, Large, 3B, and 11B. We used the small (60 million parameters), base (220 million parameters), and large model (770 million parameters) in this research. More details for the different models' parameters are shown in <ref type="table">Table 3</ref>. We set all the models' input and output length as 512 because most of the samples' length have less than 512 tokens. For the self-supervised objective, we applied the spancorruption strategy with a corruption rate of 15%. We considered a span of an average of three corrupted tokens as an entirety and used a unique mask token to replace it. For this model, the input consisted of the original input, but with some 3-gram words replaced by a unique mask token, while the target is the original corrupted 3-gram words surrounded by unique mask tokens for the uncorrupted spans. Different from the T5 models, we disabled the method of reduce concat tokens. By disabling this method, every sample will only have a single training example rather than concatenating different training examples up to the maximum training sequence length.</p><p>The T5 framework is very suitable for transfer learning, multi-task learning, and finetune the models. It has the concept of TaskRegistry and MixtureRegistry. Each task can be built as one TaskRegistry, and one or more TaskRegistries can build one MixtureRegistry. We built 13 TaskRegistries. Each programming language from each task has one 23. https://github.com/c2nes/javalang 24. https://github.com/tree-sitter/tree-sitter-php 25. https://github.com/tree-sitter/tree-sitter-go 26. https://github.com/tree-sitter/tree-sitter-javascript 27. https://github.com/andialbrecht/sqlparse 28. https://www.antlr.org/ 29. https://github.com/google-research/text-to-text-transfer-transformer  <ref type="table">Table 3</ref>: Model Parameters for different size of models, as well as the time cost, and the final loss of transfer learning and multi-task learning pre-training stage with a batch size of 4,096.</p><p>TaskRegistry. We also built one MixtureRegistry for self-supervised learning and another MixtureRegistry for multi-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Vocabulary</head><p>Vocabulary is an essential aspect of natural language processing. Vocabulary itself contains much information about the corpus, like the corpus domain, formality, tone, and target audience. Vocabulary is helpful when processing the text corpus. We need to tokenize the input into different ids mapping to the components in the vocabulary before putting the text into the model. It is also the storage to construct the output of the model. The choice of vocabulary has a critical impact on model performance and output quality. Furthermore, the token frequency in the vocabulary also indicates the different importance of the text information.</p><p>We used the SentencePiece model <ref type="bibr" target="#b31">(Kudo, 2018)</ref> to construct the vocabulary for this research, as well as to decode and encode the input/output. SentencePiece provides different tokenization methods, including the sub-word level tokenization. It extracts sub-words containing the semantic meanings and overcomes the drawback of the character level tokenization. The vocabulary generated could cover almost all the texts in the datasets. This is better than the word level tokenization, which requires an enormous vocabulary to cover most words in the datasets. We trained the SentencePiece on all the labeled and unlabeled datasets used in our experiment with the unigram language model algorithm. We set the id for padding token as 0, end of statement (EOS) token as 1, Unknown token as 2, and beginning of statement (BOS) token as 3. We set the size of the vocabulary to 32,000. The whole datasets have more than 46 million lines (each line could be considered one model input example and one SentencePiece input sentence). It is tremendous when using the unigram language model algorithm and would cause the training crash for training on the whole sentences. Therefore, we limited the "input sentence size" to 40 million, shuffled the input sentences to get random sentence inputs, and enabled the setting for training a huge corpus. We set the character coverage as 0.9999 because the corpus may contain non-English characters or meaningless symbols. In this way, we could exclude these noises from the vocabulary.</p><p>We noticed a significant amount of tokens of tokens indicating the programming languages and processes from the generated vocabulary, including "function," "String," "var," "import," etc. Furthermore, it covered most of the English vocabulary. This means our generated vocabulary is suitable for both natural language processing tasks and software engineering tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Hardware</head><p>We utilized both Graphics Processing Units (GPUs) and Tensor Processing Units (TPUs) for training and evaluating the models. We used one NVIDIA GPU Quadro RTX 8000, 30 which has 576 NVIDIA Tensor Cores, 72 NVIDIA RT Cores, and 48 GB GDDR6 with ECC GPU memory. We used this GPU for all the single-task learning for small models and for part of the base models. We had two types of Google TPUs, v2-8, and v3-8. We obtained access to two TPUs v2-8 through the Google Colab notebooks 31 and multiple TPUs v3-8 using Google Cloud console. TPUs v3-8 are mainly used for multi-task learning, transfer learning pre-training, and fine-tuning models for large datasets. Moreover, TPUs v2-8 are applied for single-task training for the base model and fine-tuning the pre-trained models on relatively small datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We clarify our experiment details in this section, including single-task learning, transfer learning, multi-task learning, and multi-task learning with fine-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Single-Task Learning</head><p>For single-task learning, we trained the six tasks (13 sub-tasks in total) separately using the T5 framework. We trained both small and base size models, which generated two models for each task and, in total, 26 models. We tuned the batch size using the grid search inside the range of 2 5 and 2 10 . We determined the training steps using early stopping concerning the models' performance on the validation sets based on the T5 built-in BLEU <ref type="bibr" target="#b42">(Post, 2018)</ref> and ROUGE <ref type="bibr" target="#b34">(Lin, 2004)</ref> scores. The optimal training steps with the corresponding batch size are listed in <ref type="table" target="#tab_8">Table 4</ref>.</p><p>We noticed the following points during the single-task training:</p><p>• The number of samples in a data-set has an essential impact on the model size and the training steps. Task API Sequence Recommendation and Code Comment Generation have the two largest datasets. The small models for these two tasks require almost seven times more training steps than the base models until they could converge. • Corpus for Source Code Summarization converges extremely fast. For SQL and CSharp data-sets in this corpus, the base model converges in 500 training steps even if the batch size is only 32, and the model has not seen the complete dataset yet. The scores on this task's validation set become worse if we train the model with more steps. So it is very easy to overfit the models for the Source Code Summarization task, mainly because of its small dataset size.</p><p>• Half of the models achieve the best performance with a batch size of 256. However, it varies slightly among different tasks, like the Source Code Summarization task requiring small batch sizes. Nevertheless, large batch sizes do not result in better performance, no matter the number of samples in the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Transfer Learning</head><p>Transfer Learning has two steps, pre-training and fine-tuning. The first uses the selfsupervised method on unlabeled data, and the latter fine-tunes the models on supervised tasks using labeled datasets. We trained the small, base, and large T5 models for transfer learning.</p><p>All the self-supervised tasks were used and combined in the pre-training step. We set the T5 model to mask the spans of input data by enabling, which makes the model predict the masked content and builds an initial language model in this way. Since our pretrained models used the datasets containing nine programming languages and one human language, these models are suitable to be fine-tuned on other downstream tasks in the software development domain and natural language processing domain. This can be seen as an expansion to current multi-language language models, which focuses on having a single model that supports only multiple human languages <ref type="bibr" target="#b50">Xue et al. (2020)</ref>. For per-training, we chose the batch size of 4096. We pre-trained the small and base model for 500,000 steps and the large model for 240,000 steps. Furthermore, we mainly used single TPU v3-8 in the pre-training. <ref type="table">Table 3</ref> shows the final loss and training time for different sizes of CodeTrans models during the pre-training.</p><p>After obtaining the pre-training model on the 500,000 training steps for the small and base models and 240,000 steps for the large model, we fine-tuned the models for the 13 supervised sub-tasks. We have noticed that half of the single-task learning models reach their best performance with a batch size of 256. So we chose 256 as the batch size for fine-tuning the downstream tasks. We applied early stopping to determine the fine-tuning steps based on the models' performance on the validation sets using the T5 built-in BLEU and ROUGE scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Multi-Task Learning</head><p>Multi-task learning trains a single model on a mixture of tasks, allowing sharing all model parameters across different tasks. This training strategy improves the data augmentation, focuses attention, and shares information for eavesdropping, learning the representation, and regularizes the weights <ref type="bibr" target="#b47">(Ruder, 2017)</ref>. Furthermore, it allows a single model to perform more than one task using the same weights. We trained 13 supervised sub-tasks together with all the self-supervised tasks. The self-supervised tasks are desired to help the model  The single-task learning (ST) training steps, transfer learning fine-tuning (TF-FT) steps, and multi-task learning fine-tuning (MT-FT) steps for the small, base, and large models. We also listed the batch size of each task in this table. The batch sizes of single-task learning are different for different tasks. However, the batch size of transfer learning and multi-task learning are both 256.</p><p>gain information about the language attributes and build a language model in both the software development domain and human language. This allows the model to understand both human written languages, in our case the English language, and the computer code languages. Simultaneously, the supervised tasks help each other make the model more generalized for all the tasks and avoid overfitting on each specific task.</p><p>We used examples-proportional mixing to select samples in proportion to the size of each task's dataset and concatenated them. This ensures that the model will see samples from small datasets as it will see samples from large datasets on every batch. We recorded the model checkpoint every 20,000 training steps, using a batch size of 4,096. Usually, all the tasks should share one same best performance checkpoint. However, <ref type="bibr" target="#b43">Raffel et al. (2020)</ref> proposed a way to relax this goal and select a different checkpoint for each task. We also evaluated the model on the validation set and selected the best checkpoint for each task. Therefore, each task could have a different checkpoint from the same model. We trained T5 small, base, and large models using only single TPU v3-8. <ref type="table">Table 3</ref> illustrate the training steps, final loss, and the time cost for the multi-task learning models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Multi-Task Learning with Fine-Tuning</head><p>We further fine-tuned the multi-task learning final checkpoint, 500k steps for the small and base model, and 260k steps for the large model, for each supervised task separately. Like the transfer learning fine-tuning, we chose the batch size of 256 and applied early stopping to determine the fine-tuning steps based on the models' performance on the validation datasets. <ref type="table" target="#tab_8">Table 4</ref> shows all the related hyper-parameters for the multi-task learning fine-tuning step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Result</head><p>We ran the final evaluation on the test dataset, and we compared the performance of CodeTrans models with different state-of-the-art models for each task. Therefore, we applied the same metrics as other SOT models for calculating the evaluation results, as shown in <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Code Documentation Generation</head><p>We evaluated the Code Documentation Generation tasks using smoothed BLEU-4 score <ref type="bibr" target="#b35">(Lin and Och, 2004)</ref> on the CodeSearchNet test dataset. The evaluation results are shown in <ref type="table">Table 5</ref>, and were compared with CodeBert <ref type="bibr" target="#b23">(Feng et al., 2020)</ref>. Overall, we outperformed CodeBert on all the programming languages in this task. Multi-task learning has, in general, the best performance and achieves the best result for three programming languages. The possible reason for this is that the CodeSearchNet dataset is involved in two self-supervised tasks during the multi-task training, and it has seen many training data related to this task. Transfer learning and multi-task learning fine-tuning CodeTrans models also have relatively good performance and are much better than single-task learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Source Code Summarization</head><p>We also applied smoothed BLEU-4 to evaluate the Source Code Summarization task. We compared CodeTrans performance with Code-NN <ref type="bibr" target="#b28">(Iyer et al., 2016)</ref>. Unfortunately, Code-NN did not provide their result for the Python code; however, we did evaluate it for future comparisons. By evaluating the SQL and CSharp code, Code-NN selected 100 samples with two additional human annotations and calculated the smoothed BLEU-4 on these samples. We followed their instruction and evaluated in the same way to have a fair comparison. CodeTrans outperformed Code-NN on the existing scores as shown in <ref type="table">Table 5</ref>. Among different CodeTrans models, multi-task learning has the best performance on two of the programming languages for this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Code Comment Generation</head><p>We compared CodeTrans' performance with DeepCom using smoothed BLEU-4 for the task Code Comment generation, with the results shown in <ref type="table">Table 5</ref>. CodeTrans transfer learning large model has the best performance, and its smoothed BLEU score is higher than DeepCom by more than one percent. CodeTrans models with multi-task learning have the worst performance. However, the score increases with the increase of model size. The reason is the Code Comment Generation dataset has the second-largest sample size with 470,451 samples, and we need bigger models to exploit them all.   <ref type="table">Table 5</ref>: Evaluation results of all the tasks in this paper. CCG means the task Code Comment Generation. Git-Gen is Git Commit Message Generation. API-Gen stands for the task API Sequence Recommendation. PS is the task Program Synthesis.</p><p>ST stands for single-task learning, TF is transfer learning, MT means multi-task learning, and MT-FT is multi-task learning with fine-tuning. We evaluate the CodeTrans on the test dataset and compare the performance of CodeTrans with state-of-the-art models. We applied smoothed BLEU-4 for Code Documentation Generation, Source Code Summarization, and Code Comment Generation. We use BLEU-4 for Git Commit Message Generation and API Sequence Recommendation, and accuracy for Program Synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Git Commit Message Generation</head><p>The task Git Commit Message Generation's evaluation result is listed in <ref type="table">Table 5</ref>. We applied the BLEU-4 <ref type="bibr" target="#b42">(Post, 2018)</ref> for the evaluation to compare it with previous research results. All the CodeTrans models, including single-task training, outperform the NMT model. Among them, the CodeTrans transfer learning large model has the best BLEU-4 score. The performance of the CodeTrans multi-task learning large model is very close to the transfer learning large model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">API Sequence Recommendation</head><p>Table 5 also contains the task API Sequence Recommendation evaluation result. We compared the CodeTrans models with the DeepAPI model. We applied the same BLEU-4 metric script as the DeepAPI used. All the CodeTrans models, including single-task training, outperform the DeepAPI model. Among the CodeTrans models, those trained using only multi-task learning performed the most poorly mainly because of this task's large data-set. However, with increasing multi-task learning model size, it started to reach a close/better performance than single-task learning and transfer learning. The CodeTrans large model with multi-task learning fine-tuning has the highest scores across all the models. The CodeTrans transfer learning large model also has a similarly good performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Program Synthesis</head><p>We used accuracy when evaluating the Program Synthesis task. This accuracy calculates whether our model output is precisely the same as the golden reference. Seq2Tree used the code accuracy to count how many of the model's outputs can pass the code tests. If the model output is identical to the reference, this output could pass the code tests. As shown in <ref type="table">Table 5</ref>, nine out of ten CodeTrans models outperform the Seq2Tree model. The CodeTrans multi-task learning fine-tuning small model achieves the best score on accuracy. For multi-task learning, the performance increases along with the model size. However, for transfer learning and multi-task fine-tuning, smaller models perform better. Generally, this task's scores are very high, which means that this is an easy task with similar validation and test sets, and bigger models may be easy to be overfitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Reflection</head><p>We discuss our results and provides a conclusion in this section. We also list our future work here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Discussion</head><p>Our CodeTrans models with the transformer architecture with both encoder and decoder outperform the thirteen sub-tasks' baseline models. This proves the effectiveness of the transformer encoder-decoder architecture for these tasks in the software engineering domain, mainly when we utilize transfer learning and multi-task learning methods with selfsupervised learning. Nevertheless, the models' performance varies a bit when using different training strategies for different sizes of models on different datasets.</p><p>(a) The Code Comment Generation task's training dataset has 470,486 samples.</p><p>(b) The Source Code Summarization -SQL task's training dataset has 22,492 samples. <ref type="figure">Figure 1</ref>: The evaluation of multi-task learning checkpoints on the validation set for two tasks. The x-axis lists the training steps. The y-axis is the T5 built-in BLEU score. Different colors indicate different sizes of models.</p><p>We noticed that the model size plays an essential role in the model's performance. For single-task learning, the larger the dataset is, the fewer training steps a bigger model requires. A bigger model reaches a lower loss under the same batch size and the same evaluation steps when applying the multi-task learning or transfer learning strategy. Although the pre-training may cost more time for bigger models, they need fewer iteration steps during fine-tuning for each task than the small models. As a result, for most of the tasks, the bigger the model is, the better the evaluation scores the model could achieve with even less fine-tuning time.</p><p>The evaluation results also prove that transfer learning and multi-task learning with fine-tuning strategies outperform the models that only used single-task learning on all the tasks. The performance of models using transfer learning is very similar to those using multi-task learning fine-tuning. It is hard to say which one is better. However, transfer learning does not require the task dataset to be involved in the pre-training steps. For a new task, the dataset only needs to be trained for relatively few fine-tuning steps, while multi-task learning with fine-tuning needs the new task dataset during pre-training. We can clearly say that transfer learning would save time and provide better results for a new task when fine-tuned on a pre-trained model checkpoint.</p><p>The performance of multi-task learning depends heavily on the data size and attributes of the task itself. For large datasets like the datasets for the task Code Comment Generation and API Sequence Recommendation, multi-task learning models are even worse than the models that only applied single-task learning as shown in <ref type="table">Table 5</ref>. <ref type="figure">Figure 1a</ref> shows that the model's performance improves significantly when we increase the model size for the Code Comment Generation task with a large dataset. Half a million multi-task training steps are not enough for this task, even using the large model. When the dataset is tiny and easy to be overfitted, small multi-task models could achieve the best result while bigger setups do not always lead to better performance. This is due to the higher probability of overfitting the small dataset when more parameters are used, even when we used regularization methods like a dropout (10% in all our models). <ref type="figure">Figure 1b</ref> shows that the Golden Reference datetime implementation in php mysql <ref type="table">Table 6</ref>: The models' output for an example of the task Source Code Summarization. We compared different CodeTrans model outputs and the golden reference for the input SQL code "select time ( col0 ) from tab0". The golden reference is the one extracted from the StackOverflow.</p><p>base model performs better overall than the small models for the source code summarization -SQL task, but the large model has several overlaps with the base model. The large model has a sign of overfitting after 120,000 training steps, and the model's performance decreases after that point. <ref type="table">Table 6</ref> lists each CodeTrans model's outputs compared with the golden reference extracted from the StackOverflow. The input for the models is "select time ( col0 ) from tab0". We can observe that all the models' outputs are readable sentences. The majority of them have a question format. Because the dataset contains questions and answers from Stack-Overflow, the models have learned how to ask questions. Outputs from single-task learning models do not make much sense. The transfer learning and multi-task learning outputs all notice that this code is about time. All the multi-task learning models also specified the mysql database system. The CodeTrans multi-task learning large model mentions the keyword datetime, which also appears in the golden reference. Besides, the transfer learning and multi-task learning fine-tuning base models have reasonable outputs as well. The CodeTrans transfer learning and multi-task learning fine-tuning models focus more on the code function and structure to summarize this code snippet. In total, our judgement for the models' performances matches the ranking of our evaluation metrics. For more examples, we chosen one example from each task and list all the models' output for this example in the Appendix.</p><p>Moreover, most of the Code Documentation Generation tasks achieved the best evaluation performance when using the multi-task learning strategy. It is possible that we have two more self-supervised tasks from the same CodeSearchNet corpus during the multi-task learning. These give more similar samples for the supervised Code Documentation Generation tasks so that the model would focus on performing better for these tasks. Furthermore, using different types of tasks during multi-task learning efficiently avoids overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Conclusion</head><p>This paper explores the CodeTrans models with transformer encoder-decoder architecture on six main tasks and, in total, thirteen sub-tasks in the software engineering domain covering nine programming languages. We have carried out experiments with different training strategies, including single-task learning, transfer learning, multi-task learning, and multi-task learning with fine-tuning. We applied different models' sizes based on the Text-To-Text Transfer Transformer framework by utilizing NVIDIA GPUs and Google Cloud TPUs.</p><p>Our CodeTrans models outperform all the baseline models and achieve the state of the art over all the tasks. Our experiments on various tasks provide us with many insights about training a neural network model on software engineering-relevant tasks. First, we find that larger models can bring a better model performance. Second, models with transfer learning perform as well as models with multi-task learning fine-tuning, and the pre-training models can be fine-tuned on the new downstream tasks efficiently while saving a significant amount of training time. Moreover, multi-task learning is very beneficial for the small dataset on which the model will overfit easily. It is very promising that these experiences can be generalized for training natural language processing tasks on different domains.</p><p>In addition to these findings, we published our models on the Hugging Face Hub so that everyone can access our models and use them for their purposes. We also provided online the pre-trained checkpoints generated from our CodeTrans during transfer learning pre-training and multi-task learning. These checkpoints are suitable for fine-tuning other software engineering tasks if the task's programming language is covered in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Future Work</head><p>When working on the Code Documentation Generation tasks, we have noticed that a programming language function has two aspects influencing the model performance: the function names/parameter names and the code structure. A well-named function would lower the difficulty for the model to generate the documentation. Further investigation about functions with disguised parameter names or function names would be valuable. In this work, we considered a function as a sentence; from this aspect, we do not fully make use of the code structure, so how to present the code is also a good research point. Experiments about finding the best way to present the features of code structure can be carried out.</p><p>We preprocessed the datasets by parsing and tokenizing the programming codes using different Python libraries for each programming language. So when using our models, applying the same preprocessing way would draw the best results. Nevertheless, not every user is a programming expert, and the preprocessing increases the complexity for users to get the best model performance. It would be meaningful to examine the effect of preprocessing for the software engineering tasks and train models with good performance, but without preprocessing like parsing and tokenizing.</p><p>Moreover, more tasks can be explored using transformer encoder-decoder architecture. It would be interesting to examine our models' performance on the unseen programming languages as <ref type="bibr" target="#b23">Feng et al. (2020)</ref> did in their experiments. Furthermore, testing the pretrained language models on human language tasks like SQuAD dataset <ref type="bibr" target="#b44">Rajpurkar et al. (2016)</ref> could be of interest. Evaluation could be run directly on similar tasks with an unseen programming language using the multi-task learning CodeTrans models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Model Output Examples for each Task.</head><p>For each sub-task we list one example containing all the models' output, the input, and the standard reference in this Appendix. These examples are all randomly selected from the supervised tasks test datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Code Documentation Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Code Documentation Generation -Python</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Code Documentation Generation -Java</head><p>Model Size Model Output</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CodeTrans</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-Task Learning</head><p>Small Casts a Cast to another type Base Casts a value into a function that returns the same type as the supplied parameter .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CodeTrans Transfer Learning</head><p>Small Casts the given class to a function . Base Casts a function to a class Large Cast a value to the given target type .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CodeTrans</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Learning</head><p>Small Casts the given class to the given target class .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base</head><p>Returns a function that casts the given value to the given type . Large Creates a function that performs a cast to the given class .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CodeTrans</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Learning Fine-Tuning</head><p>Small Creates a function that casts the given object to the target type .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base</head><p>Returns a function that casts the given object to the given class . Large Returns a function that casts its input type to the target type .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Code Documentation Generation -Go</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Code Documentation Generation -Ruby</head><p>Model Size Model Output</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CodeTrans</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Single-Task Learning</head><p>Small Log a message at the given level if the logger is present Base Writes a log message if the current log level is at or below the supplied severity .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CodeTrans Transfer Learning</head><p>Small Create an IO object at the given level with the given block . This method will not be called directly if the IO object at that level . Base Writes a message of the given severity to the log . Large Writes a message to the log device .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CodeTrans</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Learning</head><p>Small Add a message to the log . Base</p><p>Add a message to the logger . Large Add a message to the log .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CodeTrans</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Task Learning Fine-Tuning</head><p>Small Writes a message to the log with a given severity .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base</head><p>We need to overload the add method . Basibally it is the same as the original one but we add our own log format to it . Large Writes a message if the severity is high enough . This method is executed asynchronously . </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>5. https://github.com/microsoft/CodeBERT 6. https://github.com/sriniiyer/codenn 7. https://stackoverflow.com/ 8. https://github.com/xing-hu/DeepCom 9. https://sjiang1.github.io/commitgen/ 10. https://github.com/guxd/deepAPI 11. https://github.com/nearai/program_synthesis/tree/master/program_synthesis/algolisp</figDesc><table><row><cell>Task</cell><cell>Language</cell><cell>Train</cell><cell>Valid</cell><cell>Test</cell><cell>Data Source</cell><cell>Data Level</cell></row><row><cell></cell><cell>Python</cell><cell cols="3">251,820 13,914 14,918</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Java</cell><cell>164,923</cell><cell cols="2">5,183 10,955</cell><cell></cell><cell></cell></row><row><cell>Code Documentation Generation</cell><cell>Go Php</cell><cell cols="3">167,288 241,241 12,982 14,014 7,325 8,122</cell><cell>GitHub</cell><cell>Function</cell></row><row><cell></cell><cell>Ruby</cell><cell>24,927</cell><cell>1,400</cell><cell>1,261</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Javascript</cell><cell>58,023</cell><cell>3,885</cell><cell>3,291</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Python</cell><cell>12,004</cell><cell>2,792</cell><cell>2,783</cell><cell></cell><cell></cell></row><row><cell>Source Code Summarization</cell><cell>Csharp</cell><cell>52,943</cell><cell>6,601</cell><cell>108</cell><cell cols="2">StackOverflow Code Snippet</cell></row><row><cell></cell><cell>SQL</cell><cell>25,671</cell><cell>3,326</cell><cell>100</cell><cell></cell><cell></cell></row><row><cell>Code Comment Generation</cell><cell>Java</cell><cell cols="3">470,451 58,811 58,638</cell><cell>GitHub</cell><cell>Function</cell></row><row><cell cols="2">Git Commit Message Generation Java</cell><cell>26,208</cell><cell>3,000</cell><cell>3,000</cell><cell>GitHub</cell><cell>Commit</cell></row><row><cell>API Sequence Recommendation</cell><cell>Java</cell><cell>7,475,850</cell><cell cols="2">-10,000</cell><cell>GitHub</cell><cell>API</cell></row><row><cell>Program Synthesis</cell><cell>DSL</cell><cell cols="2">79,214 10,819</cell><cell>9,967</cell><cell>Homework</cell><cell>Function</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>The summarization of supervised datasets. It includes the number of samples in training, validation and testing data-sets, data source, and the programming data level for each supervised dataset. Each sample could be a code function, a code snippet, a commit diff, or a natural language sentence, depending on the data level.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>: The number of samples of each unlabeled data-set for different programming</cell></row><row><cell>languages and the English natural language. The first column lists the languages. For</cell></row><row><cell>programming languages, each sample can be considered as one function or a programming</cell></row><row><cell>file, or part of the code, depending on the code level of that dataset. For the English</cell></row><row><cell>language, one sample means one sentence.</cell></row><row><cell>forked projects, and obfuscated files. The Python code in this corpus is also a file-level code</cell></row><row><cell>like Public Git Archive datasets.</cell></row><row><cell>StaQC: For the SQL unlabeled dataset, we chose StaQC 14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>true if the Progress . Match field of ProgressState == ProgressStateSnapshot and pr . Match ¡ pr . PendingSnapshot . Base needSnapshotAbort returns true if the progress is in a snapshot and the match exceeds the pending snapshot . Large needsnapshotabort returns true if we need to abort a snapshot . Code Snippet as Input public static function update ( $ table ) { if ( ! is array ( $ table ) ) { $ table = json decode ( $ table , true ) ; } if ( ! SchemaManager :: tableExists ( $ table [ 'oldName' ] ) ) { throw SchemaException :: tableDoesNotExist ( $ table [ 'oldName' ] ) ; } $ updater = new self ( $ table ) ; $ updater -¿ updateTable ( ) ; } }</figDesc><table><row><cell>Model</cell><cell>Size</cell><cell>Model Output</cell></row><row><cell>CodeTrans</cell><cell cols="2">Small needSnapshotAbort returns true if we need to roll a snapshot .</cell></row><row><cell>Single-Task Learning</cell><cell>Base</cell><cell>needSnapshotAbort returns true if we need to call snapshot and false otherwise .</cell></row><row><cell cols="3">CodeTrans Transfer Learning Small needSnapshotAbort returns CodeTrans Small needSnapshotAbort returns true if the current state is a snapshot . Base needSnapshotAbort returns true if we need to abort the snapshot . Multi-Task Learning Large needsnapshotabort returns true if we need to abort a snapshot .</cell></row><row><cell>CodeTrans Multi-Task Learning Fine-Tuning</cell><cell cols="2">Small needSnapshotAbort returns true if the progress needs to be aborted . Base needSnapshotAbort returns true if we need to abort the snapshot Large needsnapshotabort returns true if we need to abort the snapshot .</cell></row><row><cell>Code Snippet as Input</cell><cell></cell><cell>func ( pr * Progress ) needSnapshotAbort ( ) bool { return pr . State == ProgressStateSnapshot &amp;&amp; pr . Match ¿= pr . PendingSnapshot }</cell></row><row><cell>Golden Reference</cell><cell></cell><cell>needSnapshotAbort returns true if snapshot progress s Match is equal or higher than the pendingSnapshot .</cell></row><row><cell cols="3">A.4 Code Documentation Generation -Php</cell></row><row><cell>Model</cell><cell>Size</cell><cell>Model Output</cell></row><row><cell>CodeTrans</cell><cell cols="2">Small Update table with specific data</cell></row><row><cell>Single-Task Learning</cell><cell>Base</cell><cell>Call when table is constructed</cell></row><row><cell>CodeTrans Transfer Learning</cell><cell cols="2">Small Updates a table . Base Calls the updateTable method to update a table . Large Updates a table .</cell></row><row><cell>CodeTrans Multi-Task Learning</cell><cell cols="2">Small Update a table . Base Updates a table . Large updates a database table .</cell></row><row><cell>CodeTrans Multi-Task Learning Fine-Tuning</cell><cell cols="2">Small Update a table . Base Updates a table . Large updates a database table .</cell></row><row><cell>Golden Reference</cell><cell></cell><cell>Update the table .</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>3 Source Code Summarization -SQLAppendix D. Git Commit Message Generation</head><label></label><figDesc>Code Snippet as Input def add ( severity , progname , &amp; block ) return true if io . nil? --severity ¡ level message = format message ( severity , progname , yield ) MUTEX . synchronize { io . write ( message ) } true end Golden Reference Add a message to the logger with the given severity and progname . Small Renders the given URI into a string . Override this if you want to do something special to convert the URI into ASCII . Base Render a URI Large Renders the given URI using the encoding scheme in ISO-8859-1 . Large Started work on the icq / aim protocol provider implementation Code Snippet as Input new file mode 100644 index 000000000 . . 892fda21b Binary files / dev / null and b / src / plugins / gateway / lib / joscar . jar differ</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell>Size</cell><cell>Model Output</cell></row><row><cell>CodeTrans</cell><cell></cell><cell></cell><cell>Small mysql : how to get the difference of a column in a table ?</cell></row><row><cell>Single-Task Learning</cell><cell></cell><cell></cell><cell>Base</cell><cell>how do i get the average of a date range in sql server 2005 ?</cell></row><row><cell>CodeTrans Transfer Learning</cell><cell></cell><cell></cell><cell>Small how to get the time in milliseconds since the start time of the transaction was taken ? Base how to get current date time in sql server ? Large mysql time ( ) function</cell></row><row><cell>CodeTrans Multi-Task Learning</cell><cell></cell><cell></cell><cell>Small how to get the time in mysql ? Base how can i get the time of a date in mysql ? Large how to convert datetime to time in mysql ?</cell></row><row><cell cols="4">CodeTrans Multi-Task Learning Fine-Tuning</cell><cell>Small how to get the correct time from mysql database ? Base how to convert date to time in mysql ? Large select time from mysql table</cell></row><row><cell>Code Snippet as Input</cell><cell></cell><cell></cell><cell>select time ( col0 ) from tab0</cell></row><row><cell>Golden Reference</cell><cell></cell><cell></cell><cell>datetime implementation in php mysql</cell></row><row><cell cols="4">Appendix C. Code Comment Generation</cell></row><row><cell>Model</cell><cell></cell><cell>Size</cell><cell>Model Output</cell></row><row><cell>CodeTrans</cell><cell></cell><cell cols="2">Small Renders the URI into its textual representation .</cell></row><row><cell>Single-Task Learning</cell><cell></cell><cell>Base</cell><cell>Renders the URI .</cell></row><row><cell>CodeTrans</cell><cell></cell><cell></cell></row><row><cell>Transfer Learning</cell><cell></cell><cell></cell></row><row><cell>CodeTrans Multi-Task Learning</cell><cell></cell><cell cols="2">Small Renders a URI . Base Renders the given URI into a string . Large Renders the URI as an ASCII string .</cell></row><row><cell cols="2">CodeTrans Multi-Task Learning Fine-Tuning</cell><cell cols="2">Small Render the URI as ASCII string . Base Renders a URI as a string , escaping special characters . Large Render the given URI .</cell></row><row><cell>Code Snippet as Input</cell><cell></cell><cell></cell><cell>protected String renderUri ( URI uri ) return uri . toASCIIString ( ) ;</cell></row><row><cell>Golden Reference</cell><cell></cell><cell></cell><cell>Render the URI as a string</cell></row><row><cell>Model</cell><cell cols="2">Size</cell><cell>Model Output</cell></row><row><cell>CodeTrans</cell><cell cols="3">Small Added missing Jungtaeks to log4j . jar</cell></row><row><cell>Single-Task Learning</cell><cell cols="2">Base</cell><cell>Added missing jar file</cell></row><row><cell>CodeTrans Transfer Learning</cell><cell cols="3">Small Adding missing joscar jar Base Added joscar . jar to the lib directory of the Gateway project Large added missing joscar jar</cell></row><row><cell>CodeTrans Multi-Task Learning</cell><cell cols="3">Small add joscar jar Base Add joscar jar Large Add joscar</cell></row><row><cell>CodeTrans Multi-Task Learning Fine-Tuning</cell><cell cols="3">Small Adding missing jar file Base Added joscar jar</cell></row><row><cell>Golden Reference</cell><cell></cell><cell></cell><cell>Added joscar JAR</cell></row></table><note>B.</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors primarily thank Tim Karl (TUM) and Jian Kong (TUM) for their invaluable help with hardware and software; Inga Weise and Aline Schmidt (both TUM) for support with many other aspects of this work. Thanks for invaluable support and feedback from NVIDIA, in particular to Ulrich Michaelis, Ada Sedova, Geetika Gupta, Axel Koehler, Frederic Pariente, Jonathan Lefman, and Thomas Bradley. From Google, we would like to deeply thank Jamie Kinney, Alex Schroeder, Nicole DeSantis, Andrew Stein, Vishal Mishra, Eleazar Ortiz, Nora Limbourg, Cristian Mezzanotte, and all TFRC Team for their invaluable support to set up our projects on Google Cloud and solve all the related Google TPU and servers issues. All CodeTrans models could not be easily publicly accessible without the amazing support from the Hugging Face team; that is why we are very grateful to Patrick von Platen, Julien Chaumond, and Clément Delangue from Hugging Face. Last, but not least, thanks to all those who deposit their experimental data in public databases, and to those who maintain these databases.</p><p>This work was supported by a grant from Software Campus through the German Ministry for Research and Education (BMBF: Bundesministerium fuer Bildung und Forschung). We gratefully acknowledge the support of NVIDIA Corporation with the donation of two Titan GPU used for this research development phase. We also want to thank LRZ (Leibniz Rechenzentrum) for providing us access to DGX-1(V100) for the testing phase.</p><p>Finally and most importantly, this research used resources of TPUs under TensorFlow Research Cloud grant. Furthermore, Rostlab acknowledge support from Google Cloud and Google Cloud Research Credits program to fund this project under Covid19 grants.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. API Sequence Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Size Model Output</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix F. Program Synthesis</head><p>All the CodeTrans models for this task receive more than 90% accuracy, because all the samples in the tasks have very similar questions and answers in the train, validation, and test dataset. Therefore, the models could give exactly the same outputs as the reference.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Small Returns true if the browser environment is a standard browser environment . Base Checks if the current browser environment is a standard browser environment</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Small Standard browser environment has a notion of what React Native does not support it . Base Check if the browserEnv is standard</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">{ if ( typeof navigator !== &apos;undefined&apos; &amp;&amp; ( navigator . product === &apos;ReactNative&apos; --navigator . product === &apos;NativeScript&apos; --navigator . product === &apos;NS&apos; ) ) { return false ; } return ( typeof window !== &apos;undefined&apos; &amp;&amp; typeof document !==</title>
	</analytic>
	<monogr>
		<title level="m">Code Snippet as Input function isStandardBrowserEnv (</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Small Python : Compare files containing a folder Base How to copy a text file into a variable ?</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Small How to write a string to a file in Python ? Base How to write a text file to a text file in Python ? Large How to remove a line from a file in python ? CodeTrans Multi-Task Learning Fine-Tuning Small How to include text in a file using Python ? Base Python</title>
		<imprint/>
	</monogr>
	<note>Include this text in a file</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">Code Snippet as Input with open ( CODE STRING , CODE STRING ) as in file : buf = in file . readlines ( ) with open ( CODE STRING , CODE STRING ) as out file : for line in buf : if line ==</title>
		<imprint/>
	</monogr>
	<note>Include this text &quot; : line = line + &quot; Include below &quot; out file . write ( line</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Small How to convert C # DateTime to C # Base How to convert DateTime to DateTime format in C # ? CodeTrans Transfer Learning Small How to convert double to DateTime in C # ? Base Convert double to DateTime Large How to convert double to datetime ? CodeTrans Multi-Task Learning Small How to convert unix timestamp to local time in C # ? Base How to convert a double to a DateTime in C # ? Large How to convert double to datetime in C # ? CodeTrans Multi-Task Learning Fine-Tuning Small How to convert double to DateTime in C # ? Base How to convert double to DateTime in C # ?</title>
	</analytic>
	<monogr>
		<title level="m">DateTime ParseUnixDateTime ( double unixTime ) { var dt = new DateTime ( CODE INTEGER CODE INTEGER , CODE INTEGER , CODE INTEGER , CODE INTEGER , CODE INTEGER , CODE INTEGER</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">AddSeconds ( unixTimeStamp )</title>
		<imprint/>
	</monogr>
	<note>ToLocalTime ( ). return dt</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Small Color . getRed Color . getGreen Color . getBlue Base Color . getRed Color . getGreen Color . getBlue</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">CodeTrans Multi-Task Learning Fine-Tuning Small Color . getRed Color . getGreen Color . getBlue Base Color . getRed Color . getGreen Color . getBlue</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Small</title>
		<imprint/>
	</monogr>
	<note>map a [ partial1 b -</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">CodeTrans Transfer Learning Small</title>
		<imprint/>
	</monogr>
	<note>map a [ partial1 b -</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Large</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>map a [ partial1 b -</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">CodeTrans Multi-Task Learning Small</title>
		<imprint/>
	</monogr>
	<note>map a [ partial1 b -</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Large</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>map a [ partial1 b -</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">CodeTrans Multi-Task Learning Fine-Tuning Small</title>
		<imprint/>
	</monogr>
	<note>map a [ partial1 b -</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Large</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>map a [ partial1 b -</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Code Snippet as Input you are given an array of numbers a and a number b , compute the difference of elements in a and b Golden Reference</title>
		<imprint/>
	</monogr>
	<note>map a [ partial1 b -</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Nltk: the natural language toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions</title>
		<meeting>the COLING/ACL 2006 Interactive Presentation Sessions</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="69" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pretraining of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaocheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08155</idno>
		<title level="m">A pre-trained model for programming and natural languages</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep api learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongmei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunghun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering</title>
		<meeting>the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="631" to="642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep code comment generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/ACM 26th International Conference on Program Comprehension (ICPC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="200" to="20010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Codesearchnet challenge: Evaluating the state of semantic code search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamel</forename><surname>Husain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho-Hsiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiferet</forename><surname>Gazit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09436</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Summarizing source code using a neural attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinivasan</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvin</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2073" to="2083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatically generating commit messages from diffs using neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameer</forename><surname>Armaly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Collin</forename><surname>Mcmillan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 32nd IEEE/ACM International Conference on Automated Software Engineering (ASE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="135" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language modeling in meeting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Kombrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Burget</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twelfth annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10959</idno>
		<title level="m">Subword regularization: Improving neural network translation models with multiple subword candidates</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised translation of programming languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baptiste</forename><surname>Roziere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lowik</forename><surname>Chanussot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03511</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Orange: a method for evaluating automatic evaluation metrics for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz Josef</forename><surname>Och</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2004: Proceedings of the 20th International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="501" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11504</idno>
		<title level="m">Multi-task deep neural networks for natural language understanding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Public git archive: A big code dataset for all</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Markovtsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Waren</forename><surname>Long</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th International Conference on Mining Software Repositories</title>
		<meeting>the 15th International Conference on Mining Software Repositories</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="34" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">The definitive ANTLR 4 reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terence</forename><surname>Parr</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Pragmatic Bookshelf</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Neural program search: Solving programming tasks from description and examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Skidanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04335</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Post</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.08771</idno>
		<title level="m">A call for clarity in reporting bleu scores</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/20-074.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Probabilistic model for code with decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Raychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavol</forename><surname>Bielik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Vechev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="731" to="747" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Managing the development of large software systems: concepts and techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winston W Royce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th international conference on Software Engineering</title>
		<meeting>the 9th international conference on Software Engineering</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="328" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An overview of multi-task learning in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
	</analytic>
	<monogr>
		<title level="m">deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Deep Learning in Software Engineering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cody Allen Watson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
	<note>The College of William and Mary</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11934</idno>
		<title level="m">Aditya Barua, and Colin Raffel. mt5: A massively multilingual pre-trained text-to-text transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Russ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5753" to="5763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Staqc: A systematically mined question-code dataset from stack overflow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Peng</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1693" to="1703" />
		</imprint>
	</monogr>
	<note>International World Wide Web Conferences Steering Committee</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
