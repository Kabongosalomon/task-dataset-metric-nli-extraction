<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attributional Robustness Training using Input-Gradient Spatial Alignment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Media and Data Science Research Lab</orgName>
								<address>
									<settlement>Adobe</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nupur</forename><surname>Kumari</surname></persName>
							<email>nupkumar@adobe.com</email>
							<affiliation key="aff0">
								<orgName type="department">Media and Data Science Research Lab</orgName>
								<address>
									<settlement>Adobe</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Puneet</forename><surname>Mangla</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">IIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Sinha</surname></persName>
							<email>abhishek.sinha94@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">Media and Data Science Research Lab</orgName>
								<address>
									<settlement>Adobe</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineeth</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
							<email>vineethnb@iith.ac.in</email>
							<affiliation key="aff1">
								<orgName type="institution">IIT Hyderabad</orgName>
								<address>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Krishnamurthy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Media and Data Science Research Lab</orgName>
								<address>
									<settlement>Adobe</settlement>
									<country key="IN">India</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attributional Robustness Training using Input-Gradient Spatial Alignment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Attributional robustness</term>
					<term>Adversarial robustness</term>
					<term>Explainable deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Interpretability is an emerging area of research in trustworthy machine learning. Safe deployment of machine learning system mandates that the prediction and its explanation be reliable and robust. Recently, it has been shown that the explanations could be manipulated easily by adding visually imperceptible perturbations to the input while keeping the model's prediction intact. In this work, we study the problem of attributional robustness (i.e. models having robust explanations) by showing an upper bound for attributional vulnerability in terms of spatial correlation between the input image and its explanation map. We propose a training methodology that learns robust features by minimizing this upper bound using soft-margin triplet loss. Our methodology of robust attribution training (ART) achieves the new state-of-the-art attributional robustness measure by a margin of â‰ˆ 6-18 % on several standard datasets, ie. SVHN, CIFAR-10 and GT-SRB. We further show the utility of the proposed robust training technique (ART) in the downstream task of weakly supervised object localization by achieving the new state-of-the-art performance on CUB-200 dataset. Code is available at https://github.com/nupurkmr9/Attributional-Robustness.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Attribution methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b32">53,</ref><ref type="bibr" target="#b38">59,</ref><ref type="bibr" target="#b35">56,</ref><ref type="bibr" target="#b34">55,</ref><ref type="bibr" target="#b41">62,</ref><ref type="bibr" target="#b33">54]</ref> are an increasingly popular class of explanation techniques that aim to highlight relevant input features responsible for model's prediction. These techniques are extensively used with deep learning models in risk-sensitive and safety-critical applications such as healthcare <ref type="bibr" target="#b4">[5,</ref><ref type="bibr">38,</ref><ref type="bibr" target="#b43">64,</ref><ref type="bibr">27]</ref>, where they provide a human user with visual validation of the features used by the model for predictions. E.g., in computer-assisted diagnosis, <ref type="bibr" target="#b43">[64]</ref> showed that predictions with attribution maps increased accuracy of retina specialists above that of unassisted reader   <ref type="bibr" target="#b14">[15]</ref> of different attribution maps using the target attribution of (a). Here, (b) Integrated Gradients <ref type="bibr" target="#b41">[62]</ref>, (c) GradCAM++ <ref type="bibr" target="#b11">[12]</ref> and (d) GradSHAP  <ref type="bibr" target="#b48">[69]</ref> in the class of Long Tailed Jaeger, Yellow Breasted Chat and Acadian Flycatcher respectively. or model alone. Also, in <ref type="bibr">[27]</ref>, the authors improve the analysis of skin lesions by leveraging explanation maps of prediction.</p><p>It has been recently demonstrated that one could construct targeted <ref type="bibr" target="#b14">[15]</ref> and untargeted perturbations <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref> that can arbitrarily manipulate attribution maps without affecting the model's prediction. This issue further weakens the cause of safe application of machine learning algorithms. We show an illustrative example of attributionbased attacks for image classifiers over different attribution methods in <ref type="figure" target="#fig_1">Fig. 1</ref>. This vulnerability leads to newer challenges for attribution methods, as well as robust training techniques. The intuition of attributional robustness is that if the inputs are visually indistinguishable with the same model prediction, then interpretation maps should also remain the same.</p><p>As one of the first efforts, <ref type="bibr" target="#b12">[13]</ref> recently proposed a training methodology that aims to obtain models having robust integrated gradient <ref type="bibr" target="#b41">[62]</ref> attributions. In addition to being an early effort, the instability of this training methodology, as discussed in <ref type="bibr" target="#b12">[13]</ref>, limits its usability in the broader context of robust training in computer vision. In this paper, we build upon this work by obtaining an upper bound for attributional vulnerability as a function of spatial correlation between the input image and its explanation map. Furthermore, we also introduce a training technique that minimizes this upper bound to provide attributional robustness. In particular, we introduce a training methodology for attributional robustness that uses soft-margin triplet loss to increase the spatial correlation of input with its attribution map. The triplet loss considers input image as the anchor, gradient of the correct class logit with respect to input as the positive and gradient of the incorrect class with highest logit value with respect to input as the negative. We show empirically how this choice results in learning of robust and interpretable features that help in other downstream weakly supervised tasks.</p><p>Existing related efforts in deep learning research are largely focused on robustness to adversarial perturbations <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">63]</ref>, which are imperceptible perturbations which, when added to input, drastically change the neural networks prediction. While adver-sarial robustness has been explored significantly in recent years, there has been limited progress made on the front of attributional robustness, which we seek to highlight in this work. Our main contributions can be summarized as:</p><p>-We tackle the problem of attribution vulnerability and provide an upper bound for it as a function of spatial correlation between the input and its attribution map <ref type="bibr" target="#b35">[56]</ref>. We then propose ART, a new training method that aims to minimize this bound to learn attributionally robust model. -Our method outperforms prior work in this direction, and achieves state-of-the-art attributional robustness on Integrated Gradient <ref type="bibr" target="#b41">[62]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Our work is associated with various recent development made in the field of explanation methods, robustness to input distribution shifts and weakly supervised object localization. We hence describe earlier efforts in each of these directions below.</p><p>Visual Explanation Methods: Various explanation methods have been proposed that focus on producing posterior explanations for the model's decisions. A popular approach to do so is to attribute the predictions to the set of input features <ref type="bibr" target="#b35">[56,</ref><ref type="bibr" target="#b39">60,</ref><ref type="bibr" target="#b34">55,</ref><ref type="bibr" target="#b41">62,</ref><ref type="bibr" target="#b33">54,</ref><ref type="bibr" target="#b6">7]</ref>. Sample-based explanation methods [30, 71] leverage previously seen examples to describe the prediction of the model. Concept-based explanation techniques <ref type="bibr" target="#b7">[8,</ref><ref type="bibr">29]</ref> aim to explain the decision of the model by high-level concepts. There has also been work that explores interpretability as a built-in property of architecture inspired by the characteristics of linear models <ref type="bibr" target="#b3">[4]</ref>. <ref type="bibr" target="#b58">[79,</ref><ref type="bibr" target="#b15">16]</ref> provide a survey of interpretation techniques. Another class of explanation methods, commonly referred to as attribution techniques, can be broadly divided into three categories -gradient/back-propagation, propagation and perturbation based methods. Gradient-based methods attribute an importance score for each pixel by using the derivative of a class score with respect to input features <ref type="bibr" target="#b35">[56,</ref><ref type="bibr" target="#b34">55,</ref><ref type="bibr" target="#b41">62]</ref>. Propagation-based techniques <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b33">54,</ref><ref type="bibr" target="#b56">77]</ref> leverage layer-wise propagation of feature importance to calculate the attribution maps. Perturbation-based interpretation methods generate attribution maps by examining the change in prediction of the model when the input image is perturbed <ref type="bibr" target="#b53">[74,</ref><ref type="bibr" target="#b26">47,</ref><ref type="bibr" target="#b28">49]</ref>. In this work, we primarily report results on the attribution method of Integrated Gradients IG [62] that satisfies desirable axiomatic properties and was also used in the previous work <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness of Attribution Maps:</head><p>Recently, there have been a few efforts <ref type="bibr" target="#b59">[80,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3]</ref> that have explored the robustness of attribution maps, which we call attributional robustness in this work. The authors of <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b59">80]</ref> study the robustness of a network's attribution maps and show that the attribution maps can be significantly manipulated via imperceptible input perturbations while preserving the classifier's prediction. Recently, Chen, J. et al. <ref type="bibr" target="#b12">[13]</ref> proposed a robust attribution training methodology, which is one of the first attempts at making an image classification model attributionally robust and is the current state of the art. The method minimizes the norm of difference in Integrated Gradients <ref type="bibr" target="#b41">[62]</ref> of an original and perturbed image during training to achieve attributional robustness. In this work, we approach the problem from a different perspective of maintaining spatial alignment between an image and its saliency map.</p><p>Adversarial Perturbation and Robustness: Adversarial attacks can be broadly categorized into two types: White-box <ref type="bibr">[39,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b49">70]</ref> and Black-box attacks [25, <ref type="bibr" target="#b45">66,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b25">46]</ref>. Several proposed defense techniques have been shown to be ineffective to adaptive adversarial attacks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">33,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9]</ref>. Adversarial training <ref type="bibr">[21,</ref><ref type="bibr">36,</ref><ref type="bibr" target="#b37">58]</ref>, which is a defense technique that continuously augments the data with adversarial examples while training, is largely considered the current state-of-the-art to achieve adversarial robustness. <ref type="bibr" target="#b55">[76]</ref> characterizes the trade-off between accuracy and robustness for classification problems and propose a regularized adversarial training method. Recent work of <ref type="bibr" target="#b27">[48]</ref> proposes a regularizer that encourages the loss to behave linearly in the vicinity of the training data, and <ref type="bibr" target="#b54">[75]</ref> improves the adversarial training by also minimizing the convolutional feature distance between the perturbed and clean examples. Prior works have also attempted to improve adversarial robustness using gradient regularization that minimizes the Frobenius norm of the Hessian of the classification loss with respect to input <ref type="bibr" target="#b29">[50,</ref><ref type="bibr">40,</ref><ref type="bibr">35]</ref> or weights <ref type="bibr">[26]</ref>. For a comprehensive review of the work done in the area of adversarial examples, please refer <ref type="bibr" target="#b51">[72,</ref><ref type="bibr" target="#b0">1]</ref>. We show in our work that in addition to providing attributional robustness, our proposed method helps in achieving significant performance improvement on downstream tasks such as weakly supervised object localization. We hence briefly discuss earlier efforts on this task below.</p><p>Weakly Supervised Object Localization (WSOL): The problem of WSOL aims to identify the location of the object in a scene using only image-level labels, and without any location annotations. Generally, rich labeled data is scarcely available, and its collection is expensive and time-consuming. Learning from weak supervision is hence promising as it requires less rich labels and has the potential to scale. A common problem with most previous approaches is that the model only identifies the most discriminative part of the object rather than the complete object. For example, in the case of a bird, the model may rely on the beak region for classification than the entire bird's shape. In WSOL task, ADL <ref type="bibr" target="#b13">[14]</ref>, the current state-of-the-art method, uses an attention-based dropout layer while training the model that promotes the classification model to also focus on less discriminative parts of the image. For getting the bounding box from the model, ADL and similar other techniques in this domain first extract attribution maps, generally CAM-based <ref type="bibr" target="#b60">[81]</ref>, for each image and then fit a bounding box as described in <ref type="bibr" target="#b60">[81]</ref>. We now present our methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Attributional Robustness Training: Methodology</head><p>Given an input image x âˆˆ [0, 1] n with true label y âˆˆ {1...k}, we consider a neural network model f Î¸ : R n â†’ R k with ReLU activation function that classifies x into one of k classes as arg max f (x) i where i âˆˆ {1...k}. Here, f (x) i is the i th logit of f (x). Attribution map A(x, f (x) i ) : R n â†’ R n with respect to a given class i assigns an importance score to each input pixel of x based on its relevance to the model for predicting the class i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Attribution Manipulation</head><p>It was shown recently <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref> that for standard models f Î¸ , it is possible to manipulate the attribution map A(x, f (x) y ) (denoted as A(x) for simplicity in the rest of the paper) with visually imperceptible perturbation Î´ in the input by optimizing the following loss function.</p><formula xml:id="formula_0">arg max Î´âˆˆB D[A(x + Î´, f (x + Î´) y ), A(x, f (x) y )] subject to: arg max(f (x)) = arg max(f (x + Î´)) = y<label>(1)</label></formula><p>where B is an l p ball of radius centered at x and D is a dissimilarity function to measure the change between attribution maps. The manipulation was shown for various perturbation-based and gradient-based attribution methods. This vulnerability in neural network-based classification models suggests that the model relies on features different from what humans perceive as important for its prediction. The goal of attributional robustness is to mitigate this vulnerability and ensure that attribution maps of two visually indistinguishable images are also nearly identical. In the next section, we propose a new training methodology for attributional robustness motivated from the observation that feature importance in image space has a high spatial correlation with the input image for robust models <ref type="bibr" target="#b44">[65,</ref><ref type="bibr" target="#b17">18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attributional Robustness Training (ART)</head><p>Given an input image x âˆˆ R n with ground truth label y âˆˆ {1...k} and a classification model f Î¸ , the gradient-based feature importance score is defined as âˆ‡ x f (x) i : i âˆˆ {1...k} and denoted as g i (x) in the rest of the paper. For achieving attributional robustness, we need to minimize the attribution vulnerability to attacks as defined in Equation 1. Attribution vulnerability can be formulated as the maximum possible change in g y (x) in a -neighborhood of x if A is taken as gradient attribution method <ref type="bibr" target="#b35">[56]</ref> and D is a distance measure in some norm ||.|| i.e.</p><formula xml:id="formula_1">max Î´âˆˆB ||g y (x + Î´) âˆ’ g y (x)||<label>(2)</label></formula><p>We show that Equation 2 is upper bounded by the maximum of the distance between g y (x + Î´) and x + Î´ for Î´ in neighbourhood of x.</p><formula xml:id="formula_2">||g y (x + Î´) âˆ’ g y (x)|| = ||g y (x + Î´) âˆ’ (x + Î´) âˆ’ (g y (x) âˆ’ x) + Î´|| â‰¤ ||g y (x + Î´) âˆ’ (x + Î´)|| + ||g y (x) âˆ’ x|| + ||Î´|| â‰¤ ||g y (x + Î´) âˆ’ (x + Î´)|| + max Î´âˆˆB ||g y (x + Î´) âˆ’ (x + Î´)|| + ||Î´||<label>(3)</label></formula><p>Taking max on both sides: Leveraging existing understanding <ref type="bibr" target="#b31">[52,</ref><ref type="bibr">24]</ref> that minimizing the distance between two quantities can benefit from a negative anchor, we use a triplet loss formulation as defined in <ref type="bibr">Equation 5</ref> with image x as an anchor, g y (x) as positive sample and g i * (x) as negative sample. More details about the selection of the optimization objective 5 and choice for the negative sample can be found in Appendix A.1. Hence to achieve attributional robustness, we propose a training technique ART that encourages high spatial correlation between g y (x) and x by optimizing L attr which is a triplet loss [24] with soft margin on cosine distance between g i (x) and x i.e.</p><formula xml:id="formula_3">max Î´âˆˆB ||g y (x + Î´) âˆ’ g y (x))|| â‰¤ 2 max Î´âˆˆB ||g y (x + Î´) âˆ’ (x + Î´)|| + || || (4)</formula><formula xml:id="formula_4">L attr (x, y) = log 1 + exp âˆ’ (d(g i * (x), x) âˆ’ d(g y (x), x)) where d(g i (x), x) = 1 âˆ’ g i (x).x ||g i (x)|| 2 .||x|| 2 ; i * = arg max i =y f (x) i<label>(5)</label></formula><p>Hence, the classification training objective for ART methodology is:</p><formula xml:id="formula_5">minimize Î¸ E (x,y) L ce (x + Î´, y) + Î» L attr (x + Î´, y) where Î´ = arg max ||Î´||âˆž&lt; L attr (x + Î´, y)<label>(6)</label></formula><p>Here L ce is the standard cross-entropy loss. The optimization of L attr involves computing gradient of f (x) i with respect to input x which suffers from the problem of vanishing second derivative in case of ReLU activation, i.e. âˆ‚ 2 f i /âˆ‚x 2 â‰ˆ 0. To alleviate this, following previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13]</ref>, we replace ReLU with softplus non-linearities while optimizing L attr as it has a well-defined second derivative. The softplus approximates to ReLU as the value of Î² in softplus Î² (x) = log(1+e Î²x ) Î² increases. Note that optimization of L ce follows the usual ReLU activation pathway. Thus, our training methodology consists of two steps: first, we calculate a perturbed imagex = x + Î´ that maximizes L attr through iterative projected gradient descent; secondly, we usex as the training point on which L ce and L attr is minimized with their relative weightage controlled by the hyper-parameter Î».</p><p>Note that the square root of cosine distance for unit l 2 norm vectors as used in our formulation of L attr is a valid distance metric and is related to the Euclidean distance as shown in Appendix A. <ref type="bibr" target="#b1">2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. Through experiments, we empirically show that minimizing the upper bound in Equation 4 as our training objective increases the attributional</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Attributional Robustness Training (ART)</head><p>1 Input: Classification model f Î¸ , training data X = {(xi, yi)}, batch size b, number of epochs E, number of attack steps a, step-size for iterative perturbation Î±, softplus parameter Î², weight of Lattr loss Î».</p><formula xml:id="formula_6">2 for epoch âˆˆ {1, 2, ..., E} do 3 Get mini-batch x, y = {(x1, y1)...(x b , y b )} 4x = x + U nif orm[âˆ’ , + ] 5 for i=1,2, ... , a do 6x =x + Î± * sign(âˆ‡xLattr(x, y)) 7x = P roj âˆž (x) 8 end 9 i * = arg max i =y f (x)i 10 Calculate g y (x) = âˆ‡xf (x)y 11</formula><p>Calculate g i * (x) = âˆ‡xf (x)i * ; // We calculate g y (x) and g i * (x) using softplus Î² activation as described in Section 3.2 12 loss = Lce(x, y) + Î» Â· Lattr(x, y) <ref type="bibr" target="#b12">13</ref> Update Î¸ using loss 14 end 15 return f Î¸ . robustness of the model by a significant margin. The block diagram for our training methodology is shown in <ref type="figure" target="#fig_3">Fig 2,</ref> and its pseudo-code is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Connection to Adversarial Robustness</head><p>For a given input image x, an adversarial example is a slightly perturbed image x such that ||x âˆ’ x || is small in some norm but the model f Î¸ classifies x incorrectly. Adversarial examples are calculated by optimizing a loss function L which is large when f (x) = y:</p><p>x adv = arg max</p><formula xml:id="formula_7">x :||x âˆ’x||p&lt; L(Î¸, x , y)<label>(7)</label></formula><p>where L can be the cross-entropy loss, for example. For an axiomatic attribution function A which satisfies the completeness axiom i.e.</p><formula xml:id="formula_8">n j=1 A(x) j = f (x) y , it can be shown that |f (x) y âˆ’ f (x ) y | &lt; ||A(x) âˆ’ A(x )|| 1 , as below: |f (x) y âˆ’ f (x ) y | =| n j=1 A(x) j âˆ’ n j=1 A(x ) j | â‰¤ n j=1 |A(x) j âˆ’ A(x ) j | =||A(x) âˆ’ A(x )|| 1<label>(8)</label></formula><p>The above relationship connects adversarial robustness to attributional robustness as the maximum change in f (x) y is upper bounded by the maximum change in attribution map of x in its neighborhood. Also, it was shown <ref type="bibr" target="#b44">[65]</ref> recently that for an adversarially robust model, gradient-based feature importance map g y (x) has high spatial correlation with the image x and it highlights the perceptually relevant features of the image. For classifiers with a locally affine approximation like a DNN with ReLU activations, Etmann et al. <ref type="bibr" target="#b17">[18]</ref> establish theoretical connection between adversarial robustness, and the correlation of g y (x) with image x. <ref type="bibr" target="#b17">[18]</ref> shows that for a given image x, its distance to the nearest distance boundary is upper-bounded by the dot product between x and g y (x). The authors of <ref type="bibr" target="#b17">[18]</ref> showed that increasing adversarial robustness increases the correlation between g y (x) and x. Moreover, this correlation is related to the increase in attributional robustness of model as we show in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Downstream Task: Weakly supervised Object localization (WSOL)</head><p>As an additional benefit of our approach, we show its improved performance on a downstream task -Weakly supervised Object localization (WSOL), in this case. The problem of WSOL deals with detecting objects where only class label information of images is available, and the ground truth bounding box location is inaccessible. Generally, the pipeline for obtaining bounding box locations in WSOL relies on attribution maps. Also, the task of object detection is widely used to validate the quality of attribution maps empirically. Since our proposed training methodology ART promotes attribution map to be invariant to small perturbations in input, it leads to better attribution maps identifying the complete object instead of focusing on only the most discriminative part of the object. We validate this empirically by using attribution maps obtained from our model for bounding-box detection on the CUB dataset and obtaining new state-of-theart localization results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>In this section, we first describe the implementation details of ART and evaluation setting for measuring the attributional and adversarial robustness. We then show the performance of ART on the downstream task of weakly supervised image localization task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Attributional and Adversarial Robustness</head><p>Baselines: We compare our training methodology with the following approaches: -Natural: Standard training with minimization of cross entropy classification loss.</p><p>-PGD-n: Adversarially trained model with n-step PGD attack as in <ref type="bibr">[36]</ref>, which is typically used by work in this area <ref type="bibr" target="#b12">[13]</ref>. -IG Norm and IG-SUM Norm <ref type="bibr" target="#b12">[13]</ref>: Current state-of-the-art robust attribution training technique.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets and Implementation Details:</head><p>To study the efficacy of our methodology, we benchmark on the following standard vision datasets: CIFAR-10 [32], SVHN <ref type="bibr" target="#b21">[42]</ref>, GTSRB <ref type="bibr" target="#b40">[61]</ref> and Flower <ref type="bibr" target="#b22">[43]</ref>. For CIFAR-10, GTSRB and Flower datasets, we use Wideresnet-28-10 [73] model architecture for Natural, PGD-10 and ART. For SVHN, we use WideResNet-40-2 <ref type="bibr" target="#b52">[73]</ref> architecture. We use the perturbation = 8/255 in âˆžnorm for ART and PGD-n as in <ref type="bibr">[36,</ref><ref type="bibr" target="#b12">13]</ref>. We use Î» = 0.5, a = 3 and Î² = 50 for all experiments in the paper. For training, we use SGD optimizer with step-wise learning rate schedule. More details about datasets and training hyper-parameters are given in Appendix A.3.</p><p>Evaluation: For evaluating attributional robustness, we follow <ref type="bibr" target="#b12">[13]</ref> and present our results with Integrated Gradient (IG)-based attribution maps. We show attributional robustness accuracy of ART on other attribution methods in Section 5. IG satisfies several theoretical properties desirable for an attribution method, e.g. sensitivity and completeness axioms and is defined as:</p><formula xml:id="formula_9">IG(x, f (x) i ) = (x âˆ’ x) 1 t=0 âˆ‡ x f (x + t(x âˆ’ x)) i dt<label>(9)</label></formula><p>where x is a suitable baseline at which the function prediction is neutral. For computing perturbed imagex on which IG(x) changes drastically from IG(x), we perform Iterative Feature Importance Attack (IFIA) proposed by Ghorbani et al. <ref type="bibr" target="#b18">[19]</ref> with âˆž bound of = 8/255 as used by previous work <ref type="bibr" target="#b12">[13]</ref>. For assessing similarity between A(x) and perturbed image A(x), we use Top-k intersection (IN ) and Kendall's tau coefficient (K) similar to <ref type="bibr" target="#b12">[13]</ref>. Kendall's tau coefficient is a measure of similarity of ordering when ranked by values, and therefore is a suitable metric for comparing attribution maps. Top-k intersection measures the percentage of common indices in top-k values of attribution map of x andx. We report average of IN and K metric over random 1000 samples of test-set. More details about the attack methodology and evaluation parameters can be found in Appendix A.3. For   <ref type="table" target="#tab_1">Table 1</ref> compares attributional and adversarial robustness across different datasets and training approaches. Our proposed approach ART achieves state-of-theart attributional robustness on attribution attacks <ref type="bibr" target="#b18">[19]</ref> when compared with baselines. We also observe that ART consistently achieves higher test accuracy than [36] and has adversarial robustness significantly greater than that of the Natural model.</p><p>Qualitative study of input-gradients for ART: Motivated by <ref type="bibr" target="#b44">[65]</ref> which claims that adversarially trained models exhibits human-aligned gradients (agree with human saliency), we studied the same with (ART), and the results are shown in <ref type="figure" target="#fig_4">Fig 3.</ref> Qualitative study of input-gradients shows a high degree of spatial alignment between the object and the gradient. We also show image generation from random seeds in <ref type="figure" target="#fig_5">Fig 4 using</ref> robust ART model as done in <ref type="bibr" target="#b30">[51]</ref>. The image generation process involves maximization of the class score of the desired class starting from a random seed which is sampled from some class-conditional seed distribution as defined in <ref type="bibr" target="#b30">[51]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Weakly Supervised Image Localization</head><p>This task relies on the attribution map obtained from the classification model to estimate a bounding box for objects. We compare our approach with ADL [14] 3 on the CUB dataset, which has ground truth bounding box of 5794 bird images. We adopt similar processing steps as ADL for predicting bounding boxes except that we use gradient attribution map âˆ‡ x f (x) y instead of CAM <ref type="bibr" target="#b60">[81]</ref>. As a post-processing step, we convert the attribution map to grayscale, normalize it and then apply a mean filtering of 3 Ã— 3 kernel over it. Then a bounding box is fit over this heatmap to localize the object.</p><p>We perform experiments on Resnet-50 [22] and VGG <ref type="bibr" target="#b36">[57]</ref> architectures. We use âˆž bound of = 2/255 for ART and PGD-7 training on the CUB dataset. For evaluation, we used similar metrics as in <ref type="bibr" target="#b13">[14]</ref> i.e. GT-Known Loc: Intersection over Union (IoU) of estimated box and ground truth bounding box is atleast 0.5 and ground truth is known; Top-1 Loc: prediction is correct and IoU of bounding box is atleast 0.5; Top-1 Acc: top-1 classification accuracy. Details about dataset and training hyper-parameters are given in Appendix B.1. Our approach results in higher GT-Known Loc and Top-1 Loc for both Resnet-50 and VGG-GAP <ref type="bibr" target="#b13">[14]</ref> model as shown in <ref type="table" target="#tab_2">Table 2</ref>. We also show qualitative comparison of the bounding box estimated by our approach with <ref type="bibr" target="#b13">[14]</ref> in   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Ablation Studies</head><p>To understand the scope and impact of the proposed training approach ART, we perform various experiments and report these findings in this section. These studies were carried out on the CIFAR-10 dataset. Robustness to targeted attribution attacks: In targeted attribution attacks, the aim is to calculate perturbations that minimize dissimilarity between the attribution map of a given image and a target image's attribution map. We evaluate the robustness of ART model using targeted attribution attack as proposed in <ref type="bibr" target="#b14">[15]</ref> using the IG attribution method on a batch of 1000 test examples. To obtain the target attribution maps, we randomly shuffle the examples and then evaluate ART and PGD-10 trained model on these examples. The kendall's tau coefficient and top-k intersection similarity measure between original and perturbed images on ART was 64.76 and 70.64 as compared to 36.29 and 31.81 on the PGD-10 adversarially trained model. Attributional robustness for other attribution methods: We show the efficacy of ART against attribution attack <ref type="bibr" target="#b18">[19]</ref> using gradient <ref type="bibr" target="#b35">[56]</ref> and gradSHAP[34] attribution methods in <ref type="table" target="#tab_4">Table 4</ref>. We observe that ART achieves higher attributional robustness than Natural and PGD-10 models on Top-k intersection (IN) and Kendall's tau coefficient (K) measure. We also compare the cosine similarity between x and g y (x) for all models trained on CIFAR-10 dataset and show its variance plot in <ref type="figure">Fig. 6</ref>. We can see that ART trained model achieves higher cosine similarity Natural and PGD-10 models. This empirically validates that our optimization is effective in increasing the spatial correlation between image and gradient. Robustness against gradient-free and stronger attacks: To show the absence of gradient masking and obfuscation <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>, we evaluate our model on a gradient-free adversarial optimization algorithm <ref type="bibr" target="#b45">[66]</ref> and a stronger PGD attack with a larger number of steps. We observe similar adversarial robustness when we increase the number of steps <ref type="figure">Fig. 7</ref>: Example images of weakly supervised segmentation masks obtained from different models via different attribution methods in PGD-attack. For 100 step and 500 step PGD attacks, ART achieves 37.42 % and 37.18 % accuracy respectively. On the gradient-free SPSA <ref type="bibr" target="#b45">[66]</ref> attack, ART obtains 44.7 adversarial accuracy that was evaluated over 1000 random test samples.</p><p>Robustness to common perturbations [23] and spatial adversarial perturbations <ref type="bibr" target="#b16">[17]</ref>: We compare ART with PGD-10-based adversarially trained model on the common perturbations dataset [23] for CIFAR-10. The dataset consists of perturbed images of 15 common-place visual perturbations at five levels of severity, resulting in 75 distinct corruptions. We report the mean accuracy over severity levels for all 15 types of perturbations and observe that ART achieves better generalization than other models on a majority of these perturbations, as shown in <ref type="table" target="#tab_3">Table 3</ref>. On PGD-40 2 norm attack with = 1.0 and spatial attack <ref type="bibr" target="#b16">[17]</ref> we observe robustness of 39.65%, 11.13% for ART and 29.68%, 6.76% for PGD-10 trained model, highlighting the improved robustness provided by our method. We show more results on varying in adversarial attacks and combining PGD adversarial training <ref type="bibr">[36]</ref> with ART in Appendix C. Image Segmentation: Data annotations collection for image segmentation task is timeconsuming and costly. Hence, recent efforts [31, <ref type="bibr" target="#b46">67,</ref><ref type="bibr" target="#b47">68,</ref><ref type="bibr">28,</ref><ref type="bibr" target="#b24">45,</ref><ref type="bibr" target="#b57">78,</ref><ref type="bibr" target="#b23">44]</ref> have focused on weakly supervised segmentation models, where image labels are leveraged instead of segmentation masks. Since models trained via our approach perform well on WSOL, we further evaluate it on weakly supervised image segmentation task for Flowers dataset <ref type="bibr" target="#b22">[43]</ref> where we have access to segmentation masks of 849 images. Samples of weaklysupervised segmentation mask obtained from attribution maps on various models are shown in <ref type="figure">Fig. 7</ref>. We observe that attribution maps of ART can serve as a better prior for segmentation masks as compared to other baselines. We evaluate our results using Top-1 Seg metric which considers an answer as correct when the model prediction is correct and the IoU betweeen ground-truth mask and estimated mask is atleast 0.5. We compare ART against Natural and PGD-7 trained models using gradient <ref type="bibr" target="#b35">[56]</ref> and IG <ref type="bibr" target="#b41">[62]</ref> based attribution map. Attribution maps are converted into gray-scale heatmaps and a smoothing filter is applied as a post-processing step. We obtain a Top-1 Seg performance of 0.337, 0.422, and 0.604 via IG attribution maps and 0.244, 0.246, 0.317 via gradient maps for Natural, PGD-7 and ART models respectively. Effect of Î², Î» and a on performance:</p><p>We perform experiments to study the role of Î², Î» and a as used in Algorithm 1 on the model performance by varying one parameter and fixing the others on their best-performing values, i.e. 50, 0.5 and 3 respectively. <ref type="figure">Fig. 8</ref> shows the plots of attributional robustness. <ref type="figure">Fig. 9</ref> shows the plots of test accuracy and adversarial accuracy on âˆž PGD-40 perturbations with = 8/255 along varying parameters. From <ref type="figure">Fig. 9</ref>, we observe that adversarial accuracy initially increases with increasing Î², but the trend reverses for higher values of Î². Similar is the trend for attributional robustness on varying Î² as can be seen from the <ref type="figure">Fig. 8</ref>. On varying Î», we find that the attributional and adversarial robustness of the model increases with increasing Î» and saturates after 0.75. However, the test accuracy starts to suffer as the magnitude of Î» increases. For attack steps parameter a, we find that the performance in terms of test accuracy, adversarial accuracy and attributional robustness saturates after 3 as shown in the right-side plot of <ref type="figure">Fig. 8</ref> and <ref type="figure">Fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a new method for the problem space of attributional robustness, using the observation that increasing the alignment between the object in an input and the attribution map generated from the network's prediction leads to improvement in attributional robustness. We empirically showed this for both un-targeted and targeted attribution attacks over several benchmark datasets. We showed that the attributional robustness also brings out other improvements in the network, such as reduced vulnerability to adversarial attacks and common perturbations. For other vision tasks such as weakly supervised object localization, our attributionally robust model achieves a new stateof-the-art accuracy even without being explicitly trained to achieve that objective. We In this section, we provide details of the datasets as mentioned in the main paper (Section 4.1), as well as some additional results on attributional robustness. We qualitatively show in <ref type="figure" target="#fig_1">Figure 10</ref> that attribution maps generated via ART are robust to attribution manipulation unlike Natural model. We also report the Top-1000 Intersection and Kendalls Correlation between original and perturbed saliency maps for ART and Natural models. We use target attribution attack as mentioned in <ref type="bibr" target="#b14">[15]</ref> to perturb the attributions while keeping the predictions same. For images in <ref type="figure" target="#fig_1">Figure 10</ref>, the model predictions are correct and the attribution maps are computed using Integrated Gradient <ref type="bibr" target="#b41">[62]</ref>. We observe that attributions of the Natural model are visually and quantitatively fragile as attributions are easily manipulated to resemble target attribution map that is present in the rightmost column of the figure. However, it can seen from the figure that ART models show high robustness to attribution manipulations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Choice of optimization objective L attr and its variants</head><p>Our choice for the loss function was based on the empirical analysis as reported in table 5 on CIFAR-10. We empirically observed that instead of directly minimizing 2 distance between x and g y (x) in Equation 4 of main paper, cosine distance led to better robustness. We believe this is because cosine avoids scale mismatch issues in x and g y (x) magnitudes. The triplet loss is only introduced to improve performance on attributional robustness objective. For negative sample selection, we choose i * as second most likely class, which represents most uncertainty, following standard principles of hard negative mining in triplet loss <ref type="bibr">[24,</ref><ref type="bibr" target="#b31">52]</ref>. For other choices of i * , we observed a performance drop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Cosine distance in L attr loss</head><p>Following our discussion in Sec 3.2 of the main paper, we now elaborate on the relation of cosine distance in a unit 2 -norm surface of vectors with Euclidean distance. We show below that squared Euclidean distance is proportional to the cosine distance for unit 2 norm space of vectors. Euclidean distance is a valid distance function and follows the triangle inequality which we use in Eqn 3 for obtaining the upper bound on attributional robustness as a function of the distance between an image and its attribution map. Given two vectors x andx, with unit 2 norm i.e. ||x|| 2 = 1 and ||x|| 2 = 1, cosine distance between them is related to their Euclidean distance as follows:   </p><formula xml:id="formula_10">(||x âˆ’x|| 2 ) 2 = (x âˆ’x) .(x âˆ’x) = x x +x x âˆ’ 2.x .x = ||x|| 2 + ||x|| 2 âˆ’ 2.x .x = 1 + 1 âˆ’ 2.x .x = 2(1 âˆ’ x .x) = 2.CosineDistance(x,x)<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attack Methodology and Evaluation</head><p>For evaluation, we perform the Top-K variant of Iterative Feautre Importance Attack (IFIA) proposed by <ref type="bibr" target="#b18">[19]</ref>. Feature importance function is taken as Integrated Gradients <ref type="bibr" target="#b41">[62]</ref>, and dissimilarity function is Kendall Correlation. The hyperparameters used are the same as in <ref type="bibr" target="#b12">[13]</ref> i.e. for CIFAR-10, SVHN and GTSRB datasets, k in top-k is 100, is 8/255, number of steps is 50 and step-size is 1/255. For the Flowers dataset, k is 1000, is 8/255, number of steps is 100 and step-size is 1/255. We also show the comparison by varying on CIFAR-10 dataset in Section A.4. Evaluation is also similar to <ref type="bibr" target="#b12">[13]</ref> using Top-k intersection and Kendall correlation measure and we report both numbers as percentage values. For Top-k intersection, k is 100 for CIFAR-10, SVHN and GTSRB datasets, and 1000 for Flowers dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Additional Analysis on CIFAR-10</head><p>Attributional Robustness: <ref type="figure" target="#fig_1">In Fig 11,</ref> we show the variance box plot of Kendall Correlation and Top-k Intersection with = 8/255 for Natural, ART and PGD-10 [36] models on CIFAR-10. ART has higher attributional robustness with the least variance as compared to other approaches across 1000 samples randomly selected from the test dataset. We also measure the attributional robustness of models on varying to the standard values of 2/255, 4/255, 8/255 and 12/255 in the attack methodology as explained in Section A.3. <ref type="figure" target="#fig_1">Figure 12</ref> shows the Top-k Intersection and Kendall correlation measure for the same. We can see that ART outperforms PGD-10 and Natural model over all choices of . In this section, we provide more details of the dataset used for the results presented in the main paper on weakly supervised localization (Section 4.2), as well as more qualitative examples for these experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Dataset and Implementation Details</head><p>We begin by describing the dataset used in experiments for weakly supervised localization, which we could not include in the main paper owing to space constraints. Dataset and Model: CUB-200 <ref type="bibr" target="#b48">[69]</ref> is an image dataset of 200 different bird species (mostly North American) with 11, 788 images in total. The information as a bounding box around each bird is also available. We finetune a ResNet-50 [22] model pre-trained on ImageNet for the reported approaches as in <ref type="bibr" target="#b13">[14]</ref>. Hyper-parameters for training Natural: We use SGD optimizer with an initial learning rate of 0.01, momentum of 0.9 and l 2 weight decay of 1eâˆ’4. We train the model for 200 epochs with batch size 128 and learning rate decay of 0.1 at every 60 epochs. PGD-7 [36]: We use same hyper-parameters as natural training with = 2/255. and step size = 0.5/255. for calculating adversarial examples. ART: We use SGD optimizer with an initial learning rate of 0.01, momentum of 0.9 and l 2 weight decay of 1eâˆ’4. We decay the learning rate by 0.1 at every 40 epoch till 200 epochs and train with a batch size of 90. While calculating L attr loss, we took mean over channels of images and gradients. Values of other hyper-parameters are = 2/255, step size = 1.5/255, a = 3, Î» = 0.5 and Î² = 50. <ref type="figure" target="#fig_1">Figure 13</ref> shows the estimated bounding box and heatmap derived from gradient based attribution <ref type="bibr" target="#b35">[56]</ref> on randomly sampled images for ResNet50 model trained via our approach. We observe that the estimated bounding box sometimes does not capture the complete object in cases where birds have extended wings, or the bird is in an occluded area with branches and twigs. Although, we observe qualitatively that this issue also exists for other models <ref type="bibr" target="#b13">[14]</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Qualitative Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Adversarial Robustness</head><p>In this section, we provide additional results on adversarial robustness on the CIFAR-10 dataset.</p><p>Adversarial Robustness on âˆž and 2 PGD Perturbations with Varying To analyze the adversarial robustness of ART model, we report and compare accuracy of the ART model and the PGD-10 adversarially trained model over âˆž and 2 PGD perturbations for different values of on CIFAR-10. In <ref type="figure" target="#fig_1">Figure 14</ref>, we can observe that ART adversarial robustness for âˆž perturbations is similar to PGD-10 for less than 4/255 and better for various values of 2 perturbations.</p><p>Transfer-based black-box attacks We analyse the adversarial robustness of ART models on transfer-based black box attacks. Specifically, we compute the adversarial perturbations on the test set of CIFAR-10 for different baseline models and evaluate its adversarial accuracy on ART. We see that the transfer of adversarial perturbation from ART is much better than PGD-10 on Natural model. ART also shows higher robustness than PGD-10 for transfer attack from Natural model as reported in table 6.</p><p>Comparison with other training techniques for adversarial robustness: We consider JARN <ref type="bibr" target="#b10">[11]</ref> and CURE <ref type="bibr" target="#b20">[41]</ref>, which are recently proposed training techniques for adversarial robustness that are different from adversarial training <ref type="bibr">[36]</ref>. We compare the adversarial robustness of these techniques with ART on CIFAR-10 dataset using a âˆž PGD-20 adversarial perturbation with = 8/255. JARN, CURE and ART show adversarial accuracy of 15.5%, 41.4% and 37.73% respectively and test accuracy of 93.9%, 83.1% and 89.84% respectively.</p><p>Using L attr +L ce to Compute Perturbationsx With the motive to combine the benefits from attributional and adversarial robust models, we augment the loss function of our approach with adversarial loss <ref type="bibr">[36]</ref>. We observe that the model achieves test accuracy of 85.33 and adversarial accuracy of 52.31 on PGD-40 âˆž attack with = 8/255 as compared to the PGD-10 model which has 87.32 test accuracy and 44.07 adversarial accuracy. The attributional robustness measure of Top-k intersection and kendall correlation using Integrated Gradients is 74.24 and 77.86 which is less than the attributional robustness of ART model but is âˆ¼ 5% better than PGD-10 model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Equal contribution Work done at Adobe arXiv:1911.13073v4 [cs.CV] 18 Jul 2020</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 :</head><label>1</label><figDesc>Illustration of targeted manipulation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>[34] blocks show : Top (b), (c), (d) original image and its attribution map; Bottom (b), (c), (d) perturbed image and its attribution map. Both original and perturbed images of (b), (c) and (d) are classified correctly by the ResNet-50 trained model on CUB-200</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>Block diagram summarizing our training technique for ART. Dashed line represents backward gradient flow, and bold lines denotes forward pass of the neural network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 :</head><label>3</label><figDesc>Qualitative examples of gradient attribution map<ref type="bibr" target="#b35">[56]</ref> for different models on CIFAR-10.Top to bottom: Image; attribution maps for Natural, PGD-10 and ART trained models</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 :</head><label>4</label><figDesc>Random samples (of resolution 32 Ã— 32) generated using a CIFAR-10 robustly trained ART classifier evaluating adversarial robustness, we perform 40 step PGD attack [36] using crossentropy loss with âˆž bound of = 8/255 and report the model accuracy on adversarial examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison of heatmap and estimated bounding box by VGG model trained via our method and ADL on CUB dataset; top row corresponds to our method, and the bottom row corresponds to ADL. The red bounding box is ground truth and green bounding box corresponds to the estimated box</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Fig 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 : 10 Fig. 9 :</head><label>8109</label><figDesc>Top-k Intersection (IN) and Kendall correlation (K) measure of attributional robustness on varying Î², Î» and attack steps in our training methodology on CIFAR-Test accuracy and adversarial accuracy (PGD-40 perturbations) on varying Î², Î» and attack steps in our training methodology on CIFAR-10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>hope that our work can open a broader discussion around notions of robustness and the application of robust features on other downstream tasks. Acknowledgements. This work was partly supported by the Ministry of Human Resource Development and Department of Science and Technology, Govt of India through the UAY program. 21. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial examples. ICLR (2015) 22. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. arXiv preprint arXiv:1512.03385 (2015) 23. Hendrycks, D., Dietterich, T.: Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations (2019) 24. Hermans, A., Beyer, L., Leibe, B.: In defense of the triplet loss for person re-identification. arXiv preprint arXiv:1703.07737 (2017) 25. Ilyas, A., Engstrom, L., Athalye, A., Lin, J.: Black-box adversarial attacks with limited queries and information. ICML (2018) 26. Jakubovitz, D., Giryes, R.: Improving dnn robustness to adversarial attacks using jacobian regularization. ECCV (2018) 27. Jia, X., Shen, L.: Skin lesion classification using class activation map. arXiv preprint arXiv:1703.01053 (2017) 28. Jiang, Q., Tawose, O.T., Pei, S., Chen, X., Jiang, L., Wang, J., Zhao, D.: Weakly-supervised image semantic segmentation based on superpixel region merging. Big Data and Cognitive Computing 3(2), 31 (2019) 29. Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., Sayres, R.: Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). ICML (2018) 30. Koh, P.W., Liang, P.: Understanding black-box predictions via influence functions. In: Precup, D., Teh, Y.W. (eds.) Proceedings of the 34th International Conference on Machine Learning. Proceedings of Machine Learning Research, vol. 70, pp. 1885-1894. PMLR, International Convention Centre, Sydney, Australia (06-11 Aug 2017), http: //proceedings.mlr.press/v70/koh17a.html 31. Kolesnikov, A., Lampert, C.H.: Seed, expand and constrain: Three principles for weaklysupervised image segmentation. CoRR abs/1603.06098 (2016), http://arxiv.org/ abs/1603.06098 32. Krizhevsky, A., Nair, V., Hinton, G.: Cifar-10. URL http://www.cs.toronto.edu/ kriz/cifar.html (2010) 33. Logan Engstrom, Andrew Ilyas, A.A.: Evaluating and understanding the robustness of adversarial logit pairing. NeurIPS SECML (2018) 34. Lundberg, S.M., Lee, S.I.: A unified approach to interpreting model predictions. In: Guyon, I., Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., Garnett, R. (eds.) NeurIPS (2017), http://papers.nips.cc/paper/ 7062-a-unified-approach-to-interpreting-model-predictions. pdf 35. Lyu, C., Huang, K., Liang, H.N.: A unified gradient regularization family for adversarial examples. ICDM (2015) 36. Madry, A., Makelov, A., Schmidt, L., Tsipras, D., Vladu, A.: Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 (2017) 37. MadryLab: cifar10 challenge. URL https://github.com/MadryLab/cifar10_ challenge (2017) 38. Mitani, A., Huang, A., Venugopalan, S., Corrado, G.S., Peng, L., Webster, D.R., Hammel, N., Liu, Y., Varadarajan, A.V.: Detection of anaemia from retinal fundus images via deep learning. Nature Biomedical Eng. 4, 18-27 (2020) 39. Moosavi-Dezfooli, S.M., Fawzi, A., Frossard, P.: Deepfool: a simple and accurate method to fool deep neural networks. arXiv preprint arXiv:1511.04599v3 (2016) 40. Moosavi-Dezfooli, S.M., Fawzi, A., Uesato, J., Frossard, P.: Robustness via curvature regularization, and vice versa. CVPR (2019) Appendix A Attributional Robustness: Additional Details and Results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 10 :</head><label>10</label><figDesc>Targeted attribution attack<ref type="bibr" target="#b14">[15]</ref> using integrated gradient (IG) attribution map on Natural and ART trained model. Top-1000 intersection and Kendall correlation between IG attribution map of original and perturbed images is shown below each image. The target attribution manipulation uses the attribution map as depicted in the rightmost column of this figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 11 : 10 Fig. 12 :</head><label>111012</label><figDesc>Variance box plot of Attributional Robustness measure for different models on Kendall Correlation (left) and Top-k Intersection (right) for 1000 test samples of CIFAR-Attributional robustness on varying for ART, PGD-10 and Natural models on CIFAR-10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 13 :</head><label>13</label><figDesc>Examples of estimated bounding box and heatmap by ResNet50 model trained via our approach on randomly chosen images of CUB dataset; Red bounding box is ground truth and green bounding box corresponds to the estimated box B Weakly Supervised Localization: More Details and Results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 14 :</head><label>14</label><figDesc>âˆž and 2 adversarial robustness on varying of ART, PGD-10 and Natural model on CIFAR-10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Attributional and adversarial robustness of different approaches on various datasets.Hyper-parameters for attributional attack are same as<ref type="bibr" target="#b12">[13]</ref>. Similarity measures used are IN:Top-k intersection, K:kendall's tau rank order correlation. The values denote similarity between attribution maps of original and perturbed examples<ref type="bibr" target="#b18">[19]</ref> based on Intergrated Gradient method.</figDesc><table><row><cell>Dataset</cell><cell>Approach</cell><cell cols="2">Attributional Robustness IN K</cell><cell cols="2">Accuracy Natural PGD-40 Attack</cell></row><row><cell></cell><cell>Natural</cell><cell>40.25</cell><cell>49.17</cell><cell>95.26</cell><cell>0.</cell></row><row><cell>CIFAR-10</cell><cell>PGD-10 [36]</cell><cell>69.00</cell><cell>72.27</cell><cell>87.32</cell><cell>44.07</cell></row><row><cell></cell><cell>ART</cell><cell>92.90</cell><cell>91.76</cell><cell>89.84</cell><cell>37.58</cell></row><row><cell></cell><cell>Natural</cell><cell>60.43</cell><cell>56.50</cell><cell>95.66</cell><cell>0.</cell></row><row><cell>SVHN</cell><cell>PGD-7 [36]</cell><cell>39.67</cell><cell>55.56</cell><cell>92.84</cell><cell>50.12</cell></row><row><cell></cell><cell>ART</cell><cell>61.37</cell><cell>72.60</cell><cell>95.47</cell><cell>43.56</cell></row><row><cell></cell><cell>Natural</cell><cell>68.74</cell><cell>76.48</cell><cell>99.43</cell><cell>19.9</cell></row><row><cell>GTSRB</cell><cell>IG Norm [13]</cell><cell>74.81</cell><cell>75.55</cell><cell>97.02</cell><cell>75.24</cell></row><row><cell></cell><cell cols="2">IG-SUM Norm [13] 74.04</cell><cell>76.84</cell><cell>95.68</cell><cell>77.12</cell></row><row><cell></cell><cell>PGD-7 [36]</cell><cell>86.13</cell><cell>88.42</cell><cell>98.36</cell><cell>87.49</cell></row><row><cell></cell><cell>ART</cell><cell>91.96</cell><cell>89.34</cell><cell>98.47</cell><cell>84.66</cell></row><row><cell></cell><cell>Natural</cell><cell>38.22</cell><cell>56.43</cell><cell>93.91</cell><cell>0.</cell></row><row><cell>Flower</cell><cell>IG Norm [13]</cell><cell>64.68</cell><cell>75.91</cell><cell>85.29</cell><cell>24.26</cell></row><row><cell></cell><cell cols="2">IG-SUM Norm [13] 66.33</cell><cell>79.74</cell><cell>82.35</cell><cell>47.06</cell></row><row><cell></cell><cell>PGD-7 [36]</cell><cell>80.84</cell><cell>84.14</cell><cell>92.64</cell><cell>69.85</cell></row><row><cell></cell><cell>ART</cell><cell>79.84</cell><cell>84.87</cell><cell>93.21</cell><cell>33.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Weakly Supervised Localization on CUB dataset. Bold text refers to the best GT-Known Loc and Top-1 Loc for each model architecture. * denotes directly reported from the paper. # denotes our implementation of ADL from the official code released by<ref type="bibr" target="#b13">[14]</ref> 2</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell></cell><cell cols="2">Saliency Method</cell><cell></cell><cell>Top-1 Acc</cell></row><row><cell></cell><cell></cell><cell>Grad</cell><cell></cell><cell>CAM</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="4">GT-Known Loc Top-1 Loc GT-Known Loc Top-1 Loc</cell><cell></cell></row><row><cell cols="2">ResNet50-SE ADL [14]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>62.29  *</cell><cell>80.34  *</cell></row><row><cell></cell><cell>ADL #</cell><cell>52.93</cell><cell>43.78</cell><cell>56.85</cell><cell>47.53</cell><cell>80.0</cell></row><row><cell>ResNet50</cell><cell>Natural PGD-7[36]</cell><cell>50.2 66.73</cell><cell>42.0 47.48</cell><cell>60.37 55.24</cell><cell>50.0 39.45</cell><cell>81.12 70.3</cell></row><row><cell></cell><cell>ART</cell><cell>82.65</cell><cell>65.22</cell><cell>58.87</cell><cell>46.02</cell><cell>77.51</cell></row><row><cell></cell><cell>ADL #</cell><cell>63.18</cell><cell>43.59</cell><cell>69.36</cell><cell>50.88</cell><cell>70.31</cell></row><row><cell>VGG-GAP</cell><cell>Natural ART</cell><cell>72.54 76.50</cell><cell>53.81 57.74</cell><cell>48.75 52.88</cell><cell>35.03 40.75</cell><cell>72.94 74.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Top-1 accuracy of different models on perturbed variants of test-set (GN:Gaussian Natural 49.16 61.42 59.22 83.55 53.84 79.16 79.18 84.53 91.6 94.37 87.63 84.44 74.12 79.76 65.04 PGD-10 83.32 84.33 73.73 83.09 81.27 79.60 82.07 82.68 68.81 85.97 57.86 81.68 85.56 85.56 83.64 ART 85.44 86.41 77.07 86.07 81.70 83.14 85.54 84.99 71.04 89.42 56.69 84.72 87.64 87.89 86.02</figDesc><table><row><cell cols="8">noise; SN: Shot noise; IN: Impulse noise; DB: Defocus blur; Gl-B: Glass blur; MB: Motion blur;</cell></row><row><cell cols="8">ZB: Zoom blur; S: Snow; F: Fog; B: Brightness; C: Contrast; E: Elastic transform; P: Pixelation</cell></row><row><cell>noise; J: JPEG compression; Sp-N: Speckle Noise)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models GN SN IN DB Gl-B MB ZB</cell><cell>S</cell><cell>F</cell><cell>B</cell><cell>C</cell><cell>E</cell><cell>P</cell><cell>J Sp-N</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Attributional Robustness on</cell></row><row><cell cols="4">CIFAR-10 for other attribution methods</cell></row><row><cell>Model</cell><cell cols="3">Gradient[56] GradSHAP [34] IN K IN K</cell></row><row><cell cols="2">Natural 13.72 9.5</cell><cell>4.5</cell><cell>16.52</cell></row><row><cell cols="4">PGD-10 [36] 54.8 54.06 45.05 59.80</cell></row><row><cell>ART</cell><cell cols="3">76.07 70.31 48.31 62.35</cell></row></table><note>Fig. 6: Cosine between x and âˆ‡xf (x)y for different models over test-set of CIFAR-10</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of different loss functions used as the objective function for increasing Dataset and Implementation DetailsBelow, we describe the datasets and hyper-parameters used for experiments, which we could not include in the main paper owing to space constraints. Î² = 50 and Î» = 0.5. We calculatex using = 8/255, step size 1.5/255 and number of steps a = 3.GTSRB Data and Model: German Traffic Signal Recognition Benchmark<ref type="bibr" target="#b40">[61]</ref> consists of 43 classes of traffic signals with 34, 799 training images, 4, 410 validation images and 12, 630 test images. We resize the images to 32 Ã— 32 Ã— 3 and normalize the images with its mean and standard deviation for training. To balance the number of images for each class, we use data augmentation techniques consisting of rotation, translation, and projection transforms to extend the training set to 10, 000 images per class as in</figDesc><table><row><cell cols="2">attributional robustness on CIFAR-10</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Optimization Objective</cell><cell cols="3">Attributional Robustness Test Accuracy IN K</cell><cell>Adversarial Accuracy</cell></row><row><cell>Equation 2</cell><cell>74.78</cell><cell>71.40</cell><cell>91.34</cell><cell>15.15</cell></row><row><cell>Equation 4 : 2 distance</cell><cell>68.41</cell><cell>69.75</cell><cell>91.66</cell><cell>16.64</cell></row><row><cell>Equation 4 : Cosine distance</cell><cell>91.25</cell><cell>89.28</cell><cell>89.21</cell><cell>35.95</cell></row><row><cell>Equation 5 : ART with i  *  =argmin(logit)</cell><cell>90.75</cell><cell>83.32</cell><cell>89.94</cell><cell>37.93</cell></row><row><cell>Equation 5 : ART (ours)</cell><cell>92.90</cell><cell>91.76</cell><cell>89.84</cell><cell>37.50</cell></row><row><cell>A.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison of Adversarial accuracy of different baseline models using transfer-based black-box attacks on CIFAR-10</figDesc><table><row><cell>Training Approach</cell><cell>Adversarial perturbation created using Natural PGD-10 ART</cell><cell>Clean Test Accuracy</cell></row><row><cell>Natural</cell><cell>0.00 80.35 49.09</cell><cell>95.26</cell></row><row><cell>PGD-10</cell><cell>86.44 44.07 71.34</cell><cell>87.32</cell></row><row><cell>ART</cell><cell>88.45 72.72 37.58</cell><cell>89.84</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/junsukchoe/ADL/tree/master/Pytorch</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SVHN</head><p>Data and Model: SVHN dataset <ref type="bibr" target="#b21">[42]</ref> consists of images of digits obtained from house numbers in Google Street View images, with 73257 digits for training and 26032 digits for testing over 10 classes. We perform experiments on SVHN using WideResNet-40-2 <ref type="bibr" target="#b52">[73]</ref> architecture for training on reported approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters for Training:</head><p>Natural: We use SGD optimizer with an initial learning rate of 0.1, momentum of 0.9, l 2 weight decay of 2eâˆ’4 and batch size of 256. We train it for 200 epochs with a learning rate schedule decay of 0.1 at 50 th , 80 th and 0.5 at 150 th epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PGD-7:</head><p>We use the training configuration as in [37] to perform 7-step adversarial training with = 8/255 and step size 2.5/255. ART: We use the same training configuration as mentioned for Natural model, Î² = 50 and Î» = 0.5. We calculatex using = 8/255, step size 1.5/255 and number of steps a = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>Data and Model: CIFAR-10 dataset [32] consists of 50000 training images for 10 classes with resolution of 32 Ã— 32 Ã— 3. We normalize the images with its mean and standard deviation for training. We train a WideResNet28-10 [73] model for all the experiments on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters for Training:</head><p>Natural: We use SGD optimizer with an initial learning rate of 0.1, momentum of 0.9, l 2 weight decay of 2eâˆ’4 and batch size of 256. We train it for 100 epochs with a learning rate schedule decay of 0.1 at 50 th , 80 th and 0.5 at 150 th epoch. PGD-10: We use the training configuration as mentioned in [37] to perform 10-step adversarial training with = 8/255 and step size 2/255. ART: We use the same training configuration as mentioned for Natural model with <ref type="bibr" target="#b12">[13]</ref>. We train WideResNet28-10 [73] model for carrying out experiments related to this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hyperparameters for Training:</head><p>Natural: We use SGD optimizer with an initial learning rate of 0.1, momentum of 0.9, l 2 weight decay of 2eâˆ’4 and batch size of 128. We train it for 12 epochs with a learning rate schedule decay of 0.1 at 4 th , 6 th and 0.5 at 10 th epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PGD-7:</head><p>We use the training configuration same as <ref type="bibr" target="#b12">[13]</ref> to perform 7-step adversarial training with = 8/255 and step size 2/255. IG Norm and IG-Sum Norm <ref type="bibr" target="#b12">[13]</ref>: We report the accuracy as mentioned in the paper <ref type="bibr" target="#b12">[13]</ref>. ART: We use the same training configuration as mentioned for Natural model with Î² = 50 and Î» = 0.5. We calculatex using = 8/255, step size 1.5/255 and number of steps a = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Flower</head><p>Data and Model: Flower dataset <ref type="bibr" target="#b22">[43]</ref> has 17 categories with 80 images for each class. We resize the images to 128 Ã— 128 Ã— 3 and normalize it with its mean and standard deviation for training. The training set consists of 1, 224 images with 72 images per class. The test set compromises of 136 images with 8 images per class. We use standard data augmentation techniques of rotation, translation, and projection transforms to extend the training data so that each class contains 1, 000 training examples as proposed in <ref type="bibr" target="#b12">[13]</ref>. We use WideResNet28-10 [73] model for the reported approaches.</p><p>Hyperparameters for Training: Natural: We use SGD optimizer with an initial learning rate of 0.1, momentum of 0.9, l 2 weight decay of 2eâˆ’4 and batch size of 128. We train it for 68 epochs with a learning rate schedule decay of 0.1 at 15 th , 35 th and 0.5 at 50 th epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PGD-7[36]:</head><p>We use the training configuration as mentioned in [37] to perform 7-step adversarial training with = 8/255 and step size 2.5/255. IG Norm and IG-Sum Norm <ref type="bibr" target="#b12">[13]</ref> : We report the accuracy as mentioned in the paper <ref type="bibr" target="#b12">[13]</ref>. ART: We use the same training configuration as mentioned for Natural model with Î» = 0.5 and Î² = 50. We calculatex using = 8/255, step size 1.5/255 and number of steps a = 3.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Threat of adversarial attacks on deep learning in computer vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
		<title level="m">On the robustness of interpretability methods. ICML 2018 Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Towards robust interpretability with self-explaining neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Alvarez-Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-to-end lung cancer screening with three-dimensional deep learning on low-dose chest computed tomography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ardila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Kiraly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Reicher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Etemadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Naidich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="954" to="961" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Klauschen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>MÃ¼ler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Network dissection: Quantifying interpretability of deep visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Athalye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rauber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.06705</idno>
		<title level="m">On evaluating adversarial robustness</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Symposium on Security and Privacy</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Jacobian adversarially regularized networks for robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Grad-cam++: Generalized gradient-based visual explanations for deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Howlader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.11063</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rastogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09957</idno>
		<title level="m">Robust attribution regularization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Attention-based dropout layer for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2219" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Explanations can be manipulated and geometry is to blame</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Dombrowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Anders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kessel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="13567" to="13578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.00033</idno>
		<title level="m">Techniques for interpretable machine learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring the landscape of spatial robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1802" to="1811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Etmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lunz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Maass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>SchÃ¶nlieb</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04172</idno>
		<title level="m">On the connection between adversarial robustness and saliency map interpretability</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Interpretation of neural networks is fragile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghorbani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3681" to="3688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Robustness via curvature regularization, and vice versa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9078" to="9086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A visual vocabulary for flower classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1447" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Delving deeper into the whorl of flower segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.imavis.2009.10.001</idno>
		<ptr target="http://dx.doi.org/10.1016/j.imavis.2009.10.001" />
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1049" to="1062" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Exploiting saliency for object segmentation from image level labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>abs/1701.08261</idno>
		<ptr target="http://arxiv.org/abs/1701.08261" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Practical blackbox attacks against machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">B</forename><surname>Celik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Rise: Randomized input sampling for explanation of blackbox models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Petsiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gowal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stanforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02610</idno>
		<title level="m">Adversarial robustness through local linearization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Why should i trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACM SIGKDD</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Image synthesis with a single (robust) classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>An Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3145" to="3153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shcherbina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01713</idno>
		<title level="m">Not just a black box: Learning important features through propagating activation differences</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Harnessing the vulnerability of latent layers in adversarially trained models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Machiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balasubramanian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05186</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Smilkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thorat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>ViÃ©gas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<title level="m">Smoothgrad: removing noise by adding noise. Workshop on Visualization for Deep Learning, ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<title level="m">Striving for simplicity: The all convolutional net. ICLR workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The German Traffic Sign Recognition Benchmark: A multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Neural Networks</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1453" to="1460" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Axiomatic attribution for deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundararajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Using a deep learning algorithm and integrated gradient explanation to assist grading for diabetic retinopathy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Taly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Coz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahimy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Blumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shumski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hammel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Sayres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Rastegar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ophthalmology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Robustness may be at odds with accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Santurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Adversarial risk and the dangers of evaluating against weak attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>O&amp;apos;donoghue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Weakly supervised top-down image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2006.333</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2006.333" />
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1001" to="1006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards weakly supervised semantic segmentation by means of multiple instance and multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2010.5540060</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2010.5540060" />
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="3249" to="3256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. Rep.</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erdogmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<title level="m">Structured adversarial attack: Towards general implementation and better interpretability. ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Representer point selection for explaining deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">E</forename><surname>Yen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Adversarial examples: Attacks and defenses for deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1605.07146</idno>
		<ptr target="http://arxiv.org/abs/1605.07146" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Defense against adversarial attacks using feature scattering-based adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ghaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08573</idno>
		<title level="m">Theoretically principled trade-off between robustness and accuracy</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<title level="m">Top-down neural attention by excitation backprop. ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Probabilistic graphlet cut: Exploiting spatial structure cue for weakly supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2013.249</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2013.249" />
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="1908" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00614</idno>
		<title level="m">Visual interpretability for deep learning: a survey</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00891</idno>
		<title level="m">Interpretable deep learning under fire</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
