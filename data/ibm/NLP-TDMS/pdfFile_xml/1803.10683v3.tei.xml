<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pose2Seg: Detection Free Human Instance Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Hai</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruilong</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Rosin</surname></persName>
							<email>rosinpl@cardiff.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">Cardiff University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zixi</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingcheng</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Huang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Pose2Seg: Detection Free Human Instance Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1 Codes are available: https://github.com/liruilong940607/Pose2Seg 2 Dataset is available: https://github.com/liruilong940607/OCHumanApi Figure 1: Heavily occluded people are better separated us-ing human pose than using bounding-box.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The standard approach to image instance segmentation is to perform the object detection first, and then segment the object from the detection bounding-box. More recently, deep learning methods like Mask R-CNN [14] perform them jointly. However, little research takes into account the uniqueness of the "human" category, which can be well defined by the pose skeleton. Moreover, the human pose skeleton can be used to better distinguish instances with heavy occlusion than using bounding-boxes. In this paper, we present a brand new pose-based instance segmentation framework 1 for humans which separates instances based on human pose, rather than proposal region detection. We demonstrate that our pose-based framework can achieve better accuracy than the state-of-art detectionbased approach on the human instance segmentation problem, and can moreover better handle occlusion. Furthermore, there are few public datasets containing many heavily occluded humans along with comprehensive annotations, which makes this a challenging problem seldom noticed by researchers. Therefore, in this paper we introduce a new benchmark "Occluded Human (OCHuman)" 2 , which focuses on occluded humans with comprehensive annotations including bounding-box, human pose and instance masks. This dataset contains 8110 detailed annotated human instances within 4731 images. With an average 0.67 Max-IoU for each person, OCHuman is the most complex and challenging dataset related to human instance segmentation. Through this dataset, we want to emphasize occlusion as a challenging problem for researchers to study.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, research related to "humans" in the computer vision community has become increasingly active because of the high demand for real-life applications. There has been much good research in the fields of human pose estimation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b39">40]</ref>, pedestrian detection <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b42">42]</ref>, portrait segmentation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>, and face recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44]</ref>, much of which has already produced practical value in real life. This paper focuses on multi-person pose estimation and human instance segmentation, and proposes a pose-based human instance segmentation framework.</p><p>General Object Instance Segmentation is a challenging problem which aims to predict pixel-level labels for each object instance in the image. Currently, those instance segmentation methods with highest accuracy <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b29">30]</ref> are all based on powerful object detection baseline methods, such as Fast/Faster R-CNN <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33]</ref>, YOLO <ref type="bibr" target="#b31">[32]</ref>, which mostly follow a basic rule: first generate a large number of proposal regions, then remove the redundant regions using Non-maximum Suppression (NMS). However, when two objects of the same category have a large overlap, NMS will treat one of them as a redundant proposal region and eliminates it. This means that almost all the object detection methods cannot deal with the situation of large overlaps. Moreover, even if the detection methods sometimes successfully detect two instances, the bounding-box is not suitable for instance segmentation in occluded cases. If two in-stances are heavily intertwined, they will both appear in the same bounding-box (like the case in <ref type="figure">Figure 1</ref>), which makes it hard for the segmentation network to identify which instance should be the target in this Region of Interest (RoI).</p><p>However, "human" is a special category in the computer vision community, and can be well defined by the pose skeleton. As shown in <ref type="figure">Figure 1</ref>, Human pose skeletons are more suitable for distinguishing two heavily intertwined people, because they can provide more distinct information about a person than bounding-boxes, such as the location and visibility of different body parts. Multi-Person Pose Estimation is also a very active topic in recent years, and there is already good progress <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b28">29]</ref> on tackling this problem. Although object detection methods are widely used by many multi-person pose estimation frameworks, some powerful bottom-up methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b25">26]</ref> which do not rely on object detection also achieved good performance, including the COCO keypoints challenge 2016 winner <ref type="bibr" target="#b0">[1]</ref>. The main idea of the bottom-up methods is to first detect keypoints for each body part for all the people, and then group or connect those parts to form several instances of human pose, which makes it possible to seperate two intertwined human instances with a large overlap. Based on this observation, we present a new pose-based instance segmentation framework for humans which separates instances based on human pose rather than region proposal detection. Our pose-based framework works seamlessly with existing bottom-up pose estimation methods, and works better than the detection-based framework, especially in the case of occlusion.</p><p>Generally, there is an align module in the instance segmentation framework, for example, RoI-Align in Mask R-CNN. The align module is used to crop the objects from the image using detection bounding boxes, and resize the objects to a uniform scale. Since it is hard to find a bounding box accurately from the object using human pose, we proposed an align module based on human pose, called Affine-Align, which is a combination of scale, translation, rotation and left-right flip. An extra advantage of using Affine-Align is that we can correct some objects with strange poses to a standard pose, like the inverted skiing human in <ref type="figure" target="#fig_0">Figure 2</ref>.</p><p>Additionally, the human pose and human mask are not independent. Human pose can be approximately considered as a skeleton of the mask of the human instance. So we explicitly use human pose to guide the segmentation module by concatenating the Skeleton features to the instance feature map after Affine-Align. Our experiments demonstrate our Skeleton features not only help to improve the accuracy of segmentation, but also give our network the ability to easily distinguish different instances that are heavily intertwined in the same RoI.</p><p>Severe occlusion between human bodies is often encountered in life, but current human-related public datasets either do not contain many severe occlusion situations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b20">21]</ref>, or lack comprehensive annotations of the human in- stances <ref type="bibr" target="#b33">[34]</ref>. Therefore, we introduce a new benchmark "Occluded Human (OCHuman)" in this paper, which focuses on heavily occluded humans with comprehensive annotations including bounding-boxes, human poses and instance masks. This dataset contains 8110 detailed annotated human instances within 4731 images. On average, over 67% of the bounding-box area of a human is occluded by one or several other persons, which makes this dataset the most complex and challenging dataset related to humans. Through this dataset, we want to emphasize occlusion as a challenging problem for researchers to study, and encourage current algorithms to become more practical for real life situations.</p><p>Our main contributions can be summarized as follows:</p><p>• We propose a brand new pose-based human instance segmentation framework which works better than the detection-based framework, especially in cases with occlusion.</p><p>• We propose a pose-based align module, called Affine-Align, which can align image windows into a uniform scale and direction based on human pose.</p><p>• We explicitly use artific human Skeleton features to guide the segmentation module and achieve a further improvement of the segmentation accuracy.</p><p>• We introduce a new benchmark OCHuman which focuses on the heavy occlusion problem, with comprehensive annotations including bounding-boxes, human poses and instance masks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-Person Pose Estimation</head><p>Top-down methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b28">29]</ref> first employ object detection to crop each person, and then use a singleperson pose estimation method on each human instance, which makes them all suffer from the defects of object detection methods on heavy occlusion. While other bottom-up methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b30">31]</ref> first detect body part keypoints of all the people, and then cluster these parts into instances of human pose. Pishchulin et al. <ref type="bibr" target="#b30">[31]</ref> propose a complex framework of partitioning and labeling body-parts generated using a CNN. They solve the problem as an integer linear program, and jointly generate the detection and pose estimation results. Insafutdinov et al. <ref type="bibr" target="#b16">[17]</ref> use Resnet <ref type="bibr" target="#b14">[15]</ref> to improve precision, and propose image-conditioned pairwise terms to increase speed. Cao et al. <ref type="bibr" target="#b0">[1]</ref> use knowledge of the human structure, and predict a keypoints heatmap and PAFs, and finally connect the body parts. Newell et al. <ref type="bibr" target="#b25">[26]</ref> design a tag score map for each body part and use the score map to group body part keypoints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Instance Segmentation</head><p>Some works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13</ref>] employ a multi-stage pipeline which first uses detection to generate bounding boxes and then applies semantic segmentation. Others <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref> employ a tighter integration of detection and segmentation, e.g. jointly and simultaneously performing detection and segmentation in an end-to-end framework <ref type="bibr" target="#b18">[19]</ref>. Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> is the state-of-art performing framework on the COCO <ref type="bibr" target="#b20">[21]</ref> dataset competition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Harnessing Human Pose Estimation for Instance Segmentation</head><p>There are three typical works that combine human pose estimation and instance segmentation. Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> approach detects objects while generating instance segmentation and human pose estimation simultaneously in a single framework. But in their work, Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> with mask-only performs better than combining keypoints and masks in the instance segmentation task. Pose2Instance <ref type="bibr" target="#b37">[38]</ref> proposes a cascade network to harness human pose estimation for instance segmentation. Both of these two works rely on human detection, and perform poorly when two bounding boxes have a large overlap. More recently, PersonLab <ref type="bibr" target="#b27">[28]</ref> treats instance segmentation as a pixel-wise clustering problem, and use human pose to refine the clustering results. Although their method is not based on bounding-box detection, they cannot perform as well as Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> in the segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Occluded Human Benchmark</head><p>Our "Occluded Human (OCHuman)" dataset contains 8110 human instances within 4731 images. Each human instance is heavily occluded by one or several others. We use MaxIoU to measure the severity of an object being occluded, which means the max IoU with other same category objects in a single image. Those instances with MaxIoU &gt;0.5 are referred to as heavy occlusion, and are selected to form this dataset. <ref type="figure" target="#fig_1">Figure 3</ref> shows some samples from this dataset. With an average of 0.67 MaxIoU for each person, OCHuman is the most challenging dataset related to human instances. Moreover, OCHuman also has rich annotations. Each instance is annotated with a bounding-box for object detection, an instance binary mask for instance segmentation and 17 body joint locations for pose estimation. All images are collected from real-world scenarios containing people with challenging poses and viewpoints, various appearances and in a wide range of resolutions. With OCHuman, we provide a new benchmark for the problem of occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Annotations</head><p>For each image we first annotate the bounding-box of all humans present. Then we calculate the IoU between all the person pairs, and mark those persons with MaxIoU&gt;0.5 as heavily occluded instances. Finally, we provide extra information for those occluded instance. The OCHuman dataset contains three kinds of annotations related to humans: bounding-boxes, instance binary masks and 17 body joint locations. We reference the definition of body joints from <ref type="bibr" target="#b20">[21]</ref>, which are eye, nose, ear, shoulder, elbow, wrist, hip, knee and ankle. Except for the nose, all other joints have distinct left and right instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dataset Splits</head><p>OCHuman dataset is designed for validation and testing. Since all the instances in this dataset are heavily occluded by other instances, we consider it is better to use general datasets such as COCO <ref type="bibr" target="#b20">[21]</ref> as a training set, then test the robustness of the segmentation methods to occlusion using this dataset, rather than performing training on   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dataset Statistics</head><p>We compare our dataset with the person part of COCO in <ref type="table">Table.</ref> 1, which is currently the largest public dataset that contains both instance masks and human pose key-points. Although COCO includes comprehensive annotations, it contains few occluded human cases, and so this dataset cannot help to evaluate the capability of methods when faced with occlusion. OCHuman is designed for all three most important tasks related to humans: detection, pose estimation and instance segmentation. It is the most challenging benchmark because of its heavy occlusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Overview</head><p>Our overall structure is shown in <ref type="figure" target="#fig_2">Figure 4</ref>, which takes both the image and the human pose as input. Firstly, a base network is used to extract the features of the image. Then an align module, called Affine-Align, is used to align RoIs to a uniform size, which is 64×64 in this paper, based on the human pose. In the meantime, we generate Skeleton features for each human instance and concatenate them to the RoIs. Our segmentation module, which we called SegModule, is designed based on the same residual unit in Resnet <ref type="bibr" target="#b14">[15]</ref>. We carry out experiments on how the depth of SegModule contributes to the performance of this system in Section 5.3.3. Finally, we use the estimated matrices in Affine-Align operation to reverse the alignment for each instance and get the final segmentation results. We describes our Affine-Align operation, Skeleton features and SegModule in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Affine-Align Operation</head><p>Our Affine-Align operation is inspired by the RoI-Pooling in Faster R-CNN <ref type="bibr" target="#b32">[33]</ref> and RoI-Align in Mask R-CNN <ref type="bibr" target="#b13">[14]</ref>. But unlike them, we align the people based on human pose instead of bounding-boxes. Specifically, as shown in <ref type="figure" target="#fig_2">Figure 4</ref>(a), we first cluster the poses in the dataset and use the center of each cluster as pose templates, to represent the standard poses in the dataset. Then for each pose detected in the image, we estimate the affine transformation matrix H between it and the templates, and chose the best H based on the transformation error. Finally, we apply H to the image or features and transform it to the desired resolution using bilinear interpolation. Details are introduced below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Human Pose Representation</head><p>Human poses are represented as a list of vectors. Let vector P = (C 1 , C 2 , ..., C m ) ∈ R m×3 represent the pose of a single person, where C i = (x, y, v) ∈ R 3 is a 3D vector representing the coordinates of a single part (such as rightshoulder, left-ankle) and the visibility of this body joint. m is a dataset related parameter meaning the total number of parts in a single pose, which is 17 in COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Pose Templates</head><p>We cluster the pose templates from the training set to best represent the distribution of various human poses. We use K-means clustering <ref type="bibr" target="#b6">[7]</ref> to cluster the poses (P 1 , P 2 , ..., P n ) into k(≤ n) sets S = {S 1 , S 2 , ..., S k } by optimizing Eq. 1, in which P µi is the mean of poses in S i . We define the distance between two human poses using Eq. 2 and Eq. 3, with several preprocessing steps: (1) We first crop a square-RoI of each instance using its bounding-box, and put the target into the center of the RoI, along with its pose coordinates.</p><p>(2) We resize this square-RoI to 1 × 1, so that the pose coordinates are all normalized to (0, 1). (3) We only count those poses which contain more than 8 valid points in the dataset to serve our purpose of creating the pose templates. Poses with few valid points cannot provide effective information and would act as outliers during K-means clustering.</p><formula xml:id="formula_0">arg min S K i=1 P ∈S i Dist(P, Pµi) (1) Dist(P, Pµi) = m j=1 Cj − Cµij 2 (2) Cj =      (x, y, 2) if Cj is visible (x, y, 1)</formula><p>if Cj is not visible (0.5, 0.5, 0) if Cj is not in image <ref type="formula">(3)</ref> After K-means, we use the mean value of each set P µi to form the pose template and use it to represent the whole group. We set those body joints with v &gt; 0.5 in P µi as valid points. Clustering results with different values of K on the COCO training set are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. Although the results of K-means are heavily reliant on initialization values, our multiple experimental results remain the same, which shows that there is a strong distinction between different sets of human poses. After careful observation of those pose templates, we can find the two most frequent human poses in COCO are a half-body pose and a full-body pose, which is in line with our common sense view of daily life. When K = 3 in K-means, we get a half-body pose, a full-body backview and a full-body frontview. When K ≥ 4, the difference between left and right are introduced. Since our align process copes with the left-right flip, K ≥ 4 seems unnecessary for our framework. So finally, we choose K = 3 to cluster pose templates in our approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Estimate Affine Transformation Matrix</head><p>Let vector P µ represent a pose template, and P represent a single person pose estimation result. We optimize Eq. 4 to estimate an affine transformation matrix H which transforms the pose coordinates to be as near as possible to the template coordinates. H is a 2 × 3 matrix with 5 independent variables: rotation, scale factor, x-axis translation, yaxis translation and whether to do left-right flip. Since we have K templates, we define a score for each H * based on the optimized error value, calculated by Eq. 5, to choose the best template for each estimated pose, as shown in <ref type="figure" target="#fig_2">Figure 4(a)</ref>. In order to get the unique solution from Eq. 4, P µ and P must contain at least three valid points in common, which can provide at least 6 independent equations for optimizing Eq. 4. If none of our pose templates satisfy this condition, such as the case where there is only one valid point in P , the estimated transformation matrix H * will be calculated to align the whole image to the desired solution. In most case, this is reasonable because situations lacking valid points in the image mostly correspond to a single, large person in the image. <ref type="figure" target="#fig_2">Figure 4</ref>(b) shows our Skeleton features. We adopt the part affinity fields (PAFs) from <ref type="bibr" target="#b0">[1]</ref>, which is a 2-channel vector field map for each skeleton. We use PAFs to represent the skeleton structure of a human pose. With 19 skeletons defined in the COCO dataset, PAFs is a 38-channel feature map for each human pose instance. We also use part confidence maps for body parts to emphasize the importance of those regions around the body part keypoints. For the COCO dataset, each human pose has a 17-channel part confidence map and a 38-channel PAFs map. So the total number of channels in our Skeleton features is 55 for each human instance. </p><formula xml:id="formula_1">H * = arg min H H · P − P µ . (4) score = exp(− H * · P − P µ ) )<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Skeleton Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">SegModule</head><p>Since we introduced Skeleton features after alignment to artificially extend the image features, Our segmentation module, which we called SegModule, needs to have enough receptive fields to not only fully understand these artificial features, but also learn the connections between them and the image features extracted by the base network. Therefore, we design SegModule based on the resolution of the aligned RoIs. <ref type="figure" target="#fig_2">Figure 4</ref>(c) demonstrates the overall architecture of our SegModule. It starts with a 7 × 7, stride-2 convolution layer, and is followed by several standard residual units <ref type="bibr" target="#b14">[15]</ref> to achieve a large enough receptive field for the RoIs. After that, a bilinear upsampling layer is used to restore the resolution, and another residual unit, along with a 1 × 1 convolution layer are used to predict the final result. Such a structure with 10 residual units can achieve about 50 pixels of receptive field, corresponding to our alignment size of 64 × 64. Fewer units will make the network less capable of learning, and more units enable little improvement on the learning ability. <ref type="table" target="#tab_5">Table 4</ref> shows our experiment on this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our proposed method on two datasets: (1) OCHuman, which is the largest validation dataset that is focused on heavily occluded humans, and proposed in this paper; and (2) COCOPersons (the person category of COCO) <ref type="bibr" target="#b20">[21]</ref>, which contains the most common scenarios in daily life. Note that the Small category persons in COCO is not contained in COCOPersons due to the lack of annotations of human pose.</p><p>As far as we know, there are few public datasets which have labels for both human pose and human instance segmentation. COCO is the largest dataset that meets both of these requirements, so all of our models are trained end-toend on the COCOPersons training set with the annotations of pose keypoints and segmentation masks. We compare our methods with Mask-RCNN <ref type="bibr" target="#b13">[14]</ref>, the well known detection based instance segmentation framework. For Mask-RCNN <ref type="bibr" target="#b13">[14]</ref>, we use the author's released code and configurations from <ref type="bibr" target="#b10">[11]</ref>, and retrained and evaluate the model on the same dataset as our method. Our framework is implemented using Pytorch. The input resolution of our framework is 512 × 512 in all experiments. All our models are trained using the same training schedule, which is started by learningrate = 2e − 4, decayed by 0.1 after 33 epochs, and ended after 40 epochs. Each model is trained on a single TITAN X (Pascal) with batchsize = 4 for 80 hours. No special techniques are used, such as iterative training, online hard-case mining, or multi-GPU synchronized batch normalization. Our method with images and keypoints as inputs can run about 20 FPS on a TITAX X (Pascal).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Performance on occlusion</head><p>In this experiment, we evaluate our method's capacity for handling occlusion cases compared with Mask-RCNN <ref type="bibr" target="#b13">[14]</ref> on the OCHuman dataset. All methods in this experiment are trained on COCOPersons, including our keypoint detector baseline <ref type="bibr" target="#b25">[26]</ref> which achieves 0.285 / 0.303 AP on the keypoints task of OCHuman val / test set. As shown in <ref type="table" target="#tab_2">Table 2</ref>, based on this keypoint detector baseline, our framework can achieve nearly 50% higher than the performance of Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> on this dataset. In addition, we test the upper limits of our pose-based framework using ground-truth (GT) keypoints as input, and more than double the accuracy. This demonstrates that with a better keypoint detector our framework can perform far better on occlusion problems. Some results are shown in <ref type="figure" target="#fig_4">Figure 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance on general cases</head><p>In this experiment, we evaluate our model on the CO-COPerson validation set using groundtruth keypoints as input, and get 0.582 AP on the instance segmentation task. We also evaluate the performance of our model under the predicted pose keypoints using our keypoint detector baseline <ref type="bibr" target="#b25">[26]</ref>, and achieve 0.555 AP. Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> can only achieve 0.532 AP on this same dataset. We further compare our results with a recent work, PersonLab <ref type="bibr" target="#b27">[28]</ref>. Scores of PersonLab <ref type="bibr" target="#b27">[28]</ref> are taken from their paper, in which the detector is trained and tested on the whole person category of COCO. For fair comparison, we only compare against the results of the Median and Large categories. Our results surpass theirs with a heavier backbone and multi-scale pre-   diction, as shown in <ref type="table" target="#tab_3">Table 3</ref>. <ref type="figure" target="#fig_6">Figure 8</ref> and <ref type="figure" target="#fig_5">Figure 7</ref> show some results of our instance segmentation framework and our Affine-Align operation, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Ablation Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Affine-Align v.s. RoI-Align</head><p>Occluded Cases In this experiment, we replace the align module in our framework with RoI-Align based on groundtruth (Gt) bounding-box, and re-train our model with nothing else changed. As shown in <ref type="table">Table 5</ref>, this box-based alignment strategy can achieve 0.476 AP on OCHuman validation set. Our Affine-Align based on Gt human pose can achieve 0.544 AP on this same dataset. This means that even if we do not take into account the NMS's deficiencies on handling occlusion (which is eliminated by using GT bounding-boxes), the box-based alignment strategy still does not perform as well as our pose-based alignment strategy in the instance segmentation task of occlusion. The reason is that rotation is allowed in Affine-Align, which helps  to better distinguish two heavy intertwined people by aligning into discriminative RoIs. Strong discrimination RoIs are essential for the segmentation network to locate and extract the specific target.</p><p>General Cases We also experiment on COCOPerson validation set. If we allow using both Gt bounding-box and Gt keypoint as input, the best performance is achieved by combining RoI-Align and our Skeleton features (0.648 AP). While simultaneously requiring bounding-box and keypoint as input is a rather strict requirement, and both of them can introduce error to the framework when using predicted results instead of ground-truth. If we contraint the framework to only rely on one of them, combining Affine-Align with Skeleton features can achieved better performance than using RoI-Align stategy on COCOPerson (0.582 AP v.s. 0.568 AP). What's more, the upper limits of the box-based framework is limited by NMS, especially in the case of occlusion. In comparison, our pose-based alignment strategy has no such limits.</p><p>Intuitive Pose-based Alignment An intuitive idea of pose-based alignment is to first generate bounding-boxes based on human pose key-points, and then use a box-based alignment strategy, such as RoI-Align, to align each person into a RoI. We take the maximum and minimum values of the valid key-points as the generated bounding-box, and expand the generated bounding-box by a factor α to simulate the accurate bounding-box as much as possible. We treat α as a hyperparameter and search for the best value during testing. <ref type="table">Table 5</ref> shows that no matter how this hyperparameter α is adjusted, the performance still cannot match our Affine-Align strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">With/Without Skeleton Features</head><p>We also experiment on the contribution of our artificial Skeleton features. <ref type="table">Table 5</ref> shows that our Skeleton features are good for different kinds of align strategies because manually concatenating the features of human pose can explicitly provide more information for the network, and lead to a more accurate result. This is more effective for situations where there is more than one person in the RoIs (which is very common), because Skeleton features can explicitly guide the network to focus on the specific person. Also, due to this component our framework can better segment the person under occlusion than the previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.3">SegModule</head><p>We have discussed in Section 4.4 that the receptive field is an important factor to be considered in designing the Seg-Module. So we experiment how the receptive field of Seg-Module affects our system. We achieve different receptive fields by stacking different numbers of residual units after the first convolution. Besides that, all the other components stay unchanged. As shown in <ref type="table" target="#tab_5">Table 4</ref>, our SegModule with 10 residual units can achieve about 50 pixels of receptive field, which is enough for our 64 × 64 alignment size. A large enough receptive field can provide enough learning ability to understand the image features and artificial features globally. Fewer units will make the network less capable of learning, and more units have little help with the learning ability,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a pose-based human instance segmentation framework. We designed an Affine-Align  <ref type="table">Table 5</ref>: Ablation experiments on OCHuman val set and COCOPerson val set about different alignment strategies and Skeleton features. All scores are tested using groundtruth (GT) bounding-box (BBOX) or keypoint (KPT). 'GT KPT to BBOX' represents taking the maximum and minimum values of the valid KPT as the BBOX, and expanding the BBOX by a factor. Notice that scores marked by * rely on both BBOX and KPT as input, while others rely on only one of them.</p><p>operation for selecting RoIs based on pose instead of bounding-boxes. We explicitly concatenate the human pose skeleton feature to the image feature in the network to further improve the performance. Compared with the traditional detection based instance segmentation framework, our pose-based system can achieve a better performance in the general case, and can moreover better handling occlusion. In addition, we introduce a new dataset called OCHuman, which focuses on heavily occluded humans, as a challenging benchmark on occlusion problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of box-based alignment and our pose-based alignment (Affine-Align). Objects with strange pose are corrected to a standard pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Samples of our OCHuman dataset. All the annotated people in this dataset are heavily occluded with others, and have comprehensive annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Overview of our network structure (Sec. 4.1). (a) Affine-Align operation (Sec. 4.2). (b) Skeleton features (Sec. 4.3). (c) Structure of SegModule (Sec. 4.4), in which residual unit refers to<ref type="bibr" target="#b14">[15]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Pose templates clustered using K-means on COCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Our method's results vs. Mask R-CNN<ref type="bibr" target="#b13">[14]</ref> on occlusion cases. Bounding-boxes in our results are generated using predicted masks for better visualization and comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>More results of our Affine-Align operation. (a) shows the align window on the original image. (b) shows the align results and the segmentation results of our framework. 0.222 0.261 0.150 Ours(GT Kpt) Resnet50-fpn 0.544 0.576 0.491 (a) Performance on OCHuman val set. 0.238 0.266 0.175 Ours(GT Kpt) Resnet50-fpn 0.552 0.579 0.495 (b) Performance on OCHuman test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>More results of our instance segmentation framework on COCO. Bounding-boxes are generated using predicted masks for better visualization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of different public datasets related to occluded human. "persons (oc X )" represents occluded persons with MaxIoU &gt; X. only occluded cases. We split our dataset into separate validation and test sets. Following random selection, we arrive at a unique split consisting of 2500 validation and 2231 testing image, containing 4313 and 3797 instances respectively. Furthermore, we divide instances in OCHuman dataset into two subsets: OCHuman-Moderate and OCHuman-Hard. The first subset contains instances with MaxIoU in the range of 0.5 and 0.75, while the second contains instances with MaxIoU larger than 0.75, making it the more challenging subset. With these two subsets, we can evaluate the ability of algorithms to handle occlusions of different levels of severity.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance on occlusion. All methods are trained on COCOPersons train split, and tested on OCHuman. Ours (GT Kpt) indicates our method with the input of ground-truth keypoints.</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell cols="2">AP APM APL</cell></row><row><cell>Mask R-CNN</cell><cell>Resnet50-fpn</cell><cell cols="2">0.532 0.433 0.648</cell></row><row><cell>PersonLab</cell><cell>Resnet101</cell><cell>-</cell><cell>0.476 0.592</cell></row><row><cell>PersonLab</cell><cell>Resnet101(ms scale)</cell><cell>-</cell><cell>0.492 0.621</cell></row><row><cell>PersonLab</cell><cell>Resnet152</cell><cell>-</cell><cell>0.483 0.595</cell></row><row><cell>PersonLab</cell><cell>Resnet152(ms scale)</cell><cell>-</cell><cell>0.497 0.621</cell></row><row><cell>Ours</cell><cell>Resnet50-fpn</cell><cell cols="2">0.555 0.498 0.670</cell></row><row><cell cols="2">Ours(GT Kpt) Resnet50-fpn</cell><cell cols="2">0.582 0.539 0.679</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Performance on general cases. Mask R-CNN and ours are trained on the COCOPersons train split, and tested on the COCOPersons val split (without Small category persons). Scores of PersonLab<ref type="bibr" target="#b27">[28]</ref> is referred from their paper. Ours (GT Kpt) indicates our method with the input of ground-truth keypoints.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Experiments on the depth of SegModule under 64 × 64 RoIs. 10 residual units with a receptive field of about 50 pixels is enough for this alignment size. Deeper architecture brings little benefits. All scores are tested on the COCOPerson val set.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-En</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaser</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07319</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional feature masking for joint object and stuff segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3992" to="4000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bernt Schiele, and Pietro Perona. Pedestrian detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="304" to="311" />
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuqin</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Cluster analysis of multivariate data: efficiency versus interpretability of classifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Forgy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">biometrics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="768" to="769" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable part models are convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="437" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Piotr Dollár, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simultaneous detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="297" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bharath Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A coarsefine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoli</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DeeperCut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eldar</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bjoern</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A largescale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE international conference on computer vision workshops (ICCV workshops)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Real-time avatar pose transfer and motion generation using locally encoded laplacian offsets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celong</forename><surname>Masoud Zadghorban Lifkooee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yimin</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="256" to="271" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sgn: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust facial landmark detection and tracking across poses and expressions for in-the-wild monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="47" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust sparse representation based face recognition in an adaptive weighted spatial pyramid structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fandong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuelong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jufu</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">12101</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">What can help pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3127" to="3136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A fast face detection architecture for autofocus in smart-phones and digital cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouyi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leibo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojun</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science China Information Sciences</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">122402</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08225</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Chris Bregler, and Kevin Murphy. Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nori</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to segment object candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object segmentation from bounding box annotations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rajchl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjia</forename><surname>Passerat-Palmbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mellisa</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">A</forename><surname>Damodaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">V</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Hajnal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kainz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="674" to="683" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Crowdhuman: A benchmark for detecting human in a crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">High-quality correspondence and segmentation estimation for dual-lens smart-phone portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3257" to="3266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Automatic portrait segmentation for image stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Sachs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="93" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep automatic portrait matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Pose2instance: Harnessing keypoints for person instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01152</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Joint head pose and facial landmark regression from depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changwei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Falai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="229" to="241" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A survey on human performance capture and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Kun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ze</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinxiang</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science and Technology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="536" to="554" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Is faster r-cnn doing well for pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="443" to="457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Occluded pedestrian detection through guided attention in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6995" to="7003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="94" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Extensive facial landmark localization with coarseto-fine convolutional network cascade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erjin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqiang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhimin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="386" to="391" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
