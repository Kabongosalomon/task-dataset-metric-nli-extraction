<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Requeima</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><forename type="middle">Nowozin</forename><surname>Google</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Research</forename><surname>Berlin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Cambridge Invenia Labs</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Cambridge</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">University of Cambridge Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fast and Flexible Multi-Task Classification Using Conditional Neural Adaptive Processes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The goal of this paper is to design image classification systems that, after an initial multi-task training phase, can automatically adapt to new tasks encountered at test time. We introduce a conditional neural process based approach to the multi-task classification setting for this purpose, and establish connections to the meta-learning and few-shot learning literature. The resulting approach, called CNAPS, comprises a classifier whose parameters are modulated by an adaptation network that takes the current task's dataset as input. We demonstrate that CNAPS achieves state-of-theart results on the challenging META-DATASET benchmark indicating high-quality transfer-learning. We show that the approach is robust, avoiding both over-fitting in low-shot regimes and under-fitting in high-shot regimes. Timing experiments reveal that CNAPS is computationally efficient at test-time as it does not involve gradient based adaptation. Finally, we show that trained models are immediately deployable to continual learning and active learning where they can outperform existing approaches that do not leverage transfer learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We consider the development of general purpose image classification systems that can handle tasks from a broad range of data distributions, in both the low and high data regimes, without the need for costly retraining when new tasks are encountered. We argue that such systems require mechanisms that adapt to each task, and that these mechanisms should themselves be learned from a diversity of datasets and tasks at training time. This general approach relates to methods for meta-learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> and few-shot learning <ref type="bibr" target="#b2">[3]</ref>. However, existing work in this area typically considers homogeneous task distributions at train and test-time that therefore require only minimal adaptation. To handle the more challenging case of different task distributions we design a fully adaptive system, requiring specific design choices in the model and training procedure.</p><p>Current approaches to meta-learning and few-shot learning for classification are characterized by two fundamental trade-offs. (i) The number of parameters that are adapted to each task. One approach adapts only the top, or head, of the classifier leaving the feature extractor fixed <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. While useful in simple settings, this approach is prone to under-fitting when the task distribution is heterogeneous <ref type="bibr" target="#b5">[6]</ref>. Alternatively, we can adapt all parameters in the feature extractor <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> thereby increasing  fitting capacity, but incurring a computation cost and opening the door to over-fitting in the low-shot regime. What is needed is a middle ground which strikes a balance between model capacity and reliability of the adaptation. (ii) The adaptation mechanism. Many approaches use gradient-based adaptation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref>. While this approach can incorporate training data in a very flexible way, it is computationally inefficient at test-time, may require expertise to tune the optimization procedure, and is again prone to over-fitting. Conversely, function approximators can be used to directly map training data to the desired parameters (we refer to this as amortization) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10]</ref>. This yields fixed-cost adaptation mechanisms, and enables greater sharing across training tasks. However, it may under-fit if the function approximation is not sufficiently flexible. On the other hand, high-capacity function approximators require a large number of training tasks to be learned.</p><p>We introduce a modelling class that is well-positioned with respect to these two trade-offs for the multi-task classification setting called Conditional Neural Adaptive Processes (CNAPS). 2 CNAPS directly model the desired predictive distribution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, thereby introducing a conditional neural processes (CNPs) <ref type="bibr" target="#b12">[13]</ref> approach to the multi-task classification setting. CNAPS handles varying way classification tasks and introduces a parametrization and training procedure enabling the model to learn to adapt the feature representation for classification of diverse tasks at test time. CNAPS utilize i) a classification model with shared global parameters and a small number of task-specific parameters. We demonstrate that by identifying a small set of key parameters, the model can balance the trade-off between flexibility and robustness. ii) A rich adaptation neural network with a novel auto-regressive parameterization that avoids under-fitting while proving easy to train in practice with existing datasets <ref type="bibr" target="#b5">[6]</ref>. In Section 5 we evaluate CNAPS. Recently, Triantafillou et al. <ref type="bibr" target="#b5">[6]</ref> proposed META-DATASET, a few-shot classification benchmark that addresses the issue of homogeneous train and test-time tasks and more closely resembles real-world few-shot multi-task learning. Many of the approaches that achieved excellent performance on simple benchmarks struggle with this collection of diverse tasks. In contrast, we show that CNAPS achieve state-of-the-art performance on the META-DATASET benchmark, often by comfortable margins and at a fraction of the time required by competing methods. Finally, we showcase the versatility of the model class by demonstrating that CNAPS can be applied "out of the box" to continual learning and active learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Design</head><p>We consider a setup where a large number of training tasks are available, each composed of a set of inputs x and labels y. The data for task τ includes a context set D τ = {(x τ n , y τ n )} Nτ n=1 , with inputs and outputs observed, and a target set {(x τ * m , y τ * m )} Mτ m=1 for which we wish to make predictions (y τ * are only observed during training). CNPs <ref type="bibr" target="#b12">[13]</ref> construct predictive distributions given x * as: p (y * |x * , θ, D τ ) = p (y * |x * , θ, ψ τ = ψ φ (D τ )) .</p><p>(1)</p><p>Here θ are global classifier parameters shared across tasks. ψ τ are local task-specific parameters, produced by a function ψ φ (·) that acts on D τ . ψ φ (·) has another set of global parameters φ called adaptation network parameters. θ and φ are the learnable parameters in the model (see <ref type="figure" target="#fig_1">Figure 1a</ref>).  CNAPS is a model class that specializes the CNP framework for the multi-task classification setting. The model-class is characterized by a number of design choices, made specifically for the multi-task image classification setting. CNAPS employ global parameters θ that are trained offline to capture high-level features, facilitating transfer and multi-task learning. Whereas CNPs define ψ τ to be a fixed dimensional vector used as an input to the model, CNAPS instead let ψ τ be specific parameters of the model itself. This increases the flexibility of the classifier, enabling it to model a broader range of input / output distributions. We discuss our choices (and associated trade-offs) for these parameters below. Finally, CNAPS employ a novel auto-regressive parameterization of ψ φ (·) that significantly improves performance. An overview of CNAPS and its key components is illustrated in <ref type="figure" target="#fig_1">Figure 1b</ref>.</p><p>2.1 Specification of the classifier: global θ and task-specific parameters ψ τ</p><p>We begin by specifying the classifier's global parameters θ followed by how these are adapted by the local parameters ψ τ .</p><p>Global Classifier Parameters. The global classifier parameters will parameterize a feature extractor f θ (x) whose output is fed into a linear classifier, described below. A natural choice for f θ (·) in the image setting is a convolutional neural network, e.g., a ResNet <ref type="bibr" target="#b13">[14]</ref>. In what follows, we assume that the global parameters θ are fixed and known. In Section 3 we discuss the training of θ.</p><p>Task-Specific Classifier Parameters: Linear Classification Weights. The final classification layer must be task-specific as each task involves distinguishing a potentially unique set of classes. We use a task specific affine transformation of the feature extractor output, followed by a softmax. The task-specific weights are denoted ψ τ w ∈ R d f ×C τ (suppressing the biases to simplify notation), where d f is the dimension of the feature extractor output f θ (x) and C τ is the number of classes in task τ .</p><p>Task-Specific Classifier Parameters: Feature Extractor Parameters. A sufficiently flexible model must have capacity to adapt its feature representation f θ (·) as well as the classification layer (e.g. compare the optimal features required for ImageNet versus Omiglot). We therefore introduce a set of local feature extractor parameters ψ τ f , and denote f θ (·) the unadapted feature extractor, and f θ (·; ψ τ f ) the feature extractor adapted to task τ . It is critical in few-shot multi-task learning to adapt the feature extractor in a parameter-efficient manner. Unconstrained adaptation of all the feature extractor parameters (e.g. by fine-tuning <ref type="bibr" target="#b8">[9]</ref>) gives flexibility, but it is also slow and prone to over-fitting <ref type="bibr" target="#b5">[6]</ref>. Instead, we employ linear modulation of the convolutional feature maps as proposed by Perez et al. <ref type="bibr" target="#b14">[15]</ref>, which adapts the feature extractor through a relatively small number of task specific parameters.</p><p>A Feature-wise Linear Modulation (FiLM) layer <ref type="bibr" target="#b14">[15]</ref> scales and shifts the i th unadapted feature map f i in the feature extractor FiLM(f i ; γ τ i , β τ i ) = γ τ i f i + β τ i using two task specific parameters, γ τ i and β τ i . <ref type="figure" target="#fig_3">Figure 2a</ref> illustrates a FiLM layer operating on a convolutional layer, and <ref type="figure" target="#fig_3">Figure 2b</ref> illustrates how a FiLM layer can be added to a standard Residual network block <ref type="bibr" target="#b13">[14]</ref>. A key advantage of FiLM layers is that they enable expressive feature adaptation while adding only a small number of parameters <ref type="bibr" target="#b14">[15]</ref>. For example, in our implementation we use a ResNet18 with FiLM layers after every convolutional layer. The set of task specific FiLM parameters (ψ τ f = {γ τ i , β τ i }) constitute fewer than 0.7% of the parameters in the model. Despite this, as we show in Section 5, they allow the model to adapt to a broad class of datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Computing the local parameters via adaptation networks</head><p>The previous sections have specified the form of the classifier p (y * |x * , θ, ψ τ ) in terms of the global and task specific parameters, θ and ψ τ = {ψ τ f , ψ τ w }. The local parameters could now be learned separately for every task τ via optimization. While in practice this is feasible for small numbers of tasks (see e.g., <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>), this approach is computationally demanding, requires expert oversight (e.g. for tuning early stopping), and can over-fit in the low-data regime.</p><p>Instead, CNAPS uses a function, such as a neural network, that takes the context set D τ as an input and returns the task-specific parameters, ψ τ = ψ φ (D τ ). The adaptation network has parameters φ that will be trained on multiple tasks to learn how to produce local parameters that result in good generalisation, a form of meta-learning. Sacrificing some of the flexibility of the optimisation approach, this method is comparatively cheap computationally (only involving a forward pass through the adaptation network), automatic (with no need for expert oversight), and employs explicit parameter sharing (via φ) across the training tasks.</p><p>Adaptation Network: Linear Classifier Weights. CNAPS represents the linear classifier weights ψ τ w as a parameterized function of the form</p><formula xml:id="formula_0">ψ τ w = ψ w (D τ ; φ w , ψ f , θ), denoted ψ w (D τ )</formula><p>for brevity. There are three challenges with this approach: first, the dimensionality of the weights depends on the task (ψ τ w is a matrix with a column for each class, see <ref type="figure" target="#fig_4">Figure 3</ref>) and thus the network must output parameters of different dimensionalities; second, the number of datapoints in D τ will also depend on the task and so the network must be able to take inputs of variable cardinality; third, we would like the model to support continual learning. To handle the first two challenges we follow Gordon et al. <ref type="bibr" target="#b4">[5]</ref>. First, each column of the weight matrix is generated independently from the context points from that class ψ τ w = [ψ w (D τ 1 ) , . . . , ψ w (D τ C )], an approach which scales to arbitrary numbers of classes. Second, we employ a permutation invariant architecture <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> for ψ w (·) to handle the variable input cardinality (see Appendix E for details). Third, as permutation invariant architectures can be incrementally updated <ref type="bibr" target="#b19">[20]</ref>, continual learning is supported (as discussed in Section 5).</p><p>Intuitively, the classifier weights should be determined by the representation of the data points emerging from the adapted feature extractor. We therefore input the adapted feature representation of the data points into the network, rather than the raw data points (hence the dependency of ψ w on ψ f and θ). To summarize, ψ w (·) is a function on sets that accepts as input a set of adapted feature representations from D τ c , and outputs the c th column of the linear classification matrix, i.e.,</p><formula xml:id="formula_1">ψ w (D τ c ; φ w , ψ f , θ) = ψ w ({f θ (x m ; ψ f ) |x m ∈ D τ , y m = c}; φ w ) .<label>(2)</label></formula><p>Here φ w are learnable parameters of ψ w (·). See <ref type="figure" target="#fig_4">Figure 3</ref> for an illustration.</p><p>Adaptation Network: Feature Extractor Parameters. CNAPS represents the task-specific feature extractor parameters ψ τ f , comprising the parameters of the FiLM layers γ τ and β τ in our implementation, as a parameterized function of the context-set D τ . Thus, ψ f (·; φ f , θ) is a collection of functions (one for each FiLM layer) with parameters φ f , many of which are shared across functions. We denote the function generating the parameters for the i th FiLM layer ψ i f (·) for brevity. Our experiments (Section 5) show that this mapping requires careful parameterization. We propose a novel parameterization that improves performance in complex settings with diverse datasets. Our implementation contains two components: a task-specific representation that provides context about the task to all layers of the feature extractor (denoted z τ G ), and an auto-regressive component that provides information to deeper layers in the feature extractor concerning how shallower layers have adapted to the task (denoted z i AR ). The input to the ψ i f (·) network is z i = (z τ G , z i AR ). z τ G is computed for every task τ by passing the inputs x τ n through a global set encoder g with parameters in φ f .  <ref type="figure">Figure 4</ref>: Implementation of the feature-extractor: an independently learned set encoder g provides a fixed context that is concatenated to the (processed) activations of x from the previous ResNet block. The inputs zi = (z τ G , z i AR ) are then fed to ψ i f (·), which outputs the FiLM parameters for layer i. Green arrows correspond to propagation of auto-regressive representations. Note that the auto-regressive component z i AR is computed by processing the adapted activations {f i θ (x; ψ τ f )} of the previous convolutional block.  <ref type="figure">Figure 5</ref>: Adaptation network φ f . R γib j ch and R βib j ch denote a vector of regularization weights that are learned with an l2 penalty.</p><p>To adapt the l th layer in the feature extractor, it is useful for the system to have access to the representation of task-relevant inputs from layer l − 1. While z G could in principle encode how layer l − 1 has adapted, we opt to provide this information directly to the adaptation network adapting layer l by passing the adapted activations from layer l − 1. The auto-regressive component z i AR is computed by processing the adapted activations of the previous convolutional block with a layer-specific set encoder (except for the first residual block, whose auto-regressive component is given by the unadapted initial pre-processing stage in the ResNet). Both the global and all layer-specific set-encoders are implemented as permutation invariant functions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> (see Appendix E for details). The full parameterization is illustrated in <ref type="figure">Figure 4</ref>, and the architecture of ψ i f (·) networks is illustrated in <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model Training</head><p>The previous section has specified the model (see <ref type="figure" target="#fig_1">Figure 1b</ref> for a schematic). We now describe how to train the global classifier parameters θ and the adaptation network</p><formula xml:id="formula_2">parameters φ = {φ f , φ w }.</formula><p>Training the global classifier parameters θ. A natural approach to training the model (originally employed by CNPs <ref type="bibr" target="#b12">[13]</ref>) would be to maximize the likelihood of the training data jointly over θ and φ. However, experiments (detailed in Appendix D.3) showed that it is crucially important to adopt a two stage process instead. In the first stage, θ are trained on a large dataset (e.g., the training set of ImageNet <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b5">6]</ref>) in a full-way classification procedure, mirroring standard pre-training. Second, θ are fixed and φ are trained using episodic training over all meta-training datasets in the multi-task setting. We hypothesize that two-stage training is important for two reasons: (i) during the second stage, φ f are trained to adapt f θ (·) to tasks τ by outputting ψ τ f . As θ has far more capacity than ψ τ f , if they are trained in the context of all tasks, there is no need for ψ τ f to adapt the feature extractor, resulting in little-to-no training signal for φ f and poor generalisation. (ii) Allowing θ to adapt during Residual Adapters <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> LEO <ref type="bibr" target="#b22">[23]</ref>, Proto-MAML <ref type="bibr" target="#b5">[6]</ref> CNAPS, TADAM <ref type="bibr" target="#b26">[27]</ref> VERSA <ref type="bibr" target="#b4">[5]</ref>, Proto Nets <ref type="bibr" target="#b3">[4]</ref>, Matching Nets <ref type="bibr" target="#b23">[24]</ref> MAML <ref type="bibr" target="#b6">[7]</ref> Meta-LSTM <ref type="bibr" target="#b21">[22]</ref> Model Flexibility CAVIA <ref type="bibr" target="#b24">[25]</ref> Disc. k-shot <ref type="bibr" target="#b25">[26]</ref> Figure 6: Model design space. The y-axis represents the number of task-specific parameters |ψ τ |.</p><p>Increasing |ψ τ | increases model flexibility, but also the propensity to over-fit. The x-axis represents the complexity of the mechanism used to adapt the taskspecific parameters to training data ψ(D τ ). On the right are amortized approaches (i.e. using fixed functions). On the left is gradient-based adaptation. Mixed approaches lie between. Computational efficiency increases to the right. Flexibility increases to the left, but with it over-fitting and need for hand tuning.</p><p>the second phase violates the principle of "train as you test", i.e., when test tasks are encountered, θ will be fixed, so it is important to simulate this scenario during training. Finally, fixing θ during meta-training is desireable as it results in a dramatic decrease in training time.</p><p>Training the adaptation network parameters φ. Following the work of Garnelo et al. <ref type="bibr" target="#b12">[13]</ref>, we train φ with maximum likelihood. An unbiased stochastic estimator of the log-likelihood is:</p><formula xml:id="formula_3">L (φ) = 1 M T m,τ log p (y * τ m |x * τ m , ψ φ (D τ ) , θ) ,<label>(3)</label></formula><p>where {y * τ m , x * τ m , D τ } ∼P , withP representing the data distribution (e.g., sampling tasks and splitting them into disjoint context (D τ ) and target data {(x * τ m , y * τ m )} Mt m=1 ). Maximum likelihood training therefore naturally uses episodic context / target splits often used in meta-learning. In our experiments we use the protocol defined by Triantafillou et al. <ref type="bibr" target="#b5">[6]</ref> and META-DATASET for this sampling procedure. Algorithm 1 details computation of the stochastic estimator for a single task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Our work frames multi-task classification as directly modelling the predictive distribution p(y * |x * , ψ(D τ )). The perspective allows previous work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26]</ref> to be organised in terms of i) the choice of the parameterization of the classifier (and in particular the nature of the local parameters), and ii) the function used to compute the local parameters from the training data. This space is illustrated in <ref type="figure" target="#fig_11">Figure 6</ref>, and further elaborated upon in Appendix B.</p><p>One of the inspirations for our work is conditional neural processes (CNPs) <ref type="bibr" target="#b12">[13]</ref>. CNPs directly model the predictive distribution p(y * |x * , ψ(D τ )) and train the parameters using maximum likelihood. Whereas previous work on CNPs has focused on homogeneous regression and classification datasets and fairly simple models, here we study multiple heterogeneous classification datasets and use a more complex model to handle this scenario. In particular, whereas the original CNP approach to classification required pre-specifying the number of classes in advance, CNAPS handles varying way classification tasks, which is required for e.g. the meta-dataset benchmark. Further, CNAPS employs a parameter-sharing hierarchy that parameterizes the feature extractor. This contrasts to the original CNP approach that shared all parameters across tasks, and use latent inputs to the decoder to adapt to new tasks. Finally, CNAPS employs a meta-training procedure geared towards learning to adapt to diverse tasks. Similarly, our work can be viewed as a deterministic limit of ML-PIP <ref type="bibr" target="#b4">[5]</ref> which employs a distributional treatment of the local-parameters ψ.</p><p>A model with design choices closely related to CNAPS is TADAM <ref type="bibr" target="#b26">[27]</ref>. TADAM employs a similar set of local parameters, allowing for adaptation of both the feature extractor and classification layer. However, it uses a far simpler adaptation network (lacking auto-regressive structure) and an expensive and ad-hoc training procedure. Moreover, TADAM was applied to simple few-shot learning benchmarks (e.g. CIFAR100 and mini-ImageNet) and sees little gain from feature extractor adaptation. In contrast, we see a large benefit from adapting the feature extractor. This may in part reflect the differences in the two models, but we observe that feature extractor adaptation has the largest impact when used to adapt to different datasets and that two stage training is required to see this. Further differences are our usage of the CNP framework and the flexible deployment of CNAPS to continual learning and active learning (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments and Results</head><p>The experiments target three key questions: (i) Can CNAPS improve performance in multi-task few-shot learning? (ii) Does the use of an adaptation network benefit computational-efficiency and data-efficiency? (iii) Can CNAPS be deployed directly to complex learning scenarios like continual learning and active learning? The experiments use the following modelling choices (see Appendix E for full details). While CNAPS can utilize any feature extractor, a ResNet18 <ref type="bibr" target="#b13">[14]</ref> is used throughout to enable fair comparison with Triantafillou et al. <ref type="bibr" target="#b5">[6]</ref>. To ensure that each task is handled independently, batch normalization statistics <ref type="bibr" target="#b27">[28]</ref> are learned (and fixed) during the pre-training phase for θ. Actual batch statistics of the test data are never used during meta-training or testing.</p><p>Few Shot Classification. The first experiment tackles a demanding few-shot classification challenge called META-DATASET <ref type="bibr" target="#b5">[6]</ref>. META-DATASET is composed of ten (eight train, two test) image classification datasets. The challenge constructs few-shot learning tasks by drawing from the following distribution. First, one of the datasets is sampled uniformly; second, the "way" and "shot" are sampled randomly according to a fixed procedure; third, the classes and context / target instances are sampled. Where a hierarchical structure exists in the data (ILSVRC or OMNIGLOT), task-sampling respects the hierarchy. In the meta-test phase, the identity of the original dataset is not revealed and the tasks must be treated independently (i.e. no information can be transferred between them). Notably, the meta-training set comprises a disjoint and dissimilar set of classes from those used for meta-test. Full details are available in Appendix C.1 and <ref type="bibr" target="#b5">[6]</ref>.</p><p>Triantafillou et al. <ref type="bibr" target="#b5">[6]</ref> consider two stage training: an initial stage that trains a feature extractor in a standard classification setting, and a meta-training stage of all parameters in an episodic regime. For the meta-training stage, they consider two settings: meta-training only on the META-DATASET version of ILSVRC, and on all meta-training data. We focus on the latter as CNAPS rely on training data from a variety of training tasks to learn to adapt, but provide results for the former in Appendix D.1. We pre-train θ on the meta-training set of the META-DATASET version of ILSVRC, and meta-train φ in an episodic fashion using all meta-training data. We compare CNAPS to models considered by Triantafillou et al. <ref type="bibr" target="#b5">[6]</ref>, including their proposed method (Proto-MAML) in <ref type="table" target="#tab_2">Table 1</ref>. We meta-test CNAPS on three additional held-out datasets: MNIST <ref type="bibr" target="#b28">[29]</ref>, CIFAR10 <ref type="bibr" target="#b29">[30]</ref>, and CIFAR100 <ref type="bibr" target="#b29">[30]</ref>. As an ablation study, we compare a version of CNAPS that does not make use of the auto-regressive component z AR , and a version that uses no feature extractor adaptation. In our analysis of <ref type="table" target="#tab_2">Table 1</ref>, we distinguish between two types of generalization: (i) unseen tasks (classes) in meta-training datasets, and (ii) unseen datasets.</p><p>Unseen tasks: CNAPS achieve significant improvements over existing methods on seven of the eight datasets. The exception is the TEXTURES dataset, which has only seven test classes and accuracy is highly sensitive to the train / validation / test class split. The ablation study demonstrates that removing z AR from the feature extractor adaptation degrades accuracy in most cases, and that removing all feature extractor adaptation results in drastic reductions in accuracy.</p><p>Unseen datasets: CNAPS-models outperform all competitive models with the exception of FINE-TUNE on the TRAFFIC SIGNS dataset. Removing z AR from the feature extractor decreases accuracy and removing the feature extractor adaptation entirely significantly impairs performance. The degradation is particularly pronounced when the held out dataset differs substantially from the dataset used to pretrain θ, e.g. for MNIST.</p><p>Note that the superior results when using the auto-regressive component can not be attributed to increased network capacity alone. In Appendix D.4 we demonstrate that CNAPS yields superior classification accuracy when compared to parallel residual adapters <ref type="bibr" target="#b16">[17]</ref> even though CNAPS requires significantly less network capacity in order to adapt the feature extractor to a given task.</p><p>Additional results: Results when meta-training only on the META-DATASET version of ILSVRC are given in <ref type="table">Table 3</ref>. In Appendix D.2, we visualize the task encodings and parameters, demonstrating that the model is able to learn meaningful task and dataset level representations and parameterizations. The results support the hypothesis that learning to adapt key parts of the network is more robust and achieves significantly better performance than existing approaches.    FiLM Parameter Learning Performance: Speed-Accuracy Trade-off. CNAPS generate FiLM layer parameters for each task τ at test time using the adaptation network ψ f (D τ ). It is also possible to learn the FiLM parameters via gradient descent (see <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>). Here we compare CNAPS to this approach. <ref type="figure" target="#fig_8">Figure 7</ref> shows plots of 5-way classification accuracy versus time for four held out data sets as the number of shots was varied. For gradient descent, we used a fixed learning rate of 0.001 and took 25 steps for each point. The overall time required to produce the plot was 1274 and 7214 seconds for CNAPS and gradient approaches, respectively, on a NVIDIA Tesla P100-PCIE-16GB GPU. CNAPS is at least 5 times faster at test time than gradient-based optimization requiring only a single forward pass through the network while gradient based approaches require multiple forward and backward passes. Further, the accuracy achieved with adaptation networks is significantly higher for fewer shots as it protects against over-fitting. For large numbers of shots, gradient descent catches up, albeit slowly.</p><p>Complex Learning Scenarios: Continual Learning. In continual learning <ref type="bibr" target="#b39">[40]</ref> new tasks appear over time and existing tasks may change. The goal is to adapt accordingly, but without retaining old data which is challenging for artificial systems. To demonstrate the the versatility CNAPS we show that, although it has not been explicitly trained for continual learning, we are able to apply the same model trained for the few-shot classification experiments (without the auto-regressive component) to standard continual learning benchmarks on held out datasets: Split MNIST <ref type="bibr" target="#b40">[41]</ref> and Split CIFAR100 <ref type="bibr" target="#b41">[42]</ref>. We modify the model to compute running averages for the representations of both ψ τ w and ψ τ f (see Appendix F for further details), in this way it performs incremental updates using the new data and the old model, and does not need to access old data. <ref type="figure">Figure 8 (left)</ref> shows the accumulated multiand single-head <ref type="bibr" target="#b41">[42]</ref> test accuracy averaged over 30 runs (further results and more detailed figures are in Appendix G). <ref type="figure">Figure 8</ref> (right) shows average results at the final task comparing to SI <ref type="bibr" target="#b40">[41]</ref>, EWC <ref type="bibr" target="#b42">[43]</ref>, VCL <ref type="bibr" target="#b43">[44]</ref>, and Riemannian Walk <ref type="bibr" target="#b41">[42]</ref>. <ref type="figure">Figure 8</ref> demonstrates that CNAPS naturally resists catastrophic forgetting <ref type="bibr" target="#b42">[43]</ref> and compares favourably to competing methods, despite the fact that it was not exposed to these datasets during training, observes orders of magnitude fewer examples, and was not trained explicitly to perform continual learning. CNAPS performs similarly to, or better than, the state-of-the-art Riemannian Walk method which departs from the pure continual learning setting by maintaining a small number of training samples across tasks. Conversely, CNAPS has the advantage of being exposed to a larger   range of datasets and can therefore leverage task transfer. We emphasize that this is not meant to be an "apples-to-apples" comparison, but rather, the goal is to demonstrate the out-of-the-box versatility and strong performance of CNAPS in new domains and learning scenarios.</p><p>Complex Learning Scenarios: Active Learning. Active learning <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b46">47]</ref> requires accurate dataefficient learning that returns well-calibrated uncertainty estimates. <ref type="figure">Figure 9</ref> compares the performance of CNAPS and prototypical networks using two standard active learning acquisition functions (variation ratios and predictive entropy <ref type="bibr" target="#b45">[46]</ref>) against random acquisition on the FLOWERS dataset and three representative held-out languages from OMNIGLOT (performance on all languages is presented in Appendix H). <ref type="figure">Figure 9</ref> and Appendix H show that CNAPS achieves higher accuracy on average than prototypical networks. Moreover, CNAPS achieves significant improvements over random acquisition, whereas prototypical networks do not. These tests indicates that CNAPS is more accurate and suggest that CNAPS has better calibrated uncertainty estimates than prototypical networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper has introduced CNAPS, an automatic, fast and flexible modelling approach for multitask classification. We have demonstrated that CNAPS achieve state-of-the-art performance on the META-DATASET challenge, and can be deployed "out-of-the-box" to diverse learning scenarios such as continual and active learning where they are competitive with the state-of-the-art. Future avenues of research are to consider the exploration of the design space by introducing gradients and function approximation to the adaptation mechanisms, as well as generalizing the approach to distributional extensions of CNAPS <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Algorithm for Constructing Stochastic Estimator</head><p>An algorithm for constructing the stochastic training objectiveL(φ; τ ) for a single task τ is given in Algorithm 1. CAT(·; π) denotes a the likelihood of a categorical distribution with parameter vector π. This algorithm can be used on a batch of tasks to construct an unbiased estimator for the auto-regressive likelihood of the task outputs.</p><p>Algorithm 1 Stochastic Objective Estimator for Meta-Training.</p><formula xml:id="formula_4">1: procedure META-TRAINING({x * m , y * m } M m=1 , D τ , θ, φ) 2: ψ τ f ← ψ f ({f θ (x n )|x ∈ D τ }; φ f ) 3: ψ τ c ← ψ w ({f θ (x n ; ψ f )|x ∈ D τ , y n = c}; φ w ) ∀c ∈ C τ 4:</formula><p>for m ∈ 1, ..., M do 5: The choice of task-specific parameters ψ τ . Clearly, any approach to multi-task classification must adapt, at the very least, the top-level classifier layer of the model. A number of successful models have proposed doing just this with e.g., neighbourhood-based approaches <ref type="bibr" target="#b3">[4]</ref>, variational inference <ref type="bibr" target="#b25">[26]</ref>, or inference networks <ref type="bibr" target="#b4">[5]</ref>. On the other end of the spectrum are models that adapt all the parameters of the classifier, e.g., <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b49">50]</ref>. The trade-off here is clear: as more parameters are adapted, the resulting model is more flexible, but also slow and prone to over-fitting. For this reason we modulate a small portion of the network parameters, following recent work on multi-task learning <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15]</ref>.</p><formula xml:id="formula_5">π m ← f θ (x * m ; ψ τ f ) T ψ τ</formula><p>We argue that just adapting the linear classification layer is sufficient when the task distribution is not diverse, as in the standard benchmarks used for few-shot classification (OMNIGLOT <ref type="bibr" target="#b30">[31]</ref> and mini-imageNet <ref type="bibr" target="#b21">[22]</ref>). However, when faced with a diverse set of tasks, such as that introduced recently by Triantafillou et al. <ref type="bibr" target="#b5">[6]</ref>, it is important to adapt the feature extractor on a per-task basis as well.</p><p>The adaptation mechanism ψ φ (D τ ). Adaptation varies in the literature from performing full gradient descent learning with D τ <ref type="bibr" target="#b8">[9]</ref> to relying on simple operations such as taking the mean of class-specific feature representations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b23">24]</ref>. Recent work has focused on reducing the number of required gradient steps by learning a global initialization <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> or additional parameters of the optimization procedure <ref type="bibr" target="#b21">[22]</ref>. Gradient-based procedures have the benefit of being flexible, but are computationally demanding, and prone to over-fitting in the low-data regime. Another line of work has focused on learning neural networks to output the values of ψ, which we denote amortization <ref type="bibr" target="#b4">[5]</ref>. Amortization greatly reduces the cost of adaptation and enables sharing of global parameters, but may suffer from the amortization gap <ref type="bibr" target="#b50">[51]</ref> (i.e., underfitting), particularly in the large data regime. Recent work has proposed using semi-amortized inference <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23]</ref>, but have done so while only adapting the classification layer parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimentation Details</head><p>All experiments were implemented in PyTorch <ref type="bibr" target="#b51">[52]</ref> and executed either on NVIDIA Tesla P100-PCIE-16GB or Tesla V100-SXM2-16GB GPUs. The full CNAPS model runs in a distributed fashion across 2 GPUs and takes approximately one and a half days to complete episodic training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 META-DATASET Training and Evaluation Procedure</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 Feature Extractor Weights θ Pretraining</head><p>We first reduce the size of the images in the ImageNet ILSVRC-2012 dataset <ref type="bibr" target="#b20">[21]</ref> to 84 × 84 pixels. Some images in the ImageNet ILSVRC-2012 dataset are duplicates of images in other datasets included in META-DATASET, so these were removed. We then split the 1000 training classes of the ImageNet ILSVRC-2012 dataset into training, validation, and test sets according to the criteria detailed in <ref type="bibr" target="#b5">[6]</ref>. The test set consists of the 130 leaf-node subclasses of the "device" synset node, the validation set consists of the the 158 leaf-node subclasses of the "carnivore" synset node, and the training set consists of the remaining 712 leaf-node classes. We then pretrain a feature extractor with parameters θ based on a modified ResNet-18 <ref type="bibr" target="#b13">[14]</ref> architecture on the above 712 training classes. The ResNet-18 architecture is detailed in <ref type="table" target="#tab_12">Table 8</ref>. Compared to a standard ResNet-18, we reduced the initial convolution kernel size from 7 to 5 and eliminated the initial max-pool step. These changes were made to accommodate the reduced size of the imagenet training images. We train for 125 epochs using stochastic gradient descent with momentum of 0.9, weight decay equal to 0.0001, a batch size of 256, and an initial learning rate of 0.1 that decreases by a factor of 10 every 25 epochs. During pretraining, the training dataset was augmented with random crops, random horizontal flips, and random color jitter. The top-1 accuracy after pretraining was 63.9%. For all subsequent training and evaluation steps, the ResNet-18 weights were frozen.The dimensionality of the feature extractor output is d f = 512. The hyper-parameters used were derived from the PyTorch <ref type="bibr" target="#b51">[52]</ref> ResNet training tutorial. The only tuning that was performed was on the number of epochs used for training and the interval at which the learning rate was decreased. For the number of epochs, we tried both 90 and 125 epochs and selected 125, which resulted in slightly higher accuracy. We also found that dropping the learning rate at an interval of 25 versus 30 epochs resulted in slightly higher accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.2 Episodic Training of φ</head><p>Next we train the functions that generate the parameters ψ τ f , ψ τ w for the feature extractor adapters and the linear classifier, respectively. We train two variants of CNAPS (on ImageNet ILSVRC-2012 only and all datasets -see <ref type="table" target="#tab_4">Table 2</ref>). We generate training and validation episodes using the reader from <ref type="bibr" target="#b52">[53]</ref>. We train in an end-to-end fashion for 110,000 episodes with the Adam <ref type="bibr" target="#b53">[54]</ref> optimizer, using a batch size of 16 episodes, and a fixed learning rate of 0.0005. We validate using 200 episodes per validation dataset. Note that when training on ILSVRC only, we validate on ILSVRC only, however, when training on all datasets, we validate on all datasets that have validation data (see <ref type="table" target="#tab_4">Table 2</ref>) and consider a model to be better if more than half of the datasets have a higher classification accuracy than the current best model. No data augmentation was employed during the training of φ. Note that while training φ the feature extractor f θ (·) is in 'eval' mode (i.e. it will use the fixed batch normalization statistics learned during pretraining the feature extractor weights θ with a moving average). No batch normalization is used in any of the functions generating the ψ τ parameters, with the exception of the set encoder g (that generates the global task representation z τ G ). Note that the target points are never passed through the set encoder g. Again, very little hyper-parameter tuning was performed. No grid search or other hyper-parameter search was used. For learning rate we tried both 0.0001 and 0.0005, and selected the latter. We experimented with the number of training episodes in the range of 80,000 to 140,000, with 110,000 episodes generally yielding the best results. We also tried lowering the batch size to 8, but that led to decreased accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.3 Evaluation</head><p>We generate test episodes using the reader from <ref type="bibr" target="#b52">[53]</ref>. We test all models with 600 episodes each on all test datasets. The classification accuracy is averaged over the episodes and a 95% confidence interval is computed. We compare the best validation and fully trained models in terms of accuracy and use the best of the two. Note that during evaluation, the feature extractor f θ (·) is also in 'eval' mode.</p><p>ImageNet ILSVRC-2012 <ref type="table" target="#tab_2">Train  Validation  Test  Train  Validation  Test   ILSVRC [21]</ref> ILSVRC <ref type="bibr" target="#b20">[21]</ref> ILSVRC <ref type="bibr" target="#b20">[21]</ref> ILSVRC <ref type="bibr" target="#b20">[21]</ref> ILSVRC <ref type="bibr" target="#b20">[21]</ref> ILSVRC <ref type="bibr" target="#b20">[21]</ref> Omniglot <ref type="bibr" target="#b30">[31]</ref> Omniglot <ref type="bibr" target="#b30">[31]</ref> Omniglot <ref type="bibr" target="#b30">[31]</ref> Omniglot <ref type="bibr" target="#b30">[31]</ref> Aircraft <ref type="bibr" target="#b31">[32]</ref> Aircraft <ref type="bibr" target="#b31">[32]</ref> Aircraft <ref type="bibr" target="#b31">[32]</ref> Aircraft <ref type="bibr" target="#b31">[32]</ref> Birds <ref type="bibr" target="#b32">[33]</ref> Birds <ref type="bibr" target="#b32">[33]</ref> Birds <ref type="bibr" target="#b32">[33]</ref> Birds <ref type="bibr" target="#b32">[33]</ref> Textures <ref type="bibr" target="#b33">[34]</ref> Textures <ref type="bibr" target="#b33">[34]</ref> Textures <ref type="bibr" target="#b33">[34]</ref> Textures <ref type="bibr" target="#b33">[34]</ref> Quick Draw <ref type="bibr" target="#b34">[35]</ref> Quick Draw <ref type="bibr" target="#b34">[35]</ref> Quick Draw <ref type="bibr" target="#b34">[35]</ref> Quick Draw <ref type="bibr" target="#b34">[35]</ref> Fungi <ref type="bibr" target="#b35">[36]</ref> Fungi <ref type="bibr" target="#b35">[36]</ref> Fungi <ref type="bibr" target="#b35">[36]</ref> Fungi <ref type="bibr" target="#b35">[36]</ref> VGG Flower <ref type="bibr" target="#b36">[37]</ref> VGG Flower <ref type="bibr" target="#b36">[37]</ref> VGG Flower <ref type="bibr" target="#b36">[37]</ref> VGG Flower <ref type="bibr" target="#b36">[37]</ref> MSCOCO <ref type="bibr" target="#b38">[39]</ref> MSCOCO <ref type="bibr" target="#b38">[39]</ref> MSCOCO <ref type="bibr" target="#b38">[39]</ref> Traffic Signs <ref type="bibr" target="#b37">[38]</ref> Traffic Signs <ref type="bibr" target="#b37">[38]</ref> MNIST <ref type="bibr" target="#b28">[29]</ref> MNIST <ref type="bibr" target="#b28">[29]</ref> CIFAR10 <ref type="bibr" target="#b29">[30]</ref> CIFAR10 <ref type="bibr" target="#b29">[30]</ref> CIFAR100 <ref type="bibr" target="#b29">[30]</ref> CIFAR100 <ref type="bibr" target="#b29">[30]</ref>  53.6±1.0 <ref type="table">Table 3</ref>: Few-shot classification results on META-DATASET <ref type="bibr" target="#b5">[6]</ref> using models trained on ILSVRC-2012 only. All figures are percentages and the ± sign indicates the 95% confidence interval. Bold text indicates the highest scores that overlap in their confidence intervals. Results from competitive methods from <ref type="bibr" target="#b5">[6]</ref> D Additional Few-Shot Classification Results <ref type="table">Table 3</ref> shows few-shot classification results on META-DATASET when trained on ILSVRC-2012 only. We emphasize that this scenario does not capture the key focus of our work, and that these results are provided mainly for completeness and compatibility with the work of Triantafillou et al. <ref type="bibr" target="#b5">[6]</ref>. In particular, our method relies on training the parameters φ to adapt the conditional predictive distribution to new datasets. In this setting, the model is never presented with data that has not been used to pre-train θ, and therefore cannot learn to appropriately adapt the network to new datasets. Despite this, CNAPS demonstrate competitive results with the methods evaluated by Triantafillou et al. <ref type="bibr" target="#b5">[6]</ref> even in this scenario. <ref type="figure" target="#fig_1">Figure 10</ref> shows t-SNE <ref type="bibr" target="#b54">[55]</ref> plots that visualize the output of the set encoder z G and the FiLM layer parameters following the first and last convolutional layers of the feature extractor at test time. Even with unseen test data, the set encoder has learned to clearly separate examples arising from diverse datasets. The FiLM generators learn to generate feature extractor adaptation parameters unique to each dataset. The only significant overlap in the FiLM parameter plots is between CIFAR10 and CIFAR100 datasets which are closely related.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Few-Shot Classification Results When Training on ILSVRC-2012 only</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Feature Extractor Parameter Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Joint Training of θ and φ</head><p>Our experiments in jointly training θ and φ show that the two-stage training procedure proposed in Section 3 is crucially important. In particular, we found that joint training diverged in almost all cases we attempted. We were only able to train jointly in two circumstances: (i) Using batch normalization in "train" mode for both context and target sets. We stress that this implies computing the batch statistics at test time, and using those to normalize the batches. This is in contrast to the methodology we propose in the main text: only using batch normalization in "eval" mode, which enforces that no information is transferred across tasks or datasets. (ii) "Warm-start" the training procedure with batch normalization in "train" mode, and after a number of epochs (we use 50 for the results shown below), switch to proper usage of batch normalization. All other training procedures we attempted diverged. <ref type="table" target="#tab_7">Table 4</ref> details the results of our study on training procedures. The results demonstrate that the two-stage greatly improves performance of the model, even compared to using batch normalization in "train mode", which gives the model an unfair advantage over our standard model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Comparison Between CNAPS and Parallel Residual Adapters [17]</head><p>CNAPS adds FiLM layers <ref type="bibr" target="#b14">[15]</ref> in series with each convolutional layer to adapt the feature extractor to a particular task while parallel residual adapters from Rebuffi et al. <ref type="bibr" target="#b16">[17]</ref> adds 1 × 1 convolutions in parallel with each convolution layer to do the same. However, if the number of feature channels is C, then the number of parameters required for each convolutional layer in the feature extractor is 2C for CNAPS and C 2 for parallel residual adapters. Hence, parallel residual adapters have C/2 times the capacity compared to FiLM layers. Despite this advantage, CNAPs achieves superior results as can be seen in <ref type="table" target="#tab_8">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Parallel Residual Adapter CNAPS ILSVRC <ref type="bibr" target="#b20">[21]</ref> 51.2 ± 1.0 52.3 ± 1.0 Omniglot <ref type="bibr" target="#b30">[31]</ref> 87.3 ± 0.7 88.4 ± 0.7 Aircraft <ref type="bibr" target="#b31">[32]</ref> 78.3 ± 0.7 80.5 ± 0.6 Birds <ref type="bibr" target="#b32">[33]</ref> 67.8 ± 0.9 72.2 ± 0.9 Textures <ref type="bibr" target="#b33">[34]</ref> 55.5 ± 0.7 58.3 ± 0.7 Quick Draw <ref type="bibr" target="#b34">[35]</ref> 70.9 ± 0.7 72.5 ± 0.8 Fungi <ref type="bibr" target="#b35">[36]</ref> 44.6 ± 1.1 47.4 ± 1.0 VGG Flower <ref type="bibr" target="#b36">[37]</ref> 81.7 ± 0.7 86.0 ± 0.5 Traffic Signs <ref type="bibr" target="#b37">[38]</ref> 57.2 ± 0.9 60.2 ± 0.9 MSCOCO <ref type="bibr" target="#b38">[39]</ref> 43.7 ± 1.0 42.6 ± 1.1 MNIST <ref type="bibr" target="#b28">[29]</ref> 91.1 ± 0.4 92.7 ± 0.4 CIFAR10 <ref type="bibr" target="#b29">[30]</ref> 64.5 ± 0.8 61.5 ± 0.7 CIFAR100 <ref type="bibr" target="#b29">[30]</ref> 50.4 ± 0.9 50.1 ± 1.0 Throughout our experiments in Section 5, we use a ResNet18 <ref type="bibr" target="#b13">[14]</ref> as our feature extractor, the parameters of which we denote θ. <ref type="table" target="#tab_9">Table 6</ref> and <ref type="table" target="#tab_10">Table 7</ref> detail the architectures of the basic block (left) and basic scaling block (right) that are the fundamental components of the ResNet that we employ. <ref type="table" target="#tab_12">Table 8</ref> details how these blocks are composed to generate the overall feature extractor network. We use the implementation that is provided by the PyTorch [52] 3 , though we adapt the code to enable the use of FiLM layers.  In this section, we provide the details of the architectures used for our adaptation networks. <ref type="table" target="#tab_13">Table 9</ref> details the architecture of the set encoder g : D τ → z G that maps context sets to global representations. <ref type="table" target="#tab_2">Table 10</ref> details the architecture used in the auto-regressive parameterization of z AR . In our experiments, there is one such network for every block in the ResNet18 (detailed in <ref type="table" target="#tab_12">Table 8</ref>). These networks accept as input the set of activations from the previous block, and map them (through the   permutation invariant structure) to a vector representation of the output of the layer. The representation z i = (z G , z AR ) is then generated by concatenating the global and auto-regressive representations, and fed into the adaptation network that provides the FiLM layer parameters for the next layer. This network is detailed in <ref type="table" target="#tab_2">Table 11</ref>, and illustrated in <ref type="figure">Figure 5</ref>. Note that, as depicted in <ref type="figure">Figure 5</ref>, each layer has four networks with architectures as detailed in <ref type="table" target="#tab_2">Table 11</ref>, one for each γ and β, for each convolutional layer in the block.</p><formula xml:id="formula_6">Set Encoder (φ f ): {f li θ (x; ψ τ f )} → z i AR</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output size Layers</head><p>l i channels × l i channel size Input {f li θ (x; ψ τ f )} l i channels × l i channel size AvgPool, Flatten l i channels fully connected, ReLU l i channels 2 × fully connected with residual skip connection, ReLU l i channels fully connected with residual skip connection l i channels mean pooling over instances l i channels Input from mean pooling l i channels fully connected, ReLU </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Linear Classifier Adaptation Network</head><p>Finally, in this section we give the details for the linear classifer ψ τ w , and the adaptation network that provides these task-specific parameters ψ w (·). The adaptation network accepts a class-specific representation that is generated by applying a mean-pooling operation to the adapted feature activations of each instance associated with the class in the context set:</p><formula xml:id="formula_7">z τ c = 1 N τ c x∈D τ c f θ (x; ψ τ f ), where</formula><p>N τ c denotes the number of context instances associated with class c in task τ . ψ w is comprised of two separate networks (one for the weights ψ w and one for the biases ψ b ) detailed in <ref type="table" target="#tab_2">Table 12</ref> and</p><formula xml:id="formula_8">Network (φ f ): (z G , z AR ) → (γ, β)</formula><p>Output size Layers 64 + l i channels Input from Concatenate l i channels fully connected, ReLU l i channels 2 × fully connected with residual skip connection, ReLU l i channels fully connected with residual skip connection <ref type="table" target="#tab_2">Table 11</ref>: Network φ f . <ref type="table" target="#tab_2">Table 13</ref>. The resulting weights and biases (for each class in task τ ) can then be used as a linear classification layer, as detailed in <ref type="table" target="#tab_2">Table 14</ref>.  </p><formula xml:id="formula_9">Network (φ b ): z c → ψ w,b</formula><p>Output size Layers</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>512</head><p>Input from mean pooling 512 2 × fully connected, ELU 1 fully connected</p><formula xml:id="formula_10">Linear Classifier (ψ w ): f θ (x * ; ψ τ f ) → p(y * |x * , ψ τ (D τ ), θ) Output size Layers 512 Input features f θ (x * ; ψ τ f ) 512 × C τ Input weights w 512 × 1</formula><p>Input biases b C τ fully connected C τ softmax </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Continual Learning Implementation Details</head><p>As noted in Sections 2 and 5, our model can be applied to continual learning with one small modification: we store a compact representation of our training data that can be updated at each step of the continual learning procedure. Notice that <ref type="figure" target="#fig_4">Figure 3</ref> indicates that the functional representation of our linear classification layer ψ τ w (·) contains a mean pooling layer that combines the per-class output of our feature extractor {f θ (x τ m ; ψ f ) |x τ m ∈ D τ , y τ m = c}. The result of this pooling,</p><formula xml:id="formula_11">z c = 1 M f θ (x τ m ; ψ f )<label>(4)</label></formula><p>where M = |{f θ (x τ m ; ψ f ) |x τ m ∈ D τ , y τ m = c}|, is supplied as input to the network ψ w (·). This network yields the class conditional parameters of the linear classifier ψ τ w , resulting in (along with the feature extractor parameters ψ τ f ) the full paramterization of ψ τ . We store z c as the training dataset representation for, class c. Similar to the input to ψ τ w (·), the input to ψ τ f (·) also contains a mean-pooled representation, this time of the entire training dataset z τ G . This representation is also stored and updated in the same way. One issue with our procedure is that it is not completely invariant to the order in which we observe the sequence of training data during our continual learning procedure. The feature extractor adaptation parameters are only conditioned on the most recent training data, meaning that if data from class c is not present in the most recent training data, z c was generated using "old" feature extractor adaptation parameters (from a previous time step). This creates a potential disconnect between the classification parameters from previous time steps and the feature extractor output. Fortunately, in our experiment we noticed little within dataset variance for the adaptation parameters. Since all of our experiments on continual learning were within a single dataset, this did not seem to be an issue as CNAPS were able to achieved good performance. However, for continual learning experiments that contain multiple datasets, we anticipate that this issue will need to be addressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Additional Continual Learning Results</head><p>In Section 5 we provided results for continual learning experiments with Split MNIST <ref type="bibr" target="#b40">[41]</ref> and Split CIFAR100 <ref type="bibr" target="#b41">[42]</ref>. The results showed the average performance as more tasks were observed for the single and multi head settings. Here, we provide more complete results, detailing the performance through "time" at the task level. <ref type="figure" target="#fig_1">Figure 11</ref> details the performance of CNAPS (with varying number of observed examples) and Riemannian Walk (RWalk) <ref type="bibr" target="#b41">[42]</ref> on the five tasks of Split MNIST through time. Note that RWalk makes explicit use of training data from previous time steps when new data is observed, while CNAPS do not. <ref type="figure" target="#fig_1">Figure 11</ref> implies that CNAPS is competitive with RWalk in this scenario, despite seeing far less data per task, and not using old data to retrain the model at every time-step. Further, we see that CNAPS is naturally resistant to forgetting, as it uses internal task representations to maintain important information about tasks seen at previous time-steps. <ref type="figure" target="#fig_1">Figure 12</ref> demonstrates that CNAPS maintains similar results when scaling up to considerably more difficult datasets such as CIFAR100. Here too, CNAPS has not been trained on this dataset, yet demonstrates performance comparable to (and even better than) RWalk, a method explicitly trained for this task that makes use of samples from previous tasks at each time step.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Additional Active Learning Results</head><p>In Section 5 we provided active learning results for CNAPS and Prototypical Networks on the VGG Flowers dataset and three held out test languages from the Omniglot dataset. Here, we provide the results from all twenty held-out languages in Omniglot. <ref type="figure" target="#fig_1">Figure 13</ref> demonstrates that in almost all held-out languages, using the predictive distribution of CNAPS not only improves overall performance, but also enables the model to make use of standard acquisition functions <ref type="bibr" target="#b45">[46]</ref> to improve data efficiency over random acquisition. In contrast, we see that in most cases, random acquisition performs as well or better than acquisition functions that rely on the predictive distribution of Prototypical Networks. This provides empirical evidence that in addition to achieving overall better performance, the predictive distribution of CNAPS is more calibrated, and thus better suited to tasks such as active learning that require uncertainty in predictions. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>(a) Probabilistic graphical model detailing the CNP [13] framework. (b) Computational diagram depicting the CNAPS model class. Red boxes imply parameters in the model architecture supplied by adaptation networks. Blue shaded boxes depict the feature extractor and the gold box depicts the linear classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>A ResNet basic block with FiLM layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>(Left) A FiLM layer operating on convolutional feature maps indexed by channel ch. (Right) How a FiLM layer is used within a basic Residual network block<ref type="bibr" target="#b13">[14]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Implementation of functional representation of the class-specific parameters ψw. In this parameterization, ψ c w are the linear classification parameters for class c, and φw are the learnable parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Comparing CNAPS to gradient based feature extractor adaptation: accuracy on 5-way classification tasks from withheld datasets as a function of processing time. Dot size reflects shot number (1 to 25 shots).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Continual learning classification results on Split MNIST and Split CIFAR100 using a model trained on all training datasets. (Left) The plots show accumulated accuracy averaged over 30 runs for both singleand multi-head scenarios. (Right) Average accuracy at final task computed over 30 experiments (all figures are percentages). Errors are one standard deviation. Additional results from [42, 45]. Accuracy vs active learning iterations for held-out classes / languages. (Top) CNAPS and (bottom) prototypical networks. Error shading is one standard error. CNAPS achieves better accuracy than prototypical networks and improvements over random acquisition, whereas prototypical networks do not.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>w 6 :</head><label>6</label><figDesc>log p(y * m |π m ) ← log CAT(y * m ; π m ) y * m |π m ) 9: end procedure B Additional Related Work Details</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>t-SNE plots of the output of the set encoder zG and the FiLM layer parameters at the start (β 1b1 , γ 1b1 ) and end (β 4b2 , γ 4b2 ) of the feature extraction process at test time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>If at any point in our continual learning procedure we observe new training data for class c we can update our representation for class c by computing z c = 1 M f θ x τ m ; ψ f the pooled average resulting from M new training examples x τ m for class c. We then update z c with the weighted average: z c ← M zc+N zc M +N . At prediction time, we supply z c to ψ w (·) to produce classification parameters for class c.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Continual learning results on Split MNIST. Top row is multi-head, bottom row is single-head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 12 :</head><label>12</label><figDesc>Continual learning results on Split CIFAR100. Top two rows are multi-head, bottom two rows are single-head.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 13 :</head><label>13</label><figDesc>Active learning results on all twenty held-out OMNIGLOT languages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Few-shot classification results on META-DATASET<ref type="bibr" target="#b5">[6]</ref> using models trained on all training datasets. All figures are percentages and the ± sign indicates the 95% confidence interval over tasks. Bold text indicates the scores within the confidence interval of the highest score. Tasks from datasets below the dashed line were not used for training. Competing methods' results from<ref type="bibr" target="#b5">[6]</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Datasets used to train, validate, and test models.</figDesc><table><row><cell>Dataset</cell><cell cols="5">Finetune MatchingNet ProtoNet fo-MAML Proto-MAML</cell><cell>CNAPS</cell></row><row><cell>ILSVRC [21]</cell><cell>45.8±1.1</cell><cell>45.0±1.1</cell><cell>50.5±1.1</cell><cell>36.1±1.0</cell><cell>51.0±1.1</cell><cell>50.6±1.1</cell></row><row><cell>Omniglot [31]</cell><cell>60.9±1.6</cell><cell>52.3±1.3</cell><cell>60.0±1.4</cell><cell>38.7±1.4</cell><cell>63.0±1.4</cell><cell>45.2±1.4</cell></row><row><cell>Aircraft [32]</cell><cell>68.7±1.3</cell><cell>49.0±0.9</cell><cell>53.1±1.0</cell><cell>34.5±0.9</cell><cell>55.3±1.0</cell><cell>36.0±0.8</cell></row><row><cell>Birds [33]</cell><cell>57.3±1.3</cell><cell>62.2±1.0</cell><cell>68.8±1.0</cell><cell>49.1±1.2</cell><cell>66.9±1.0</cell><cell>60.7±0.9</cell></row><row><cell>Textures [34]</cell><cell>69.1±0.9</cell><cell>64.2±0.9</cell><cell>66.6±0.8</cell><cell>56.5±0.8</cell><cell>67.8±0.8</cell><cell>67.5±0.7</cell></row><row><cell>Quick Draw [35]</cell><cell>42.6±1.2</cell><cell>42.9±1.1</cell><cell>49.0±1.1</cell><cell>27.2±1.2</cell><cell>53.7±1.1</cell><cell>42.3±1.0</cell></row><row><cell>Fungi [36]</cell><cell>38.2±1.0</cell><cell>34.0±1.0</cell><cell>39.7±1.1</cell><cell>23.5±1.0</cell><cell>38.0±1.1</cell><cell>30.1±0.9</cell></row><row><cell cols="2">VGG Flower [37] 85.5±0.7</cell><cell>80.1±0.7</cell><cell>85.3±0.8</cell><cell>66.4±1.0</cell><cell>86.9±0.8</cell><cell>70.7±0.7</cell></row><row><cell cols="2">Traffic Signs [38] 66.8±1.3</cell><cell>47.8±1.1</cell><cell>47.1±1.1</cell><cell>33.2±1.3</cell><cell>51.2±1.1</cell><cell>53.3±0.9</cell></row><row><cell>MSCOCO [39]</cell><cell>34.9±1.0</cell><cell>35.0±1.0</cell><cell>41.0±1.1</cell><cell>27.5±1.1</cell><cell>43.4±1.1</cell><cell>45.2±1.1</cell></row><row><cell>MNIST [29]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70.4±0.8</cell></row><row><cell>CIFAR10 [30]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>65.2±0.8</cell></row><row><cell>CIFAR100 [30]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Few-shot classification results on META-DATASET [6] comparing joint training for θ and φ (columns 2 and 3) to two-stage training (column 4). All figures are percentages and the ± sign indicates the 95% confidence interval. Bold text indicates the highest scores that overlap in their confidence intervals.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Few-shot classification results on META-DATASET<ref type="bibr" target="#b5">[6]</ref> using models trained on all training datasets for Parallel Residual Adapters<ref type="bibr" target="#b16">[17]</ref> and CNAPS. All figures are percentages and the ± sign indicates the 95% confidence interval over tasks. Bold text indicates the scores within the confidence interval of the highest score. Tasks from datasets below the dashed line were not used for training.</figDesc><table><row><cell>E Network Architecture Details</cell></row><row><cell>E.1 ResNet18 Architecture details</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>ResNet-18 basic block b.</figDesc><table><row><cell>Layers</cell></row><row><cell>Input</cell></row><row><cell>Conv2d (3 × 3, stride 1, pad 1)</cell></row><row><cell>BatchNorm</cell></row><row><cell>FiLM (γ b,1 , β b,1 )</cell></row><row><cell>ReLU</cell></row><row><cell>Conv2d (3 × 3, stride 1, pad 1)</cell></row><row><cell>BatchNorm</cell></row><row><cell>FiLM (γ b,2 , β b,2 )</cell></row><row><cell>Sum with Input</cell></row><row><cell>ReLU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>ResNet-18 basic scaling block b.</figDesc><table><row><cell>Layers</cell></row><row><cell>Input</cell></row><row><cell>Conv2d (3 × 3, stride 2, pad 1)</cell></row><row><cell>BatchNorm</cell></row><row><cell>FiLM (γ b,1 , β b,1 )</cell></row><row><cell>ReLU</cell></row><row><cell>Conv2d (3 × 3, stride 1, pad 1)</cell></row><row><cell>BatchNorm</cell></row><row><cell>FiLM (γ b,2 , β b,2 )</cell></row><row><cell>Downsample Input by factor of 2</cell></row><row><cell>Sum with Downsampled Input</cell></row><row><cell>ReLU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>ResNet-18 Feature Extractor (θ) with FiLM Layers: x → f θ (x; ψ τ f ), x * → f θ (x * ; ψ τ f )</figDesc><table><row><cell>Stage</cell><cell>Output size</cell><cell>Layers</cell></row><row><cell>Input</cell><cell>84 × 84 × 3</cell><cell>Input image</cell></row><row><cell>Pre-processing</cell><cell cols="2">41 × 41 × 64 Conv2d (5 × 5, stride 2, pad 1, BatchNorm, ReLU)</cell></row><row><cell>Layer 1</cell><cell cols="2">41 × 41 × 64 Basic Block × 2</cell></row><row><cell>Layer 2</cell><cell cols="2">21 × 21 × 128 Basic Block, Basic Scaling Block</cell></row><row><cell>Layer 3</cell><cell cols="2">11 × 11 × 256 Basic Block, Basic Scaling Block</cell></row><row><cell>Layer 4</cell><cell>6 × 6 × 512</cell><cell>Basic Block, Basic Scaling Block</cell></row><row><cell>Post-Processing</cell><cell>512</cell><cell>AvgPool, Flatten</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>ResNet-18 feature extractor network.</figDesc><table><row><cell cols="2">Set Encoder (g): x → z τ G</cell></row><row><cell cols="2">Output size Layers</cell></row><row><cell cols="2">84 × 84 × 3 Input image</cell></row><row><cell cols="2">42 × 42 × 64 Conv2d (3 × 3, stride 1, pad 1, ReLU), MaxPool (2 × 2, stride 2)</cell></row><row><cell cols="2">21 × 21 × 64 Conv2d (3 × 3, stride 1, pad 1, ReLU), MaxPool (2 × 2, stride 2)</cell></row><row><cell cols="2">10 × 10 × 64 Conv2d (3 × 3, stride 1, pad 1, ReLU), MaxPool (2 × 2, stride 2)</cell></row><row><cell>5 × 5 × 64</cell><cell>Conv2d (3 × 3, stride 1, pad 1, ReLU), MaxPool (2 × 2, stride 2)</cell></row><row><cell>2 × 2 × 64</cell><cell>Conv2d (3 × 3, stride 1, pad 1, ReLU), MaxPool (2 × 2, stride 2)</cell></row><row><cell>64</cell><cell>AdaptiveAvgPool2d</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9 :</head><label>9</label><figDesc>Set encoder g.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 10 :</head><label>10</label><figDesc>Network of set encoder φ f .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Network φw.</figDesc><table><row><cell cols="2">Network (φ w ):</cell></row><row><cell>z c → ψ w,w</cell><cell></cell></row><row><cell cols="2">Output size Layers</cell></row><row><cell>512</cell><cell>Input from mean pooling</cell></row><row><cell>512</cell><cell>2 × fully connected, ELU</cell></row><row><cell>512</cell><cell>fully connected</cell></row><row><cell>512</cell><cell>Sum with Input</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 :</head><label>13</label><figDesc>Network φ b .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 :</head><label>14</label><figDesc>Linear classifier network.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Source code available at https://github.com/cambridge-mlg/cnaps.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://pytorch.org/docs/stable/torchvision/models.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Ambrish Rawat for helpful discussions and David Duvenaud, Wessel Bruinsma, Will Tebbutt Adrià Garriga Alonso, Eric Nalisnick, and Lyndon White for the insightful comments and feedback. Richard E. Turner is supported by Google, Amazon, Improbable and EPSRC grants EP/M0269571 and EP/L000776/1.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
		</imprint>
		<respStmt>
			<orgName>Technische Universität München</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Learning to learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorien</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4080" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meta-learning probabilistic inference for prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkxStoC5F7" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03096</idno>
		<title level="m">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<title level="m">Reptile: a scalable metalearning algorithm</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hod</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03466</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the prediction of observables: a selective update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seymour</forename><surname>Geisser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
		<respStmt>
			<orgName>University of Minnesota</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Predictive inference. Routledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seymour</forename><surname>Geisser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Shanahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Eslami</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01613</idno>
		<title level="m">Conditional neural processes</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">FiLM: Visual reasoning with a general conditioning layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harm De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning multiple visual domains with residual adapters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient parametrization of multidomain deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Sylvestre-Alvise Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8119" to="8127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3394" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A meta-learning perspective on cold-start recommendations for items</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manasi</forename><surname>Vartak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conrado</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeshua</forename><surname>Bratman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7266-a-meta-learning-perspective-on-cold-start-recommendations-for-items.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="6904" to="6914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Meta-learning with latent embedding optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">CAML: Fast context adaptation via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Luisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyriacos</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vitaly</forename><surname>Shiarlis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Kurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whiteson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.03642</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Discriminative k-shot learning using probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Bartłomiejświątkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.00326</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">TADAM: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boris N Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pau</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rodriguez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10123</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">MNIST handwritten digit database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist" />
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">18</biblScope>
		</imprint>
		<respStmt>
			<orgName>AT&amp;T Labs</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Finegrained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esa</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juho</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mircea</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sammy</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3606" to="3613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A neural representation of sketch drawings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03477</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Fgvcx fungi classification challenge at fgvc5</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brigit</forename><surname>Schroeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/fungi-challenge-fgvc-2018" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maria-Elena</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixth Indian Conference on Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Detection of traffic signs in real-world images: The german traffic sign detection benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Houben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2013 international joint conference on neural networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Child: A first step towards continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="77" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedemann</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3987" to="3995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Riemannian walk for incremental learning: Understanding forgetting and intransigence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arslan</forename><surname>Chaudhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Puneet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thalaiyasingam</forename><surname>Dokania</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Ajanthan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="532" to="547" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kieran</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiago</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agnieszka</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national academy of sciences</title>
		<meeting>the national academy of sciences</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="3521" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Cuong V Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10628</idno>
		<title level="m">Variational continual learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Improving and understanding variational continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Swaroop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cuong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Thang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02099</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Active learning with statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>David A Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="129" to="145" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Burr</forename><surname>Settles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Artificial Intelligence and Machine Learning</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="114" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Danilo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.01622</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Neural processes. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attentive neural processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunjik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yee Whye</forename><surname>Teh</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SkE6PjC9KX" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Bayesian model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ousmane</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03836</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cremer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.03558</idno>
		<title level="m">Inference suboptimality in variational autoencoders</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Code for &quot;meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<ptr target="https://github.com/google-research/meta-dataset" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Visualizing data using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
