<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PointCNN: Convolution On X -Transformed Points</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University ‡ Huawei Inc. § Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">†</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University ‡ Huawei Inc. § Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University ‡ Huawei Inc. § Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University ‡ Huawei Inc. § Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University ‡ Huawei Inc. § Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhan</forename><surname>Di</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University ‡ Huawei Inc. § Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University ‡ Huawei Inc. § Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PointCNN: Convolution On X -Transformed Points</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a simple and general framework for feature learning from point clouds. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point clouds are irregular and unordered, thus directly convolving kernels against features associated with the points will result in desertion of shape information and variance to point ordering. To address these problems, we propose to learn an X -transformation from the input points to simultaneously promote two causes: the first is the weighting of the input features associated with the points, and the second is the permutation of the points into a latent and potentially canonical order. Element-wise product and sum operations of the typical convolution operator are subsequently applied on the X -transformed features. The proposed method is a generalization of typical CNNs to feature learning from point clouds, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spatially-local correlation is a ubiquitous property of various types of data that is independent of the data representation. For data that is represented in regular domains, such as images, the convolution operator has been shown to be effective in exploiting that correlation as the key contributor to the success of CNNs on a variety of tasks <ref type="bibr" target="#b24">[25]</ref>. However, for data represented in point cloud form, which is irregular and unordered, the convoralution operator is ill-suited for leveraging spatially-local correlations in the data. in all the cases ((i) − (iv)), and we have one kernel K = [k α , k β , k γ , k δ ] T of shape 4 × C. In (i), by following the canonical order given by the regular grid structure, the features in the local 2 × 2 patch can be cast into [f a , f b , f c , f d ] T of shape 4 × C, for convolving with K, yielding</p><formula xml:id="formula_0">f i = Conv(K, [f a , f b , f c , f d ] T ),</formula><p>where Conv(·, ·) is simply an element-wise product followed by a sum <ref type="bibr" target="#b1">2</ref> . In (ii), (iii), and (iv), the points are sampled from local neighborhoods, and thus their ordering may be arbitrary. By following orders as illustrated in the figure, the input feature set F can be cast into [f a , f b , f c , f d ] T in (ii) and (iii), and [f c , f a , f b , f d ] T in (iv). Based on this, if the convolution operator is directly applied, the output features for the three cases could be computed as depicted in Eq. 1a. Note that f ii ≡ f iii holds for all cases, while f iii = f iv holds for most cases. This example illustrates that a direct convolution results in deserting shape information (i.e., f ii ≡ f iii ), while retaining variance to the ordering (i.e., f iii = f iv ).</p><p>In this paper, we propose to learn a K × K X -transformation for the coordinates of K input points (p 1 , p 2 , ..., p K ), with a multilayer perceptron <ref type="bibr" target="#b38">[39]</ref>, i.e., X = MLP (p 1 , p 2 , ..., p K ). Our aim is to use it to simultaneously weight and permute the input features, and subsequently apply a typical convolution on the transformed features. We refer to this process as X -Conv, and it is the basic building block for our PointCNN. The X -Conv for (ii), (iii), and (iv) in <ref type="figure" target="#fig_6">Figure 1</ref> can be formulated as in Eq. 1b, where the X s are 4 × 4 matrices, as K = 4 in this figure. Note that since X ii and X iii are learned from points of different shapes, they can differ so as to weight the input features accordingly, and achieve f ii = f iii . For X iii and X iv , if they are learned to satisfy X iii = X iv × Π, where Π is the permutation matrix for permuting (c, a, b, d) into (a, b, c, d), then f iii ≡ f iv can be achieved.</p><p>From the analysis of the example in <ref type="figure" target="#fig_6">Figure 1</ref>, it is clear that, with ideal X -transformations, X -Conv is capable of taking the point shapes into consideration, while being invariant to ordering. In practice, we find that the learned X -transformations are far from ideal, especially in terms of the permutation equivalence aspect. Nevertheless, PointCNN built with X -Conv is still significantly better than a direct application of typical convolutions on point clouds, and on par or better than state-of-the-art neural networks designed for point cloud input data, such as PointNet++ <ref type="bibr" target="#b34">[35]</ref>. Section 3 contains the details of X -Conv, as well as PointCNN architectures. We show our results on multiple challenging benchmark datasets and tasks in Section 4, together with ablation experiments and visualizations for a better understanding of PointCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Feature Learning from Regular Domains. CNNs have been very successful for leveraging spatially-local correlation in images -pixels in 2D regular grids <ref type="bibr" target="#b25">[26]</ref>. There has been work in extending CNNs to higher dimensional regular domains, such as 3D voxels <ref type="bibr" target="#b51">[52]</ref>. However, as both the input and convolution kernels are of higher dimensions, the amount of both computation and memory inflates dramatically. Octree <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b46">47]</ref>, Kd-Tree <ref type="bibr" target="#b21">[22]</ref> and Hash <ref type="bibr" target="#b40">[41]</ref> based approaches have been proposed to save computation by skipping convolution in empty space. The activations are kept sparse in <ref type="bibr" target="#b12">[13]</ref> to retain sparsity in convolved layers. <ref type="bibr" target="#b16">[17]</ref> and <ref type="bibr" target="#b3">[4]</ref> partition point cloud into grids and represent each grid with grid mean points and Fisher vectors respectively for convolving with 3D kernels. In these approaches, the kernels themselves are still dense and of high dimension. Sparse kernels are proposed in <ref type="bibr" target="#b27">[28]</ref>, but this approach cannot be applied recursively for learning hierarchical features. Compared with these methods, PointCNN is sparse in both input representation and convolution kernels.</p><p>Feature Learning from Irregular Domains. Stimulated by the rapid advances and demands in 3D sensing, there has been quite a few recent developments in feature learning from 3D point clouds. PointNet <ref type="bibr" target="#b32">[33]</ref> and Deep Sets <ref type="bibr" target="#b57">[58]</ref> proposed to achieve input order invariance by the use of a symmetric function over inputs. PointNet++ <ref type="bibr" target="#b34">[35]</ref> and SO-Net <ref type="bibr" target="#b26">[27]</ref> apply PointNet hierarchically for better capturing of local structures. Kernel correlation and graph pooling are proposed for improving PointNet-like methods in <ref type="bibr" target="#b41">[42]</ref>. RNN is used in <ref type="bibr" target="#b17">[18]</ref> for processing features aggregated by pooling from ordered point cloud slices. <ref type="bibr" target="#b49">[50]</ref> proposed to leverage neighborhood structures in both point and feature spaces. While these symmetric pooling based approaches, as well as those in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b35">36]</ref>, have guarantee in achieving order invariance, they come with a price of throwing away information. <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b43">44]</ref> propose to first "interpolate" or "project" features into predefined regular domains, where typical CNNs can be applied. In contrast, the regular domain is latent in our method. CNN kernels are represented as parametric functions of neighborhood point positions to generalize CNNs for point clouds in <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b52">53]</ref>. The kernels associated with each point are parametrized individually in these methods, while the X -transformations in our method are learned from each neighborhood, thus could potentially by more adaptive to local structures.</p><p>Besides as point clouds, sparse data in irregular domains can be represented as graphs, or meshes, and a few works have been proposed for feature learning from such representations <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b29">30]</ref>. We refer the interested reader to <ref type="bibr" target="#b4">[5]</ref> for a comprehensive survey of work along these directions. Spectral graph convolution on a local graph is used for processing point clouds in <ref type="bibr" target="#b45">[46]</ref>. Invariance vs. Equivariance. A line of pioneering work aiming at achieving equivariance has been proposed to address the information loss problem of pooling in achieving invariance <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b39">40]</ref>. The X -transformations in our formulation, ideally, are capable of realizing equivariance, and are demonstrated to be effective in practice. We also found similarity between PointCNN and Spatial Transformer Networks <ref type="bibr" target="#b19">[20]</ref>, in the sense that both of them provided a mechanism to "transform" input into latent canonical forms for being further processed, with no explicit loss or constraint in enforcing the canonicalization. In practice, it turns out that the networks find their ways to leverage the mechanism for learning better. In PointCNN, the X -transformation is supposed to serve for both weighting and permutation, thus is modelled as a general matrix. This is different than that in <ref type="bibr" target="#b7">[8]</ref>, where a permutation matrix is the desired output, and is approximated by a doubly stochastic matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PointCNN</head><p>The hierarchical application of convolutions is essential for learning hierarchical representations via CNNs. PointCNN shares the same design and generalizes it to point clouds. First, we introduce hierarchical convolutions in PointCNN, in analogy to that of image CNNs, then, we explain the core X -Conv operator in detail, and finally, present PointCNN architectures geared toward various tasks. In regular grids, convolutions are recursively applied on local grid patches, which often reduces the grid resolution (4 × 4 → 3 × 3 → 2 × 2), while increasing the channel number (visualized by dot thickness). Similarly, in point clouds, X -Conv is recursively applied to "project", or "aggregate", information from neighborhoods into fewer representative points (9 → 5 → 2), but each with richer information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchical Convolution</head><p>Before we introduce the hierarchical convolution in PointCNN, we briefly go through its well known version for regular grids, as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref> upper. The input to grid-based CNNs is a feature map F 1 of shape R 1 × R 1 × C 1 , where R 1 is the spatial resolution, and C 1 is the feature channel depth. The convolution of kernels K of shape K × K × C 1 × C 2 against local patches of shape K × K × C 1 from F 1 , yields another feature map F 2 of shape R 2 × R 2 × C 2 . Note that in <ref type="figure" target="#fig_0">Figure 2</ref> upper, R 1 = 4, K = 2, and R 2 = 3. Compared with F 1 , F 2 is often of lower resolution (R 2 &lt; R 1 ) and of deeper channels (C 2 &gt; C 1 ), and encodes higher level information. This process is recursively applied, producing feature maps with decreasing spatial resolution (4 × 4 → 3 × 3 → 2 × 2 in <ref type="figure" target="#fig_0">Figure 2</ref> upper), but deeper channels (visualized by increasingly thicker dots in <ref type="figure" target="#fig_0">Figure 2</ref> upper).</p><p>The input to PointCNN is F 1 = {(p 1,i , f 1,i ) : i = 1, 2, ..., N 1 }, i.e., a set of points {p 1,i : p 1,i ∈ R Dim }, each associated with a feature {f 1,i : f 1,i ∈ R C1 }. Following the hierarchical construction of grid-based CNNs, we would like to apply X -Conv on F 1 to obtain a higher level representation {p 1,i } and F 2 is of a smaller spatial resolution and deeper feature channels than F 1 , i.e., N 2 &lt; N 1 , and C 2 &gt; C 1 . When the X -Conv process of turning F 1 into F 2 is recursively applied, the input points with features are "projected", or "aggregated", into fewer points (9 → 5 → 2 in <ref type="figure" target="#fig_0">Figure 2</ref> lower), but each with increasingly richer features (visualized by increasingly thicker dots in <ref type="figure" target="#fig_0">Figure 2</ref> lower).</p><formula xml:id="formula_1">F 2 = {(p 2,i , f 2,i ) : f 2,i ∈ R C2 , i = 1, 2, ..., N 2 }, where {p 2,i } is a set of representative points of ( #,# , #,# ) ',# ',' ',' ',# ',' ',# [ , ( #,# − ',# ), #,# ] a b c ( #,# − ',' , #,# ) [ , ( #,# − ',# ), #,# ] ( #,# − ',# , #,# )</formula><p>The representative points {p 2,i } should be the points that are beneficial for the information "projection" or "aggregation". In our implementation, they are generated by random down-sampling of {p 1,i } in classification tasks, and farthest point sampling in segmentation tasks, since segmentation tasks are more demanding on a uniform point distribution. We suspect some more advanced point selections which have shown promising performance in geometry processing, such as Deep Points <ref type="bibr" target="#b50">[51]</ref>, could fit in here as well. We leave the exploration of better representative point generation methods for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">X -Conv Operator</head><p>X -Conv is the core operator for turning F 1 into F 2 . In this section, we first introduce the input, output and procedure of the operator, and then explain the rationale behind the procedure.</p><p>ALGORITHM 1: X -Conv Operator Input :K, p, P, F Output :Fp Features "projected", or "aggregated", into representative point p 1: P ← P − p Move P to local coordinate system of p 2:</p><formula xml:id="formula_2">F δ ← MLP δ (P ) Individually lift each point into C δ dimensional space 3: F * ← [F δ , F] Concatenate F δ and F, F * is a K × (C δ + C1) matrix 4: X ← MLP (P )</formula><p>Learn the K × K X -transformation matrix 5: FX ← X × F * Weight and permute F * with the learnt X 6: Fp ← Conv(K, FX )</p><p>Finally, typical convolution between K and FX To leverage spatially-local correlation, similar to convolution in grid-based CNNs, X -Conv operates in local regions. Since the output features are supposed to be associated with the representative points {p 2,i }, X -Conv takes their neighborhood points in {p 1,i }, as well as the associated features, as input to convolve with. For simplicity, we denote a representative point in {p 2,i } as p, the features with p as f and its K neighbors in {p 1,i } as N, thus the X -Conv input for this specific p is</p><formula xml:id="formula_3">S = {(p i , f i ) : p i ∈ N}.</formula><p>Note that S is an unordered set. Without loss of generality, S can be cast into a K × Dim matrix P = (p 1 , p 2 , ..., p K ) T , and a K × C 1 matrix F = (f 1 , f 2 , ..., f K ) T , and K denotes the trainable convolution kernels. With these inputs, we would like to compute the features F p , which are the "projection", or "aggregation", of the input features into the representative point p.</p><p>We detail the X -Conv operator in Algorithm 1, and summarize it concisely as:</p><formula xml:id="formula_4">F p = X −Conv(K, p, P, F) = Conv(K, MLP (P − p) × [MLP δ (P − p), F]),<label>(2)</label></formula><p>where MLP δ (·) is a multilayer perceptron applied individually on each point, as in PointNet <ref type="bibr" target="#b32">[33]</ref>. Note that all the operations involved in building X -Conv, i.e., Conv(·, ·), MLP (·), matrix multiplication (·) × (·), and MLP δ (·), are differentiable. Accordingly. X -Conv is differentiable, and can be plugged into a neural network for training by back propagation.</p><p>Lines 4-6 in Algorithm 1 are the core X -transformation as described in Eq. 1b in Section 1. Here, we explain the rationale behind lines 1-3 of Algorithm 1 in detail. X -Conv is designed to work on local point regions, and the output should not be dependent on the absolute position of p and its neighboring points, but on their relative positions. To that end, we position local coordinate systems at the representative points (line 1 of Algorithm 1, <ref type="figure" target="#fig_1">Figure 3b</ref>). It is the local coordinates of neighboring points, together with their associated features, that define the output features. However, the local coordinates are of a different dimensionality and representation than the associated features.</p><p>To address this issue, we first lift the coordinates into a higher dimensional and more abstract representation (line 2 of Algorithm 1), and then combine it with the associated features (line 3 of Algorithm 1) for further processing <ref type="figure" target="#fig_1">(Figure 3c</ref>).</p><p>Lifting coordinates into features is done through a point-wise MLP δ (·), as in PointNet-based methods. Differently, however,the lifted features are not processed by a symmetric function. Instead, along with the associated features, they are weighted and permuted by the X -transformation that is jointly learned across all neighborhoods. The resulting X is dependent on the order of the points, and this is desired, as X is supposed to permute F * according to the input points, and therefore has to be aware of the specific input order. For an input point cloud without any additional features, i.e., F is empty, the first X -Conv layer uses only F δ . PointCNN can thus handle point clouds with or without additional features in a robust uniform fashion.</p><p>For more details about the X -Conv operator, including the actual definition of MLP δ (·), MLP (·) and Conv(·, ·), please refer to Supplementary Material Section 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PointCNN Architectures</head><p>From <ref type="figure" target="#fig_0">Figure 2</ref>, we can see that the Conv layers in grid-based CNNs and X -Conv layers in PointCNN only differ in two aspects: the way the local regions are extracted (K × K patches vs. K neighboring points around representative points) and the way the information from local regions is learned (Conv vs. X -Conv). Otherwise, the process of assembling a deep network with X -Conv layers highly resembles that of grid-based CNNs.  Figure 4a depicts a simple PointCNN with two X -Conv layers that gradually transform the input points (with or without features) into fewer representation points, but each with richer features. After the second X -Conv layer, there is only one representative point left, and it aggregates information from all the points from the previous layer. In PointCNN, we can roughly define the receptive field of each representative point as the ratio K/N , where K is the neighboring point number, and N is the point number in the previous layer. With this definition, the final point "sees" all the points from the previous layer, thus has a receptive field of 1.0 -it has a global view of the entire shape, and its features are informative for semantic understanding of the shape. We can add fully connected layers on top of the last X -Conv layer output, followed by a loss, for training the network.</p><p>Note that the number of training samples for the top X -Conv layers drops rapidly <ref type="figure" target="#fig_3">(Figure 4a</ref>), making it inefficient to train them thoroughly. To address this problem, we propose PointCNN with denser connections <ref type="figure" target="#fig_3">(Figure 4b)</ref>, where more representative points are kept in the X -Conv layers. However, we aim to maintain the depth of the network, while keeping the receptive field growth rate, such that the deeper representative points "see" increasingly larger portions of the entire shape. We achieve this goal by employing the dilated convolution idea from grid-based CNNs in PointCNN. Instead of always taking the K neighboring points as input, we uniformly sample K input points from K × D neighboring points, where D is the dilation rate. In this case, the receptive field increases from K/N to (K × D)/N , without increasing actual neighboring point count or kernel size.</p><p>In the second X -Conv layer of PointCNN in <ref type="figure" target="#fig_3">Figure 4b</ref>, dilation rate D = 2 is used, thus all the four remaining representative points "see" the entire shape, and all of them are suitable for making   <ref type="bibr" target="#b51">[52]</ref> and Scan-Net <ref type="bibr" target="#b8">[9]</ref>. The reported performances are based on 1024 input points, unless otherwise noted by P# (# input points) or PN# (# input points with normals).</p><p>predictions. Note that, in this way, we can train the top X -Conv layers more thoroughly, as much more connections are involved in the network, compared to PointCNN in <ref type="figure" target="#fig_3">Figure 4a</ref>. In test time, the output from the multiple representative points is averaged right before the sof tmax to stabilize the prediction. This design is similar to that of Network in Network <ref type="bibr" target="#b28">[29]</ref>. The denser version of PointCNN <ref type="figure" target="#fig_3">(Figure 4b</ref>) is the one we used for classification tasks.</p><p>For segmentation tasks, high resolution point-wise output is required, and this can be realized by building PointCNN following Conv-DeConv <ref type="bibr" target="#b31">[32]</ref> architecture, where the DeConv part is responsible for propagating global information into high resolution predictions (see <ref type="figure" target="#fig_3">Figure 4c</ref>). Note that both the "Conv" and "DecConv" in the PointCNN segmentation network are the same X -Conv operator. The only differences between the "Conv" and "DeConv" layers is that the latter has more points but less feature channels in its output vs. its input, and its higher resolution points are forwarded from earlier "Conv" layers, following the design of U-Net <ref type="bibr" target="#b37">[38]</ref>.</p><p>Dropout is applied before the last fully connected layer to reduce over-fitting. We also employed the "subvolume supervision" idea from <ref type="bibr" target="#b33">[34]</ref>, to further address the over-fitting problem. In the last X -Conv layers, the receptive field is set to be less than 1, such that only partial information is "seen" by the representative points. The network is pushed to learn harder from the partial information during training, and performs better at test time. In this case, the global coordinates of the representative points matter, thus they are lifted into feature space R Cg with M LP g (·) (detailed in Supp. Material Section 1) and concatenated into X -Conv for further processing by follow-up layers.</p><p>Data augmentation. To train the parameters in X -Conv, it is evidently not beneficial to keep using the same set of neighboring points, in the same order, for a specific representative point. To improve generalization, we propose to randomly sample and shuffle the input points, such that both the neighboring point sets and order may differ from batch to batch. To train a model that takes N points as input, N (N, (N/8) 2 ) points are used for training, where N denotes a Gaussian distribution. We found that this strategy is crucial for successful training of PointCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We conducted an extensive evaluation of PointCNN for shape classification on six datasets (Model-Net40 <ref type="bibr" target="#b51">[52]</ref>, ScanNet <ref type="bibr" target="#b8">[9]</ref>, TU-Berlin <ref type="bibr" target="#b10">[11]</ref>, Quick Draw <ref type="bibr" target="#b14">[15]</ref>, MNIST, CIFAR10), and segmentation task on three datasets (ShapeNet Parts <ref type="bibr" target="#b53">[54]</ref>, S3DIS <ref type="bibr" target="#b1">[2]</ref>, and ScanNet <ref type="bibr" target="#b8">[9]</ref>). The details of the datasets and how we convert and feed data into PointCNN, are described in Supp. Material Section 2, and the PointCNN architectures for the tasks on these datasets can be found in Supp. Material Section 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Classification and Segmentation Results</head><p>We summarize our 3D point cloud classification results on ModelNet40 and ScanNet in <ref type="table" target="#tab_1">Table 1</ref>, and compare to several neural network methods designed for point clouds. Note that a large portion of the 3D models from ModelNet40 are pre-aligned to the common up direction and horizontal facing direction. If a random horizontal rotation is not applied on either the training or testing sets, then the relatively consistent horizontal facing direction is leveraged, and the metrics based on this setting is not directly comparable to those with the random horizontal rotation. For this reason, we ran PointCNN and reported its performance in both settings. Note that PointCNN achieved top performance on both ModelNet40 and ScanNet.</p><p>ShapeNet Parts S3DIS ScanNet pIoU mpIoU mIoU OA SyncSpecCNN <ref type="bibr" target="#b54">[55]</ref> 84.74 82.0 --Pd-Network <ref type="bibr" target="#b21">[22]</ref> 85.49 82.7 --SSCN <ref type="bibr" target="#b11">[12]</ref> 85.98 83.3 --SPLATNet <ref type="bibr" target="#b42">[43]</ref> 85.4 83.7 --SpiderCNN <ref type="bibr" target="#b52">[53]</ref> 85.3 81.7 --SO-Net <ref type="bibr" target="#b26">[27]</ref> 84.9 81.0 --PCNN <ref type="bibr" target="#b2">[3]</ref> 85.1 81.8 --KCNet <ref type="bibr" target="#b41">[42]</ref> 83.7 82.2 --SpecGCN <ref type="bibr" target="#b45">[46]</ref> 85.4 ---Kd-Net <ref type="bibr" target="#b21">[22]</ref> 82.3 77.4 --3DmFV-Net <ref type="bibr" target="#b3">[4]</ref> 84.3 81.0 --RSNet <ref type="bibr" target="#b17">[18]</ref> 84  We evaluate PointCNN on the segmentation of ShapeNet Parts, S3DIS, and ScanNet datasets, and summarize the results in <ref type="table" target="#tab_3">Table 2</ref>. More detailed segmentation result comparisons can be found in Supplementary Material Section 4. We note that PointCNN outperforms all the compared methods, including SSCN <ref type="bibr" target="#b11">[12]</ref>, SP-Graph <ref type="bibr" target="#b23">[24]</ref> and SGPN <ref type="bibr" target="#b48">[49]</ref>, which are specialized segmentation networks with state-of-the-art performance. Note that the part averaged IoU metric for ShapeNet Parts is the one used in <ref type="bibr" target="#b55">[56]</ref>. Compared with mean IoU, the part averaged IoU puts more emphasis on the correct prediction of small parts.</p><p>Sketches are 1D curves in 2D space, thus can be more effectively represented with point clouds, rather than with 2D images. We evaluate PointCNN on TU-Berlin and Quick Draw sketches, and present results in <ref type="table" target="#tab_5">Table 3</ref>, where we compare its performance with the competitive PointNet++, as well as image CNN based methods. PointCNN outperforms PointNet++ on both datasets, with a more prominent advantage on Quick Draw (25M data samples), which is significantly larger than TU-Berlin (0.02M data samples). On the TU-Berlin dataset, while the performance of PointCNN is slightly better than the generic image CNN AlexNet <ref type="bibr" target="#b22">[23]</ref>, there is still a gap with the specialized Sketch-a-Net <ref type="bibr" target="#b56">[57]</ref>. It is interesting to study whether architectural elements from Sketch-a-Net can be adopted and integrated into PointCNN to improve its performance on the sketch datasets.</p><p>Since X -Conv is a generalization of Conv, ideally, PointCNN should perform on par with CNNs, if the underlying data is the same, but only represented differently. To verify this, we evaluate PointCNN on the point cloud representation of MNIST and CIFAR10, and show results in <ref type="table" target="#tab_6">Table 4</ref>. For MNIST data, PointCNN achieved comparable performance with other methods, indicating its effective learning of the digits' shape information. For CIFAR10 data, where there is mostly no "shape" information, PointCNN has to learn mostly from the spatially-local correlation in the RGB features, and it performed reasonably well on this task, though there is a large gap between PointCNN and the mainstream image CNNs. From this experiment, we can conclude that CNNs are still the better choice for general images.</p><p>Method TU-Berlin Quick Draw Sketch-a-Net <ref type="bibr" target="#b56">[57]</ref> 77.95 -AlexNet <ref type="bibr" target="#b22">[23]</ref> 68.60 -PointNet++ <ref type="bibr" target="#b34">[35]</ref> 66. <ref type="bibr" target="#b52">53</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Experiments and Visualizations</head><p>Ablation test of the core X -Conv operator. To verify the effectiveness of the X -transformation, we propose PointCNN without it as a baseline, where lines 4-6 of Algorithm 1 are replaced by    Visualization of X -Conv features. Each representative point, with its neighboring points in a particular order, has a corresponding F * and F X in R K×C , where C = C δ + C 1 . For the same representative point, if its neighboring points in different orders are fed into the network, we get a set of F * and F X , and we denote them as F * and F X . Similarly, we define the set of F * in PointCNN w/o X as F o . Clearly, F * can be quite scattering in the R K×C space, since differences in input point order will result in a different F * . On the other hand, if the learned X can perfectly canonize F * , F X is supposed to stay at a canonical point in the space.</p><p>To verify this, we show T-SNE visualization of F o , F * and F X of 15 randomly picked representative points from the ModelNet40 dataset in <ref type="figure" target="#fig_5">Figure 5</ref>, each with one color, and consistent in the sub-figures. Note that F o is quite "blended", which indicates that the features from different representative points are not discriminative against each other <ref type="figure" target="#fig_5">(Figure 5a</ref>). While F * is better than F o , it is still "fuzzy" <ref type="figure" target="#fig_5">(Figure 5b</ref>). In <ref type="figure" target="#fig_5">Figure 5c</ref>, F * are "concentrated" by X , and the features of each representative point become highly discriminative. To give an quantitative reference of the "concentration" effect, we firstly compute the feature centers of different representative points, then classify all the feature points to the representative points they belong to, based on nearest search to the centers. The classification accuracies are 76.83%, 89.29% and 94.72% for F o , F * and F X , respectively. With the qualitative visualization and quantitative investigation, we conclude that though the "concentration" is far from reaching a point, the improvement is significant, and it explains the performance of PointCNN in feature learning.  Optimizer, model size, memory usage and timing. We implemented PointCNN in tensorflow <ref type="bibr" target="#b0">[1]</ref>, and use ADAM optimizer <ref type="bibr" target="#b20">[21]</ref> with an initial learning rate 0.01 for the training of our models. As shown in <ref type="table" target="#tab_10">Table 6</ref>, we summarize our running statistics based with the model for classification with batch size 16, 1024 input points on nVidia Tesla P100 GPU, in comparison with several other methods. PointCNN achieves 0.031/0.012 second per batch for training/inference on this setting. In addition, the model for segmentation with 2048 input points has 4.4M parameters runs on nVidia Tesla P100 with batch size 12 at 0.61/0.25 second per batch for training/inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We proposed PointCNN, which is a generalization of CNN into leveraging spatially-local correlation from data represented in point cloud. The core of PointCNN is the X -Conv operator that weights and permutes input points and features before they are processed by a typical convolution. While X -Conv is empirically demonstrated to be effective in practice, a rigorous understanding of it, especially when being composited into a deep neural network, is still an open problem for future work. It is also interesting to study how to combine PointCNN and image CNNs to jointly process paired point clouds and images, probably at the early stages. We open source our code at https://github.com/yangyanli/PointCNN to encourage further development.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PointCNN Supplementary Material 1 X -Conv Details</head><p>We implement MLP δ (·) in Line 2 of Algorithm 1 with two fully connected (FC) layers, each followed by ELU <ref type="bibr" target="#b6">[7]</ref> activation function and batch normalization (BN) <ref type="bibr" target="#b18">[19]</ref>, i.e., F C(3,</p><formula xml:id="formula_5">C δ ) → ELU → BN → F C(C δ , C δ ) → ELU → BN . We set C δ to C1/4.</formula><p>MLP (·) in Line 4 of Algorithm 1 can be implemented in a similar way: F C(3 * K, K * K) → ELU → BN → F C(K * K, K * K) → ELU → BN → F C(K * K, K * K) → BN → Reshape(K * K, K × K), where K * K denotes a vector in R K * K , and K × K denotes a K-by-K square matrix. However, this implementation results in O(K 4 ) parameters. To reduce the parameter number and computation, as well as the overfitting risk due to a large number of parameters, we propose to replace F C(K * K, K * K) with a depthwise convolution DC(R × C, C * F ), which applies F different filters to each of the C column of the input R × C matrix, yielding a C * F vector, where R * C * F parameters are involved. More specifically,</p><formula xml:id="formula_6">MLP (·) is implemented with F C(3 * K, K * K) → ELU → BN → Reshape(K * K, K × K) → DC(K × K, K * K) → ELU → BN → Reshape(K * K, K × K) → DC(K × K, K * K) → BN → Reshape(K * K, K × K), which results in O(K 3 ) parameters.</formula><p>Conv(·, ·) in Line 6 of Algorithm 1, if implemented with typical convolution, has K * (C1 + C δ ) * C2 trainable parameters. We implemented it with separable convolution <ref type="bibr" target="#b5">[6]</ref>, which has K * (C1 + C δ ) * DM + (C1 + C δ ) * DM * C2 trainable parameters, where DM is the depth multiplier, and we use DM = C2/(C1 + C δ ) in our implementation. Separable convolution reduces both parameter number and computation compared with that of a typical convolution.</p><p>M LPg(·) is used to harvest the global position information of the representative points in the last X -Conv layer. It is implemented similar to MLP δ (·), i.e., F C(3, Cg) → ELU → BN → F C(Cg, Cg) → ELU → BN . We set Cg to C2/4. The Cg dimensional output of M LPg(·) is concatenated with the C2 dimensional output of the last X -Conv layer for further processing.</p><p>In our implementation, a K nearest neighbor search is applied for extracting the K neighboring points. This assumes a more or less uniform distribution of input points. For point clouds with non-uniform distribution, a radius search can be applied first, and then K points can be randomly picked out of the radius search results.</p><p>In theory, the X -transformation can be applied on either the features or the kernels. We opt to apply it on the features, in which way the follow up operation is a standard convolution operation that is highly optimized by popular deep learning frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dataset Details</head><p>We conducted extensive evaluation of PointCNN on datasets of various types and scales. Here we introduce the details of the datasets, as well as how we pre-process and feed them into PointCNN:</p><p>• Object datasets: ModelNet40 <ref type="bibr" target="#b51">[52]</ref> and ShapeNet Parts <ref type="bibr" target="#b53">[54]</ref>.</p><p>-ModelNet40 is composed of 12311 3D mesh models from 40 categories, with a 9843/2468 training/testing split. Both the gravity and "facing" directions of the models are mostly aligned in the dataset.</p><p>In the "Pre-aligned" setting, the models are used for training and testing, without random horizontal rotations. In which way, the relative consistent "facing" direction is leveraged by the network. In the "Unaligned" setting, random horizontal rotations are explicitly applied on either the training or the testing models, not as a data augmentation, but to "forget" the relative consistent "facing" directions thus better approximate the scenarios in real world applications, where the "facing" direction of the objects are often unknown. We use the point cloud conversion of ModelNet40 provided by <ref type="bibr" target="#b32">[33]</ref> as our input, where 2048 points are sampled from each mesh, and we further sample N (1024, 128 2 ) points to train a model for testing with 1024 points on the classification task. -ShapeNet Parts contains 16880 models (14006/2874 training/testing split) from 16 shape categories, each annotated with 2 to 6 parts and there are 50 different parts in total. Each point sampled from the models is associated with a part label. The task is to predict the part label for each point, thus a segmentation task, and can be treated as a dense point-wise classification problem. The category label for each model is given, and can be used for trimming irrelevant predictions, same as that in <ref type="bibr" target="#b11">[12]</ref>. N (2048, 256 2 ) points are sampled from each point cloud to train a model for testing with 2048 input points on the segmentation task. Each testing point cloud is sampled multiple times to make sure all the points are evaluated at least r (r = 10 in our experiments) times at testing time.</p><p>• Indoor scene datasets: S3DIS <ref type="bibr" target="#b1">[2]</ref> and ScanNet <ref type="bibr" target="#b8">[9]</ref>. While ModelNet40 and ShapeNet models are mostly made by 3D modeling tools, S3DIS and ScanNet are from real scans of indoor environments. -S3DIS contains 3D scans from Matterport scanners in 6 areas including 271 rooms. Each point with RGB features in the scan is annotated with one of the semantic labels from 13 categories. The task is segmentation. The data is firstly split by room, and then the rooms are sliced into 1.5m by 1.5m blocks, with 0.3m padding on each side. The points in the padding areas serve as context of the internal points, and themselves are not linked to loss in the training phase, nor used for prediction in the testing phase. Each block is moved to a local coordinate system defined by its center. Random horizontal rotations are applied on the sliced blocks for data augmentation. The rotated blocks are handled in the same way as the object point clouds in ShapeNet Parts.</p><p>-ScanNet contains 1513 scanned and reconstructed indoor scenes, with 1201/312 scenes for training/testing in semantic voxel labeling of 17 categories. We firstly prepare data in the same way as that of S3DIS to train a segmentation model, and the segmentation results on testing data are then converted into semantic voxel labeling, as that in <ref type="bibr" target="#b34">[35]</ref>, for a fair comparison with previous methods. The 9305/2606 training/testing object instances from the 17 categories in ScanNet are also used for evaluating classification task. Note that ScanNet comes with RGB information for each point. However, they are not used in previous methods. To make fair comparisons, we do not use them either.</p><p>• 2D sketch datasets: TU-Berlin <ref type="bibr" target="#b10">[11]</ref> and Quick Draw <ref type="bibr" target="#b14">[15]</ref>. Similar to surfaces in 3D space, line sketches in 2D are inherently of less dimension than the ambient space, and can be represented as point cloud, thus we consider 2D sketches good arena for evaluating neural networks that are designed to consume point cloud data. TU-Berlin has sketches from 250 categories, with 80 sketches from each category, where 2/3 are used for training and the rest 1/3 for testing. Quick Draw is the largest available sketch dataset, with sketches from 345 categories, each with 70000/2500 training/testing samples. We sample N (512, 64 2 ) points from the sketch stokes to train a model for testing with 512 points on sketch classification task.</p><p>• Image datasets: MNIST and CIFAR10. MNIST and CIFAR10 are widely used for sanity check of image CNNs. Since PointCNN is a generalization of CNNs, we would like to evaluate PointCNN on the point cloud representation of MNIST and CIFAR10. For MNIST, we randomly sample 160 foreground pixels and convert them into point cloud representation, with the gray-scale pixel value as the input feature. For CIFAR10, we randomly sampled 512 pixels out of the 32 × 32 pixels for converting into point cloud with RGB features. Note that there is "shape" information in the MNIST point cloud, sine the point cloud follow the digits' structure, but this is not the case for the CIFAR10 point cloud, where the points are mostly the same blob for all the data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PointCNN Model Zoo</head><p>In <ref type="figure" target="#fig_6">Figure 1</ref>, we list the PointCNNs used for classification and segmentation tasks on multiple benchmark datasets.</p><p>PointCNNs are easy to implement, setup, and tune. Larger C are used for layers with more abstract/semantic information, such as the top layers in classification networks, and middle layers in "Conv-DeConv" segmentation networks. To relax the memory demand, smaller Ks are used at layers with large number of representative points, such as bottom layers of classification networks, and top and bottom layers of segmentation networks. Deeper PointCNN with larger receptive field in the last X -Conv layer are used for larger or harder datasets. The skip-links, together with the dilation parameter D, make it easy to fuse information from different scales (receptive fields), as illustrated in (d) and (e), which is essential for segmentation tasks.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Detailed Segmentation Results</head><p>We show detailed segmentation result comparisons on ShapeNet Parts in <ref type="table" target="#tab_1">Table 1</ref>, we can see our approach achieves the best overall performance and are best on 7 of the 16 categories.</p><p>We show detailed segmentation result comparisons on S3DIS in <ref type="table" target="#tab_3">Table 2</ref>, we can see our approach achieves the best overall performance and are best on 6 of the 13 categories. The detailed segmentation result comparisons on S3DIS Area 5 are summarized in <ref type="table" target="#tab_5">Table 3</ref>, as some of the literatures only report the performance on this area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Hierarchical convolution on regular grids (upper) and point clouds (lower).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The process for converting point coordinates to features. Neighboring points are transformed to the local coordinate systems of the representative points (a and b). The local coordinates of each point are then individually lifted and combined with the associated features (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>PointCNN architecture for classification (a and b) and segmentation (c), where N and C denote the output representative point number and feature dimensionality, K is the neighboring point number for each representative point, and D is the X -Conv dilation rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>F p ← Conv(K, F * ). Compared with PointCNN, the baseline has less trainable parameters, and is more "shallow" due to the removal of MLP (·) in line 4 of Algorithm 1. For a fair comparison, we further propose PointCNN w/o X -W/D, which is wider/deeper, and has approximately the same amount of parameters as PointCNN. The model depth of PointCNN w/o X (deeper) also compensates for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>T-SNE visualization of features without (a/F o ), before (b/F * ) and after (c/F X ) Xtransformation. the decrease in depth caused by the removal of MLP (·) from PointCNN. The comparison results are summarized in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 :</head><label>1</label><figDesc>PointCNN model zoo, where (a) is used for ModelNet40 (channel number in bold) and ScanNet classification, (b) is used for TU-Berlin sketch classification, (c) is used for Quick Draw sketch classification, (d) is used for ScanNet and S3DIS segmentation, and (e) is used for ShapeNet Parts segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparisons of mean per-class accuracy (mA) and overall accuracy (OA) (%) on ModelNet40</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Segmentation comparisons on ShapeNet Parts in part-averaged IoU (pIoU, %) and mean per- class pIoU (mpIoU, %), S3DIS in mean per-class IoU (mIoU, %) and ScanNet in per voxel overall accuracy (OA, %).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Sketch classification results.</figDesc><table><row><cell>Method</cell><cell cols="2">MNIST CIFAR10</cell></row><row><cell>LeNet [26]</cell><cell>99.20</cell><cell>84.07</cell></row><row><cell>Network in Network [29]</cell><cell>99.53</cell><cell>91.20</cell></row><row><cell>PointNet++ [33]</cell><cell>99.49</cell><cell>10.0 3</cell></row><row><cell>PointCNN</cell><cell>99.54</cell><cell>80.22</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Image classification results.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 .</head><label>5</label><figDesc>Clearly, PointCNN outperforms the proposed variants by a significant margin, and the gap between PointCNN and PointCNN w/o X is not due to model parameter number, or model depth. With these comparisons, we conclude that X -Conv is the key to the performance of PointCNN.</figDesc><table><row><cell></cell><cell>PointCNN</cell><cell>w/o X</cell><cell cols="2">w/o X -W w/o X -D</cell></row><row><cell>Core Layers</cell><cell cols="3">X -Conv×4 Conv×4 Conv×4</cell><cell>Conv×5</cell></row><row><cell># Parameter</cell><cell>0.6M</cell><cell>0.54M</cell><cell>0.63M</cell><cell>0.61M</cell></row><row><cell>Accuracy (%)</cell><cell>92.2</cell><cell>90.7</cell><cell>90.8</cell><cell>90.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Ablation tests on ModelNet40.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Parameter number, FLOPs and running time comparisons.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 1 :</head><label>1</label><figDesc>Segmentation result comparisons on ShapeNet Parts [54] in part-averaged IoU (pIoU, %) , mean per-class pIoU (mpIoU, %) and per-class pIoU (%).</figDesc><table><row><cell>Method</cell><cell cols="2">pIoU mpIoU</cell><cell>air</cell><cell>bag</cell><cell>cap</cell><cell>car</cell><cell>chair</cell><cell>ear</cell><cell cols="9">guitar knife lamp laptop motor mug pistol rocket skate table</cell></row><row><cell></cell><cell></cell><cell></cell><cell>plane</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>phone</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>bike</cell><cell></cell><cell></cell><cell>board</cell></row><row><cell cols="6">SyncSpecCNN [55] 84.74 82.0 Method OA mAcc mIoU ceiling floor</cell><cell>wall</cell><cell cols="5">beam column window door</cell><cell cols="2">table chair</cell><cell cols="4">sofa bookcase board clutter</cell></row><row><cell>PointNet [33]</cell><cell>78.5</cell><cell>66.2</cell><cell>47.6</cell><cell>88.0</cell><cell>88.7</cell><cell>69.3</cell><cell>42.4</cell><cell cols="2">23.1</cell><cell>47.5</cell><cell>51.6</cell><cell>54.1</cell><cell>42.0</cell><cell>9.6</cell><cell>38.2</cell><cell>29.4</cell><cell>35.2</cell></row><row><cell cols="2">SPGraph [24] 85.5</cell><cell>73.0</cell><cell>62.1</cell><cell>89.9</cell><cell>95.1</cell><cell>76.4</cell><cell>62.8</cell><cell cols="2">47.1</cell><cell>55.3</cell><cell>68.4</cell><cell>73.5</cell><cell>69.2</cell><cell>63.2</cell><cell>45.9</cell><cell>8.7</cell><cell>52.9</cell></row><row><cell>RSNet [18]</cell><cell>-</cell><cell cols="6">66.45 56.47 92.48 92.83 78.56 32.75</cell><cell cols="2">34.37</cell><cell>51.62</cell><cell cols="4">68.11 60.13 59.72 50.22</cell><cell>16.42</cell><cell cols="2">44.85 52.03</cell></row><row><cell>PointCNN</cell><cell cols="4">88.14 75.61 65.39 94.78</cell><cell cols="3">97.3 75.82 63.25</cell><cell cols="2">51.71</cell><cell>58.38</cell><cell cols="4">57.18 71.63 69.12 39.08</cell><cell>61.15</cell><cell cols="2">52.19 58.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 2 :</head><label>2</label><figDesc>Segmentation result comparisons on the S3DIS [2] dataset in overall accuracy (OA, %), micro-averaged accuracy (mAcc, %), micro-averaged IoU (mIoU, %) and per-class IoU (%).</figDesc><table><row><cell>Method</cell><cell>OA</cell><cell>mAcc mIoU ceiling floor</cell><cell cols="4">wall beam column window door</cell><cell>table chair</cell><cell>sofa bookcase board clutter</cell></row><row><cell>PointNet [33]</cell><cell>-</cell><cell cols="2">48.98 41.09 88.80 97.33 69.80 0.05</cell><cell>3.92</cell><cell>46.26</cell><cell cols="3">10.76 58.93 52.61 5.85</cell><cell>40.28</cell><cell>26.38 33.22</cell></row><row><cell>SPGraph [24]</cell><cell cols="3">86.38 66.50 58.04 89.35 96.87 78.12 0.00</cell><cell>42.81</cell><cell>48.93</cell><cell cols="3">61.58 84.66 75.41 69.84</cell><cell>52.60</cell><cell>2.10</cell><cell>52.22</cell></row><row><cell>SegCloud [45]</cell><cell>-</cell><cell cols="2">57.35 48.92 90.06 96.05 69.86 0.00</cell><cell>18.37</cell><cell>38.35</cell><cell cols="3">23.12 70.40 75.89 40.88</cell><cell>58.42</cell><cell>12.96 41.60</cell></row><row><cell>PCCN [48]</cell><cell>-</cell><cell cols="2">67.01 58.27 92.26 96.20 75.89 0.27</cell><cell>5.98</cell><cell>69.49</cell><cell cols="3">63.45 66.87 65.63 47.28</cell><cell>68.91</cell><cell>59.10 46.22</cell></row><row><cell>PointCNN</cell><cell cols="3">85.91 63.86 57.26 92.31 98.24 79.41 0.00</cell><cell>17.60</cell><cell>22.77</cell><cell cols="3">62.09 74.39 80.59 31.67</cell><cell>66.67</cell><cell>62.05 56.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 3 :</head><label>3</label><figDesc>Segmentation result comparisons on the S3DIS [2] Area 5 in overall accuracy (OA, %), micro-averaged accuracy (mAcc, %), micro-averaged IoU (mIoU, %) and per-class IoU (%).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Actually, this is a special instance of convolution -a convolution that is applied in one spatial location. For simplicity, we call it convolution as well.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">PointNet++ performs no better than random choice on CIFAR10. We suspect the reason is that, in PointNet++, the RGB features become in-discriminative after being processed by the max-pooling. Together with the lack of "shape" information, PointNet++ fails completely on this task.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Point convolutional neural networks by extension operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Atzmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno>71:1-71:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">3d point cloud classification and segmentation using 3d modified fisher vector representation for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhak</forename><surname>Ben-Shabat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lindenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anath</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08241</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02357</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeppermnet: Visual permutation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">Santa</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Scannet: Richly-annotated 3d reconstructions of indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Exploiting cyclic symmetry in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1889" to="1898" />
		</imprint>
	</monogr>
	<note>ICML&apos;16</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">How do humans sketch objects?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Alexa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ToG</publisher>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">and Laurens van der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10275</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<title level="m">Submanifold sparse convolutional networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabian</forename><surname>Groh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Wieschollek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hendrik</forename><forename type="middle">P A</forename><surname>Lensch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07289</idno>
		<title level="m">Flex-convolution (deep learning beyond grid-worlds)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A neural representation of sketch drawings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03477</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida D</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Point-wise convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Binh-Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Khoi</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai-Kit</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yeung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Recurrent slice networks for 3d segmentation on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Large-scale point cloud semantic segmentation with superpoint graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<idno>abs/1711.09869</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">So-net: Self-organizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gim Hee</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fpnn: Field probing neural networks for 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on surfaces via seamless toric covers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meirav</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miri</forename><surname>Aigerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Trope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Dym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
		<idno>71:1-71:10</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emanuele</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV, ICCV &apos;15</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="77" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Deep learning with sets and point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04500</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<editor>Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</title>
		<editor>David E. Rumelhart, James L. McClelland, and CORPORATE PDP Research Group</editor>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjia</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanlin</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. H-Cnn</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11385</idno>
		<title level="m">spatial hashing based CNN for 3d shape analysis</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Splatnet: Sparse lattice networks for point cloud processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tangent convolutions for dense prediction in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaesik</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian-Yi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Segcloud: Semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
		<idno>3DV</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Local spectral graph convolution for point set feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><surname>Samari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaleem</forename><surname>Siddiqi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05827</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">O-cnn: Octree-based convolutional neural networks for 3d shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<idno>72:1-72:11</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Deep parametric continuous convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chiu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Pokrovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">SGPN: similarity group proposal network for 3d point cloud instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiangui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Neumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<title level="m">Dynamic graph cnn for learning on point clouds</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<title level="m">Deep points consolidation. ToG</title>
		<imprint>
			<date type="published" when="2015-10" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingye</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11527</idno>
		<title level="m">Spidercnn: Deep learning on point sets with parameterized convolutional filters</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<idno>210:1-210:12</idno>
	</analytic>
	<monogr>
		<title level="j">ToG</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Syncspeccnn: Synchronized spectral cnn for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="6584" to="6592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Large-scale 3d shape reconstruction and segmentation from shapenet core55</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06104</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sketch-a-net: A deep neural network that beats humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="425" />
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><forename type="middle">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
		<idno>2017. 81.55 81.74 81.94 75.16 90.24 74.88 92.97 86.1 84.65 95.61 66.66 92.73 81.61 60.61 82.86 82.13</idno>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<biblScope unit="page" from="3394" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pd-Network</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>22</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
