<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">EV-Action: Electromyography-Vision Multi-Modal Action Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Robinson</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taotao</forename><surname>Jing</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Khoury College of Computer Science</orgName>
								<orgName type="institution" key="instit2">Northeastern University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">EV-Action: Electromyography-Vision Multi-Modal Action Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-modal human action analysis is a critical and attractive research topic. However, the majority of the existing datasets only provide visual modalities (i.e., RGB, depth and skeleton). To make up this, we introduce a new, largescale EV-Action dataset in this work, which consists of RGB, depth, electromyography (EMG), and two skeleton modalities. Compared with the conventional datasets, EV-Action dataset has two major improvements: (1) we deploy a motion capturing system to obtain high quality skeleton modality, which provides more comprehensive motion information including skeleton, trajectory, acceleration with higher accuracy, sampling frequency, and more skeleton markers. (2) we introduce an EMG modality which is usually used as an effective indicator in the biomechanics area, also it has yet to be well explored in motion related research. To the best of our knowledge, this is the first action dataset with EMG modality. The details of EV-Action dataset are clarified, meanwhile, a simple yet effective framework for EMG-based action recognition is proposed. Moreover, state-of-the-art baselines are applied to evaluate the effectiveness of all the modalities. The obtained result clearly shows the validity of EMG modality in human action analysis tasks. We hope this dataset can make significant contributions to human motion analysis, computer vision, machine learning, biomechanics, and other interdisciplinary fields.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>There are a wide range of applications for human motion analysis (e.g., event detection, behavior prediction, gait analysis, joint mechanics, prosthetic designs, sports medicines <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b50">[51]</ref>). The availability of datasets tends to directly impact the progress of research. From the start, action datasets only consisted RGB modality <ref type="bibr" target="#b35">[36]</ref>. Later on, as 3D sensors became more accessible, several datasets included the depth modality <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b22">[23]</ref>. This paved a way for researchers to propose more effective approaches in terms of multi-modal methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b49">[50]</ref>, <ref type="bibr" target="#b40">[41]</ref>. After that, skeleton data was introduced by some works <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b54">[55]</ref>. However, most skeleton information of these datasets was directly obtained from Kinect sensors <ref type="bibr" target="#b29">[30]</ref>, resulting in low localization accuracy. Skeleton modal captured by more accurate devices was released <ref type="bibr" target="#b8">[9]</ref>, while RGB-D modals were not included.</p><p>We introduce EV-Action dataset, which includes all visual modalities mentioned above (i.e., RGB, depth, and two skeleton modalities). An optical tracking-based Vicon system <ref type="bibr" target="#b30">[31]</ref> is deployed to capture high-quality skeleton motion information. Compared with Kinect, Vicon achieves significantly higher sampling rate (100 vs. 30 fps), higher</p><p>We thank our volunteers, Allyson Vakhovskaya, Daniel J. Peluso, Emily Freed, Kasey Lee, Yue Bai, and Yunyu Liu from Northeastern University for their substantial contributions to our project in data collection, labeling, and analytical procedures. localization accuracy, and more skeleton markers <ref type="bibr">(39 vs. 26)</ref>. It provides more comprehensive skeleton motion information in terms of location, trajectory, velocity, and acceleration. We further collected Electromyography (EMG) signals to measure the electrical activity of human skeletal muscles as a function of the intensity of force <ref type="bibr" target="#b9">[10]</ref>. EMG is regularly used in medical and biomechanics fields. It has not yet been well explored in the fields of human motion analysis. In EV-Action, all modalities are captured simultaneously with action labels frame by frame. The goal of EV-Action is exploring the latent correlation across different modalities and improving the performances of action analytic tasks. EV-Action could contribute significantly to the research fields of human motion analysis, multimedia, computer vision, machine learning, biomechanics, and other interdisciplinary sub-fields. The contributions of our paper are shown below: 1) We designed and constructed a data collection center with optical tracking system and Kinect-V2 systems. This allowed us to capture the four visual modalities (i.e., RGB, Depth, Skeleton-K, and Skeleton-V). 2) EMG signal from skeletal muscles is extracted. This is the first action dataset including EMG, which provides complimentary information and reveals valuable correlations between visual and non-visual modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3) A simple yet effective EMG recognition framework</head><p>is proposed which achieves highest performance and reveals unique characteristics of EMG in human actions. 4) We defined experimental settings and provided the state-of-the-art benchmarks for each modality. EMG is merged with other modalities which further demonstrates the complementary of the EMG modal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORKS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. RGB/D and Skeleton Datasets</head><p>Small-scale datasets included tens of action classes (e.g., Weizmann <ref type="bibr" target="#b16">[17]</ref>) are initially deployed for action analytic tasks <ref type="bibr" target="#b57">[58]</ref>. Upon the arrival of deep learning, large-scale RGB datasets were introduced (e.g., UCF101 <ref type="bibr" target="#b39">[40]</ref> and Kinetics <ref type="bibr" target="#b21">[22]</ref>). Later on, RGB-D datasets were released (e.g., MSR-Action3D <ref type="bibr" target="#b23">[24]</ref>, RGBD-HuDaAct <ref type="bibr" target="#b26">[27]</ref>). Due to the space and budgeting constraints, most RGB-D datasets were collected using low-cost Kinect sensors <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b37">[38]</ref>. In addition, Kinect sensors can extract skeleton data, as introduced in MAD <ref type="bibr" target="#b19">[20]</ref>, UCF-Kinect <ref type="bibr" target="#b13">[14]</ref> and NTU-RGBD <ref type="bibr" target="#b36">[37]</ref>. However, the accuracy and stability of Kinect are low, which limits the potential research of action analysis. <ref type="figure">Fig. 1</ref>: Visualization of sample frames in EV-Action dataset. Colored boxes show the correlations between visual modalities and EMG (i.e., Take Off and Touch Down). We can clearly observe that EMG responds early and last longer than visual modalities which provides unique view for action analysis. All modalities were well aligned and labeled. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EMG</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Right Arm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left Leg</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Right Leg</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skeleton-V</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left Arm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skeleton-K</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left Arm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Multi-Modal Action Datasets</head><p>We consider the dataset containing more than RGB-D modalities as multi-modal dataset. Currently, only a few datasets provide additional modalities. NTU-RGBD <ref type="bibr" target="#b36">[37]</ref> and PKU-MMD <ref type="bibr" target="#b7">[8]</ref> contained infrared frames captured by Kinect sensors. CMU-MMAC <ref type="bibr" target="#b8">[9]</ref> utilized an optical tracking technique to capture action sequences. UTD-MHAD <ref type="bibr" target="#b4">[5]</ref> utilized a single wearable inertial sensor to capture inertial signals. However, the modality is severely limited and sporadic due to the inconsistent collection manner. Our EV-Action dataset utilizes 39 markers to capture precise location, trajectory and acceleration information at a high frame rate (100 fps). To this end, EV-Action is the most accurate and comprehensive dataset of this kind.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. EMG Signal</head><p>Electromyography (EMG) is an electrodiagnostic technique to evaluate the electrical activity produced by skeletal muscles <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b9">[10]</ref>. Typically, EMG is used in neural science, biomechanics, and signal processing fields (e.g., hand gesture <ref type="bibr" target="#b5">[6]</ref>, robot arm control <ref type="bibr" target="#b14">[15]</ref>, face expression <ref type="bibr" target="#b33">[34]</ref>. Since EMG activates before visual signal which could foresee potential information such as intention, force, and even mental activities information that cannot be recognized in visual domain. To this end, we consider EMG as another crucial clue for exploring actions. To the best of our knowledge, no existing work associates EMG with other action modalities. Considering the potential applications that could be had, we generated EV-Action with EMG as one of the critical modalities for human action analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EV-ACTION DATASET</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Sensors and Setup</head><p>There are 1 Kinect-V2 sensor <ref type="bibr" target="#b0">[1]</ref>, 4 wireless EMG sensors, and 8 Vicon-T40s cameras in the data collection system.</p><p>Kinect <ref type="bibr" target="#b0">[1]</ref> captures RGB-D modalities from subjects. Skeleton information is further extracted from the depth image. We used a second generation Kinect <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b29">[30]</ref> (Kinect-V2) which has a high resolution camera (1,920×1,080) at 30 fps with a wide field of view (70 • ×60 • ). Moreover, the resolution of the depth sensor is 512×424. It is more robust and efficient for pose estimation with reference to 26 joints <ref type="figure">(Figure 3(a)</ref>). In the collection procedure, a Kinect-V2 captures the subjects in the front view ( <ref type="figure">Figure 3</ref> Vicon System utilizes optical tracking-based technology to capture skeleton data with more accurate and comprehensive motion information <ref type="bibr" target="#b30">[31]</ref>. We deploy 8 Vicon-T40s infrared cameras to capture the stickup marks on each subject <ref type="figure">(Figure 3(a)</ref>). The cameras sample data points as 10-bit grayscale frames at 100 fps and with a resolution of 2336×1728. Then, the frames were calibrated and labeled to obtain skeleton information. We follow the standard scheme <ref type="bibr" target="#b42">[43]</ref> by placing 39 markers around human body <ref type="figure">(Figure 3</ref>(a)). It captures precise and comprehensive motion information, such as the second bounce in the Fall Down action class. Also, due to the high frame rate and accuracy, high quality trajectories and accelerations were obtainable in reference to ground coordinates. <ref type="figure">Figure 2</ref> shows the Kick action viewed across time and at different angles, with the blue curve indicating the trajectory of the toe marker. No other action datasets provides such detailed information. EMG Sensor captures EMG signals from human muscles. We deploy wireless EMG sensors which captures 16-bit EMG signal at 1000 Hz. This enables the sensors to cover the whole frequency spectrum of skeletal EMG (i.e., 20-450 Hz) signal. We attached 4 sensors to each subject: the middle of each forearm and the shank muscles ( <ref type="figure">Figure 3</ref>(a)). There are 3 reasons: 1), the common actions usually utilize arms and legs; 2), the location of each muscle (mid-line of the muscle in the belly that is between the myotendinous junction and the nearest innervation zone) gives off a signal of highest amplitude, which makes the signal most responsive to the corresponding action <ref type="bibr" target="#b9">[10]</ref>; 3), the crosstalk noise generated by neighboring muscles has the potential to get misinterpreted for originating from a muscle of interest, and placing the sensor mid-line makes it less susceptible to this noise. Data Collection Center consists of 8 the Vicon cameras placed around the parameter of a 4.6m × 4.6m room which has a detectable area of 3m × 3m. All traceable markers fell in the Vicon cameras field of view. There was a single Kinect sensor centered facing front, and each action was performed with the face of this Kinect sensor as the front. 4 EMG sensors were connected to each subject ( <ref type="figure">Figure 3</ref>(b)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset Description</head><p>Completeness, comprehensiveness, and diversity were highly considered when building EV-Action. To make it practical and generalizable, we included 20 common actions <ref type="table" target="#tab_0">(Table III)</ref>, 10 were done by a single subject and the other 10 were done by that same subject interacting with different objects. The dataset includes 70 subjects performing the actions 5 times (i.e., 100 action clips per subject). To introduce diversities, the subjects intentionally perform slightly different style in each loop. All-in-all, resulting in 7000 action clips at multiple views. <ref type="table" target="#tab_0">Table II</ref> summarizes these statistics compared with recent and popular multi-modal action datasets. It is clear that EV-Action is one of the largest multimodal datasets, as it significantly surpasses other datasets in terms of modal diversity, number of subjects, and number of samples. And it includes non-visual EMG signal for the first   <ref type="figure">figure 1</ref>, we notice that the EMG signal activates prior to the actions (i.e., Take Off and Touch Down).</p><p>We also notice that the duration of EMG are typically longer than the visual modalities. These patterns are unrecognizable from any visual modal. It demonstrates that EMG does provide unique and complementary information for more deep and sophisticated action analytical research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Data Labeling</head><p>There are two steps for data labeling: 1) annotating the actions in RGB modality, and 2) aligning the Skeleton-K and Skeleton-V. Since other modalities are captured synchronously with either Kinect or Vicon, thus, the rest of the modalities are also well aligned automatically. Video Anotation: We built a MATLAB labeling tool to facilitate the labeling process <ref type="figure" target="#fig_1">(Figure 4(a)</ref>). The tool displays a video sample for a human labeler to tag start and end frames of actions selected from the predefined lists shown in <ref type="table" target="#tab_0">Table III</ref>. Data Alignment: We then align the clip with the clip captured by the Vicon system. We develop another MATLAB tool that allows us to visually align single frame of the RGB and the skeleton from the Kinect and the Vicon systems, respectively <ref type="figure" target="#fig_1">(Figure 4(b)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. DATA ANALYSIS</head><p>Histograms of all video length are shown in <ref type="figure">Figure 5</ref>, and box plots ( <ref type="figure">Figure 6</ref>) depict action-specific statistics from longest to shortest. We observe that the video lengths for different actions varied. For instance, Read Book tends to be the longest, with Stand Up the shortest. Moreover, variation in video length exists for the same action across different subjects, which is especially true for repetitive actions. For instance, when preforming actions such as Boxing or Jump, subjects prefer to choose the exact number of reps. For nonrepetitive actions there tended to be less variation across different subjects. Since subjects perform different actions continuously without intentional pause during collection; another observation is that subjects tended to move faster between actions. This sometimes results in the end of the current action and the beginning of the next getting mixed across frames in between (i.e., overlap between actions). For example, when a subject Put Down Phone and then Check Watch immediately, there might be an overlap. These situations make more of challenges while make it better suited for more practical research. Root Mean Squared (RMS) is an effective method to preprocess EMG data <ref type="bibr" target="#b9">[10]</ref>. We obtain the average of action RMS and surprisingly notice that the shank muscles have significantly higher (2 times) amplitude than forearm muscles since the stronger and bigger muscles around the shank region. We separately illustrated these four channels in <ref type="figure" target="#fig_3">Figure 7</ref> and found more interesting observations. For instances, most subjects utilize right hand for Throwing Ball, while they are also utilizing their right legs simultaneously (might for balance requirement). Moreover, subjects use left legs even for Check Watch (might for body balance; in order to rise left arm to check watch, they should hold/balance their body by left leg to take over left arm). These observations are unique and valuable for action understanding but cannot be obtained in any visual modality. We wish more interesting discoveries could be revealed by exploring EMG modal.</p><p>Since Vicon tracks the markers pasted on subjects, there were situations that several markers were obstructed to the cameras, such as Fall Down and Sit Down. Once the occluded marker is again detected, Vicon could re-localized the re- spective point. In response for the missing situation, we split the data as two types, unlabeled marker locations data and labeled skeleton data. Thus, advanced skeleton reconstruction methods or label independent research can also explore EV-Action. This also leads to believe that more sophisticated algorithms are needed to achieve higher performance rating (e.g., missing-modality and multi-view algorithms). The rest modalities (i.e., RGB, Depth, Skeleton-K and EMG) are stable across all actions without noticeable errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>State-of-the-art approaches were used to benchmark the different modalities. Specifically, single-modal benchmarks using RGB, Skeleton-K, Skeleton-V were done. In the multimodal scenario, RGB-D, Skeleton-K + EMG, and Skeleton-V + EMG were conducted. We achieved considerable performance improvements by employing a simple, yet effective fusion technique (i.e., fused at the feature-level). This project is the first to model the non-visual EMG signal for action recognition. This is also a great promise for further improvement by providing more sophisticated learning frameworks and fusion techniques. Considering the information captured in an EMG signal, it is capable of discriminating between action types in itself, thus, it is complimentary to visual evidence. Thus, the EMG modality could both improve our current action recognition capabilities and serve as a necessity for certain applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental Settings</head><p>Benchmarks on EV-Action followed conventional classification settings. The action clips from 56 subjects were used during training (i.e., 5600 clips), while the other 14 subjects were set aside for testing (i.e., 1400 clips). All experiments were evaluated in terms of classification accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EMG Signal</head><p>Signal processing methods associated with hand-crafted features are usually deployed for EMG analysis. We design a novel deep-structure framework for EMG recognition. We first introduce the conventional procedures for EMG classification. As noise of raw EMG signals occurred during collecting period, choosing the best way to extract features and reduce the dimension are crucial to achieve high classification performance. Butterworth filter <ref type="bibr" target="#b3">[4]</ref> yields a flat frequency response which is effective to filter out EMG noise. The generalized equation is:</p><formula xml:id="formula_0">H jω = ( 1 + ε 2 ( ω ω p ) 2n ) −1 ,</formula><p>where n is the filter order and we received the best classification performance when n = 5. ω is the radian frequency and ω = 2π f . ω p is the pass band frequency and ε is the maximum pass band. H is the frequency response. The low frequency cutoff value is set to 10 Hz, which removes the static electricity variation caused by friction and movement. We set high frequency cutoff value at 500 Hz, since it is the highest frequency of the EMG signal. RMS is further deployed. The expression of RMS is</p><formula xml:id="formula_1">R k = ( 1 N ∑ N i=1 x 2 i ) 1 2</formula><p>, where R k is the RMS value and x i is the EMG signal of the i-th frame in the k-th time window period. N is the size of sliding window. We obtain the RMS values from each channel to obtain RMS features. Linear Discriminative Analysis (LDA) <ref type="bibr" target="#b25">[26]</ref> and Principal Component Analysis (PCA) <ref type="bibr" target="#b20">[21]</ref> are utilized for dimension reduction. Three classifiers are tested including SVM <ref type="bibr" target="#b34">[35]</ref>, K-Nearest Neighbors (KNN) <ref type="bibr" target="#b12">[13]</ref>, and Random Forests (RF) <ref type="bibr" target="#b24">[25]</ref>. The results <ref type="table" target="#tab_0">(Table IV)</ref> indicates that RF after PCA has the best classification performance which is 35.12%.</p><p>We then introduce our deep modal-based EMG action recognition approach <ref type="figure" target="#fig_4">(Figure 8</ref>). It is an effective and efficient framework in an end-to-end scenario, while both the noise elimination and the recognition are done simultaneously. Sliding windows are first employed to extract EMG signal from each channel. Differently, we utilize Fast Fourier Transform (FFT) <ref type="bibr" target="#b2">[3]</ref> instead of RMS as the initial approach. This strategy has two advantages. Firstly, FFT decomposes the time series EMG data in frequency domain which automatically separates EMG with high/low noise. Thus, denoising procedures can be omitted. Secondly, FFT preserves more comprehensive information. Then, we utilize the amplitude of each frequency as feature vector, and concatenate the four channels (i.e., left/right forearm/shank) together and input them into a Long Short-Term Memory (LSTM) <ref type="bibr" target="#b18">[19]</ref> networks. LSTM outputs the feature representations. A classifier, C(·), is utilized to obtain the final label. The loss function is shown as below.</p><formula xml:id="formula_2">L = Y −C(G(F(X)) 2 F ,<label>(1)</label></formula><p>where Y is the instance label, X is the original data in time domain. F(·) is the FFT feature extractor. G(·) is an LSTM network. In the implementation, we deploy halfoverlapping sliding window associated with the window size 200 to extract data blocks. By deploying FFT to the extract block, we obtain a 100-dimension feature vector in frequency domain. Since there are four EMG channels, each data block is represented by a 400-dimension vector. The LSTM structure has a hidden layer of 1024-dimension. The result <ref type="table" target="#tab_0">(Table IV)</ref> denotes that the our approach significantly outperforms conventional methods which also indicates the effectiveness of EMG in action analytical tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. RGB &amp; Depth</head><p>We evaluated single-view action recognition baselines on EV-Action. Details are introduced below: Action Vector of Local Aggregated Descriptor (Action-VLAD) <ref type="bibr" target="#b15">[16]</ref> is an effective video descriptor. A trainable aggregation framework is designed to capture spatio-temporal features. We fine-tune the last layer before SoftMax pretrained by ImageNet <ref type="bibr" target="#b10">[11]</ref> our evaluation. Temporal Segment Networks (TSN) <ref type="bibr" target="#b51">[52]</ref> sparsely samples the videos to capture the temporal information in supervised scenario. In this way, the entire video was learned effectively. Long-term Recurrent Convolutional Networks (LRCN) <ref type="bibr" target="#b11">[12]</ref> deploys a hierarchical visual representation learning associated with a temporal dynamic recognition module. LRCN is capable of end-to-end training. Weighted Depth Motion Maps (WDMM) <ref type="bibr" target="#b1">[2]</ref> recognizes actions from depth videos. It utilizes a video summarization step for hierarchical representation learning. WDMM effectively increases inter-class dissimilarities and intra-class similarities. 110 is set as the number of PCA components and 80 is set as the visual words for extracting depth feature. Weighted Hierarchical Depth Motion Maps (WHDMM) <ref type="bibr" target="#b52">[53]</ref> utilizes a convolutional neural network to extract three-channel features. A hierarchical depth motion extractor is further deployed for action recognition. We only use the front view to train and test. The remaining parameters followed the original work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Skeleton-Kinect</head><p>We introduce the action recognition baselines based on skeleton modal in this section. Temporal Convolutional Networks (TCN) <ref type="bibr" target="#b38">[39]</ref> learns an interpretable spatio-temporal representation. To train the model on the Kinect Skeleton modality, we modify the data as the same format of NTU-RGB-D <ref type="bibr" target="#b36">[37]</ref>. Two Stream Recurrent Neural Network (TSRNN) <ref type="bibr" target="#b43">[44]</ref> provides an RNN framework with two-stream structure to explore spatial configurations and temporal dynamics for action classification. We process the data in the same way as TCN. The batch size is set to 256, the maximum iteration number is set to 2, 000, and the learning rate is set to 0.02. Spatial Temporal Graph Convolution Network (STGCN) <ref type="bibr" target="#b55">[56]</ref> learns both the spatial and temporal patterns from data simultaneously. It overcomes the limited expressive power and difficulties of generalization. The data is processed in the same way as TCN. We train the model with 80 epochs, using SGD as the optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Skeleton-Vicon</head><p>Vicon system captures skeleton data with higher localization quality. In our evaluation, we deploy the same baselines as (Skeleton-K) while modify the data format to satisfy the requirements of each baselines. For TCN <ref type="bibr" target="#b38">[39]</ref> approach, we change the spatial connection graph from 25 joints to 39 joints. Vicon data contains higher frame rate, and we also increase frames for other models. The remaining parameters are kept the same. The dimension of the feature is increased to 273. TSRNN <ref type="bibr" target="#b43">[44]</ref> needs the part of the body (i.e., one trunk, two legs, and two arms, as well as whole body). We use the index groups of different body parts via the 39 joints of Vicon and keep other parameters be consistent. STGCN <ref type="bibr" target="#b55">[56]</ref> needs a joint adjacency graph. Thus, we generate the connection graph for 39-joint while other parameters are consistent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Skeleton-(Kinect/Vicon) &amp; EMG</head><p>To prove the effectiveness and complementariness of EMG modal, we combine EMG with skeleton modalities together in low-level domain. TCN-RMS. We first obtain the EMG features. The time window has the same size as the sampling time between two frames of the Skeleton-K. We then concatenate the RMS directly with the skeleton data and input the combination data to TCN <ref type="bibr" target="#b38">[39]</ref> for classification. The hyper parameters we used are the same as the parameters aforementioned. And we find out that such combination features have improved the performance on the action recognition task. TCN-FFT. FFTbased feature merging strategy is also evaluated since EMG is a temporal signal which can be explored in frequency space. Similar as TCN-RMS, we set a time window to extract the frequency distribution feature of the signal in each channel, and forward to classifiers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Results and Analysis</head><p>We used top-1 accuracy to evaluate each baseline <ref type="table" target="#tab_5">(Table  V)</ref>. The left column shows the modals utilized for evaluation. SK-K and SK-V indicate the single-view skeleton data Skeleton-K and Skeleton-V respectively. SK-K-E and SK-V-E denote the multi-modal setting where EMG modal is combined with Skeleton-K and Skeleton-V respectively. For RGB modality, TSN outperformed other benchmarks with ). Thus, we conclude that the actions with slightly movements (i.e., Check Watch, Answer Phone) have been improved with EMG features. Since the EMG signal can react to the action without obvious motion, while the visual feature is indistinctive. As a consequence, the result is reasonable. The reason why EMG with FFT does not have significant improvement may be that the FFT features are complicated and significantly different compared with skeleton motion structure. If these two modalities are simply and directly concatenated together, the extra dynamic information of FFT features could not be fully utilized. The EMG with LSTM-FFT also proves EMG is useful. The accuracies of several actions are extremely high (i.e., Jump). However, similar actions(i.e., Wave Hand and Raise Hand; Stand Up and Sit down), which can be easily classified by other modality, are sometimes hard for EMG. Therefore, a good fusion technique may help us get better results. We conjecture there are two reasons for the relatively low baseline performances of Vicon data. (1) Some missing points make the data more challenging.</p><p>(2) The generation strategy of spatial graph for Vicon data is different from the Kinect skeleton model default setting. Comparing our skeleton dataset with NTU-RGBD <ref type="bibr" target="#b36">[37]</ref> dataset, which included the same skeleton baselines as ours, we consider the differences in scores are justifiable in two-fold. (1) Our dataset contains less but more challenging action clips. <ref type="formula">(2)</ref> We only utilized 3D reference frame from the skeleton modality, while <ref type="bibr" target="#b36">[37]</ref> fully utilized more motion information such as orientation. Regardless, the evaluation of our dataset can be further boosted with the added non-visual modality, the EMG signal. We believe more advanced feature extraction methods and multi-modality fusion strategies could further improve the learning performance. To this end, a lot of open questions and challenges are left for future exploration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We have introduced a new multi-modal human action dataset in this paper which is called EV-Action dataset. The proposed dataset consists of RGB, depth, skeleton, and EMG data. All modalities have been labeled and aligned across 7, 000 samples collected from 70 human subjects. In general, EV-Action has two major advantages over the other action-based video collections. (1) we have utilized an optical tracking based Vicon system to capture more accurate and comprehensive skeletal data; (2) we have introduced a nonvisual EMG modality associated with other visual modalities. We also have provided several state-of-the-art benchmarks for each modality to prove the effectiveness. Moreover, we have designed effective and simple approach for EMG-based action recognition task and achieved highest performance. Further, the experiments have demonstrated that the effective and complimentary information is extractable from EMG for human action analytical tasks. Overall, EV-Action can serve in widespread research and applications concerning human motion understanding. We hope EV-Action can have a significant impact on motion understanding, computer vision, biomechanics, and other interdisciplinary areas.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>(a) Video labeling tool used to label action sequences, and (b) tool used to precisely align modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>Histogram of the length distribution of all videos. Video length distribution of each action.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FFig. 7 :</head><label>7</label><figDesc>The average of Root Mean Square (RMS) value of the EMG recordings in different actions. We separate the value of upper body (left and right forearm) and lower body (left and right shank) for better discussion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 :</head><label>8</label><figDesc>EMG modal based action recognition framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc></figDesc><table><row><cell>Sensors</cell><cell>Modality</cell><cell>Resolution</cell><cell cols="3">Frame Rate (fps/Hz) Skeleton Joints Field of View</cell><cell>Sensor Number</cell><cell>Range</cell><cell>Sensitivity</cell></row><row><cell></cell><cell>RGB</cell><cell>1, 920 × 1, 080</cell><cell>30</cell><cell>-</cell><cell>84.1 • × 53.8 •</cell><cell>1</cell><cell>-</cell><cell>8-bit</cell></row><row><cell>Kinect-V2</cell><cell>Depth</cell><cell>512 × 424</cell><cell>30</cell><cell>-</cell><cell>70.6 • × 60.0 •</cell><cell>1</cell><cell>0.5-4.5 m</cell><cell>16-bit</cell></row><row><cell></cell><cell>Skeleton-K</cell><cell>-</cell><cell>30</cell><cell>26</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Vicon-T40s</cell><cell cols="2">Skeleton-V 2, 336 × 1, 728</cell><cell>100</cell><cell>39</cell><cell>98.1 • × 50.1 •</cell><cell>8</cell><cell>12 m</cell><cell>10-bit</cell></row><row><cell>Delsys-Trigno</cell><cell>EMG</cell><cell>-</cell><cell>1, 000</cell><cell>-</cell><cell>-</cell><cell>4</cell><cell>± 22 mV</cell><cell>16-bit</cell></row></table><note>Technical specifications of the sensors used in EV-Action dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Vicon SK Angle Time(s)</head><label></label><figDesc></figDesc><table><row><cell>1</cell><cell>14</cell><cell>25</cell><cell>39</cell><cell>52</cell><cell>64</cell><cell>77</cell><cell>90</cell><cell>102</cell></row><row><cell>0°45°90°135°180°225°270°315°360°0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>.00</cell><cell>0.13</cell><cell>0.25</cell><cell>0.40</cell><cell>0.53</cell><cell>0.65</cell><cell>0.78</cell><cell>0.91</cell><cell>1.04</cell></row><row><cell cols="9">Fig. 2: Visualization of a subject performing a kicking action across view angles and time. The blue curve highlights the</cell></row><row><cell cols="9">trajectory of a marker. Clearly, EV-Action contains the precise detailed motion information of the actions. Frame numbers</cell></row><row><cell cols="6">are shown in left bottom which indicate the high sampling rate of Vicon system.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">EMG Sensors</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Kinect Skeleton</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>&amp; Joints</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Vicon Markers</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Front</cell><cell>Back</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vicon</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cameras</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kinect Sensor</cell><cell></cell><cell cols="2">EMG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Detectable Area</cell><cell></cell><cell cols="2">Sensor</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Calibrated Ground</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(b)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Fig. 3: (a) Sensor placement schemes. Orange lines and spots</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">indicate Kinect skeleton with 26 joints. Small gray points</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">denote Vicon markers. And blue blocks indicate the EMG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">sensors. (b) Data collection center environment setup.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>Comparison between EV-Action dataset and other popular multi-modal datasets. EV-Action is one of the largest multi-modal datasets and significantly outperforms other datasets in modal diversity, subject numbers, and sample clips.</figDesc><table><row><cell>Datasets</cell><cell cols="4">Samples Classes Subjects Framerate (fps)</cell><cell>Sensors</cell><cell>Modalities</cell></row><row><cell>RGBD-HUDA [27]</cell><cell>1189</cell><cell>13</cell><cell>30</cell><cell>30</cell><cell>KinectV1</cell><cell>RGB+D</cell></row><row><cell>MSR-Action3D [24]</cell><cell>567</cell><cell>20</cell><cell>10</cell><cell>30</cell><cell>RGB-Cam</cell><cell>D+SK</cell></row><row><cell>CAD-60 [41]</cell><cell>60</cell><cell>12</cell><cell>4</cell><cell>30</cell><cell>KinectV1</cell><cell>RGB+D+SK</cell></row><row><cell>Action4 2 [7]</cell><cell>6844</cell><cell>14</cell><cell>24</cell><cell>30</cell><cell>KinectV1</cell><cell>RGB+D</cell></row><row><cell>CAD-120 [23]</cell><cell>120</cell><cell>20</cell><cell>4</cell><cell>30</cell><cell>KinectV1</cell><cell>RGB+D+SK</cell></row><row><cell>Multiview 3D Event [54]</cell><cell>3815</cell><cell>8</cell><cell>8</cell><cell>30</cell><cell>KinectV1</cell><cell>RGB+D+SK</cell></row><row><cell>Online RGB+D Action [57]</cell><cell>336</cell><cell>7</cell><cell>24</cell><cell>30</cell><cell>KinectV1</cell><cell>RGB+D+SK</cell></row><row><cell>Northwestern-UCLA [45]</cell><cell>1475</cell><cell>10</cell><cell>10</cell><cell>30</cell><cell>KinectV1</cell><cell>RGB+D+SK</cell></row><row><cell>UWA3D Multiview [32]</cell><cell>900</cell><cell>30</cell><cell>10</cell><cell>30</cell><cell>KinectV1</cell><cell>RGB+D+SK</cell></row><row><cell>Office Activity [46]</cell><cell>1180</cell><cell>20</cell><cell>10</cell><cell>30</cell><cell>KinectV1</cell><cell>RGB+D</cell></row><row><cell>UTD-MHAD [5]</cell><cell>861</cell><cell>27</cell><cell>8</cell><cell>30+50</cell><cell>KinectV1+WIS</cell><cell>RGB+D+SK</cell></row><row><cell>3D Action Pairs [29]</cell><cell>360</cell><cell>12</cell><cell>10</cell><cell>30</cell><cell>KinectV1</cell><cell>RGB+D+SK</cell></row><row><cell>UWA3D Multiview II [28]</cell><cell>1075</cell><cell>30</cell><cell>10</cell><cell>30</cell><cell>KinectV1</cell><cell>RGB+D+SK</cell></row><row><cell>EV-Action (Ours)</cell><cell>7000</cell><cell>20</cell><cell>70</cell><cell cols="3">30+100+1000 KinectV2+Vicon+EMG RGB+D+SKK+SKV+EMG</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>A list of the 20 actions included in EV-Action.</figDesc><table><row><cell cols="2">Single Person Actions</cell><cell cols="2">Person-Objects Actions</cell></row><row><cell>1. Walk</cell><cell>6. Bend Over</cell><cell cols="2">1. Answer Phone 6. Throw Ball</cell></row><row><cell>2. Boxing</cell><cell cols="2">7. Turn Around 2. Check Watch</cell><cell>7. Drink Water</cell></row><row><cell cols="2">3. Wave Hands 8. Kick</cell><cell>3. Stand Up</cell><cell>8. Tie Shoes</cell></row><row><cell cols="2">4. Clap Hands 9. Raise Hand</cell><cell>4. Sit Down</cell><cell>9. Read Book</cell></row><row><cell>5. Jump</cell><cell cols="2">10. Fall Down 5. Grab Bag</cell><cell>10. Move Table</cell></row><row><cell cols="2">time. Referencing</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>EMG classification accuracy based on different dimension reduction (Dim-Red) approaches and classifiers.</figDesc><table><row><cell>Methods</cell><cell cols="3">Dimension Reduction (None) LDA PCA</cell></row><row><cell>Random Forest</cell><cell cols="3">33.72 16.81 35.12</cell></row><row><cell>KNN</cell><cell cols="3">22.16 13.55 26.18</cell></row><row><cell>SVM</cell><cell cols="3">23.74 16.12 25.65</cell></row><row><cell cols="2">FFT-LSTM (Ours) 44.13</cell><cell>-</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Action recognition accuracy scores (%) for all benchmarks. Box, Raise Hand, and MoveTable). However, Wave Hand only got 19.8% accuracy. This is because this kind of action has low visual distinctiveness especially when the subjects wear black suit in data collection procedure. For depth modality, WDMM obtained 35.1% average accuracy, while WHDMM greatly outdid that with 40.2%. Compared with the 82.6% accuracy obtained from the skeleton data alone, the added EMG signal improved this by 1.4% (i.e., TCN-FFT with 84.0%</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Single-Person</cell><cell></cell><cell></cell><cell cols="2">Person-Object</cell></row><row><cell></cell><cell></cell><cell>W al k</cell><cell>B o x</cell><cell>W av e H an d C la p H an d s</cell><cell>Ju m p</cell><cell>B en d</cell><cell>T u rn A ro u n d</cell><cell>K ic k</cell><cell>R ai se H an d F al l D o w n</cell><cell>A n s. P h o n e C h ec k W at ch S ta n d U p S it D o w n G ra b B ag</cell><cell>T h ro w B al l D ri n k W at er T ie S h o es R ea d B o o k M o v e T ab le</cell><cell>ACC</cell></row><row><cell></cell><cell>TSN [52]</cell><cell cols="8">56.1 94.1 25.3 83.9 88.5 94.3 68.3 95.6 95.1 86.2</cell><cell cols="2">69.5 37.6 87.0 54.3 86.9 75.7 56.8 84.8 96.7 59.1</cell><cell>74.7</cell></row><row><cell>RGB</cell><cell>LRCN [12]</cell><cell cols="8">44.2 84.0 19.8 69.4 71.6 78.0 57.9 82.1 90.0 71.3</cell><cell cols="2">55.6 28.5 72.1 43.4 72.0 62.5 46.8 70.2 85.4 44.2</cell><cell>62.4</cell></row><row><cell></cell><cell>VLAD [16]</cell><cell cols="8">47.5 91.8 21.6 75.9 78.3 85.3 63.3 89.7 98.4 77.9</cell><cell cols="2">60.7 31.1 78.8 47.5 78.7 68.3 50.8 76.7 93.4 48.3</cell><cell>68.2</cell></row><row><cell>Dep</cell><cell>WDMM [2] WHDMM [53]</cell><cell cols="8">44.3 76.3 11.4 31.4 36.5 43.7 17.2 47.4 72.7 36.2 78.5 84.5 62.7 64.7 66.1 12.3 17.2 72.3 67.9 20.1</cell><cell cols="2">27.9 12.3 45.1 16.8 27.2 48.2 23.4 28.4 42.1 13.5 12.5 11.7 61.1 10.1 16.7 22.5 17.0 11.2 71.5 23.5</cell><cell>35.1 40.2</cell></row><row><cell></cell><cell>TCN[39]</cell><cell cols="8">91.2 82.0 71.4 86.0 92.2 91.7 87.6 93.0 89.2 92.6</cell><cell cols="2">57.5 76.0 92.9 87.8 66.8 70.5 95.0 76.1 76.1 76.4</cell><cell>82.6</cell></row><row><cell>SK-K</cell><cell>TSRNN [44]</cell><cell cols="8">90.0 85.0 70.6 81.0 91.0 90.5 86.6 91.8 86.6 91.4</cell><cell cols="2">56.7 75.1 91.7 86.8 66.0 69.7 93.8 75.1 65.1 85.4</cell><cell>81.5</cell></row><row><cell></cell><cell>STGCN [56]</cell><cell cols="8">90.6 83.5 71.0 83.5 91.6 91.1 87.1 92.4 88.7 92.0</cell><cell cols="2">57.1 75.6 92.3 87.3 66.4 70.1 94.4 75.6 75.6 75.9</cell><cell>82.1</cell></row><row><cell></cell><cell>TCN [39]</cell><cell cols="8">82.1 77.2 67.2 87.2 83.8 83.3 80.1 84.4 81.4 84.0</cell><cell cols="2">36.0 50.9 64.3 60.3 43.4 46.4 66.0 50.9 50.9 51.1</cell><cell>64.1</cell></row><row><cell>SK-V</cell><cell>TSRNN [44]</cell><cell cols="8">83.0 77.2 67.1 77.4 82.1 84.4 80.5 84.9 79.9 84.1</cell><cell cols="2">38.4 64.1 58.3 64.0 46.3 49.4 70.1 54.1 64.1 64.3</cell><cell>67.5</cell></row><row><cell></cell><cell>STGCN[56]</cell><cell cols="8">57.7 53.2 45.2 53.2 58.4 58.0 55.5 58.9 56.5 59.6</cell><cell cols="2">36.4 48.2 58.7 55.6 42.3 44.6 60.1 45.2 25.2 54.3</cell><cell>50.7</cell></row><row><cell>EMG</cell><cell>LSTM-FFT</cell><cell cols="8">72.3 51.6 35.1 54.8 90.6 40.0 30.3 36.6 11.9 72.8</cell><cell cols="2">51.2 56.5 16.1 41.6 17.3 48.4 45.7 31.4 46.2 33.0</cell><cell>44.1</cell></row><row><cell>SK-K-E</cell><cell>TCN-RMS TCN-FFT</cell><cell cols="8">91.1 83.0 73.4 88.0 93.2 94.7 87.8 91.0 91.4 95.6 92.0 83.7 72.1 85.7 94.0 93.5 87.3 94.8 91.0 94.4</cell><cell cols="2">60.5 79.8 91.9 88.8 70.8 72.5 94.0 74.1 78.1 74.4 60.6 78.5 91.3 89.6 70.1 71.9 94.8 79.5 77.6 77.9</cell><cell>83.6 84.0</cell></row><row><cell>SK-V-E</cell><cell>TCN-RMS TCN-FFT</cell><cell cols="8">86.7 80.7 70.3 87.9 87.1 84.5 83.6 85.1 82.1 83.6 82.2 77.5 67.3 87.3 83.8 83.4 80.5 84.7 81.7 84.5</cell><cell cols="2">63.5 51.6 64.4 60.3 45.4 46.0 65.8 50.5 51.2 51.1 37.0 51.4 64.5 60.0 43.5 47.4 64.0 53.9 52.9 51.1</cell><cell>69.1 66.8</cell></row><row><cell cols="10">an accuracy of 74.8%. For each action, we noticed that</cell><cell></cell></row><row><cell cols="10">some actions were easy to recognize and received over 90%</cell><cell></cell></row><row><cell cols="2">accuracy (i.e.,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluation of the spatial resolution accuracy of the face tracking system for kinect for windows v1 and v2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Amon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fuhrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAA</title>
		<meeting>AAAA</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="16" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dynamic 3D hand gesture recognition by learning weighted depth motion maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Azad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Asadi-Aghbolaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kasaei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Escalera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TCSVT</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1729" to="1740" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The fast Fourier transform and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">O</forename><surname>Brigham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">O</forename><surname>Brigham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">1</biblScope>
			<pubPlace>prentice Hall Englewood Cliffs, NJ</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the theory of filter amplifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Butterworth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Wireless Engineer</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="536" to="541" />
			<date type="published" when="1930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">NTD-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICIP</title>
		<meeting>ICIP</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hand gesture recognition research based on surface EMG sensors and 2D-accelerometers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ISWC</title>
		<meeting>IEEE ISWC</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="11" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human daily action analysis with multi-view and color-depth data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="52" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">PKU-MMD: A large scale benchmark for continuous multi-modal human action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chunhui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yueyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yanghao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sijie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiaying</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07475</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A. Bargteil, and others. Guide to the carnegie mellon university multimodal activity (CMU-MMAC) database. Robotics Institute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hodgins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">135</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The use of surface electromyography in biomechanics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>De Luca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied biomechanics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="163" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Duda hart pattern classification and scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Dreher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Exploring the trade-off between accuracy and observational latency in action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Masood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Tappen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Laviola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="420" to="436" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A human-assisting manipulator teleoperated by EMG signals and arm motions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fukuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tsuji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaneko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Otsuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Robotics and Automation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="222" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Action-VLAD: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Actions as space-time shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gorelick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">3D CNNs on distance matrices for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hernandez Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1087" to="1095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sequential maxmargin event detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La Torre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="410" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IESS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1094" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from RGB-D videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJRR</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="951" to="970" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3D points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="9" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Classification and regression by randomforest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wiener</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="18" to="22" />
		</imprint>
	</monogr>
	<note>R news</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fisher discriminant analysis with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ratsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Mullers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE NNSP</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">RGBD-HuDaAct: A color-depth video database for human daily activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer Depth Cameras for Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="193" to="208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint angles similarities and HOG2 for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPRW</title>
		<meeting>IEEE CVPRW</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="465" to="470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">HON4D: Histogram of oriented 4D normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Calibration of kinect for Xbox One and comparison between the two generations of microsoft sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pagliari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pinto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="27569" to="27589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Comparative abilities of Microsoft Kinect and Vicon 3D motion capture for gait analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bronner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Noah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMET</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="274" to="280" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">HOPC: Histogram of oriented principal components of 3D pointclouds for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Q</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="742" to="757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Caldwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hamill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kamen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Whittlesey</surname></persName>
		</author>
		<title level="m">Research methods in biomechanics. Human Kinetics</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Enhanced facial EMG activity in response to dynamic facial expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fujimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Suzuki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Psychophysiology</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="70" to="74" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning with kernels: support vector machines, regularization, optimization, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A 3-dimensional SIFT descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A large scale dataset for 3D human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">3d with kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Smisek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jancosek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Consumer depth cameras for computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Interpretable 3D human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Human activity detection from RGBD images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3D skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The standard vicon full-body model (plug-in gait) marker placement scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viconsystem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Modeling temporal dynamics and spatial configurations of actions using two-stream recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2649" to="2656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">3D human activity recognition with reconfigurable convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM MM</title>
		<meeting>ACM MM</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="97" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adaptive graph guided embedding for multi-label annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2798" to="2804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning transferable subspace for human motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Low-rank transfer human motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1023" to="1034" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generative multi-view human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6212" to="6221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dual relation semisupervised multi-label learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Action recognition from depth maps using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE THMS</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="498" to="509" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Modeling 4D human-object interactions for event and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3272" to="3279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3D joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="20" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07455</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Discriminative orderlet mining for realtime recognition of human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACCV</title>
		<meeting>ACCV</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="50" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">RGB-D-based action recognition datasets: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Ogunbona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="86" to="105" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
