<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accurate RGB-D Salient Object Detection via Collaborative Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Li</surname></persName>
							<email>jingjing.dlut@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongri</forename><surname>Piao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<settlement>Dalian</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Pengcheng Lab</orgName>
								<address>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Accurate RGB-D Salient Object Detection via Collaborative Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Benefiting from the spatial cues embedded in depth images, recent progress on RGB-D saliency detection shows impressive ability on some challenge scenarios. However, there are still two limitations. One hand is that the pooling and upsampling operations in FCNs might cause blur object boundaries. On the other hand, using an additional depth-network to extract depth features might lead to high computation and storage cost. The reliance on depth inputs during testing also limits the practical applications of current RGB-D models. In this paper, we propose a novel collaborative learning framework where edge, depth and saliency are leveraged in a more efficient way, which solves those problems tactfully. The explicitly extracted edge information goes together with saliency to give more emphasis to the salient regions and object boundaries. Depth and saliency learning is innovatively integrated into the high-level feature learning process in a mutual-benefit manner. This strategy enables the network to be free of using extra depth networks and depth inputs to make inference. To this end, it makes our model more lightweight, faster and more versatile. Experiment results on seven benchmark datasets show its superior performance. Recent researches on RGB-D salient object detection have gradually broken the performance bottleneck of traditional methods and RGB-based methods, especially when dealing with complex scenarios like similar foreground and background. However, there are some limitations with the introduction of FCNs [40,51] and depth images. Firstly, the emergence of FCNs enables automatic extraction of multi-level and multi-scale features. The high-level features means equal contribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The goal of salient object detection (SOD) is to locate and segment the most attractive and noticeable regions in an image. As a fundamental and pre-processing task, salient object detection plays an important role in various computer vision tasks, e.g., visual tracking <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b51">52]</ref>, video SOD <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b19">20]</ref>, object detection <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b13">14]</ref>, semantic segmentation <ref type="bibr" target="#b36">[37]</ref>, and human-robot interaction <ref type="bibr" target="#b12">[13]</ref>.  <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b45">46]</ref>). (b) Using tailor-made depth subnetworks to compensate for RGB representations (e.g. <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b64">65]</ref>).</p><p>with rich semantic information can better locate salient objects but the pooling and upsampling operations in FCNs might result in coarse and blur object boundaries (see <ref type="figure" target="#fig_0">Fig. 1 (left)</ref>). The low-level features contain rich local details but suffer from excessive background noises and might cause information chaos. Secondly, the spatial layout information from depth images can better express 3D scenes and help locate salient objects. However, previous RGB-D methods either adopted two-stream architectures that process RGB and depth images separately with various cross-modal fusion strategies (see <ref type="figure" target="#fig_0">Fig. 1a</ref>) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b45">46]</ref>, or utilized subnetworks tailored for depth image to compensate for RGB representations (see <ref type="figure" target="#fig_0">Fig. 1b</ref>) <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b64">65]</ref>. In those methods, the additional depth-networks might lead to high computation and storage cost, and cannot work without depth input, seriously limiting their practical applications.</p><p>In this paper, we propose a novel collaborative learning framework (CoNet) to confront the aforementioned limitations. In collaborative learning, multiple group members work together to achieve learning goals through exploratory learning and timely interaction. In our framework, three mutually beneficial collaborators are well-designed from different perspectives of the SOD task, namely edge detection, coarse salient object detection, and depth estimation. On the one hand, a edge collaborator is proposed to explicitly extracts edge information from the overabundant low-level features and then goes together with saliency knowledge to jointly assign greater emphasis to salient regions and object boundaries. On the other hand, considering the strong consistencies among global semantics and geometrical properties of image regions <ref type="bibr" target="#b53">[54]</ref>, we innovatively integrate depth and saliency learning into the high-level feature learning process in a mutualbenefit manner. Instead of directly taking depth image as input, this learning strategy enables the network to be free of using an extra depth network to make inference from an extra input. Compared with previous RGB-D models which utilize additional subnetworks to extract depth features and rely on depth images as input, our network is more lightweight, faster and more versatile. To our best knowledge, this is the first attempt to use depth images in such a way in RGB-D SOD research. Finally, a unified tutor named knowledge collector is designed to accomplish knowledge transfer from individual collaborators to the group, so as to more comprehensively utilize the learned edge, saliency and depth knowledges to make accurate saliency prediction. Benefiting from this learning strategy, our framework produces accurate saliency results with sharp boundary preserved and simultaneously avoids the reliance on depth images during testing.</p><p>In summary, our main contributions are as follows:</p><p>-We propose a novel collaborative learning framework (CoNet) where edge, depth, and saliency are leveraged in a different but more efficient way for RGB-D salient object detection. The edge exploitation makes the boundaries of saliency maps more accurate. -This learning strategy enables our RGB-D network to be free of using an additional depth network and depth input during testing, and thus being more lightweight and versatile. -Experiment results on seven datasets show the superiority of our method over other state-of-the-art approaches. Moreover, it supports the faster frame rate as it runs at 34 FPS, meeting the needs of real-time prediction (enhances FPS by 55% compared with current best performing method DMRA <ref type="bibr" target="#b45">[46]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early works <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b60">61]</ref> for saliency detection mainly rely on hand-crafted features. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b54">55]</ref> are some comprehensive surveys. Recently, traditional methods have been gradually surpassed by deep learning ones. Among those researches, 2D methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b59">60]</ref> based on RGB images have achieved remarkable performance and lone been the mainstream of saliency detection. However, 2D saliency detection appears to make a downgrade when handling complex scenarios due to the lack of spatial information in single RGB image. The introduction of depth images in RGB-D saliency researches <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b64">65]</ref> has made great promotions for those complex cases thanks to the embedded rich spatial information of depth images. The first CNNs-based method <ref type="bibr" target="#b47">[48]</ref> for RGB-D SOD uses hand-crafted features extracted from RGB and depth images for training. Then, Chen et al. propose to use two-stream models <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7]</ref> to process RGB and depth image separately and then combine cross-modal features to jointly predict saliency. They subsequently design a progressive fusion network <ref type="bibr" target="#b4">[5]</ref> to better fuse cross-modal multi-level features and propose a three-stream network <ref type="bibr" target="#b5">[6]</ref> which adopts the attention mechanism to adaptively select complement from RGB and depth features. Afterwards, Piao et al. <ref type="bibr" target="#b45">[46]</ref> utilize residual structure and depth-scale feature fusion module to fuse paired RGB and depth features. The network structures in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b45">46]</ref> can be represented as two-stream architectures shown in <ref type="figure" target="#fig_0">Fig. 1a</ref>. Another kind of structure is the use of subnetworks tailored for depth images to extract depth features and make compensation for RGB representations <ref type="bibr" target="#b67">[68,</ref><ref type="bibr" target="#b64">65]</ref>  <ref type="figure" target="#fig_0">(Fig. 1b</ref>). Zhu et al. <ref type="bibr" target="#b67">[68]</ref> utilize an auxiliary network to extract depth-induced features and then use them to enhance a pre-trained RGB prior model. In <ref type="bibr" target="#b64">[65]</ref>, Zhao et al. first enhance the depth map by contrast prior and then think of it as an attention map and integrate it with RGB features.</p><p>Those methods have some limitations. Using additional depth networks to extract depth features leads to high computation and storage cost. The reliance on depth images as input during testing also severely limits the practical applications of current RGB-D models. Moreover, we found that the boundaries of the produced saliency maps in those methods are a bit coarse and blur. This is mainly because the pooling and upsampling operations in FCNs might lead to the loss of local details and current RGB-D methods have not taken steps to emphasize the boundaries of salient objects.</p><p>Some RGB-based SOD methods attempt to enhance the boundary accuracy through adding edge constraints or designing boundary-aware losses. An edge guidance network <ref type="bibr" target="#b65">[66]</ref> couples saliency and edge features to better preserve accurate object boundary. Liu et al. <ref type="bibr" target="#b37">[38]</ref> train their pooling-based network with edge detection task and successfully enhance the details of salient regions. A predictrefine architecture <ref type="bibr" target="#b46">[47]</ref> equipped with a hybrid loss segments salient regions and refines the structure with clear boundaries. An attentive feedback module <ref type="bibr" target="#b20">[21]</ref> employs a boundary-enhanced loss for learning exquisite boundaries.</p><p>In this paper, we propose a novel collaborative learning framework where edge, depth and saliency are leveraged in a different but more efficient way. Different from previous RGB methods using edge supervision <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b37">38]</ref> or boundaryaware losses <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b20">21]</ref>, we further combine the learned edge knowledge with saliency knowledge to give extra emphasis to both salient regions and boundaries. For the use of depth, we innovatively integrate it into the high-level feature learning process in a mutual-benefit manner, instead of directly taking depth images as input. Free of using the depth subnetworks and depth input during testing makes our network more lightweight and versatile. In Section 3, we will elaborate on our collaborative learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Collaborative Learning Framework</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Overall Architecture</head><p>In this paper, we propose a novel CoNet for RGB-D SOD. The overall architecture is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. In this framework, three mutually beneficial collaborators, namely edge detection, coarse salient object detection and depth estimation, work together to aid accurate SOD through exploratory learning and timely interaction. From different perspectives of the SOD target, knowledges from edge, depth and saliency are fully exploited in a mutual-benefit manner to enhance the detector's performance. A simplified workflow is given below.</p><p>First, a backbone network is used to extract features from original images. Five transition layers and a global guidance module (GGM) are followed to per-  form feature preprocessing and generate the integrated low-level feature f l and high-level feature f h (details are shown in Sec. 3.2). Then an edge collaborator is assigned to f l to extract edge information from the overabundant low-level feature. For the high-level feature f h , saliency collaborator and depth collaborator work together to jointly enhance the high-level feature learning process of global semantics in a mutual-benefit manner. Finally, all learned knowledges from three collaborators (Att edge , Att sal and Att depth ), as well as the integrated low-level and high-level feature (F g ), are uniformly handed to a knowledge collector (KC). Here, acting as a tutor, KC summarizes the learned edge, depth and saliency knowledges and utilizes them to predict accurate saliency results. We elaborate on the three collaborators and the KC in Sec. 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Feature Preprocessing</head><p>Backbone Network. We use the widely used ResNet <ref type="bibr" target="#b23">[24]</ref> suggested by other deep-learning-based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b59">60]</ref> as backbone network, where the last fully connected layers are truncated to better fit for the SOD task. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, five side-out features generated from the backbone network are transferred to five transition layers to change their sizes and the number of channels. Detailed parameters are listed in <ref type="table">Table.</ref> 1, and the five output features are defined as Global Guidance Module. In order to obtain richer global semantics and alleviate information dilution in the decoder, a Global Guidance Module (GGM) is applied on high-level features (i.e. f 3 , f 4 , and f 5 )(see <ref type="figure" target="#fig_2">Fig. 3</ref>).</p><formula xml:id="formula_0">{f 1 , f 2 , f 3 , f 4 , f 5 }.</formula><p>Its key component, global perception module (GPM), takes the progressively integrated feature as input, followed by four parallel dilated convolution operations <ref type="bibr" target="#b61">[62]</ref> (kernel size = 3, dilation rates = 1/6/12/18 ) and one 1×1 traditional convolution operation, to obtain rich global semantics.</p><p>Benefiting from the dilated convolution <ref type="bibr" target="#b61">[62]</ref>, the GPM captures affluent multi-scale contextual information without sacrificing image resolution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>. Here, we define the process of GPM as F = Φ(F ), where F denotes the input feature map and F means the output feature. In GGM, we take the summation of the feature in current layer and the output features of all high-level GPMs as input to alleviate information dilution. Finally, three output features of GPMs are concatenated and an integrated high-level feature f h is produced, which is computed by:</p><formula xml:id="formula_1">f i = Φ(f i + 5 m=i+1 f m ), i = 3, 4, 5,<label>(1)</label></formula><formula xml:id="formula_2">f h = U p(W h * Concat( f 3 , f 4 , f 5 ) + b h ),<label>(2)</label></formula><p>where * means convolution operation. W h and b h are convolution parameters. U p(·) means the upsampling operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Collaborative Learning</head><p>Edge Collaborator. Existing 3D methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b64">65]</ref> have achieved remarkable performance in locating salient regions, but they still suffer from coarse object boundaries. In our framework, we design an edge collaborator to explicitly extract edge information from the overabundant low-level feature and use this information to give more emphasis to object boundaries. Specifically, we first formulate this problem by adding edge supervision on the top of integrated low-level feature f l . The used edge ground truths (GT) (shown in <ref type="figure" target="#fig_0">Fig. 1</ref>) are derived from saliency GT using canny operator <ref type="bibr" target="#b3">[4]</ref>. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, f l is processed by a 1×1 convolution operation and a softmax function to generate the edge map M edge . Then, binary cross entropy loss (denoted as loss e ) is adopted to calculate the difference between M edge and edge GT. As the edge maps M edge in <ref type="figure" target="#fig_1">Fig. 2</ref> and <ref type="figure" target="#fig_5">Fig. 5</ref> show, edge detection constraint is beneficial for predicting accurate boundaries of salient objects. Additionally, we also transfer the learned edge knowledge before the softmax function (denoted as Att edge ) to the knowledge collector (KC), where the edge information is further utilized to emphasize object boundaries. The reason why we use Att edge rather than M edge is to alleviate the negative influence brought by accuracy decrement of M edge . Saliency and Depth Collaborators. When addressing scene understanding tasks like semantic segmentation and salient object detection, there exist strong consistencies among the global semantics and geometric properties of image regions <ref type="bibr" target="#b53">[54]</ref>. In our framework, a saliency collaborator and a depth collaborator work together to jointly enhance the feature learning process of high-level semantics in a mutual-benefit manner.</p><p>Stage one: The high-level feature f h is first processed by a 1 × 1 convolution operation and a softmax function to predict a coarse saliency map S coarse . Here, binary cross entropy loss (denoted as loss s ) is used for training. Then, the learned saliency knowledge acts as a spatial attention map to refine the high-level feature in a similar way like <ref type="bibr" target="#b59">[60]</ref>. But different from <ref type="bibr" target="#b59">[60]</ref> which considers S coarse as attention map directly, we use the more informative feature map before softmax function (denoted as Att sal ) to emphasize or suppress each pixel of f h . Identify mapping is adopted to alleviate the errors in Att sal to be propagated to depth learning and accelerate network convergence. Formally, this procedure can be defined as:</p><formula xml:id="formula_3">Att sal = W s * f h + b s ,<label>(3)</label></formula><formula xml:id="formula_4">f h = Att sal f h + f h ,<label>(4)</label></formula><p>where means element-wise multiplication. f h denotes the output saliencyenhanced feature.</p><p>Stage two: As pointed out in previous RGB-D researches <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b64">65]</ref>, the spatial information within depth image is helpful for better locating salient objects in a scene. In our network, we innovatively integrate depth learning into the high-level feature learning process, instead of directly taking depth image as input. This learning strategy enables our network to be free of using an extra depth network to make inference from an extra depth input, and thus being more lightweight and versatile. As in <ref type="figure" target="#fig_1">Fig. 2</ref>, a depth head with three convolution layers (defined as Ψ (·)) is first used to make feature f h adapt to depth estimation. Then, its output Ψ ( f h ) is followed by a 1 × 1 convolution operation to generate the estimated depth map Att depth . Here, depth images act as GTs for supervision and we use smooth L 1 loss <ref type="bibr" target="#b21">[22]</ref> to calculate the difference between Att depth and depth GT, where smooth L 1 loss is a robust L 1 loss proposed in <ref type="bibr" target="#b21">[22]</ref> that is less sensitive to outliers than L 2 loss. Formally, the depth loss can be defined as:</p><formula xml:id="formula_5">Loss d = 1 W × H W x=1 H y=1 0.5 × | (x, y)| 2 , if | (x, y)| ≤ 1, | (x, y)| − 0.5, if (x, y) &lt; −1 or (x, y) &gt; 1,<label>(5)</label></formula><p>where W and H denote the width and height of the depth map. (x, y) means the error between prediction Att depth and the depth GT in each pixel (x, y).</p><p>Since each channel of a feature map can be considered as a feature detector <ref type="bibr" target="#b58">[59]</ref>, the depth knowledge Att depth is further employed to learn a channel-wise attention map M c for choosing useful semantics. Identify mapping operation is also adopted to enhance the fault-tolerant ability. This procedure can be defined as:</p><formula xml:id="formula_6">Att depth = W d * Ψ ( f h ) + b d ,<label>(6)</label></formula><formula xml:id="formula_7">M c = σ(GP (W c * Att depth + b c )),<label>(7)</label></formula><formula xml:id="formula_8">f hc = M c ⊗ f h + f h ,<label>(8)</label></formula><p>where w * and b * are parameters to be learned. GP (·) means global pooling operation. σ(·) is the softmax function. ⊗ denotes channel-wise multiplication. After these two stages, two collaborators can cooperatively generate optimal feature which contains affluent spatial cues and possesses strong ability to distinguish salient and non-salient regions. Knowledge Collector. In our framework, the KC works as a unified tutor to complete knowledge transfer from individual collaborators to the group.</p><p>As illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>, all knowledges learned from three collaborators (i.e. Att edge , Att sal , and Att depth ) and the concatenated multi-level feature F g = Concat(f l , f hc ) are uniformly transferred to the KC. Those information are comprehensively processed in a triple-attention manner to give more emphasis to salient regions and object boundaries. In <ref type="figure" target="#fig_1">Fig. 2</ref>, we show a detailed diagram with visualized attention maps for better understanding. To be specific, Att edge and Att sal are first concatenated together to jointly learn a fused attention map Att f , where the locations and boundaries of the salient objects are considered uniformly. Then, F g is in turn multiplied with the depth attention map Att depth and the fused attention map Att f , which significantly enhances the contrast between salient and non-salient areas. Ablation analysis shows the ability of the KC to enhance the performance significantly.</p><p>There is a vital problem worth thinking about. The quality of Att depth and Att f might lead to irrecoverable inhibition of salient areas. Therefore, we add several residual connection operations <ref type="bibr" target="#b23">[24]</ref> to the KC to retain the original features. Formally, this process can be defined as:</p><formula xml:id="formula_9">Att f = σ(W f * Concat(Att sal , Att edge ) + b f ),<label>(9)</label></formula><formula xml:id="formula_10">F g = Att depth F g + F g ,<label>(10)</label></formula><formula xml:id="formula_11">F = Att f F g + F g .<label>(11)</label></formula><p>In the end, F is followed by a 1 × 1 convolution operation and an upsampling operation to generate the final saliency map S f inal . Here, binary cross entropy loss (denoted as loss f ) is used to calculate the difference between S f inal and saliency GT. Thus, the total loss L can be represented as:</p><formula xml:id="formula_12">L = λ e Loss e + λ s Loss s + λ d Loss d + λ f Loss f ,<label>(12)</label></formula><p>where Loss e , Loss s , and Loss f are cross entropy loss and Loss d is a smooth L 1 loss. In this paper, we set λ e = λ s = λ f = 1 and λ d = 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>To evaluate the performance of our network, we conduct experiments on seven widely used benchmark datasets. DUT-D <ref type="bibr" target="#b45">[46]</ref>: contains 1200 images with 800 indoor and 400 outdoor scenes paired with corresponding depth images. This dataset contains many complex scenarios. NJUD <ref type="bibr" target="#b27">[28]</ref>: contains 1985 stereo images (the latest version). They are gathered from the Internet, 3D movies and photographs taken by a Fuji W3 stereo camera. NLPR <ref type="bibr" target="#b43">[44]</ref>: includes 1000 images captured by Kinect under different illumination conditions. SIP <ref type="bibr" target="#b18">[19]</ref>: contains 929 salient person samples with different poses and illumination conditions. LFSD <ref type="bibr" target="#b33">[34]</ref>: is a relatively small dataset with 100 images captured by Lytro camera. STEREO <ref type="bibr" target="#b42">[43]</ref>: contains 797 stereoscopic images downloaded from the Internet. RGBD135 <ref type="bibr" target="#b10">[11]</ref>: consists of seven indoor scenes and contains 135 images captured by Kinect.</p><p>For training, we split 800 samples from DUT-D, 1485 samples from NJUD, and 700 samples from NLPR as in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b45">46]</ref>. The remaining images and other public datasets are all for testing to comprehensively evaluate the generation abilities of models. To reduce overfitting, we augment the training set by randomly flipping, cropping and rotating those images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental setup</head><p>Evaluation metrics. We adopt 6 widely used evaluation metrics to verify the performance of various models, including precision-recall (PR) curve, mean Fmeasure (F β ) <ref type="bibr" target="#b0">[1]</ref>, mean absolute error (MAE) <ref type="bibr" target="#b2">[3]</ref>, weighted F-measure (F w β ) <ref type="bibr" target="#b41">[42]</ref> and recently proposed S-measure (S) <ref type="bibr" target="#b16">[17]</ref> and E-measure (E) <ref type="bibr" target="#b17">[18]</ref>. Saliency maps are binarized using a series of thresholds and then pairs of precision and recall are computed to plot the PR curve. The F-measure is a harmonic mean of average precision and average recall. Here, we calculate the mean F-measure which uses adaptive threshold to generate binary saliency map. The MAE represents the average absolute difference between the saliency map and ground truth. Weighted F-measure intuitively generalizes F-measure by alternating the way to calculate the Precision and Recall. S-measure contains two terms: object-aware and region-aware structural similarities. E-measure jointly captures image level statistics and local pixel matching information. Details of those evaluation metrics can refer to <ref type="bibr" target="#b54">[55]</ref>. For MAE, lower value is better. For others, higher is better. Implementation details. We implement our proposed framework using the Pytorch toolbox and train it with a GTX 1080 Ti GPU. All training and test images are uniformly resized to 256×256. Our network is trained in an end-to-end manner using the standard SGD optimizer, and it converges after 50 epochs with batch size of 2. The momentum, weight decay and learning rate are set as 0.9, 0.0005 and 1e-10, respectively. Any post-processing procedure (e.g., CRF <ref type="bibr" target="#b28">[29]</ref>) is not applied in this work. The model size of our network has only 167.6M and the inference speed for a 256 × 256 image only takes 0.0290s (34FPS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Analysis</head><p>Overview of Performance. We show the quantitative and qualitative results of different modules of our proposed network in Tab. 2 and <ref type="figure" target="#fig_3">Fig. 4</ref>. The backbone network (denoted as B) is constructed by directly concatenating low-level feature f l and high-level feature f h without using GGM for prediction. Comparison of the results (a) and (b) shows that adding our GGM can more effectively extract rich semantic features and prevent information dilution in the decoding stage. <ref type="table">Table 2</ref>. Quantitative results of the ablation analysis on two benchmark datasets. B means the backbone network. E and S represent edge supervision and saliency supervision respectively. SSA + DCA means our mutual-benefit learning strategy between depth and saliency. +KC means adding our knowledge collector on (e).  After introducing edge supervision (denoted as E), the boundaries of the saliency maps are sharper (b vs c in <ref type="figure" target="#fig_3">Fig. 4</ref>). The edge maps (M edge ) in <ref type="figure" target="#fig_1">Fig. 2</ref> and <ref type="figure" target="#fig_5">Fig. 5</ref> also show the ability of our network in explicitly extracting object boundaries. By adding additional saliency supervision on f h (denoted as S), the performance can be further improved. However, by comparing (d) and (e), we can see that our mutual-benefit learning style between saliency collaborator and depth collaborator (denoted as S SA and D CA ) can further improve the detector's ability to locate salient objects. This also verifies the strong correlation between saliency and depth. Finally, by using our proposed (KC), all learned edge, depth and saliency knowledges from three collaborators can be effectively summarized and utilized to give more emphasis to salient regions and object boundaries, improving the average MAE performance on two datasets by nearly 9.6% points. By comparing (e) and (f) in <ref type="figure" target="#fig_3">Fig. 4</ref>, we can also see that salient regions in (f) are more consistent with the saliency GT and the object boundaries are explicitly highlighted benefiting from the comprehensive knowledge utilization. Those advances demonstrate that using our collaborative learning strategy is beneficial for accurate saliency prediction. We list some numerical results here for better understanding. The Root Mean Squared Error (RMSE) of depth prediction on NJUD and NLPR datasets are 0.3684 and 0.4696, respectively. The MAE scores of edge prediction are 0.053 and 0.044, respectively. The Interactions between Collaborators. Saliency and Edge. To explore the correlation between saliency and edge, we gradually add edge detection supervision (denoted as E) and saliency supervision (denoted as S l ) on the low-level feature f l . From the quantitive re-sults in Tab. 3, we can see that adding edge supervision can explicitly extract clear boundary information and significantly enhance the detection performance, especially for the F-measure scores. However, when adding saliency supervision on f l , the performances on both datasets decrease dramatically. <ref type="table">Table 3</ref>. Ablation analysis of the interactions between three collaborators. The meaning of indexes (b)-(f) can refer to <ref type="table">Table.</ref> 2. +S l means adding saliency supervision on low-level feature. D means depth supervision.  This is partly because the low-level features contain too much information and are relatively too coarse to predict saliency, and partly because the two tasks are to some extent incompatible, in which one is for highlighting the boundaries and another is for highlighting the whole salient objects. Hence, it is optimal to only add edge detection supervision on the low-level feature. Saliency and Depth. In order to verify the effectiveness of the proposed mutual-benefit learning strategy on high-level feature f h , we gradually add two collaborators and their mutual-benefit operations to the baseline model (c). As shown in Tab. 3, adding saliency supervision (denoted as S) and adding depth supervision (denoted as D) are all beneficial for extracting more representative high-level semantic features. In addition, by gradually introducing our proposed mutual-benefit learning strategy between two collaborators (denoted as S SA and D CA ), spatial layouts and global semantics of high-level feature can be greatly enhanced, which consequently brings additional accuracy gains on both datasets. These results further verify the effectiveness of our collaborative learning framework. Saliency, Edge and Depth. In our knowledge collector, all knowledge learned from three collaborators are summarized and utilized in a triple-attention manner. As the visualized attention maps in <ref type="figure" target="#fig_1">Fig. 2 and Fig. 5</ref> show, the edge knowledge (Att edge ) can help highlight object boundaries, and the depth and saliency knowledge (Att depth and Att sal ) can also be used to emphasize salient regions and suppress non-salient regions. We can see from Tab. 3 that both Att edge and Att sal are beneficial for enhancing the feature representation and improving the F-measure and MAE performance. In our framework, we adopt a better strategy  that Att edge and Att sal are concatenated together to jointly emphasize salient objects and their boundaries. Finally, by comparing the results in the last two lines of Tab. 3, we can see that by further utilizing the learned depth knowledge, the detector's performance can be further improved. We visualize all internal results of the KC in <ref type="figure" target="#fig_5">Fig. 5</ref> for better understanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NJUD NLPR indexes Modules</head><formula xml:id="formula_13">F β ↑ M AE ↓ F β ↑ M AE ↓</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-arts</head><p>We compare results from our method with various state-of-the-art approaches on seven public datasets. For fair comparisons, the results from competing methods are generated by authorized codes or directly provided by authors. Quantitative Evaluation. Tab. 4 shows the quantitative results of our method over other 13 RGB-D ones on seven benchmark datasets. We can see that our proposed collaborative learning framework achieves superior performance. Noted that our method avoids the reliance on depth images and only takes RGB image as input in the testing stage. To comprehensively verify the effectiveness of our model, we additionally conduct comparisons with 9 state-of-the-art RGB methods on three public datasets. Results in Tab. 5 consistently show that our  method also achieves comparable results compared to 2D methods. The PR curves in <ref type="figure">Fig. 7</ref> also verify the superiority of our method. Qualitative Evaluation. <ref type="figure" target="#fig_7">Fig. 6</ref> shows some representative samples of results comparing our method with some top-ranking CNNs-based RGB and RGB-D approaches. For the complex scenes with lower-contrast (the 4 th and 5 th rows) or multiple objects (the 8 th row), our method can better locate the salient objects thanks to the useful spatial information in depth image and sufficient extraction and utilization of edge information. Thus, our method can produce accurate saliency results with sharp boundaries preserved. Complexity Evaluation. We also compare the model size and run time (Frame Per Second, FPS) of our method with 11 representative models in Tab. 6. Thanks to the well-designed depth learning strategy, our network is free of using extra depth networks and depth inputs to make inference. It can also be seen that our method achieves outstanding scores with a smaller model size and higher  <ref type="figure">Fig. 7</ref>. The PR curves of our method compared to other state-of-the-art approaches on four datasets. <ref type="table">Table 6</ref>. Complexity comparisons of various methods. The best three results are shown in blue, red, and green fonts respectively. FPS means frame per second.</p><p>NJUD <ref type="bibr" target="#b27">[28]</ref> NLPR <ref type="bibr" target="#b43">[44]</ref> Types FPS (enhances FPS by 55% compared to current best performing RGB-D model DMRA). Those results confirm that our model is suitable for the pre-processing task in terms of model size and running speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we propose a novel collaborative learning framework for accurate RGB-D salient object detection. In our framework, three mutually beneficial collaborators, i.e., edge detection, coarse salient object detection and depth estimation, jointly accomplish the SOD task from different perspectives. Benefiting from the well-designed mutual-benefit learning strategy between three collaborators, our method can produce accurate saliency results with sharp boundaries preserved. Free of using extra depth subnetworks and depth inputs during testing also makes our network more lightweight and versatile. Experiment results on seven benchmark datasets show that our method achieves superior performance over 22 state-of-the-art RGB and RGB-D methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>(Left) First two rows: feature maps in different layers of CNNs. Last two rows: RGB image, depth image, edge map, saliency ground truth (GT) and saliency results of several state-of-the-art methods. * means RGB-D methods. (Right) Two kinds of previous RGB-D SOD network structures. (a) Processing RGB input and depth input separately and then combining the complementary RGB and depth features through cross-modal fusion (e.g.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The overall architecture of our collaborative learning framework. Details of the Global Guidance Module can be found inFig. 3. Here, Att * = 1 − Att * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>The architecture of global guidance module (GGM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>(b)+E+S SA +D CA 0.864 0.051 0.841 0.035 (f) (e)+KC 0Visual saliency maps of ablation analysis. The meaning of the indexes (a)-(f) can refer to</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>AE ↓ Fβ ↑ M AE ↓ Saliency &amp; Edge (b) 0.839 0.060 0.813 0.044 (b)+E (c) 0.851 0.056 0.825 0.041 (b)+E+Sl 0.835 0.062 0.807 0.044 Saliency &amp; Depth (c) 0.851 0.056 0.825 0.041 (c)+S (d) 0.857 0.054 0.833 0.038 (c)+S+D 0.859 0.054 0.835 0.037 (c)+S+DCA 0.861 0.053 0.837 0.036 (c)+SSA+DCA (e</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Internal results in the knowledge collector. The results of another sample can be seen inFig. 2. Here, F = 1 − F .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>0.813 0.844 0.844 0.850 0.872 0.872 M AE ↓ 0.448 0.201 0.167 0.202 0.181 0.151 0.085 0.062 0.079 0.061 0.059 0.053 0.051 0.047 NLPR [44] E ↑ 0.735 0.772 0.684 0.814 0.785 0.838 0.869 0.876 0.871 0.916 0.916 0.924 0.942 0.936 S ↑ 0.582 0.591 0.550 0.714 0.724 0.769 0.860 0.835 0.855 0.886 0.873 0.888 0.899 0.907 F w β ↑ 0.259 0.320 0.265 0.574 0.512 0.524 0.691 0.659 0.688 0.789 0.772 0.820 0.845 0.850 Fβ ↑ 0.583 0.520 0.328 0.637 0.591 0.682 0.723 0.740 0.729 0.795 0.794 0.822 0.855 0.848 M AE ↓ 0.301 0.119 0.196 0.089 0.114 0.099 0.056 0.064 0.059 0.041 0.044 0.036 0.031 0.031 STEREO [43] E ↑ 0.451 0.781 0.838 0.693 0.801 0.844 0.870 0.903 0.890 0.911 0.905 0.897 0.920 0.923 S ↑ 0.473 0.567 0.745 0.579 0.727 0.763 0.853 0.874 0.856 0.877 0.880 0.871 0.886 0.908 F w β ↑ 0.277 0.369 0.551 0.445 0.595 0.576 0.727 0.799 0.747 0.811 0.810 0.818 0.850 0.871 Fβ ↑ 0.223 0.716 0.761 0.572 0.680 0.761 0.786 0.833 0.812 0.849 0.845 0.827 0.868 0.885 M AE ↓ 0.417 0.179 0.150 0.178 0.149 0.142 0.087 0.064 0.080 0.060 0.061 0.054 0.047 0.041 SIP [19] E ↑ 0.742 0.722 0.787 0.715 0.721 0.794 0.824 0.802 0.886 0.893 0.898 0.899 0.863 0.909 S ↑ 0.616 0.523 0.684 0.624 0.597 0.651 0.716 0.691 0.833 0.835 0.844 0.850 0.806 0.858 F w β ↑ 0.352 0.286 0.426 0.474 0.411 0.411 0.551 0.503 0.726 0.762 0.777 0.798 0.750 0.814 Fβ ↑ 0.646 0.593 0.646 0.573 0.494 0.672 0.684 0.620 0.795 0.809 0.824 0.818 0.819 0.842 M AE ↓ 0.300 0.182 0.186 0.163 0.224 0.186 0.139 0.166 0.086 0.075 0.071 0.064 0.085 0.063 LFSD [34] E ↑ 0.475 0.742 0.842 0.631 0.737 0.841 0.851 0.872 0.840 0.845 0.846 0.867 0.899 0.897 S ↑ 0.440 0.558 0.754 0.538 0.658 0.796 0.796 0.845 0.787 0.801 0.800 0.828 0.847 0.862 F w β ↑ 0.278 0.379 0.605 0.401 0.524 0.645 0.700 0.738 0.668 0.723 0.720 0.779 0.814 0.819 Fβ ↑ 0.228 0.708 0.815 0.543 0.634 0.810 0.781 0.824 0.779 0.794 0.794 0.813 0.849 0.848 M AE ↓ 0.415 0.211 0.155 0.218 0.199 0.142 0.120 0.109 0.132 0.111 0.112 0.088 0.075 00.750 0.782 0.763 0.819 0.857 0.861 M AE ↓ 0.289 0.097 0.194 0.102 0.119 0.130 0.055 0.050 0.064 0.045 0.049 0.037 0.029 0.027</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Visual comparisons of our method with other state-of-the-art CNNs-based methods in some representative scenes. * means RGB-D methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Methods Years Size FPS F w β ↑ M AE ↓ F w β ↑ M AE ↓ 2D DSS 2017'CVPR 447.3MB 22 0.678 0.108 0.614 0.076 Amulet 2017'ICCV 132.6 MB 16 0.758 0.085 0.716 0.062 PiCANet 2018'CVPR 197.2 MB 7 0.768 0.071 0.707 0.053 PoolNet 2019'CVPR 278.5 MB 32 0.816 0.057 0.771 0.046 CPD 2019'CVPR 183 MB 62 0.834 0.054 0.820 0.036 3D PCA 2018'CVPR 533.6 MB 15 0.811 0.059 0.772 0.044 TANet 2019'TIP 951.9 MB 14 0.812 0.061 0.789 0.041 MPCI 2019'PR 929.7 MB 19 0.749 0.079 0.688 0.059 PDNet 2019'ICME 192 MB 19 0.798 0.062 0.659 0.064 CPFP 2019'CVPR 278 MB 6 0.837 0.053 0.820 0.036 DMRA 2019'ICCV 238.8 MB 22 0.853 0.051 0.845 0.031 * Ours 167.6 MB 34 0.856 0.047 0.850 0.031</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table><row><cell>Transition Input Size</cell><cell>Transition Operators</cell><cell>Output Size</cell></row><row><cell>trans1 128 × 128 × 64</cell><cell>U psample×2</cell><cell>256 × 256 × 64</cell></row><row><cell>trans2 64 × 64 × 256</cell><cell>U psample×4</cell><cell>256 × 256 × 256</cell></row></table><note>Detailed information of the five transition layers in Fig. 2. We show the input size and output size of the feature maps before and after those transition layers, and represent their specific transition operators for better understanding.trans3 32 × 32 × 512 U psample×2, Conv3×3 + BN + P Relu 64 × 64 × 64 trans4 16 × 16 × 1024 U psample×4, Conv3×3 + BN + P Relu 64 × 64 × 64 trans5 16 × 16 × 2048 U psample×4, Conv3×3 + BN + P Relu 64 × 64 × 64</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Quantitative comparisons on seven benchmark datasets. The best three results are shown in blue, red, and green fonts respectively.</figDesc><table><row><cell>Dataset</cell><cell cols="13">Metric DES LHM DCMC MB CDCP DF CTMF PDNet MPCI TANet PCA CPFP DMRA Ours</cell></row><row><cell></cell><cell>[11]</cell><cell>[44]</cell><cell>[12]</cell><cell>[69]</cell><cell>[70]</cell><cell>[48]</cell><cell>[23]</cell><cell>[68]</cell><cell>[7]</cell><cell>[6]</cell><cell>[5]</cell><cell>[65]</cell><cell>[46]</cell></row><row><cell>DUT-D [46]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>Quantitative comparisons with state-of-the-art 2D methods.</figDesc><table><row><cell>Dataset</cell><cell cols="9">Metric DSS Amulet R 3 Net PiCANet PAGRN EGNet PoolNet BASNet CPD Ours</cell></row><row><cell></cell><cell></cell><cell>[26]</cell><cell>[63]</cell><cell>[15]</cell><cell>[39]</cell><cell>[64]</cell><cell>[66]</cell><cell>[38]</cell><cell>[47]</cell><cell>[60]</cell></row><row><cell>NJUD [28]</cell><cell>S ↑ F w β ↑</cell><cell cols="2">0.807 0.843 0.678 0.758</cell><cell>0.837 0.736</cell><cell>0.847 0.768</cell><cell>0.829 0.746</cell><cell>0.871 0.812</cell><cell>0.872 0.816</cell><cell>0.872 0.876 0.894 0.839 0.834 0.856</cell></row><row><cell></cell><cell cols="3">M AE ↓ 0.108 0.085</cell><cell>0.092</cell><cell>0.071</cell><cell>0.081</cell><cell>0.057</cell><cell>0.057</cell><cell>0.055 0.054 0.047</cell></row><row><cell>NLPR [44]</cell><cell>S ↑ F w β ↑</cell><cell cols="2">0.816 0.848 0.614 0.716</cell><cell>0.798 0.611</cell><cell>0.834 0.707</cell><cell>0.844 0.707</cell><cell>0.861 0.760</cell><cell>0.867 0.771</cell><cell>0.890 0.887 0.907 0.834 0.820 0.850</cell></row><row><cell></cell><cell cols="3">M AE ↓ 0.076 0.062</cell><cell>0.101</cell><cell>0.053</cell><cell>0.051</cell><cell>0.046</cell><cell>0.046</cell><cell>0.036 0.036 0.031</cell></row><row><cell>STEREO [43]</cell><cell>S ↑ F w β ↑</cell><cell cols="2">0.841 0.881 0.718 0.811</cell><cell>0.855 0.752</cell><cell>0.868 0.774</cell><cell>0.851 0.792</cell><cell>0.897 0.847</cell><cell>0.898 0.849</cell><cell>0.896 0.899 0.908 0.873 0.865 0.871</cell></row><row><cell></cell><cell cols="3">M AE ↓ 0.087 0.062</cell><cell>0.084</cell><cell>0.062</cell><cell>0.067</cell><cell>0.045</cell><cell>0.045</cell><cell>0.042 0.042 0.041</cell></row></table><note>* DMRA* CPFP* TANet* PCA** Depth</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ssstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1597" to="1604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Salient object detection: a benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>ECCV</publisher>
			<biblScope unit="page" from="414" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="679" to="698" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Progressively complementarity-aware fusion network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3051" to="3060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Three-stream attention-aware network for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2825" to="2835" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multi-modal fusion network with multi-scale multi-path and cross-modal interactions for rgb-d salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PR</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="376" to="385" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="833" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="409" to="416" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Depth enhanced saliency detection method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIMCS. pp</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Saliency detection for stereoscopic images based on depth confidence analysis and multiple cues fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SPL</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="819" to="823" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Environment exploration for object-based visual saliency learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Craye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Filliat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Goudou</surname></persName>
		</author>
		<editor>ICRA</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2303" to="2309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">R-fcn: object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="379" to="387" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">R 3 net: Recurrent residual refinement network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI. pp</title>
		<imprint>
			<biblScope unit="page" from="684" to="690" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Salient objects in clutter: Bringing salient object detection to the foreground</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="196" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Structure-measure: A new way to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4558" to="4567" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhanced-alignment measure for binary foreground map evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI. pp</title>
		<imprint>
			<biblScope unit="page" from="698" to="704" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Rethinking rgb-d salient object detection: Models, datasets, and large-scale benchmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06781</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Shifting more attention to video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8554" to="8564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attentive feedback network for boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1623" to="1632" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cnns-based rgb-d saliency detection via cross-view transfer and multiview fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3171" to="3183" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Online tracking by learning discriminative saliency map with convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="597" to="606" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="815" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Depth saliency based on anisotropic center-surround difference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP. pp</title>
		<imprint>
			<biblScope unit="page" from="1115" to="1119" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krhenbhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep saliency with encoded low level distance map and high level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="660" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A three-pathway psychobiological framework of salient object detection using stereoscopic technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCVW</publisher>
			<biblScope unit="page" from="3008" to="3014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Visual saliency detection based on multiscale deep cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5012" to="5024" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Saliency detection on light field</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1605" to="1616" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepsaliency: Multi-task deep neural network model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3919" to="3930" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5168" to="5177" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A simple pooling-based design for real-time salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3917" to="3926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Picanet: Learning pixel-wise contextual attention for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3089" to="3098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Non-local deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6593" to="6601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">How to evaluate foreground maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Margolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Leveraging stereopsis for saliency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Rgbd salient object detection: A benchmark and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="92" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krhenbhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Depth-induced multi-scale recurrent attention network for saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Piao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Basnet: Boundary-aware salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jagersand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7479" to="7489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rgbd salient object detection via deep fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2274" to="2285" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Exploiting global priors for rgb-d saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPRW</publisher>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Faster r-cnn: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<biblScope unit="volume">2015</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1442" to="1468" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep networks for saliency detection via local estimation and global search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3183" to="3192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2800" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Salient object detection in the deep learning era: An in-depth survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09146</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Deep visual attention prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2368" to="2378" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Salient object detection driven by fixation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1711" to="1720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Revisiting video saliency prediction in the deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Cascaded partial decoder for fast and accurate salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3907" to="3916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="202" to="211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Progressive attention guided recurrent network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Contrast prior and fluid pyramid integration for rgbd salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Egnet: Edge guidance network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Saliency detection by multi-context deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1265" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Pdnet: Prior-model guided depthenhanced network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="199" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A multilayer backpropagation saliency detection algorithm based on depth mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CAIP. pp</title>
		<imprint>
			<biblScope unit="page" from="14" to="23" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">An innovative salient object detection using center-dark channel prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICCVW</publisher>
			<biblScope unit="page" from="1509" to="1515" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
