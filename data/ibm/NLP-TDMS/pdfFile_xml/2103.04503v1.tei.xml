<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Human Object Interaction Detection with HOI Transformer</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zou</surname></persName>
							<email>zoucheng@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Wang</surname></persName>
							<email>wangbohan@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
							<email>huyue@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
							<email>liujunqi@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wu</surname></persName>
							<email>wuqian@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhao</surname></persName>
							<email>zhaoyu03@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhang</surname></persName>
							<email>liboxunzhangchenguang@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
							<email>zhangchi@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
							<email>weiyichen@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Human Object Interaction Detection with HOI Transformer</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose HOI Transformer to tackle human object interaction (HOI) detection in an end-to-end manner. Current approaches either decouple HOI task into separated stages of object detection and interaction classification or introduce surrogate interaction problem. In contrast, our method, named HOI Transformer, streamlines the HOI pipeline by eliminating the need for many hand-designed components. HOI Transformer reasons about the relations of objects and humans from global image context and directly predicts HOI instances in parallel. A quintuple matching loss is introduced to force HOI predictions in a unified way. Our method is conceptually much simpler and demonstrates improved accuracy. Without bells and whistles, HOI Transformer achieves 26.61% AP on HICO-DET and 52.9% AP role on V-COCO, surpassing previous methods with the advantage of being much simpler. We hope our approach will serve as a simple and effective alternative for HOI tasks. Code is available at https: //github.com/bbepoch/HoiTransformer. Pairs Enumerate Object Detection Feature Extraction Object Stream Human Stream Pairwise Stream HOI Category Object Detection Pairs Enumerate</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two-stage(a)</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End-to-End(c)</head><p>One-stage (b)    Feature Extraction HOI Transformer</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human-Object Interaction (HOI) detection plays an important role in the high level human-centric scene understanding, and has attracted considerable research interest recently. The HOI research can also contribute to other tasks, such as action analysis, weakly-supervised object detection, and visual question answering, etc.</p><p>The goal of HOI detection aims at localizing human and object, as well as recognizing the interaction between them. Previous studies <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b2">3]</ref> present promising results on HOI detection by decouple this task into object detection and interaction classification ( <ref type="figure">Fig. 1(a)</ref>). More specifically, human and object detection results is first obtained by pre-trained object detector, then interaction classification is * Equal contribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Matching</head><p>Interaction(Poi nt) Detection <ref type="figure">Figure 1</ref>: Comparison of recent approaches on HOI detection. (a) two-stage methods, typically use pre-trained detectors to generate human, object proposal and enumerate all (human, object) pairs, followed by a multi-stream architecture to clarify interaction. (b) one-stage methods, detect interaction point/box and object proposals simultaneously, followed by complex matching process to assign interactions to object pairs. (c) our end-to-end method, input an image and predict HOI instances directly.</p><p>conducted on the pair-wisely combined human-object proposals. The limitations of these methods are mainly caused by the separated two stages. The independent optimization on two sub-problems may lead to sub-optimal solution. The generated human-object proposals have relative low quality for interaction classification <ref type="bibr" target="#b18">[19]</ref>, because only object-level confidence has been taken into account. Moreover, all pairwise proposals need to be processed, which brings large re-dundant computation cost.</p><p>More recent approaches <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b18">19]</ref> have introduced a surrogate interaction detection problem to optimize HOI detection indirectly ( <ref type="figure">Fig. 1(b)</ref>). Firstly, the interaction proposal has been pre-defined based on human priors. For example, UnionDet <ref type="bibr" target="#b2">[3]</ref> defines the interaction proposal as union box of the human and object box. PPDM <ref type="bibr" target="#b18">[19]</ref> uses the center point between human and object as interaction point. Secondly, the human, object and interaction proposals are detected in parallel. Finally, each interaction result is assigned to one (human, object) pair based on pre-defined matching strategy in post processing. However, such definition of interaction proposal are not always valid under different circumstance and make the pipeline more complex and costly in computation.</p><p>For HOI detection, how to capture the dependencies, especially long range, between human and object in the image space is the main problem. The above methods used complex but sub-optimal strategies, i.e. decouple into twostages or introduce surrogate proposals to empower models the ability of capturing dependencies. However, the transformer network <ref type="bibr" target="#b31">[32]</ref> is designed to exhaustively capture the long range dependencies, which inspire us to address the problem with transformer.</p><p>In this paper, we propose a new architecture to directly predict the HOI instance, i.e. (human, object, interaction), in an end-to-end manner. Our method consists of two parts, a transformer encoder-decoder architecture and a quintuple HOI matching loss. The architecture first use CNN backbone to extract high-level image features, then the encoder is leveraged to generate global memory feature, which models the relation between the image feature explicitly. Next the global memory from encoder and the HOI queries are send to decoder to generate the output embeddings. Finally, a multi-layer perception is used to predict HOI instances based on the output embeddings of decoder. Meanwhile, a quintuple HOI matching loss is proposed to supervise the learning of HOI instance prediction. Our method achieves state-of-the-art results on different challenging HOI benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work 2.1. Two-Stage HOI Detection</head><p>Modern two-stage HOI detection methods usually consists of an object detector in the first stage and an interaction classifier in the second stage. More specifically, In the first stage, a fine-tuned object detector is used to get the humans and objects bounding boxes and class labels. In the second stage, a multi-stream architecture is used to predict the interactions for each human-object pair.</p><p>Typically there are three streams in the mentioned multistream interaction classifier: human stream, object stream, and pairwise stream. Both human stream and object stream usually encode visual features for human and object boxes respectively <ref type="bibr" target="#b6">[7]</ref>. In FCMNet <ref type="bibr" target="#b21">[22]</ref>, object visual feature is replaced by word embedding for the reason that detailed visual appearance of the object is often not crucial for the interaction category. Besides visual features, Bansal et. al <ref type="bibr" target="#b0">[1]</ref> introduced word embedding in human stream for feature augmentation. PDNet <ref type="bibr" target="#b37">[38]</ref> introduced word embedding for all the streams to get language prior-guided channel attention and feature augmentation. Plenty of researches have been done on the pairwise stream. This stream usually encodes the relationship between the human and object. A two-channel binary image representation is first advocated in iCAN <ref type="bibr" target="#b6">[7]</ref> to encode the spatial relation, but in FCM-Net <ref type="bibr" target="#b21">[22]</ref>, a fine-grained version from human parsing is proposed to amplify the key cues. Apart from spatial relation, graph neural networks in DRG <ref type="bibr" target="#b5">[6]</ref>, CHG <ref type="bibr" target="#b33">[34]</ref>, RPNN <ref type="bibr" target="#b38">[39]</ref> were proposed to explicitly model the interactions between human and objects, which sure improved the model's representation capability.</p><p>Auxiliary models can be easily introduced to two-stage pipeline to help improving HOI, e.g. human pose feature, human body-part <ref type="bibr" target="#b16">[17]</ref>, language model <ref type="bibr" target="#b25">[26]</ref> and graph model <ref type="bibr" target="#b36">[37]</ref>, etc. Interestingly Bansal et.al <ref type="bibr" target="#b0">[1]</ref> and Hou et.al <ref type="bibr" target="#b13">[14]</ref> introduced feature level augmentation, which is proved to be effective to HOI. However, these methods suffer from heavy complexity and low efficiency due to the sequential and separated two-stage architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">One-Stage HOI Detection</head><p>InteractNet <ref type="bibr" target="#b8">[9]</ref> may be one of the earliest detection-based methods but it needs cascaded inference. PPDM <ref type="bibr" target="#b18">[19]</ref> and IPNet <ref type="bibr" target="#b34">[35]</ref> treated HOI as a point detection problem and directly detect interactions in a one stage manner by introducing a novel definition of interaction point. Further, PPDM predicts both object detection and HOI detection in a unified CenterNet-based <ref type="bibr" target="#b40">[41]</ref> model. In UnionDet <ref type="bibr" target="#b2">[3]</ref>, HOI detection is regarded as a union box detection problem and based on the popular RetinaNet <ref type="bibr" target="#b19">[20]</ref>, another unified one stage HOI detection model is proposed, an extra union branch for detecting union box is added parallel to the conventional object detection branch.</p><p>Compared to two-stage methods, the pipeline becomes simpler, faster, more efficient and easier to deploy for real world applications. However, one-stage methods still need complex post processing to group object detection results and the interaction predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">End-to-End Object Detection</head><p>Russell <ref type="bibr" target="#b29">[30]</ref> proposed an end-to-end people detection method by LSTM-based encoder-decoder, which is an autoregressive model that predicts the output sequence one element at a time. DETR <ref type="bibr" target="#b3">[4]</ref> improved it by by replacing  LSTM with transformer, which decodes N objects in parallel by leveraging the recent transformers with parallel decoding <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b7">8]</ref>. Both methods use Hungarian algorithm to match the ground truths and predictions though different matching costs are used. Unlike traditional object detectors, the end-to-end methods, usually have an NMS free architecture, and to make this reality, a good one-to-one matching strategy for duplicates reduction is important, and Hungarian matching seems to be a better choice so far.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Different from previous methods, we solve humanobject interaction detection in an end-to-end manner both in training and inference: input an image and then output the HOI relations directly, without any post processing. The proposed method consists of two main parts, an end-to-end transformer encoder-decoder architecture and a quintuple HOI instance matching loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>The proposed architecture illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref> consists of three main parts: (i) a backbone to extract visual feature from the input image, (ii) a transformer encoder-decoder to digest backbone feature and produce output embeddings, and (iii) a multi-layer perception (MLP) to predict HOI instances. Backbone: A CNN backbone is used to extract visual feature from the input image. First, a color image is fed into the backbone and generate a feature map of shape (H, W, C) which contains high level semantic concepts. A 1 × 1 convolution layer is used to reduce the channel dimension from C to d. A flatten operator is used to collapse the spatial dimension into one dimension. After that, a feature map of shape [H × W, d] is obtained, denoted as flatten feature in <ref type="figure" target="#fig_0">Fig. 2</ref>. The spatial dimension transformation is important because the following transformer encoder requires a sequence as input, thus the feature map can be interpreted as a sequence of length H ×W , and the value at each time step is a vector of size d. We use ResNet <ref type="bibr" target="#b12">[13]</ref> as our backbone and reduce the dimension of feature conv-5 from C = 2048 to d = 256.</p><p>Encoder: The encoder layer is built upon standard transformer architecture with a multi-head self-attention module and a feed-forward network (FFN). Theoretically the transformer architecture is permutation invariant. To enable it distinguish relative position in the sequence, position encoding <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b1">2]</ref> is added to the input of each attention layer. The sum of flatten feature and positional encoding is fed into the transformer encoder to summarize global information. The output of the encoder is denoted as global memory in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Decoder: The decoder layer is also built upon the transformer architecture. Different from encoder layer, it contains an additional multi-head cross attention layer. The decoder transforms N learnt positional embeddings (denoted as HOI queries in <ref type="figure" target="#fig_0">Fig. 2</ref>) into N output embeddings. They are then decoded into HOI instances by the following MLP, which will be detailed in next section. In general, the decoder has three inputs, one is the global memory from encoder, one is HOI queries, and one is positional encoding. For multi-head cross attention layer, the Value comes from global memory directly. The Key is the sum of global memory and the input position encoding. The Query is the sum of input position encoding and the input HOI queries. For self-attention layer, all of the Query, Key, Value come from the HOI queries or the output of previous decoder layer. The output of the decoder is denoted as output embeddings in <ref type="figure" target="#fig_0">Fig. 2</ref>. This architecture design follows <ref type="bibr" target="#b3">[4]</ref>. MLP for HOI Prediction: We define each HOI instance as a quintuple of (human class, interaction class, object class, human box, object box). The output embedding for each HOI query is decoded into one HOI instance by several multi-layer perception (MLP) branches. Specifically, there are three one-layer MLP branches to predict the human confidence, object confidence and interaction confidence respectively, and two three-layer MLP branches to predict human box and object box. All one-layer MLP branches for predicting confidence use a softmax function. For human confidence branch, the output size is 2, implies the confidences for foreground and background. For object confidence branch and interaction confidence branch, the output size is C + 1, which implies the confidences for all C kinds of objects or verbs defined in the dataset plus one for background. For both human and object box branches, the output size is 4, implies the normalized center coordinates (x c , y c ), height and width of the box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">HOI Instance Matching</head><p>The HOI instance is a quintuple of (c</p><formula xml:id="formula_0">h , c r , c o , b h , b o ), where (c h , c r , c o ) denotes human, interaction and object class confidence, (b h , b o )</formula><p>is the bounding box of the human and object. Two-stage HOI detectors first predict the object proposals (c h , b h ), (c o , b o ) with an object detector, then enumerate the detected (human, object) pairs to predict the c r by interaction classification. In other words, they are trying to approximate the following probability in a given dataset,</p><formula xml:id="formula_1">p(h, r, o) = p(h, o)p(r|h, o) ≈ p(h)p(o)p(r|h, o)<label>(1)</label></formula><p>where p(h) and p(o) indicate the confidence of human and object bounding box, respectively. p(r|h, o) denotes the probability of interaction r given human box h and object box o, often implemented by a multi-stream interaction recognition model. In this method, the object detector and the interaction classifier are separately optimized.</p><p>On the contrary, we treat HOI detection as a set prediction problem of bipartite matching between predictions and ground truth. Our method directly predicts the elements in HOI set and optimizes the proposed HOI matching loss in a unified way.</p><p>As shown in <ref type="figure" target="#fig_1">Fig. 3(a)</ref>, suppose a ground truth (human, fly, object) is in the image, and the model predicts two HOI instances: the yellow one (human, fly, object), and the blue one (human, hold, object). The yellow one not only predict the interaction correctly but localize the human and object more accurately as well. To minimize the matching cost, it is more suitable to assign the black one to the yellow one, and assign ∅ (implies nothing) to the blue one. A precise and complete matching strategy is formulated in the following.</p><p>Assume the model outputs a fixed-size set of N predictions, where N is chosen to be larger than the number of HOI relations in one image. Let us denote the set of predicted HOIs as P = p i , i = 1, 2, ..., N , the set of ground truth HOIs as G = g i , i = 1, 2, ..., M, ∅, ..., ∅, where M ≤ N . M represents the number of ground truth in an image. By padding ∅ to the ground truth set, we make the length of two sets equal.</p><p>We denote the matching as an injective function: σ G→P , where σ(i) is the index of predicted HOI assigned to the i-th groundtruth. The matching cost function is defined as:</p><formula xml:id="formula_2">L cost = N i L match g i , p σ(i)<label>(2)</label></formula><p>where L match g i , p σ(i) is a matching cost between ground truth g i and prediction p σ(i) . In each step of training, we should first find an optimal one-to-one matching between the ground truth set and the current prediction set. We design the following matching cost for HOI:</p><formula xml:id="formula_3">L match g i , p σ(i) = β1 j∈h,o,r αjL j cls + β2 k∈h,o L k box<label>(3)</label></formula><p>where L j cls = L cls g i j , p σ(i) j , j ∈ h, o, r represents human, object, and interactions, g i j denotes the category label of j on ground-truth g i . We use standard softmax cross entropy loss in the paper. L k box is box regression loss for human box and object box, the weighted sum of GIoU <ref type="bibr" target="#b27">[28]</ref> loss and L 1 loss are used. α and β are hyper-parameters of loss weights, which will be discussed later in ablation study.</p><p>We use Hungarian algorithm <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4]</ref> to solve the following problem to find a bipartite matching.</p><formula xml:id="formula_4">σ = arg min σ∈S N L cost<label>(4)</label></formula><p>where S N denotes the one-to-one matching solution space.</p><p>After the optimal one-to-one matching between the groundtruth and predictions is found, the network loss is calculated between the matched pairs, using the same loss function as Eq. 3. Although these two processes share the same formulation, the hyper-parameters of them are different theoretically and may have different optimal values. However, in practice, due to the considerable computation cost brought by large hyper-parameter search space, we made them the same, just as DETR <ref type="bibr" target="#b3">[4]</ref> does.</p><p>Different from conventional HOI detection methods which optimize object detector and interaction classifier separately, the proposed HOI matching loss takes both the classification and localization into account. Human and object boxes will be produced simultaneously with their interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setting</head><p>Datasets: We conduct experiments on HICO-DET <ref type="bibr" target="#b4">[5]</ref> and V-COCO <ref type="bibr" target="#b10">[11]</ref> benchmark to evaluate the proposed methods. HICO-DET consists of 47,776 images with more than 150K human-object pairs <ref type="bibr" target="#b37">(38,</ref><ref type="bibr">118</ref> images in training set and 9,658 in test set). It has 600 HOI categories over 117 interactions and 80 objects. Further, 600 HOI categories has been split into 138 Rare and 462 Non-Rare based on the number of training instances. V-COCO is a subset of MS-COCO <ref type="bibr" target="#b20">[21]</ref>, consists of 5,400 images in the trainval dataset and 4946 images in test set. Each human is annotated with binary labels for 29 different action categories (five of them do not involve associated objects). Evaluation Metric: Following the standard evaluation, we use the commonly used role mean average precision (mAP) to examine the model performance for both datasets. An HOI detection is considered as true positive if and only if it localizes the human and object accurately (i.e. the Interaction-over-Union (IOU) ratio between the predicted box and ground-truth is greater than 0.5) and predict the interaction correctly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>Data Augmentation: First, we adjust the brightness and contrast with a probability of 0.5 as image level augmenta-tion. Specifically, for both brightness and contract, a parameter is randomly chosen from the range [0.8, 1.2], meaning only slight change is performed to original image. Next, we use scale augmentation, scaling the input image such that the shortest side is at least 480 and at most 800 pixels while the longest at most 1333 <ref type="bibr" target="#b35">[36]</ref>. And also, we use random flip with a probability of 0.5. Finally, we apply random crop augmentations: an image is cropped with probability 0.5 to a random rectangular patch followed by another scale augmentation to ensure its shape, it is noteworthy that if any box in a given ground truth human-object pair is outside the cropped patch, its label will be removed.</p><p>Training Settings: The input image to the model is first scaled to [0, 1] and then normalized by channel-wise mean and std. The experiments are conducted on two popular backbone, ResNet-50 and ResNet-101. The models are trained with AdamW <ref type="bibr" target="#b22">[23]</ref> setting the transformer's learning rate to 1e-4, the backbone's to 1e-5, and weight decay to 1e-4. The number of encoder layer and decoder layer are both set to 6, the number of HOI query is set to 100, and we use a COCO pre-trained DETR <ref type="bibr" target="#b3">[4]</ref> model to initialize the weights of both backbone and transformer encoder-decoder. The batch size for ResNet-50 is set to <ref type="bibr" target="#b15">16</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparisons with State-of-the-Art methods</head><p>We report the main quantitative resutls in terms of AP on HICO-DET in <ref type="table">Table 1</ref> and AP role on V-COCO in <ref type="table" target="#tab_3">Table 2</ref>.</p><p>For the HICO-DET dataset, our method compares against state-of-the-art algorithms. We achieve 4.88% point gain over one-stage methods <ref type="bibr" target="#b18">[19]</ref> on Full categories, especially 5.37% point on Rare categories. Compared with those two-stage methods, our method achieves best performance on Full categories without introducing additional human pose feature and languages prior, which shows great potential of our method. Meanwhile, we humbly regard how to improve performance on Rare due to the long-tail distribution of HOI detection as a significant future work in our framework.</p><p>For the V-COCO dataset, our method also achieves the competitive performance compared with state-of-theart methods without introducing external data, e.g. human pose, HICO-DET data, language prior knowledge. We obtain 1.9% point gain over previous one-stage method <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>In our ablation study, we explore how the matching strategy, loss weight, and data augmentation influence the final performance.  <ref type="table">Table 1</ref>: Comparison with the state-of-the-art methods on HICO-DET test set. For the Detector, COCO means that the detector is trained on COCO, while HICO-DET means that the detector is first trained on COCO and then fine-tuned on HICO-DET. Pose means that human pose feature extracted by pre-trained skeleton model. Language means that languages prior, e.g. words2vec. Our methods achieve best performance on Full categories without introducing additional human pose feature, external language prior, and object detector.</p><p>ResNet-50 backbone models, and the models are trained for 250 epochs with once learning rate decay at epoch 200. The number of encoder layer and decoder layer are both set to 6, the number of HOI query is set to 100, and the batch size is set to 16. We use a COCO pre-trained DETR model to initialize the weights of both backbone and transformer encoder-decoder. Our baseline set all the loss weight hyper-parameters to 1.0, and uses only brightness, contrast and random flip augmentation. Matching Strategy: Consider the situation in <ref type="figure" target="#fig_1">Fig. 3(b)</ref>, black boxes indicate a ground truth pair, green boxes indicate a predicted pair, and red boxes indicate another predicted pair. For simplicity, we assume that both human and object boxes are in the right place and the object class category is correct. Then we can see, for the red pair, both human and object boxes have higher overlap than the black ones, but its interaction prediction 'hit' is wrong, note that the interaction label of ground truth is 'kick'. For the green pair, the interaction prediction 'kick' is right, while its human box is obviously far from the ground truth. So in this case, which pair to match with the ground truth is confused, location first or category first? We conduct ablation study to find the relative importance in matching. In Eq. 3, β 1 , β 2 dominates the weight of classification and localization respectively. As shown in <ref type="table">Table.</ref> 3b, the best result is obtained under β 1 = 2.0, β 2 = 1.0, which reflects that classification plays a more important role than localization during the matching process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Ablation:</head><p>The human/object/interaction are 2/81/117-category classification problems in HICO-DET. 'human' has sufficient training data because it appears in each HOI instance. Therefore, we assume human classification as the simplest one and set α h = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Pose Language AProle Two-stage methods VSRL <ref type="bibr" target="#b10">[11]</ref> ResNet-50-FPN 31.8 InteractNet <ref type="bibr" target="#b8">[9]</ref> ResNet-50-FPN 40.0 GPNN <ref type="bibr" target="#b26">[27]</ref> ResNet-101 44.0 RPNN <ref type="bibr" target="#b38">[39]</ref> ResNet50 47.5 VCL <ref type="bibr" target="#b13">[14]</ref> ResNet101 48.3 TIN * <ref type="bibr" target="#b17">[18]</ref> ResNet-50 48.7 Zhou et al. <ref type="bibr" target="#b39">[40]</ref> ResNet-50 48.9 PastaNet <ref type="bibr" target="#b16">[17]</ref> ResNet-50 51.0 DRG <ref type="bibr" target="#b5">[6]</ref> ResNet-50-FPN 51.0 VSGNet <ref type="bibr" target="#b30">[31]</ref> ResNet-152 51.8 CHG <ref type="bibr" target="#b33">[34]</ref> ResNet-50 52.7 PMFNet <ref type="bibr" target="#b32">[33]</ref> ResNet-50-FPN 52.0 PD-Net <ref type="bibr" target="#b37">[38]</ref> ResNet-152 52.6 FCMNet <ref type="bibr" target="#b21">[22]</ref> ResNet-50 53.1 ACP * <ref type="bibr" target="#b14">[15]</ref> ResNet-152 53.2 One-stage methods UnionDet <ref type="bibr" target="#b2">[3]</ref> ResNet-50-FPN 47.5 IPNet <ref type="bibr" target="#b34">[35]</ref> Hourglass-104 51.0 IPNet * <ref type="bibr" target="#b34">[35]</ref> Hourglass-104 52.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ours</head><p>ResNet-101 52.9 We conduct experiments to evaluate the relative importance of 'object' and 'interaction' in our experiments. In Eq. 3, α o and α r dominate the weight of object and interaction respectively in training loss. As shown in <ref type="table">Table.</ref> 3a, our method obtains best result when α r = 2.0 and α o = 1.0, indicating that interaction tends to be more important than object in our framework. Data Augmentation: We mainly study two kinds of data augmentation in our experiments: multi-scale training and random crop. We conduct ablation experiments on combination of them, results can be found in <ref type="table">Table.</ref> 3c. Considerable improvements have been made, multi-scale training attains 4.29% point gain on Full categories and random crop achieves 5.08%, and the combination of them gets even better results, mainly because these two augmentations help the attention layers to learn scale-invariant and shift-invariant features much easier on a small dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head><p>We formulate HOI detection as a set prediction problem and directly predict instances in an end-to-end manner. So, compared with previous methods, which are trained on HOI instance level, treating human-object pair as data point, our  method regards a whole image as data point. There seems some differences. Here we briefly discuss our understanding on two aspects. Image-level data point. First, the training loss for interaction in our method is the sum of classification loss between each matched (pred, GT) pair. It is relevant to the number of positive samples, which is consistent with prior instance-level work. Second, random crop augmentation provides sufficient training instances. For example, let's think random crop in an extreme way, each time only one positive sample is cropped, then it is close to prior work. <ref type="figure">Figure 5</ref>: Visualization of attention map in decoder for predicted HOI instance (images from HICO-DET dataset). As can be seen from the figure, our method attends to the discriminative part (e.g. telephone for talk on, human face for kiss) and can capture long-distance interaction (e.g. fly and pull). Moreover, it can be seen from the figure that different interaction categories share little common pattern, which suggested a unified definition for all interaction proposal may be sub-optimal.</p><p>Out of distribution test. Noting that in training data there are at most 3 'drink with bottle' HOIs in a single image, we create a synthetic image containing 6 'drink with bottle' HOIs, which is out of distribution. As shown in <ref type="figure">Fig. 4</ref>, the result indicates that the model can learn to recognize HOIs with good generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Analysis</head><p>As can be seen from <ref type="figure">Fig. 5</ref>, we visualize the decoder attention map for predicted HOI instances. The interaction heatmap highlights both the human and object area, meaning that our model reasons about the relations between human and object from a more global image context, not focusing on human or object only. It is obvious that decoder has ability to find the discriminative part for the interaction category. The model can predict different instances based on similar attention heatmap, which implies that, the MLP of the model have the ability to tell from fine-grained interaction features. Meanwhile, some local area with relatively higher attention may indicate the localization (boundaries) of human or object, because the visualized attention map is immediately followed by the MLP head for classification as well as regression. Moreover, it can be seen from the figure that different interaction categories share little common pattern, which suggests that the empirically unified definition of interaction proposal in one-stage methods, i.e. interaction point/box, is sub-optimal. And thanks to the large receptive field of attention layers, our model can easily handle long distance interaction, e.g. fly kite.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel HOI Transformer to directly predict the HOI instances in an end-to-end manner. Our core idea is to build a transformer encoder-decoder architecture to directly predict HOI instances, and a quintuple matching loss for HOI to enable supervision in a unified way. We validate the proposed method on two challenging HOI benchmark and achieve a considerable performance boost over state-of-the-art results. It is worth noting that our method not only abandon the additional features, but desert the complex post processing as well. Moreover, based on the attention map of the decoder, we found that our model has ability to dynamically attains the discerning feature for different HOI queries. We hope our method will be useful for human activity understanding research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Illustration of Inference Process</head><p>With parallel decoding <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b7">8]</ref>, the network can simultaneously output a group of N HOI instances. In training phase, the N output HOI instances first find an optimal one-to-one matching to the N ground truth, and then get the loss between matched pairs to optimize the network, as shown in the right part of <ref type="figure" target="#fig_0">Fig. 2</ref>. While in inference phase, no one-to-one matching is needed, the network directly outputs N HOI instances.</p><p>Regardless of the parallel decoding technology used in our architecture, the inference process can be comprehended in an auto-regressive way <ref type="bibr" target="#b29">[30]</ref>. A high level global memory full of rich context is first extracted by the encoder. At each time step, the decoder outputs a new HOI instance considering both the global feature and the previous detected HOI instances. Only the undetected HOI instances will be figured out at each time step. The most likely HOI candidates are encouraged to be produced at first.</p><p>As shown in <ref type="figure" target="#fig_3">Fig. 6</ref>, global memory from an image containing person ride/jump/straddle horse is first extracted. At time step 1, the most likely interaction 'jump horse' is produced. When it comes to time step 2, the model looks through the global memory and the previous output and find that 'jump horse' has already been predicted. So, it outputs the second most likely interaction 'ride horse'. It keeps producing HOI instances until there is a stop token, i.e. the confidence is lower than a predefined threshold, or has reached the maximum number.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Overall architecture. Our model first use a CNN backbone to extract visual feature from input image. Then the feature is reduced in channel-dimension, flatten in spatial-dimension and complemented by positional encoding. The transformer encoder generates the global memory feature based on K, Q, V . The transformer decoder transforms N learnt positional embeddings (denoted as HOI queries) into N output embeddings. Finally, the multi-layer perception (MLP) predict the quintuple HOI instance based on the embeddings. The HOI instances are directly output simultaneously.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of the matching strategy between HOI ground-truth (black) and prediction (other colors). An HOI instance is represented by a pair of boxes in the same color. h and o represent human and object respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Examples of decoder inference process. At each time step, the decoder outputs a new HOI instance considering both the global memory and the previously detected HOI instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>while 8 for ResNet-101. All the models are trained for 250 epochs with once learning rate decay at epoch 200. Training our network takes 7 hours on 8 NVIDIA 2080TI GPU on V-COCO and 70 hours on HICO-DET. At test times, our model runs at 24 fps on a single 2080TI GPU.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of the state-of-the-art on V-COCO test set. Pose denotes whether human skeleton feature has been introduced and Language denotes external-language prior. Character * indicates that HICO-DET training data was incorporated into training data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Ablation experiments for HOI Transformer. All</cell></row><row><cell>models use ResNet-50 backbone to extract feature; number</cell></row><row><cell>of queries is set to 100; batch size is set for 16; trained for</cell></row><row><cell>250 epochs; learning rate decay from 1e-4 to 1e-5 at epoch</cell></row><row><cell>200.</cell></row><row><cell>Figure 4: Out of distribution generalization for HOI predic-</cell></row><row><cell>tion. Even though no images in the training set has more</cell></row><row><cell>than 3 'drink with bottle' HOIs, our method generalize well</cell></row><row><cell>on the synthetic image with 6 of them.</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research is supported by China's 'scientific and technological innovation 2030 -major projects' (No. 2020AAA0104400).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Detecting human-object interactions via functional generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankan</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Sai Saketh Rambhatla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Uniondet: Union-level detector towards real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Bumsoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Choi</forename><surname>Taeho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Jaewoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Kim</forename><surname>Hyunwoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunfan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xieyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huayi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 ieee winter conference on applications of computer vision (wacv)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Drg: Dual relation graph for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="696" to="712" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">ican: Instancecentric attention network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10437</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mask-predict: Parallel decoding of conditional masked language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09324</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Detecting and recognizing human-object interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="8359" to="8367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02281</idno>
		<title level="m">Non-autoregressive neural machine translation</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nofrills human-object interaction detection: Factorization, layout encodings, and training techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9677" to="9685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.12407</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Detecting human-object interactions with action co-occurrence priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Jin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08728</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pastanet: Toward human activity knowledge engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Transferable interactiveness knowledge for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao-Shu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ppdm: Parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="482" to="490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Amplifying key cues for human-object-interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingchao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="248" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parallel wavenet: Fast high-fidelity speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Babuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Cobo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Stimberg</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05751</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">Image transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detecting unseen visual relations using analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Peyre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning human-object interactions by graph parsing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoxiong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scaling human-object interaction recognition through zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serena</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1568" to="1576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iftekhar</forename><surname>Ulutan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vsgnet</surname></persName>
		</author>
		<title level="m">Spatial attention network for detecting human object interactions using graph convolutions. arXiv: Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Pose-aware multi-level feature network for human object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Desen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Contextual heterogeneous graph network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Yingbiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10001</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning human-object interaction detection using interaction points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiancai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Detectron2</surname></persName>
		</author>
		<idno>2019. 5</idno>
		<ptr target="https://github.com/facebookresearch/detectron2" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to detect human-object interactions with knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Polysemy deciphering network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xubin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Relation parsing neural network for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Penghao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingmin</forename><surname>Chi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Cascaded human-object interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4263" to="4272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<title level="m">Objects as points</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
