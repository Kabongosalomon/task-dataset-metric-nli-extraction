<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Semi-Supervised Neural Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science and Technology of China</orgName>
								<address>
									<settlement>Hefei</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
							<email>cheneh@ustc.edu.cn2xuta</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
							<email>tyliu@microsoft.com</email>
						</author>
						<title level="a" type="main">Semi-Supervised Neural Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Neural architecture search (NAS) relies on a good controller to generate better architectures or predict the accuracy of given architectures. However, training the controller requires both abundant and high-quality pairs of architectures and their accuracy, while it is costly to evaluate an architecture and obtain its accuracy. In this paper, we propose SemiNAS, a semi-supervised NAS approach that leverages numerous unlabeled architectures (without evaluation and thus nearly no cost). Specifically, SemiNAS 1) trains an initial accuracy predictor with a small set of architecture-accuracy data pairs; 2) uses the trained accuracy predictor to predict the accuracy of large amount of architectures (without evaluation); and 3) adds the generated data pairs to the original data to further improve the predictor. The trained accuracy predictor can be applied to various NAS algorithms by predicting the accuracy of candidate architectures for them. SemiNAS has two advantages: 1) It reduces the computational cost under the same accuracy guarantee. On NASBench-101 benchmark dataset, it achieves comparable accuracy with gradientbased method while using only 1/7 architecture-accuracy pairs. 2) It achieves higher accuracy under the same computational cost. It achieves 94.02% test accuracy on NASBench-101, outperforming all the baselines when using the same number of architectures. On ImageNet, it achieves 23.5% top-1 error rate (under 600M FLOPS constraint) using 4 GPU-days for search. We further apply it to LJSpeech text to speech task and it achieves 97% intelligibility rate in the low-resource setting and 15% test error rate in the robustness setting, with 9%, 7% improvements over the baseline respectively. * The work was done when the first author was an intern at Microsoft Research Asia. 2 Although a variety of metrics including accuracy, model size, and inference speed have been used as search criterion, the accuracy of an architecture is the most important and costly one, and other metrics can be easily calculated with almost zero computation cost. Therefore, we focus on accuracy in this work. 34th</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural architecture search (NAS) for automatic architecture design has been successfully applied in several tasks including image classification and language modeling <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6]</ref>. NAS typically contains two components, a controller (also called generator) that controls the generation of new architectures, and an evaluator that trains candidate architectures and evaluates their accuracy 2 . The controller learns to generate relatively better architectures via a variety of techniques (e.g., reinforcement learning <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>, evolution <ref type="bibr" target="#b19">[20]</ref>, gradient optimization <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>, Bayesian optimization <ref type="bibr" target="#b38">[39]</ref>), and plays an important role in NAS <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b38">39]</ref>. To ensure the performance of the controller, a large number of high-quality pairs of architectures and their corresponding accuracy are required as the training data.</p><p>However, collecting such architecture-accuracy pairs is expensive, since it is costly for the evaluator to train each architecture to accurately get its accuracy, which incurs the highest computational cost in NAS. Popular methods usually consume hundreds to thousands of GPU days to discover eventually good architectures <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17]</ref>. To address this problem, one-shot NAS <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35]</ref> uses a supernet to include all candidate architectures via weight sharing and trains the supernet to reduce the training time. While greatly reducing the computational cost, the quality of the training data (architectures and their corresponding accuracy) for the controller is degraded <ref type="bibr" target="#b23">[24]</ref>, and thus these approaches suffer from accuracy decline on downstream tasks.</p><p>In various scenarios with limited labeled training data, semi-supervised learning <ref type="bibr" target="#b39">[40]</ref> is a popular approach to leverage unlabeled data to boost the training accuracy. In the scenario of NAS, unlabeled architectures can be obtained through random generation, mutation <ref type="bibr" target="#b19">[20]</ref>, or simply going through the whole search space <ref type="bibr" target="#b31">[32]</ref>, which incur nearly zero additional cost. Inspired by semi-supervised learning, in this paper, we propose SemiNAS, a semi-supervised approach for NAS that leverages a large number of unlabeled architectures. Specifically, SemiNAS 1) trains an initial accuracy predictor with a set of architecture-accuracy data pairs; 2) uses the trained accuracy predictor to predict the accuracy of a large number of unlabeled architectures; and 3) adds the generated architecture-accuracy pairs to the original data to further improve the accuracy predictor. The trained accuracy predictor can be incorporated to various NAS algorithms by predicting the accuracy of unseen architectures.</p><p>SemiNAS can be applied to many NAS algorithms. We take the neural architecture optimization (NAO) <ref type="bibr" target="#b16">[17]</ref> algorithm as an example, since NAO has the following advantages: 1) it takes architectureaccuracy pairs as training data to train a accuracy predictor to predict the accuracy of architectures, which can directly benefit from SemiNAS; 2) it supports both conventional methods which train each architecture from scratch <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17]</ref> and one-shot methods which train a supernet with weight sharing <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17]</ref>; and 3) it is based on gradient optimization which has shown better effectiveness and efficiency. Although we implement SemiNAS on NAO, it is easy to be applied to other NAS methods, such as reinforcement learning based methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b17">18]</ref> and evolutionary algorithm based methods <ref type="bibr" target="#b19">[20]</ref>.</p><p>SemiNAS shows advantages over both conventional NAS and one-shot NAS. Compared to conventional NAS, it can significantly reduce computational cost while achieving similar accuracy, and achieve better accuracy with similar cost. Specifically, on NASBench-101 benchmark, SemiNAS achieves similar accuracy (93.89%) as gradient based methods <ref type="bibr" target="#b16">[17]</ref> using only 1/7 architectures. Meanwhile it achieves 94.02% mean test accuracy surpassing all the baselines when evaluating the same number of architectures (with the same computational cost). Compared to one-shot NAS, SemiNAS achieves higher accuracy using similar computational cost. For image classification, within 4 GPU days for search, we achieve 23.5% top-1 error rate on ImageNet under the mobile setting. For text to speech (TTS), using 4 GPU days for search, SemiNAS achieves 97% intelligibility rate in the low-resource setting and 15% sentence error rate in the robustness setting, which outperforms human-designed model by 9 and 7 points respectively. To the best of our knowledge, we are the first to develop NAS algorithms on text to speech (TTS) task. We carefully design the search space and search metric for TTS, and achieve significant improvements compared to human-designed architectures. We believe that our designed search space and metric are helpful for future studies on NAS for TTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>From the perspective of the computational cost of training candidate architectures, previous works on NAS can be categorized into conventional NAS and one-shot NAS.</p><p>Conventional NAS includes <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17]</ref>, which achieve significant improvements on several benchmark datasets. Obtaining the accuracy of the candidate architectures is expensive in conventional NAS, since they train every single architecture from scratch and usually require thousands of architectures to train. The total cost is usually more than hundreds of GPU days <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17]</ref>.</p><p>To reduce the huge cost in NAS, one-shot NAS was proposed with the help of weight sharing mechanism. <ref type="bibr" target="#b1">[2]</ref> proposes to include all candidate operations in the search space within a supernet and share parameters among candidate architectures. Each candidate architecture is a sub-graph in the supernet and only activates the parameters associated with it. The algorithm trains the supernet and then evaluates the accuracy of candidate architectures by the corresponding sub-graphs in the supernet. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b8">9]</ref> also leverage the one-shot idea to perform efficient search while using different search algorithms. Such weight sharing mechanism successfully cuts down the computational cost to less than 10 GPU days <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref>. However, the supernet requires careful design and the training of supernet needs careful tuning. Moreover, it shows inferior performance and reproducibility compared to conventional NAS. One main cause is the short training time and inadequate update of individual architecture <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>, which leads to an inaccurate ranking of the architectures, and provides relatively low-quality architecture-accuracy pairs for the controller.</p><p>To sum up, there exists a trade-off between computational cost and accuracy. We formalize the computational cost of the evaluator by C = N × T , where N is the number of architecture-accuracy pairs for the controller to learn, and T is the training time of each candidate architecture. In conventional NAS, the evaluator trains each architecture from scratch and the T is typically several epochs 3 to ensure the accuracy of the evaluation, leading to large C. In one-shot NAS, the T is reduced to a few mini-batches, which is inadequate for training and therefore produces low-quality architecture-accuracy pairs. Our SemiNAS handles this computation and accuracy trade-off from a new perspective which reduces N by leveraging a large number of unlabeled architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SemiNAS</head><p>In this section, we first describe the semi-supervised training of the accuracy predictor, and then introduce the implementation of the proposed SemiNAS algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">The Semi-Supervised Training of the Accuracy Predictor</head><p>To learn from both labeled architecture-accuracy pairs and unlabeled architectures without corresponding accuracy numbers, SemiNAS trains an accuracy predictor via semi-supervised learning. Specifically, we utilize a large number of unevaluated architectures (M ) to improve the accuracy predictor. To utilize numerous unlabeled data, we leverage self-supervised learning by predicting the accuracy of unevaluated architectures <ref type="bibr" target="#b10">[11]</ref> and then combine them with ground-truth data to further improve the accuracy predictor. Following <ref type="bibr" target="#b33">[34]</ref>, we apply dropout as noise during the training.</p><p>However, a simple accuracy predictor is hard to learn information from architectures with pseudo labels via regression task although with techniques in <ref type="bibr" target="#b33">[34]</ref>. Inspired by <ref type="bibr" target="#b16">[17]</ref>, we use an accuracy predictor framework consisting of an encoder f e , a predictor f p and a decoder f d . The encoder is implemented as an LSTM network to map the discrete architecture x to continuous embedding representations e x , and the predictor uses fully connected layers to predict the accuracy of the architecture taking the continuous embedding e x as input. The decoder is an LSTM to decode the continuous embedding back to discrete architecture in an auto-regressive manner. The three components are trained jointly via the regression task and the reconstruction task. The semi-supervised learning of the accuracy predictor can be decomposed into 3 steps:</p><p>• Train the encoder f e , predictor f p and the decoder f d with N architecture-accuracy pairs where each architecture is trained and evaluated. • Generate M unlabeled architectures and use the trained encoder f e and predictor f p to predict their accuracy. • Use both the N architecture-accuracy pairs and the M self-labeled pairs together to train a better accuracy predictor.</p><p>The accuracy predictor learns information from limited number of architecture-accuracy pairs, while there are still numerous unseen architectures. With the help of the decoder, the encoder and the decoder together act like an autoencoder to learn the hidden representation of architectures. Therefore it is able for the whole framework to learn the information of architectures in an unsupervised way without the requirement of ground-truth labels (evaluated accuracy), and further improves the accuracy predictor as the three components are trained jointly. The trained accuracy predictor can be incorporated to various NAS algorithms by predicting the accuracy of unseen architectures for them.</p><p>SemiNAS brings advantages over both conventional NAS and one-shot NAS, which can be illustrated under the computational cost formulation C = N × T . Compared to conventional NAS which is costly, SemiNAS can reduce the computational cost C with smaller N but using more additional unlabeled architectures to avoid accuracy drop, and can also further improve the performance with same computational cost. Compared to one-shot NAS which has inferior accuracy, SemiNAS can improve the accuracy by using more unlabeled architectures under the same computational cost C. Specifically, in order to get more accurate evaluation of architectures and improve the quality of architecture-accuracy pairs, we can extend the average training time T for each individual architecture. Meanwhile, we reduce the number of architectures to be trained (i.e., N ) to keep the total budget C unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Implementation of SemiNAS</head><p>We now describe the implementation of our SemiNAS algorithm. We take NAO <ref type="bibr" target="#b16">[17]</ref> as our implementation since it has following advantages: 1) it contains an encoder-predictor-decoder framework, where the encoder and the predictor can predict the accuracy for large number of architectures without evaluation, and is straightforward to incorporate our method; 2) it performs architecture search by applying gradient ascent which has shown better effectiveness and efficiency; 3) it can incorporate both conventional NAS (whose evaluator trains each architecture from scratch) and one-shot NAS (whose evaluator builds a supernet to train all the architectures via weight sharing).</p><p>NAO <ref type="bibr" target="#b16">[17]</ref> uses an encoder-predictor-decoder framework as the controller, where the encoder f e maps the discrete architecture representation x into continuous representation e x = f e (x) and uses the predictor f p to predict its accuracyŷ = f p (e x ). Then it uses a decoder f d that is implemented based on a multi-layer LSTM to reconstruct the original discrete architecture from the continuous representation x = f d (e x ) in an auto-regressive manner.</p><p>After the controller is trained, for any given architecture x as the input, NAO moves its representation e x towards the direction of the gradient ascent of the accuracy prediction f p (e x ) to get a new and better continuous representation e x as follows: e x = e x + η ∂fp(ex) ∂ex , where η is a step size. e x can get higher prediction accuracy f p (e x ) after gradient ascent. Then it uses the decoder f d to decode e x into a new architecture x , which is supposed to be better than architecture x. The process of the architecture optimization is performed for L iterations, where newly generated architectures at the end of each iteration are added to the architecture pool for evaluation and further used to train the controller in the next iteration. Finally, the best performing architecture in the architecture pool is selected out as the final result. Step size η. 2: Generate N architectures. Use the evaluator to train each architecture for T steps (in conventional way or weight sharing way). 3: Evaluate the N architectures to obtain the accuracy and form the labeled dataset D. 4: for l = 1, · · · , L do 5:</p><p>Train fe, fp and f d jointly using D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>6:</head><p>Randomly generate M architectures and use fe and fp to predict their accuracy and forming datasetD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Set D = D D . 8:</p><p>Train fe, fp and f d using D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9:</head><p>Pick K architectures with top accuracy among D. For each architecture, obtain a better architecture by applying gradient ascent optimization with step size η.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>10:</head><p>Evaluate the newly generated architectures using the evaluator and add them to D. 11: end for 12: Output: The architecture in D with the best accuracy.</p><p>With the semi-supervised method proposed in Section 3.1, we propose our SemiNAS as shown in Alg. 1. First we train the encoder-predictor-decoder on limited number (N ) of architectureaccuracy pairs (line 5). Then we train the encoder-predictor-decoder with additional M unlabeled architectures (line <ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. Finally, we perform the step of generating new architectures as the same in <ref type="bibr" target="#b16">[17]</ref> (line9-10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussions</head><p>Although our SemiNAS is mainly implemented based on NAO in this paper, the key idea of utilizing the trained encoder f e and predictor f p to predict the accuracy of numerous unlabeled architectures can be extended to a variety of NAS methods. For reinforcement learning based algorithms <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b17">18]</ref> where the controller is usually an RNN model, we can predict the accuracy of the architectures generated by the RNN and take the predicted accuracy as the reward to train the controller. For evolution based methods <ref type="bibr" target="#b19">[20]</ref>, we can predict the accuracy of the architectures generated through mutation and crossover, and then take the predicted accuracy as the fitness of the generated architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Application to Image Classification</head><p>In this section, we demonstrate the effectiveness of SemiNAS on image classification tasks. We first conduct experiments on NASBench-101 <ref type="bibr" target="#b36">[37]</ref> and then on the commonly used large-scale ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">NASBench-101</head><p>Dataset NASBench-101 <ref type="bibr" target="#b36">[37]</ref> designs a cell-based search space following the common practice <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b14">15]</ref>. It includes 423, 624 CNN architectures and trains each architecture CIFAR-10 for 3 times. Querying the accuracy of an architecture from the dataset is equivalent to training and evaluating the architecture. We hope to discover comparable architectures with less computational cost or better architectures with comparable computational cost. Specifically, on this dataset, reducing the computational cost can be regarded as decreasing the number of queries.</p><p>Setup Both the encoder and the decoder consist of a single layer LSTM with a hidden size of 16, and the predictor is a three-layer fully connected network with hidden sizes of 16, 64, 1 respectively. We use Adam optimizer with a learning rate of 0.001. During search, only valid accuracy is used. After search, we report the mean test accuracy of the selected architecture over the 3 runs. We report two settings of SemiNAS. For the first setting, we use N = 100, M = 10000 and up-sample N labeled data by 100x (directly duplicate the labeled data). We generate 100 new architectures based on top K = 100 architectures following line 9 in Alg. 1 at each iteration and run for L = 2 iterations. The algorithm totally evaluates 100 + 100 × 2 = 300 architectures. For the second setting, we set N = 1100, M = 10000 and up-sample N labeled data by 10x. We generate 300 new architectures based on top K = 100 architectures at each iteration and run for L = 3 iterations. The algorithm totally queries 1100 + 300 × 3 = 2000 architectures. For comparison, we evaluate random search, regularized evolution (RE) <ref type="bibr" target="#b19">[20]</ref> and NAO as baselines, where RE is validated as the best-performing algorithm in the NASBench-101 publication. We limit the number of queries of the baselines to be 2000 for fair comparison. Particularly, we run NAO with two settings using 300 and 2000 architectures for better comparison considering our SemiNAS is mainly implemented based on NAO in this paper. Additionally, we also combine our semi-supervised trained accuracy predictor with RE and name it SemiNAS (RE) for comparison to show the potential of SemiNAS. All the experiments are conducted for 500 times and we report the averaged results. Since the best test accuracy in the dataset is 94.32% and several algorithms are reaching it, we also report test regret (gap to 94.32%) following the guide by <ref type="bibr" target="#b36">[37]</ref> and the ranking of the accuracy number among the whole dataset to better illustrate the improvements of our method.</p><p>Results All the results are listed in which is merely better than random search. This demonstrates that with the help of unlabeled data, SemiNAS indeed outperforms baselines when using the same number of labeled architectures, and can achieve similar performance while using much less resources. Further, SemiNAS (RE) achieves 93.97% which is on par with baseline RE while using only a half number of labeled architectures, and outperforms RE with 94.03% when using same number of labeled architectures <ref type="bibr">(2000)</ref>. This implies the potential of using semi-supervised learning in NAS for speeding up the search and applying to various NAS algorithms. We also conduct experiments to study the effect of different number of unlabeled architectures (M ) and up-sampling ratio in SemiNAS, and the results are in Section 7.2.  <ref type="table" target="#tab_0">Table 1</ref>: Performances of different NAS methods on NASBench-101 dataset. "#Queries" is the number of architecture-accuracy pairs queried from the dataset. "SD" is standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ImageNet</head><p>Previous experiments on NASBench-101 dataset verify the effectiveness and efficiency of SemiNAS in a well-controlled environment. We further evaluate our approach to the large-scale ImageNet dataset.</p><p>Search space We adopt a MobileNet-v2 <ref type="bibr" target="#b22">[23]</ref> based search space following ProxylessNAS <ref type="bibr" target="#b3">[4]</ref>. It consists of multiple stacked layers. We search the operation of each layer. Candidate operations include mobile inverted bottleneck convolution layers <ref type="bibr" target="#b22">[23]</ref> with various kernel sizes {3, 5, 7} and expansion ratios {3, 6}, as well as zero-out layer.</p><p>Setup We randomly sample 50, 000 images from the training data as valid set for architecture search. Since training ImageNet is too expensive, we adopt weight sharing mechanism <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b3">4]</ref> to perform one-shot search. We train the supernet on 4 GPUs for 20000 steps with a batch size of 128 per card. We set N = 100, M = 4000 and run the search process for L = 3 iterations. In each iteration, 100 new better architectures are generated based on top K = 100 architectures following line 9 in Alg. 1. The search runs for 1 day on 4 V100 GPUs. To fairly compare with other works, we limit the FLOPS of the discovered architecture to be less than 600M. The discovered architecture is trained for 300 epochs with a total batch size of 256. We use the SGD optimizer with an initial learning rate of 0.05 and a cosine learning rate schedule <ref type="bibr" target="#b15">[16]</ref>. More training details are in Section 7.1. For NAO, we use the open source code 4 and train it on the same search space used in this paper. In both SemiNAS and NAO, we train the supernet for 20000 steps at each iteration to keep the same cost while NAO uses larger N = 1000. For ProxylessNAS, since it also optimizes latency as additional target, for fair comparison, we use their open source code <ref type="bibr" target="#b4">5</ref> and rerun the search while optimizing accuracy without considering latency. We limit the FLOPS of discovered architecture to be less than 600M. We run all the experiments for 5 times.</p><p>Results From the results in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Application to Text to Speech</head><p>In this section, we further explore the application of SemiNAS to a new task: text to speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model/Method</head><p>Top-1 (%) Top-5 (%) Params (Million) FLOPS (Million) MobileNetV2 <ref type="bibr" target="#b22">[23]</ref> 25.3 -6.9 585 ShuffleNet 2× (v2) <ref type="bibr" target="#b37">[38]</ref> 25.  <ref type="table" target="#tab_2">Table 2</ref>: Performances of different methods on ImageNet. For fair comparison, we run NAO on the same search space used in this paper, and run ProxylessNAS by optimizing accuracy without latency.</p><p>Text to speech (TTS) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b20">21]</ref> is an import task aiming to synthesize intelligible and natural speech from text. The encoder-decoder based neural TTS <ref type="bibr" target="#b24">[25]</ref> has achieved significant improvements. However, due to the different modalities between the input (text) and the output (speech), popular TTS models are still complicated and require many human experiences when designing the model architecture. Moreover, unlike many other sequence learning tasks (e.g., neural machine translation) where the Transformer model <ref type="bibr" target="#b29">[30]</ref> is the dominate architecture, RNN based Tacotron <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b24">25]</ref>, CNN based Deep Voice <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19]</ref>, and Transformer based models <ref type="bibr" target="#b12">[13]</ref> show comparable accuracy in TTS, without one being exclusively better than others.</p><p>The complexity of the model architecture in TTS indicates great potential of NAS on this task. However, applying NAS on TTS task also has challenges, mainly in two aspects: 1) Current TTS model architectures are complicated, including many human designed components. It is difficult but important to design the network bone and the corresponding search space for NAS. 2) Unlike other tasks (e.g., image classification) whose evaluation is objective and automatic, the evaluation of a TTS model requires subject judgement and human evaluation in the loop (e.g., intelligibility rate for understandability and mean opinion score for naturalness). It is impractical to use human evaluation for thousands of architectures in NAS. Thus, it is difficult but also important to design a specific and appropriate objective metric as the reward of an architecture during the search process. Next, we design the search space and evaluation metric for NAS on TTS, and apply SemiNAS on two specific TTS settings: low-resource setting and robustness setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Settings</head><p>Search space After surveying the previous neural TTS models, we choose a multi-layer encoderdecoder based network as the network backbone for TTS. We search the operation of each layer of the encoder and the decoder. The search space includes 11 candidate operations in total: convolution layer with kernel size {1, 5, 9, 13, 17, 21, 25}, Transformer layer <ref type="bibr" target="#b12">[13]</ref> with number of heads of {2, 4, 8} and LSTM layer. Specifically, we use unidirectional LSTM layer, causal convolution layer, causal self-attention layer in the decoder to avoid seeing the information in future positions. Besides, every decoder layer is inserted with an additional encoder-decoder-attention layer to catch the relationship between the source and target sequence, where the dot-product multi-head attention in Transformer <ref type="bibr" target="#b29">[30]</ref> is adopted.</p><p>Evaluation metric It has been shown that the quality of the attention alignment between the encoder and decoder is an important influence factor on the quality of synthesized speech in previous works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref>, and misalignment can be observed for most mistakes (e.g., skipping and repeating). Accordingly, we consider the diagonal focus rate (DFR) of the attention map between the encoder and decoder as the metric of an architecture. DFR is defined as:</p><formula xml:id="formula_0">DF R = I i=1 ki+b o=ki−b Ao,i I i=1 O o=1 Ao,i ,</formula><p>where A ∈ R O×I denotes the attention map, I and O are the length of the source input sequence and the target output sequence, k = O I is the slope factor and b is the width of the diagonal area in the attention map. DFR measures how much attention lies in the diagonal area with width b in the attention matrix, and ranges in [0, 1] which is the larger the better. In addition, we have also tried valid loss as the search metric, but it is inferior to DFR according to our preliminary experiments.</p><p>Task setting Current TTS systems are capable of achieving near human-parity quality when trained on adequate data and tested on regular sentences <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b12">13]</ref>. However, current TTS models have poor performance on two specific TTS settings: 1) low-resource setting, where only few paired speech and text data is available. 2) Robustness setting, where the test sentences are not regular (e.g., too short, too long, or contain many word pieces that have the same pronunciations). Under these two settings, the synthesized speech of a human-designed TTS model is usually not accurate and robust (i.e., some words are skipped or repeated). Thus we apply SemiNAS on these two settings to improve the accuracy and robustness. We conduct experiments on the LJSpeech dataset <ref type="bibr" target="#b9">[10]</ref> which contains 13100 text and speech data pairs with approximately 24 hours of speech audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results on Low-Resource Setting</head><p>Setup To simulate the low-resource scenario, we randomly split out 1500 paired speech and text samples as the training set, where the total audio length is less than 3 hours. We use N = 100, M = 4000, T = 3. We adopt the weight sharing mechanism and train the supernet on 4 GPUs for 1000 epochs. The search runs for 1 day on 4 P40 GPUs. Besides, we train vanilla NAO as a baseline where N = 1000. The discovered architecture is trained on the training set for 80k steps on 4 GPUs, with a batch size of 30K speech frames on each GPU. More details are provided in Section 7.1. In the inference process, the output mel-spectrograms are transformed into audio samples using Griffin-Lim <ref type="bibr" target="#b7">[8]</ref>. We run all the experiments for 5 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model/Method</head><p>Intelligibility Rate (%) DFR (%)</p><p>Transformer TTS <ref type="bibr" target="#b12">[13]</ref> 88 86 NAO <ref type="bibr" target="#b16">[17]</ref> 94 88 SemiNAS 97 90 <ref type="table">Table 3</ref>: Results on LJSpeech under the low-resource setting. "DFR" is diagonal focus rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We test the performance of SemiNAS, NAO <ref type="bibr" target="#b16">[17]</ref> and Transformer TTS (following <ref type="bibr" target="#b12">[13]</ref>) on the 100 test sentences and report the results in <ref type="table">Table 3</ref>. We measure the performances in terms of word level intelligibility rate (IR), which is a commonly used metric to evaluate the quality of generated audio <ref type="bibr" target="#b21">[22]</ref>. IR is defined as the percentage of test words whose pronunciation is considered to be correct and clear by human. It is shown that SemiNAS achieves 97% IR, with significant improvements of 9 points over human designed Transformer TTS and 3 points over NAO. We also list the DFR metric for each method in <ref type="table">Table 3</ref>, where SemiNAS outperforms Transformer TTS and NAO in terms of DFR, which is consistent with the results on IR and indicates that our proposed search metric DFR can indeed guide NAS algorithms to achieve better accuracy. We also use MOS (mean opinion score) <ref type="bibr" target="#b27">[28]</ref> to evaluate the naturalness of the synthesized speech. Using Griffin-Lim as the vocoder to synthesize the speech, the ground-truth mel-spectrograms achieves 3.26 MOS, Transformer TTS achieves 2.25, NAO achieves 2.60 and SemiNAS achieves 2.66. SemiNAS outperforms other methods in terms of MOS, which also demonstrates the advantages of SemiNAS. We also attach the discovered architecture by SemiNAS in Section 7.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results on Robustness Setting</head><p>Setup We train on the whole LJSpeech dataset as the training data. For robustness test, we select the 100 sentences as used in <ref type="bibr" target="#b18">[19]</ref> (attached in Section 7.4) that are found hard for TTS models. Training details follow the same as in the low-resource TTS experiment. We also attach the discovered architecture in Section 7.3. We run all the experiments for 5 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>We report the results in  <ref type="table" target="#tab_4">Table 4</ref>: Robustness test on the 100 hard sentences. "DFR" stands for diagonal focus rate.</p><p>a repeating or skipping word. SemiNAS is better than Transformer TTS <ref type="bibr" target="#b12">[13]</ref> and NAO <ref type="bibr" target="#b16">[17]</ref> on all the metrics. It reduces the error rate by 7% and 4% compared to Transformer TTS structure designed by human experts and the searched architecture by NAO respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>High-quality architecture-accuracy pairs are critical to NAS; however, accurately evaluating the accuracy of an architecture is costly. In this paper, we proposed SemiNAS, a semi-supervised learning method for NAS. It leverages a small set of high-quality architecture-accuracy pairs to train an initial accuracy predictor, and then utilizes a large number of unlabeled architectures to further improve the accuracy predictor. Experiments on image classification tasks (NASBench-101 and ImageNet) and text to speech tasks (the low-resource setting and robustness setting) demonstrate 1) the efficiency of SemiNAS on reducing the computation cost over conventional NAS while achieving similar accuracy and 2) its effectiveness on improving the accuracy of both conventional NAS and one-shot NAS under similar computational cost. In the future, we will apply SemiNAS to more tasks such as automatic speech recognition, text summarization, etc. Furthermore, we will explore advanced semi-supervised learning methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b2">3]</ref> to improve SemiNAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>This work focuses on neural architecture search. It has the following potential positive impact in the society: 1) Improve the performance of neural networks for better applications. 2) Reduce the human efforts in designing neural architectures. At the same time, it may have some negative consequences because architecture search may cost many resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.3">TTS</head><p>We adopt the weight sharing mechanism for search and train the supernet on 4 GPUs. The discovered architecture is trained on the training set for 80k steps on 4 GPUs, with a batch size of 30K speech frames on each GPU. We use the Adam optimizer with β 1 = 0.9, β 2 = 0.98, = 1e − 9 and follow the same learning rate schedule in <ref type="bibr" target="#b12">[13]</ref> with 4000 warmup steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Study of SemiNAS</head><p>In this section, we conduct experiments on NASBench-101 to study SemiNAS, including the number of unlabeled architectures M and the up-sampling ratio of labeled architectures.  <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. Notice that M = 0 is equivalent to NAO without using any additional evaluated architectures. We can see that the test accuracy increases as M increases, indicating that utilizing unlabeled architectures indeed helps the training of the controller and generating better architectures.</p><p>Up-sampling ratio Since N is much smaller than M , we do up-sampling to balance the data. We study how the up-sampling ratio affects the effectiveness of SemiNAS on NASBench-101. We set N = 100, M = 10000 and range the up-sampling ratio in {1, 2, 5, 10, 20, 50, 100} where 1 means no up-sampling. The results are depicted in <ref type="figure" target="#fig_0">Figure 1(b)</ref>. We can see that the final accuracy would benefit from up-sampling but will not continue to improve when the ratio is high (e.g., larger than 10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Discovered Architectures</head><p>We show the discovered architectures for the tasks by SemiNAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.1">ImageNet</head><p>We adopt the ProxylessNAS <ref type="bibr" target="#b3">[4]</ref>  where MBConv is mobile inverted bottleneck convolution, k is the kernel size and r is the expansion ratio <ref type="bibr" target="#b22">[23]</ref>. Our discovered architecture for ImageNet is depicted in <ref type="figure">Fig. 2</ref> Figure 2: Architecture for ImageNet discovered by SemiNAS. "MBConv3" and "MBConv6" denote mobile inverted bottleneck convolution layer with an expansion ratio of 3 and 6 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.2">TTS</head><p>We adopt encoder-decoder based architecture as the backbone, and search the operation of each layer. Candidate operations include:</p><p>• Convolution layer with kernel size of 1</p><p>• Convolution layer with kernel size of 5</p><p>• Convolution layer with kernel size of 9</p><p>• Convolution layer with kernel size of 13</p><p>• Convolution layer with kernel size of 17</p><p>• Convolution layer with kernel size of 21</p><p>• Convolution layer with kernel size of 25</p><p>• Transformer layer with head number of 2</p><p>• Transformer layer with head number of 4</p><p>• Transformer layer with head number of 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>• LSTM layer</head><p>Low-Resource Setting The discovered architecture by SemiNAS for low-resource setting is shown in <ref type="figure" target="#fig_2">Fig. 3</ref> Robustness Setting The discovered architecture by SemiNAS for robustness setting is shown in <ref type="figure" target="#fig_3">Fig. 4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Robustness Test Sentences</head><p>We list the 100 sentences we use for robustness setting: a b c.</p><p>x y z. hurry. warehouse. referendum. is it free? justifiable. environment.   nine adam baye study on the two pieces. an unfriendly decay conveys the outcome. abstraction is often one floor above you. a played lady ranks any publicized preview. he told us a very exciting adventure story. on august twenty eight mary plays the piano. into a controller beams a concrete terrorist. i often see the time eleven eleven on clocks. it was getting dark and we weren't there yet. against every rhyme starves a choral apparatus. everyone was busy so i went to the movie alone. i checked to make sure that he was still alive. a dominant vegetarian shies away from the g o p. joe made the sugar cookies susan decorated them. i want to buy a onesie but know it won't suit me. a former override of q w e r t y outside the pope. f b i says that c i a says i'll stay way from it. any climbing dish listens to a cumbersome formula. she wrote him a long letter but he didn't read it. dear beauty is in the heat not physical i love you. an appeal on january fifth duplicates a sharp queen. a farewell solos on march twenty third shakes north. he ran out of money so he had to stop playing poker. for example a newspaper has only regional distribution t. i currently have four windows open up and i don't know why. next to my indirect vocal declines every unbearable academic.</p><p>opposite her sounding bag is a m c's configured thoroughfare. from april eighth to the present i only smoke four cigarettes. i will never be this young again every oh damn i just got older. a generous continuum of amazon dot com is the conflicting worker. she advised him to come back at once the wife lectures the blast. a song can make or ruin a person's day if they let it get to them. she did not cheat on the test for it was not the right thing to do. he said he was not there yesterday however many people saw him there. should we start class now or should we wait for everyone to get here? if purple people eaters are real where do they find purple people to eat? on november eighteenth eighteen twenty one a glittering gem is not enough. a rocket from space x interacts with the individual beneath the soft flaw. malls are great places to shop i can find everything i need under one roof. i think i will buy the red car or i will lease the blue one the faith nests. italy is my favorite country in fact i plan to spend two weeks there next year. i would have gotten w w w w dot google dot com but my attendance wasn't good enough. nineteen twenty is when we are unique together until we realise we are all the same. my mum tries to be cool by saying h t t p colon slash slash w w w b a i d u dot com. he turned in the research paper on friday otherwise he emailed a s d f at yahoo dot org. she works two jobs to make ends meet at least that was her reason for no having time to join us. a remarkable well promotes the alphabet into the adjusted luck the dress dodges across my assault. a b c d e f g h i j k l m n o p q r s t u v w x y z one two three four five six seven eight nine ten. across the waste persists the wrong pacifier the washed passenger parades under the incorrect computer. if the easter bunny and the tooth fairy had babies would they take your teeth and leave chocolate for you? sometimes all you need to do is completely make an ass of yourself and laugh it off to realise that life isn't so bad after all. she borrowed the book from him many years ago and hasn't yet returned it why won't the distinguishing love jump with the juvenile? last friday in three week's time i saw a spotted striped blue worm shake hands with a legless lizard the lake is a long way from here. i was very proud of my nickname throughout high school but today i couldn't be any different to what my nickname was the metal lusts the ranging captain charters the link. i am happy to take your donation any amount will be greatly appreciated the waves were crashing on the shore it was a lovely sight the paradox sticks this bowl on top of a spontaneous tea. a purple pig and a green donkey flew a kite in the middle of the night and ended up sunburn the contained error poses as a logical target the divorce attacks near a missing doom the opera fines the daily examiner into a murderer. as the most famous singer-songwriter jay chou gave a perfect performance in beijing on may twenty fourth twenty fifth and twenty sixth twenty three all the fans thought highly of him and took pride in him all the tickets were sold out. if you like tuna and tomato sauce try combining the two it's really not as bad as it sounds the body may perhaps compensates for the loss of a true metaphysics the clock within this blog and the clock on my laptop are on hour different from each other. someone i know recently combined maple syrup and buttered popcorn thinking it would taste like caramel popcorn it didn't and they don't recommend anyone else do it either the gentleman marches around the principal the divorce attacks near a missing doom the color misprints a circular worry across the controversy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Demo of TTS</head><p>We provide demo for both low-resource setting and robustness setting of TTS experiments. Specifically, we provide 10 test cases for each setting respectively and provide their ground-truth audio (if exist), generated audio by Transformer TTS and generated audio by SemiNAS. We provide the demo at this link <ref type="bibr" target="#b5">6</ref> which is a web page and one can directly listen to the audio samples.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1</head><label>1</label><figDesc>Semi-Supervised Neural Architecture Search 1: Input: Number of architectures N to evaluate. Number of unlabeled architectures M to use. The set of architecture-accuracy pairs D = ∅ to train the encoder-predictor-decoder. Number of architectures K based on which to generate better architectures. Training steps T to evaluate each architecture. Number of optimization iterations L.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Study of SemiNAS on NASBench-101. (a): Performances with different M . (b): Performances with different up-sampling ratios. Number of unlabeled architectures M We study the effect of different M on SemiNAS. Given N = 100, we range M within {0, 100, 200, 500, 1000, 2000, 5000, 10000}, and plot the results in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Architecture for low-resource setting discovered by SemiNAS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Architecture for robustness setting discovered by SemiNAS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Random search achieves 93.64% test accuracy with a confidence interval of [93.61%, 93.67%] (alpha=99%). This implies that even 0.1% is a significant difference and there exists a large margin for improvement. We can see that, when using the same number of architectures-accuracy pairs (2000), SemiNAS outperforms all the baselines with 94.02% test accuracy and corresponding 0.30% test regret, which ranks top 43 in the whole space. SemiNAS with only 300 architectures achieves 93.89% test accuracy and 0.63% test regret which is on par with NAO with 2000 architectures. Moreover, NAO using 300 architectures only achieves 93.69%</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 ,</head><label>2</label><figDesc>SemiNAS achieves 23.5% top-1 test error rate on ImageNet under the 600M FLOPS constraint, which outperforms all the other NAS works. Specifically, it significantly outperforms the baseline algorithm NAO based on which SemiNAS is mainly implemented by 1.0%, and outperforms ProxylessNAS where our search space is based on by 0.5%. The discovered architecture is depicted in Section 7.3.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>, including the DFR, the number of sentences with repeating and skipping words, and the sentence level error rate. A sentence is counted as an error if it contains</figDesc><table><row><cell>Model/Method</cell><cell cols="4">DFR (%) Repeat Skip Error (%)</cell></row><row><cell>Transformer TTS[13]</cell><cell>15</cell><cell>1</cell><cell>21</cell><cell>22</cell></row><row><cell>NAO [17]</cell><cell>25</cell><cell>2</cell><cell>18</cell><cell>19</cell></row><row><cell>SemiNAS</cell><cell>30</cell><cell>2</cell><cell>14</cell><cell>15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>search space which is built on the MobileNet-V2<ref type="bibr" target="#b22">[23]</ref> backbone. It contains several different stages and each stage consists of multiple layers. We search the operation of each individual layer. There are 7 candidate operations in the search space:</figDesc><table><row><cell>• zero-out layer</cell></row><row><cell>• MBConv (k=3, r=3)</cell></row><row><cell>• MBConv (k=3, r=6)</cell></row><row><cell>• MBConv (k=5, r=3)</cell></row><row><cell>• MBConv (k=5, r=6)</cell></row><row><cell>• MBConv (k=7, r=3)</cell></row><row><cell>• MBConv (k=7, r=6)</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">One epoch means training on the whole dataset for once.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/renqianluo/NAO_pytorch 5 https://github.com/mit-han-lab/proxylessnas</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://speechresearch.github.io/seminas</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the first setting where N = 100, M = 10000, 100 new architectures are generated based on top K = 100 architectures at each iteration. In the second setting where N = 1100, M = 10000, 300 new architectures are generated based on top K = 100 architectures at each iteration. We use λ = 0.8 as the trade-off parameter to balance the regression loss and the reconstruction loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">ImageNet</head><p>We build the supernet following <ref type="bibr" target="#b3">[4]</ref>. We train the supernet on 4 GPUs for 20000 steps with a batch size of 128 per card. We use SGD optimizer with a learning rate of 1.6 and decay the learning rate by a factor of 0.97 per epoch. The discovered architecture is trained on 4 P40 cards for 300 epochs with a batch size of 64 per card. We use the SGD optimizer with an initial learning rate of 0.05 and a cosine learning rate schedule <ref type="bibr" target="#b15">[16]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep voice: Realtime neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Sercan Ö Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Chrzanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongguo</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="195" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Understanding and simplifying one-shot architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="549" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Mixmatch: A holistic approach to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Progressive differentiable architecture search: Bridging the depth gap between search and evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12760</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7036" to="7045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep voice 2: Multi-speaker neural text-to-speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sercan</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kainan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2962" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Signal estimation from modified short-time fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="236" to="243" />
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Single path one-shot neural architecture search with uniform sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zichao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyuan</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00420</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The lj speech dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Ito</surname></persName>
		</author>
		<ptr target="https://keithito.com/LJ-Speech-Dataset/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on challenges in representation learning, ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.07638</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural speech synthesis with transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naihan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shujie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6706" to="6713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.00559</idno>
		<title level="m">Progressive neural architecture search</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.09055</idno>
		<title level="m">Darts: Differentiable architecture search</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renqian</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.07233</idno>
		<title level="m">Neural architecture optimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melody</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4092" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep voice 3: Scaling text-to-speech with convolutional sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kainan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gibiansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sercan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Raiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07654</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.01548</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fastspeech: Fast, robust and controllable text to speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangjun</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3165" to="3174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Almost unsupervised text to speech and automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.06791</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.08142</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Natural tts synthesis by conditioning wavenet on mel spectrogram predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rj</forename><surname>Skerrv-Ryan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4779" to="4783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The evolved transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5877" to="5886" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Single-path nas: Designing hardware-efficient convnets in less than 4 hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Stamoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhou</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Lymberopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodhi</forename><surname>Priyantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Marculescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.02877</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Mean opinion score (mos) revisited: methods and applications, limitations and alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Streijl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">S</forename><surname>Winkler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hands</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Systems</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="213" to="227" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daisy</forename><surname>Skerry-Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Stanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongheng</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10135</idno>
		<title level="m">Towards end-to-end speech synthesis</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.00848</idno>
		<title level="m">Neural predictor for neural architecture search</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Snas: Stochastic neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hehui</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pc-darts: Partial channel connections for memory-efficient differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongkai</forename><surname>Xiong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">NAS-bench-101: Towards reproducible neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<editor>Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="9" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Bayesnas: A bayesian approach for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongpeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04919</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Introduction to semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis lectures on artificial intelligence and machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="130" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01578</idno>
		<title level="m">Neural architecture search with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning transferable architectures for scalable image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8697" to="8710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">We implement the core architecture search algorithm following NAO [17] 7 . For downstream tasks, we implement the code following corresponding baselines. For ImageNet experiment, we build our code based on ProxylessNAS implementation 8 . For TTS experiment</title>
	</analytic>
	<monogr>
		<title level="m">with version 1.2</title>
		<imprint/>
	</monogr>
	<note>Implementation Details We implement all the code in Pytorch. we build the code following Transformer TTS [13] which is originally in Tensorflow</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
