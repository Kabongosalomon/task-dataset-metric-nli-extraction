<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Good Features to Correlate for Visual Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhan</forename><surname>Gundogdu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">A</forename><surname>Aydın Alatan</surname></persName>
						</author>
						<title level="a" type="main">IEEE TRANSACTIONS ON IMAGE PROCESSING 1 Good Features to Correlate for Visual Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TIP.2018.2806280</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-visual tracking</term>
					<term>correlation filters</term>
					<term>deep feature learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>During the recent years, correlation filters have shown dominant and spectacular results for visual object tracking. The types of the features that are employed in these family of trackers significantly affect the performance of visual tracking. The ultimate goal is to utilize robust features invariant to any kind of appearance change of the object, while predicting the object location as properly as in the case of no appearance change. As the deep learning based methods have emerged, the study of learning features for specific tasks has accelerated. For instance, discriminative visual tracking methods based on deep architectures have been studied with promising performance. Nevertheless, correlation filter based (CFB) trackers confine themselves to use the pre-trained networks which are trained for object classification problem. To this end, in this manuscript the problem of learning deep fully convolutional features for the CFB visual tracking is formulated. In order to learn the proposed model, a novel and efficient backpropagation algorithm is presented based on the loss function of the network. The proposed learning framework enables the network model to be flexible for a custom design. Moreover, it alleviates the dependency on the network trained for classification. Extensive performance analysis shows the efficacy of the proposed custom design in the CFB tracking framework. By fine-tuning the convolutional parts of a state-of-the-art network and integrating this model to a CFB tracker, which is the top performing one of VOT2016, 18% increase is achieved in terms of expected average overlap, and tracking failures are decreased by 25%, while maintaining the superiority over the state-of-the-art methods in OTB-2013 and OTB-2015 tracking datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>One of the major problems in computer vision is single object visual tracking, which has potential applications including visual surveillance, security and defense applications and human computer interaction. Although the definition of this problem varies according to the application and the type of the target object, it can be described as tracking an object, which is marked by the user at the beginning of a video sequence. Tracking is accomplished by predicting the state of the object at each frame. The benchmark datasets <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b2">[2]</ref>, which <ref type="bibr">E</ref> are useful tools to assess the performances of the tracking algorithms, define the ground truth object state as the bounding box surrounding the object in the image domain. Thus, if there is more overlap between the prediction and the ground truth bounding box, more accurate localization of the target is obtained. In order to improve the accuracy of the tracking, various machine learning concepts have been borrowed, such as sparse generative methods <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref>, support vector machines <ref type="bibr" target="#b7">[5]</ref> and deep learning <ref type="bibr" target="#b8">[6]</ref>, <ref type="bibr" target="#b9">[7]</ref>. During the last decade, a substantial amount of effort has been put on the correlation filter based (CFB) trackers, while the pioneering study of Bolme et.al. <ref type="bibr" target="#b10">[8]</ref> has triggered the use of correlation filters for visual tracking. Concretely, the attractive participants and the winners of the Visual Object Tracking (VOT) challenges are from the CFB tracking family in the last three years. This being the case, various improvements over the base correlation filter formulation are frequently proposed to enhance the accuracy of the tracking. The best performing trackers of VOT2015 <ref type="bibr" target="#b11">[9]</ref> and VOT2016 <ref type="bibr" target="#b12">[10]</ref> challenges utilize the pre-trained deep features <ref type="bibr" target="#b13">[11]</ref> specifically trained on the large scale image recognition datasets <ref type="bibr" target="#b14">[12]</ref>, <ref type="bibr" target="#b15">[13]</ref> for object classification. In order to employ these networks to the CFB frameworks, only convolutional layers are utilized, since the shift invariance property is intended to be maintained due to the nature of the correlation operation. The correlation capability of the features are limited to the classification network, which will hopefully generate good features to correlate. Nevertheless, learning deep convolutional features for CFB tracking cost function is still unexplored.</p><p>In order to break the limits of the aforementioned models of object recognition, we address the problem of learning a arXiv:1704.06326v2 [cs.CV] 10 Mar 2018 fully convolutional neural network which generates useful feature maps for correlation operation. The proposed framework consists of a single fully convolutional network. Training of the model is performed by propagating two image patches, which contain the same visual object, through the model. Once the feature maps of the top layer are obtained for each image patch, the correlation filter is calculated from the template patch, and the difference between the estimated and desired correlation response is to be minimized. The reduction of the difference between these two signals is obtained by the backpropagation of the error and the stochastic gradient descent procedure as in the case of training a classification based CNN architecture.</p><p>In this study, our contributions are summarized as follows:</p><p>• A framework to train a fully convolutional deep network is presented for the correlation filter cost function. <ref type="bibr">•</ref> We provide an efficient formulation to backpropagate the network according to the CFB loss. • In order to efficiently train networks with large number of feature channels at their final layer, we propose to include an auxiliary layer with few number of feature channels as the final layer of the network. • The network trained on the dataset generated for our specific scenario is integrated into the CFB tracking methods DSST <ref type="bibr" target="#b17">[14]</ref>, SAMF <ref type="bibr" target="#b18">[15]</ref> and CCOT <ref type="bibr" target="#b20">[16]</ref>. This significantly boosts the performance of the integrated trackers in benchmark datasets, VOT2016, OTB-2013 and OTB-2015 1 . In the remaining part of the manuscript, we first present the closely related work to our visual tracking framework in Section II. The ultimate goal is to obtain robust feature maps to be employed in CFB trackers. Hence, the CFB formulation is explained in Section III. In Section IV, the proposed feature learning method is given with detailed derivations. Section V reports the experimental results as well as the implementation details and dataset generation. Finally, Section VI discusses conclusive remarks about the proposed methodology and promising feature work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Numerous methods have been proposed to solve the visual tracking problem for decades. In this section, our aim is to give a literature survey as comprehensive and recent as possible to link the proposed method and the state-of-the-art trackers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Discriminative Trackers</head><p>Discriminative methods utilize a classifier model, which is responsible for the classification of a visual sample as either the object or background. Model training is performed by collecting positive and negative examples from the region of interest that is provided at the beginning of the tracking. The object localization is generally performed by looking for the candidate location with the highest classifier score. The Haar-like features <ref type="bibr" target="#b21">[17]</ref> and/or Local Binary Patterns <ref type="bibr" target="#b22">[18]</ref> are employed in <ref type="bibr" target="#b23">[19]</ref> and <ref type="bibr" target="#b24">[20]</ref> to train an ensemble of classifiers, since these features can be efficiently extracted by integral images. Support Vector Machines are employed in <ref type="bibr" target="#b7">[5]</ref> and further improved in <ref type="bibr" target="#b25">[21]</ref>, <ref type="bibr" target="#b26">[22]</ref>. Deep learning based track-byclassification methods are also studied in the works, such as <ref type="bibr" target="#b9">[7]</ref> and <ref type="bibr" target="#b8">[6]</ref>. Nevertheless, discriminative methods must evaluate their classifiers at each candidate location, thus bringing a significant computational load.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Generative Methods</head><p>Unlike the discriminative tracking approaches, generative methods describe appearance model for the object and optionally for the background. The object location is estimated as the one which contains the test instance with the most similarity to the appearance model. The model is updated with the object instance gathered from the predicted location. The study in <ref type="bibr" target="#b27">[23]</ref> proposes an online subspace learning method. Another method in <ref type="bibr" target="#b28">[24]</ref> models the object appearance in terms of the brightness histogram of the object patch. On the other hand, sparse visual trackers are proposed in <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b30">[25]</ref> and <ref type="bibr" target="#b31">[26]</ref>, which mainly obtain a sparse projection of the object instances with respect to a dictionary consisting of the object templates. In <ref type="bibr" target="#b32">[27]</ref> and <ref type="bibr" target="#b33">[28]</ref>, a joint sparsity constraint is forced in such a way that the resulting sparse coefficients are not only sparse themselves, but also their usage for different samples are sparse as well. Non-negative matrix factorization is also casted to the visual tracking problem in <ref type="bibr" target="#b34">[29]</ref> to learn a nonnegative dictionary. Generative methods suffer from the same problem, the evaluation of the objectness at each candidate location, as the discriminative trackers do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Correlation Filter Based Trackers</head><p>Correlation filters have become popular by the pioneering study in <ref type="bibr" target="#b10">[8]</ref>, which mainly attempts to minimize the sum of squared error between the desired correlation response and the circular correlation of the filter and the object patch. Utilizing the Convolution Theorem and properties of Fast Fourier Transform, the minimization of correlation filter cost is efficiently computed in the frequency domain. The work in <ref type="bibr" target="#b17">[14]</ref> extends <ref type="bibr" target="#b10">[8]</ref> by formulating the multi-channel support and employing HOG feature maps. In addition, the method in <ref type="bibr" target="#b17">[14]</ref> has a multi scale search support to estimate the scale of the object and to increase the tracking performance.</p><p>Kernelized correlation filters (KCF) are proposed in <ref type="bibr" target="#b35">[30]</ref>. <ref type="bibr" target="#b36">[31]</ref> generalizes <ref type="bibr" target="#b35">[30]</ref> for multi-channel support. Various extensions of KCF is proposed in <ref type="bibr" target="#b37">[32]</ref>, <ref type="bibr" target="#b38">[33]</ref> and <ref type="bibr" target="#b39">[34]</ref> for scale estimation as well as part-based proposal combinations. The imperfect training example issue is addressed in <ref type="bibr" target="#b40">[35]</ref> and <ref type="bibr" target="#b41">[36]</ref> by applying a spatial regularization on the corelation filter to increase the search range. In <ref type="bibr" target="#b42">[37]</ref>, spatial constraints are forced on the correlation filter. The study in <ref type="bibr" target="#b43">[38]</ref> simultaneously learns the correlation filter and the desired correlation response to circumvent the problems of circularly shifted training patches. To distinguish the object from the background, color statistics are used in <ref type="bibr" target="#b44">[39]</ref> as a complementary model to correlation filters. On the other hand, the tracker in <ref type="bibr" target="#b45">[40]</ref> learns the correlation filter by considering the background patches surrounding the target object, and achieves a good trade-off between computational complexity and accuracy.</p><p>Pre-trained deep CNN models are utilized in <ref type="bibr" target="#b46">[41]</ref> and <ref type="bibr" target="#b47">[42]</ref> as the feature maps to correlate. Moreover, the method in <ref type="bibr" target="#b20">[16]</ref> presents a continuous domain correlation filter learning to address the utilization of feature maps with different resolution.</p><p>Concurrently with our proposed method, two independent studies ( <ref type="bibr" target="#b48">[43]</ref> and <ref type="bibr" target="#b49">[44]</ref>) have recently been presented to train fully convolutional networks for correlation filter based tracking. Valmadre et al. <ref type="bibr" target="#b48">[43]</ref> propose to learn a fully convolutional network along with the element-wise logistic loss function while considering the correlation filter formulation in their framework as we do. However, our proposed cost function reduces the squared error between the desired and estimated correlation responses, which is the multiple channel correlation filter cost function. Moreover, we train a larger network model than those of <ref type="bibr" target="#b48">[43]</ref> and <ref type="bibr" target="#b49">[44]</ref>, and observe a significant amount of tracking performance increase in benchmark datasets, while <ref type="bibr" target="#b48">[43]</ref> and <ref type="bibr" target="#b49">[44]</ref> mainly concentrate on lightweight architectures for less computational complexity. In this study, the backpropagation formulations are based on the generalized chain rule and the conjugate symmetry property of real signals in the Fourier domain, however, the backpropagation derivations are performed in <ref type="bibr" target="#b48">[43]</ref> by exploiting the adjoint of differentials. Furthermore, the proposed framework has been trained on our generated dataset from ILSVRC Video dataset, which attempts to handle the imperfect training example issue, whereas both the test image and the template patch have the target object centered in <ref type="bibr" target="#b48">[43]</ref>, and a relatively smaller dataset is adopted in <ref type="bibr" target="#b49">[44]</ref>. Finally, our proposed feature learning framework achieves favorable performance against these concurrent works since the proposed method mainly focuses on training a relatively larger network, and achieves the state-of-the-art results in benchmark tracking datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Custom Architectures for Visual Tracking</head><p>Recently, various deep architectures with customized layers or objective functions have been proposed. An application of Siamese feature learning to the visual object tracking is proposed in <ref type="bibr" target="#b50">[45]</ref> where the network learns to output similar features for various appearances of the target object and dissimilar ones for the target and non-target samples. Nevertheless, evaluation of many candidates are quite expensive. Hence, a CNN model is introduced in <ref type="bibr" target="#b51">[46]</ref>, which directly learns to output the relative location of the object with respect to a reference object instance and avoids the expensive candidate evaluations and the feature matching phase. Unlike the model in <ref type="bibr" target="#b51">[46]</ref> employing fully connected layers, a fully convolutional neural network is presented in <ref type="bibr" target="#b53">[47]</ref>. In this approach <ref type="bibr" target="#b53">[47]</ref>, the object template and the test frame are passed through the same convolutional layers, and their correlation is obtained by the sliding window approach. The sliding window stage is operated in the convolutional layer format, since the standard deep learning libraries are efficiently exploited in order not to sacrifice much from the computation time, while still suffering from the satisfactory tracking performance.</p><p>Another popular concept is Recurrent Neural Networks (RNNs) <ref type="bibr" target="#b54">[48]</ref>, which is a useful neural network model, es-pecially in natural language processing. RNNs are employed in <ref type="bibr" target="#b55">[49]</ref> in order to estimate the confidence map of the target object by modeling the spatial relationships between the object and the background. Another spatial perspective is to spatially model the object structure <ref type="bibr" target="#b56">[50]</ref>. This study successfully applies this idea to the visual tracking problem in order to assist the CNN layers. Unlike the use of RNNs for the spatial relationships, <ref type="bibr" target="#b57">[51]</ref> and <ref type="bibr" target="#b59">[52]</ref> propose to learn an RNN model to directly estimate the motion of the object by modeling their RNNs to learn the relationships between the frames sequentially. Nevertheless, the visual tracking experiments are conducted on the simulation data, and they lack the performance on the standard benchmarks, such as VOT challenges <ref type="bibr" target="#b60">[53]</ref>, <ref type="bibr" target="#b11">[9]</ref>, <ref type="bibr" target="#b12">[10]</ref> or Online Tracking Benchmarks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b2">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Combining Trackers</head><p>Combination of multiple online trackers is another research path. For instance, multiple correlation trackers are run at different parts of the object in <ref type="bibr" target="#b39">[34]</ref>. A part-based version of MOSSE <ref type="bibr" target="#b10">[8]</ref> has been proposed in <ref type="bibr" target="#b61">[54]</ref> to accomplish object detection task. Reliable patches are tracked in <ref type="bibr" target="#b62">[55]</ref> using KCF <ref type="bibr" target="#b36">[31]</ref> as the base tracker. The work in MEEM <ref type="bibr" target="#b63">[56]</ref> selects the best SVM-based discriminative tracker according to an entropy minimization criterion. Markov Chain Monte Carlo sampling is also used to sample trackers and combine them <ref type="bibr" target="#b64">[57]</ref>. On the other hand, various trackers with mixed feature types are combined in <ref type="bibr" target="#b65">[58]</ref>. Hybrid methods combining generative and discriminative approaches are proposed in <ref type="bibr" target="#b66">[59]</ref>, <ref type="bibr" target="#b67">[60]</ref>. Since deep discriminative networks <ref type="bibr" target="#b8">[6]</ref> have an impact in the visual tracking literature, in <ref type="bibr" target="#b68">[61]</ref>, a tree-structure stores different appearances in the nodes of the tree as CNN models. This provides a robustness to significant appearance changes, while suffering from the heavy computational load.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CORRELATION FILTER FORMULATION</head><p>In this section, we briefly summarize the two correlation filter based tracking methods, Discriminative Scale Space Tracker (DSST) <ref type="bibr" target="#b17">[14]</ref> and Continuous Convolution Operator Tracker (CCOT) <ref type="bibr" target="#b20">[16]</ref> for completeness. The learned features from the proposed framework are integrated into these trackers due to their notable performance in the benchmark sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Multiple Channel Linear Correlation Filters</head><p>DSST <ref type="bibr" target="#b17">[14]</ref> is the multiple channel extension of MOSSE <ref type="bibr" target="#b10">[8]</ref>. The feature maps {y 1 , ..., y d } correspond to the training example y, which consists of particular feature maps, such as HOG orientation maps or deep feature maps with the same dimension as the object patch. The desired correlation mask of the training example y is denoted byĝ.</p><formula xml:id="formula_0">L(h t ) = t i=1 ( d l=1 h l t y l i ) −ĝ i 2 + λ d l=1 h l t 2<label>(1)</label></formula><p>Here, λ is the control parameter for l 2 regularization term of the filter, is the circular correlation operation, which can be described as:</p><formula xml:id="formula_1">a b = i a[i]b[n + i] = F −1 {A * B},<label>(2)</label></formula><p>where F −1 , and * are the inverse DFT, element-wise multiplication and the conjugation operations, respectively. The lowercase letters represent the signals in the spatial domain, whereas the uppercase letters denote the signals in the DFT domain. Moreover, the subscripts of the variables indicate the time indices, i.e., h l t is the l th feature channel of the corelation filter calculated at time t. As (1) suggests, a set of filters {h l t } d l=1 are to be estimated such that the correlation operation between h l t 's and x l i 's are summed and the error between the desired responseĝ i 's and the summed correlation results </p><formula xml:id="formula_2">H l = Y l Ĝ * d k=1 Y k Y k * + λ , ∀l ∈ 1, ..., d,<label>(3)</label></formula><p>where the time dependency is dropped for convenience. At time t, the filter H l t is updated by applying moving average to the numerator and denominator of (3) separately via the following relations:</p><formula xml:id="formula_3">A l t = (1 − µ)A l t−1 + µĜ * t Y l t , B t = (1 − µ)B t−1 + µ d k=1 Y k t Y k * t ,<label>(4)</label></formula><p>where µ is the model update rate. The correlation of an object patch z and the model H l t is calculated by using the updated numerator A l t and denominator B t of H l t in the frequency domain using:</p><formula xml:id="formula_4">c = F −1 {( d l=1 A l * t Z l )/(B t + λ )},<label>(5)</label></formula><p>where the spatial domain correlation mask is obtained by taking the inverse Fourier transform. The new location of the object in the next frame is estimated as the location giving the maximum value at c in <ref type="bibr" target="#b7">(5)</ref>. For scale estimation, DSST extractsd-dimensional HOG features for S scale factors. The base target size is multiplied by the scale factor. The corresponding region is cropped and described byd-dimensional features similar to the location estimation procedure. Then, the scale correlation filter h scale is calculated for the scale samples y s ∈ Rd ×S . The optimal scale is determined as the scale index giving the highest value on the correlation response of the test instance z scale and h scale .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Continuous Convolution Operators for Visual Tracking</head><p>A continuous domain formulation for correlation filters is proposed in CCOT <ref type="bibr" target="#b20">[16]</ref> to combine feature maps of different resolutions, especially deep feature maps at different layers.</p><p>Unlike the constant dimension assumption for all of the feature maps, each training sample y j is allowed to have the feature maps with different dimensions as y d j ∈ R N d . To implicitly model the signals in the continuous domain, the interval [0, T ) is assumed to be the support interval. For each feature map d, the interpolation operator is expressed as:</p><formula xml:id="formula_5">J d {y d }(t) = N d −1 n=0 y d [n]b d t − T N d n ,<label>(6)</label></formula><p>where b d ∈ L 2 (T ) is the interpolation function. A linear convolution (or a correlation) operator S f is required such that a sample y is mapped to a target confidence response</p><formula xml:id="formula_6">s(t) = S f {y}(t). Since there exist d feature maps, the correlation filters f = (f 1 , f 2 , ...f D ) ∈ L 2 (T ) D is intended to be estimated.</formula><p>The convolution operator in the continuous domain is described as:</p><formula xml:id="formula_7">S f {x} = D d=1 f d * J d {y d }<label>(7)</label></formula><p>In the above relation, * is the continuous domain correlation.</p><p>Although the initial signals are discrete, they are first converted to the continuous domain by using the operation</p><formula xml:id="formula_8">J d {y d }.</formula><p>Moreover, there should be continuous desired values g j for each training example y j . The correlation filter cost function is defined in the continuous domain by:</p><formula xml:id="formula_9">E(f ) = m j=1 α j S f {y j } − g j 2 + D d=1 wf d 2 ,<label>(8)</label></formula><p>Here, α j represents the importance of the sample y j , and w is a spatial penalty function to regularize the correlation filter in the spatial domain for suppressing the boundaries. In order to learn the filter f minimizing the cost in <ref type="formula" target="#formula_9">(8)</ref>, the operations are projected to the discrete frequency domain. Then, the cost in (8) is converted to a set of normal equations. The Conjugate Gradient Descent is utilized to iteratively optimize this cost. The implementation details can be found in <ref type="bibr" target="#b20">[16]</ref>. Once the object is localized, a multi-scale search is adopted with S scales to find the best matching scale by looking at the correlation response at every scale.</p><p>Thus far, the correlation filter based tracking methods that are tested in this study are summarized and these techniques are utilized to assess the effectiveness of the proposed feature learning method. The proposed framework is presented next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROPOSED FRAMEWORK FOR FEATURE LEARNING A. Preliminaries</head><p>In order to perform the training of the proposed framework in <ref type="figure" target="#fig_0">Figure 1</ref>, a set of triplet training samples is required. A triplet is represented by T i {x i , y i , g i }. y i is the template image patch which contains the object at its center. x i is the test image patch including the non-centered object. g i is the desired correlation response which has a peak at the location shifted from the center of the patch by the amount of the correct motion of the object between x i and y i . Throughout the intermediate derivation and equivalences between them, these three discrete signals are assumed to be 1-dimensional. The derivations are also valid for 2-dimensional case, since all of the utilized operations are separable for horizontal and vertical dimensions, such as Discrete Fourier Transform (DFT). For a signal x, x[n] denotes its n th component, and x[n + i] is its shifted version by an integer amount i to the left circularly. The circular shift is important to exploit the Correlation Theorem for real signals.</p><p>It is notable that a feature generation function f (.) of the image patch I, which is typically integrated into the CFB trackers, should carry the shift invariance property, i.e. if</p><formula xml:id="formula_10">I θ [x][y] = f (I[x][y]) and Y θ [x][y] = f (I[x − kδ x ][y − kδ y ]), then Y θ [x][y] ≈ I θ [x − δ x ][y − δ y ] should be satisfied, where I[.]</formula><p>[.] is a 2-D discrete signal and k is the scale factor of the transformation function f (.). Thus, we employ fully convolutional CNN models, which contain convolutional, batch normalization, pooling and ReLU layers. These layers do not violate this property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. The Proposed Loss Function for Parameter Learning</head><p>The proposed learning methodology utilizes the stochastic gradient descent (SGD) as in most of the deep learning frameworks. Our cost function for N triplet examples is defined as:</p><formula xml:id="formula_11">L = N i=1 L(θ) i ,<label>(9)</label></formula><p>where</p><formula xml:id="formula_12">L(θ) i = d l=1 h l i (θ) x l i (θ) − g i 2<label>(10)</label></formula><p>In the equation above, x l i (θ) is the network output with parameters θ for the input patch x i . In <ref type="formula" target="#formula_0">(10)</ref> and <ref type="bibr" target="#b11">(9)</ref>, θ represents the parameters of the fully convolutional network model. As it is given in</p><formula xml:id="formula_13">(3), h l i (θ) = F −1 {(Y l i (θ) Ĝ * i )/( d k=1 Y k i (θ) Y k * i (θ) + λ )}</formula><p>is the minimizing correlation filter for the feature map l, which is a function of {y l i (θ)} d l=1 (y l i (θ) is the output of the network for y i ). Note that, in this formulation,Ĝ i represents the DFT of the centered desired Guassian shaped correlation response for the template y i as in <ref type="bibr" target="#b3">(3)</ref>. The goal of the proposed method is to learn appropriate values for θ that will help to reduce the cost in <ref type="bibr" target="#b11">(9)</ref>.</p><p>The major difference between the correlation filter cost in (1) and the proposed one in <ref type="bibr" target="#b11">(9)</ref> is that the cost in <ref type="formula" target="#formula_11">(9)</ref> is minimized with respect to the network parameters θ for the given correlation filter solution in <ref type="formula" target="#formula_2">(3)</ref>, whereas the cost in (1) is minimized with respect to the correlation filters</p><formula xml:id="formula_14">{h l i } d l=1 (θ), ∀i.</formula><p>The regularization part in the second term of (1) is removed in the proposed cost function, since the correlation filter solution in (3) already penalizes the norm of the correlation filters. We hypothesize that the correlation quality will increase during test time in a visual tracking application, if the proposed cost function is reduced with respect to the parameters of the network by a stochastic training process on an appropriately generated dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Gradient of the Loss, L(θ) i</head><p>In order to learn a model with parameter set θ, the gradient of the loss with respect to θ is required. By the multivariable chain rule, the gradient of the loss in (10) can be written as 2 :</p><formula xml:id="formula_15">∇ θ L = l dL dx l dx l dθ + l dL dy l dy l dθ ,<label>(11)</label></formula><p>By applying the multivariable chain rule again, the first multiplicand in the second term of (11) becomes:</p><formula xml:id="formula_16">dL dy l = d k=1 dL dh k dh k dy l<label>(12)</label></formula><p>If the error signal e[n] is defined as:</p><formula xml:id="formula_17">e[n] = l i h l [i]x l [i + n] − g[n],<label>(13)</label></formula><p>the terms dL dx l and dL dh k in <ref type="formula" target="#formula_0">(11)</ref> and <ref type="formula" target="#formula_0">(12)</ref> can be written as:</p><formula xml:id="formula_18">dL dh k = F −1 {E * X k } dL dx l = F −1 {E H l }<label>(14)</label></formula><p>The partial derivatives in <ref type="bibr" target="#b17">(14)</ref> are derived in Appendix A. The Jacobians of the vectors y l and x l with respect to the model parameters θ ( dy l dθ and dx l dθ ) can be efficiently calculated by using the standard backpropagation tools of the existing deep learning libraries.</p><p>Till now, all of the terms to calculate the gradient in (11) are presented, except for the Jacobian dh k dy l . Since the relations between h k and y l are in the DFT domain as described in <ref type="formula" target="#formula_2">(3)</ref>, this term should be first converted to the DFT domain as follows:</p><formula xml:id="formula_19">dh k dy l = dh k dH k dH k dY l dY l dy l = F H dH k dY l F,<label>(15)</label></formula><p>where F and F H are DFT and inverse DFT matrices, respectively. The relation between H k and Y l are expressed independently for each frequency component as in <ref type="formula" target="#formula_2">(3)</ref>. Hence, the derivative of the division rule enables us to write:</p><formula xml:id="formula_20">dH k dY l = I(k == l)diag Ĝ * / (D(Y )) −diag Ĝ * Y k Y l * (D(Y )) 2 −diag Ĝ * Y k Y l (D(Y )) 2 M<label>(16)</label></formula><p>where I(.) is the indicator function yielding 1 when its argument is true, and 0 otherwise, and</p><formula xml:id="formula_21">D(Y ) = d m=1</formula><p>Y m Y m * + λ. In the above relations, the signals are treated as individual complex variables, and M is the matrix for the circular time reversal operation, equal to dY l * dY l (i.e. the Jacobian of Y l * with respect to Y l ) due to the conjugate symmetry property of real signals in DFT domain. In other words, M v is the time reversed version of the signal v by fixing its first element.</p><p>If the following intermediate signals are defined:</p><formula xml:id="formula_22">K kl 1 = I(k == l)Ĝ * D(Y ) K kl 2 =Ĝ * Y k Y l * (D(Y )) 2 K kl 3 =Ĝ * Y k Y l (D(Y )) 2 ,<label>(17)</label></formula><p>then <ref type="bibr" target="#b18">(15)</ref> can be simplified to:</p><formula xml:id="formula_23">dh k dy l = F H (diag(K kl 1 − K kl 2 ) − diag(K kl 3 )M )F<label>(18)</label></formula><p>All of the operations performed in the DFT domain are element-wise multiplication of the signals or their reciprocals, conjugation operation and so on, which do not violate the real property of the resulting signals. Hence, the conjugation operation in the spatial domain keeps the imaginary parts of the gradient terms to be zero.</p><p>Finally, if dh k dy l in (18) and dL dh k in <ref type="formula" target="#formula_0">(14)</ref> are replaced in <ref type="formula" target="#formula_0">(12)</ref>, and the Hermitian operation is taken for the overall expression,</p><formula xml:id="formula_24">∇ y l L = dL dy l = F −1 { d k=1 (K kl 1 − K kl 2 ) * A k − (K kl 3 A k * )}<label>(19)</label></formula><p>is obtained as the gradient of the loss in (10) for the l th feature map of the template image patch y. In <ref type="bibr" target="#b23">(19)</ref>, A k stands for the DFT of dL dh k . During the training process, the gradient of the cost with respect to the parameters and the activations of the network are computed as explicitly derived and formulated above for</p><formula xml:id="formula_25">B N triplet examples ({T i } B N i=1</formula><p>) in a batch. Then, the GD optimization is performed for all of the randomly sampled batches.</p><p>It should be noted that the proposed method can be modified so that feature channels are independent from each other. However, treating the feature channels independently makes it difficult to reduce the loss function due to the decrease of the frequency components in the denominator of (3) and probably causing the gradient overflow in spite of the careful selection of learning rate. Moreover, the employed formulation is optimal according to <ref type="bibr" target="#b0">(1)</ref>. Due to the aforementioned reasons, we prefer the current loss and leave the independence assumption on the feature channels as a future study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Computational Complexity and Its Reduction</head><p>It is notable that all of the necessary gradient terms in <ref type="bibr" target="#b13">(11)</ref> and <ref type="bibr" target="#b14">(12)</ref> can be efficiently computed in the DFT domain with only element-wise multiplications, divisions, summations and DFT transform operations. The main computational burden results from the DFT calculation with the complexity of O(P log(P )) (P is the length of the signal). Moreover, the operation in <ref type="formula" target="#formula_0">(19)</ref> is performed for each feature index out of d feature maps. Hence, the final complexity of backpropagating one triplet through the network has the complexity of O(dP log(P )). Depending on the value of d (typically ranging between 64 and 512), this complexity could be impractical. In our Matlab implementation, the training speeds are 36, 16 and 6 frames per second for d values 32, 64 and 128, respectively.</p><p>To train the convolutional parts of VGG as fast as the classification networks such as Alexnet <ref type="bibr" target="#b69">[62]</ref> and VGG <ref type="bibr" target="#b13">[11]</ref>, an auxiliary layer with relatively fewer feature maps has been added on top of the conv − 5 layer to reduce the computation time. It is observed that the robustness of the localization improves as the number of feature maps increase <ref type="bibr" target="#b20">[16]</ref>, <ref type="bibr" target="#b47">[42]</ref>. However, it can be claimed that if the correlation quality of a layer is enhanced, then this quality is expected to be transferred to the lower layers. This claim is analyzed in Appendix B for a layer with two feature maps and a layer with single feature map which is the summation of the two feature maps of the previous layer under mild assumptions. Moreover, the amount of quality improvement reduces as the distance between the layers increases. In this way, the outputs of all the convolutional layers before top one can be used as "good" features without increasing the complexity of the training stage.</p><p>In the following section, the implementation details and the conducted experiments are presented to show the validity of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTAL RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Performance Evaluation</head><p>The proposed tracker configurations are evaluated on OTB-2013 <ref type="bibr" target="#b0">[1]</ref>, OTB-2015 <ref type="bibr" target="#b2">[2]</ref> and VOT2016 <ref type="bibr" target="#b12">[10]</ref> datasets. OTB-2013 is a subset of OTB-2015 whereas VOT2016 is the 2016 challenge dataset of the Visual Object Tracking (VOT) committee.</p><p>For OTB, there exist two main performance metrics. 1) Success curve is computed by the ratio of successfully tracked frames according to a threshold on the overlap ratio, which is defined as the intersection over union of the predicted and ground-truth bounding boxes. The trackers are ranked according to the Area-Under-Curve (AUC) score of the success curve. Overlap precision (OP) is also a respectable metric which orders the trackers according to the value of the average success on the threshold 0.5. 2) Precision curve is plotted according to the center localization error and the ratio of the frames with a localization error below a threshold is accept as the distance precision (DP). The curve is plotted by varying the threshold and the trackers are ranked according to the average distance precision value at 20 pixels.</p><p>VOT2016 has a different tracking assessment technique including three major metrics. 1) Accuracy is the mean intersection over union of the frames in a sequence. 2) Failure is the mean number of failures per sequence. These two metrics are raw metrics. The ranking of a particular metric (failure or accuracy) is obtained by ordering the compared trackers with respect to that metric, and the statistically significant tracker rankings are merged. 3) Expected average overlap (EAO) is estimated for a selected range of sequence lengths. Concretely, a specific expected average overlap φ Ns is estimated by averaging the accuracy values in the segments that are longer than N s while discarding the segments shorter than N s with no failure termination. The segments shorter than N s with a failure are zero-padded; hence, penalizing the failure case for that particular N s length. These φ Ns values are determined for the set {φ Ns } N hi N lo and the final score is the mean of these expected values in the set. N lo and N hi are determined according to the sequence length histogram of the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Dataset Generation</head><p>The proposed method is realized by generating two datasets with appropriate fully convolutional models. For the first dataset, we generate 200K training examples by utilizing the VOT2015 dataset <ref type="bibr" target="#b11">[9]</ref>, consisting of 60 sequences with different attributes. The bounding box of each object is provided for each frame. We crop approximately two times larger area of the object size and resize the images to the appropriate size of the network (101×101 in our experiments). To keep the aspect ratio of the objects, we crop the squares from the region of interests of the object, where the side length of the square is 2 √ W H (W and H are the width and height of the object, respectively.). Generated y raw centers the object, since these patches are indeed templates for us. However, x raw is obtained by shifting the center of the object, since our aim is to break the influence of the circular translation over the actual translation. The shift amount is determined by a random variable which is uniformly distributed in the range of values [−0.3×W, 0.3×W ] and [−0.3×H, 0.3×H] for horizontal and vertical translations. The frame difference between y raw and x raw is a Gaussian random variable with standard deviation of 5 frames. We entitle this dataset as Convolutional Features for Correlation Filters VOT2015 (CFCF VOT2015 for short).</p><p>The custom architecture model has been trained on a dataset generated by using VOT2015 dataset including 60 video sequences. 11 of the sequences in VOT2015 also exist in OTB-2013 Benchmark dataset <ref type="bibr" target="#b0">[1]</ref>. Thus, it prevents the evaluation to be fulfilled on the full OTB-2013 sequences. Moreover, VOT2015 is not a large-scale dataset even though the generated samples are over 200K. This situation discourages to train or fine-tune the state-of-the-art convolutional networks such as <ref type="bibr" target="#b13">[11]</ref>, <ref type="bibr" target="#b70">[63]</ref>. In order to handle this situation, a new dataset is generated from the large-scale video sequences of ILSVRC challenge dataset <ref type="bibr" target="#b15">[13]</ref>.</p><p>The existing benchmarks OTB-2013 <ref type="bibr" target="#b0">[1]</ref>, OTB-2015 <ref type="bibr" target="#b2">[2]</ref>, VOT2014 <ref type="bibr" target="#b60">[53]</ref>, VOT2015 <ref type="bibr" target="#b11">[9]</ref>, VOT2016 <ref type="bibr" target="#b12">[10]</ref>, NUS-PRO <ref type="bibr" target="#b71">[64]</ref> and ALOV <ref type="bibr" target="#b72">[65]</ref> have limited number of sequences (less than a thousand). Moreover, some of these datasets have overlapping sequences (e.g., VOT2015 and VOT2016 is a subset of ALOV and OTB datasets.). Thus, the total number of sequences are not large enough to conveniently train a large network model. As a secondary note, all of these datasets contain sequences with similar appearances and challenges, hence increasing the risk of memorization. Due to the aforementioned reasons, in our study, we also employ ILSVRC Video dataset for our training and OTB and VOT datasets for testing as in <ref type="bibr" target="#b53">[47]</ref>. In the 2015 challenge organized by ILSVRC <ref type="bibr" target="#b15">[13]</ref>, a new dataset is presented for the challenge, namely "Object Detection from Video", which has more than 4000 videos. In each video, an object out of 30 classes acts and the bounding box for each frame is provided. This rich amount of annotated data  is utilized to generate our 200K triplet samples (the desired response, the localized and unlocalized patches) as it is done for VOT2015, and called as CFCF ILSVRC for short.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. CNN Architectures</head><p>1) The Custom Architectures for CFCF VOT2015 Dataset: Two custom architectures are designed and illustrated in <ref type="figure" target="#fig_3">Figure 2a</ref> and 2b. The first one outputs a single feature map, whereas the second one yields multiple feature maps. The trackers utilizing these networks and the tracker DSST <ref type="bibr" target="#b17">[14]</ref> will be called DSST CFCF and DSST MCFCF for single and multiple channel correlation filters, respectively. CFCF VOT2015 is a medium scale dataset. Hence, we opt to design a relatively small architecture with respect to the state-of-theart CNNs, such as <ref type="bibr" target="#b13">[11]</ref>. For this purpose, the input to our network is 3 channel input image in 101 × 101 dimensions. Our architecture consists of 4 convolutional layers. All of these layers have a batch normalization layer after the convolutional layer part. The first three of them have a rectified linear unit (ReLU) <ref type="bibr" target="#b69">[62]</ref> layer with a leak of 0.1 <ref type="bibr" target="#b73">[66]</ref>, <ref type="bibr" target="#b74">[67]</ref>. In order to keep the spatial size of the feature maps constant, convolutional layers have the appropriate padding (e.g. padding value is 1 for a 3 × 3 kernel). The number of feature maps are shown in <ref type="figure" target="#fig_3">Figure 2</ref>. The final layer outputs the map which will be utilized for the correlation task (x(θ) and y(θ) of <ref type="figure" target="#fig_0">Figure 1)</ref>.</p><p>2) Fine-tuning VGG-M <ref type="bibr" target="#b13">[11]</ref> for CFCF ILSVRC Dataset: Unlike the network described above, the VGG-M network in <ref type="bibr" target="#b13">[11]</ref> is exploited in such a way that this network is cut from the first fully connected layer (fc6), since the built framework only accepts the convolutional layers due to their shift invariance property. Although any other network model could be selected, VGG-M is fine-tuned to fairly compare against the CCOT tracker <ref type="bibr" target="#b20">[16]</ref>, which utilizes the zeroth, the first and the fifth convolutional layers of VGG-M. In the literature, there exist efforts for the investigation of the useful feature maps. For instance, the study in <ref type="bibr" target="#b46">[41]</ref> performs analysis on the effect of different convolutional layers of AlexNet <ref type="bibr" target="#b69">[62]</ref> and VGG-VD [63] models in a correlation filter based tracking framework, where conv-5 layer results in better performance than conv-3 and conv-4. Moreover, the analysis in deepSRDCF <ref type="bibr" target="#b47">[42]</ref> also verifies that the conv-5 layer of <ref type="bibr" target="#b13">[11]</ref> is the best performing layer. Since our baseline tracker CCOT is based on the tracker deepSRDCF <ref type="bibr" target="#b47">[42]</ref>, we prefer utilizing conv-1 and conv-5 layers along with the RGB channels of the image.</p><p>In order to train convolutional layers of VGG-M, an auxiliary layer with 32 feature maps is added as the layer to be optimized by our cost function in (9) by following the discussion in Section IV-D and Appendix B. This augmentation is necessary, because the final convolutional layer of VGG-M has 512 feature maps and the training with respect to the proposed loss becomes infeasible. The tracker obtained by integrating fully convolutional layers of VGG-M <ref type="bibr" target="#b13">[11]</ref>, fine-tuned in CFCF ILSVRC dataset with our cost function, into CCOT is simply called CFCF. The decrease of the cost in <ref type="bibr" target="#b11">(9)</ref> and the localization error, i.e., Euclidean distance between the desired and estimated object location, are plotted in <ref type="figure" target="#fig_6">Figure 3</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Evaluation in OTB-2013 by Training on CFCF VOT2015</head><p>1) Comparison with respect to hand-crafted features: In order to understand the effects of different feature types on the tracking performance, a comparative analysis has been carried out. For this purpose, we work on DSST <ref type="bibr" target="#b17">[14]</ref>, which is a state-of-the-art multi-channel CFB tracker with a scale search support. For fair comparisons, the only revised part in this tracker is its feature extraction stage. Tracking performance of different feature configurations are presented in <ref type="table">Table I</ref>. The proposed single and multiple feature map configurations (DSST CFCF and DSST MCFCF, respectively) perform favorably against the use of hand-crafted features in terms of mean OP and mean DP, although the number of maps are fewer than the hand-crafted ones of DSST. In other words, DSST CFCF and DSST MCFCF have 4 and 11 feature maps, respectively, while DSST employs 28 HOG maps. DSST MCFCF has the best performance among the compared feature combinations.</p><p>2) Comparison with respect to the state-of-the-art trackers: For comparing our learned features against the recently proposed trackers, CCOT <ref type="bibr" target="#b20">[16]</ref> (winner of VOT2016), which allows the use of multi-resolution feature maps, is adopted. For this purpose, we integrate our last layer features as well as the zeroth and first layers after the ReLU part, resulting in 27 feature maps compared to 611 feature maps of CCOT <ref type="bibr" target="#b20">[16]</ref>. This configuration, called MCFCF CCOT, is also compared against deepSRDCF <ref type="bibr" target="#b47">[42]</ref> (the 2 nd best of VOT2015 challenge) utilizing 96 feature maps of <ref type="bibr" target="#b13">[11]</ref>. A recent work SiamFC <ref type="bibr" target="#b53">[47]</ref>, where a fully convolutional model is trained for sliding window matching, is also compared with the proposed method. <ref type="figure" target="#fig_7">Figure 4</ref> presents OPE results on 40 sequences of <ref type="bibr" target="#b0">[1]</ref>. Regarding average OP values (the left of <ref type="figure" target="#fig_7">Figure 4</ref>), the proposed 27 feature maps yield a close performance to CCOT with 611 features. Meanwhile, it outperforms deepSRDCF, which utilizes 96 feature maps. On the other hand, the proposed method performs favorably against deepSRDCF and SiamFC in terms of CLE values (the right of <ref type="figure" target="#fig_7">Figure 4</ref>).</p><p>In <ref type="table" target="#tab_2">Table II</ref>, AUC values of OP are presented for 11 attributes. For most attributes, the proposed features perform close to CCOT, such as in the sequences with scale variation (SV), deformation (DEF) and background clutter (BC).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Evaluations in OTB-2013 and OTB-2015 by Training on CFCF ILSVRC</head><p>The fine-tuned VGG-M is tested on OTB-2013 with 51 videos and OTB-2015 with 100 videos. As in <ref type="bibr" target="#b20">[16]</ref>, the zeroth, first and fifth convolutional layers of VGG are employed. For the remaining part of the simulations, CCOT <ref type="bibr" target="#b20">[16]</ref> and SAMF <ref type="bibr" target="#b18">[15]</ref> are utilized to integrate our learned features, and the proposed configurations are denoted as SAMF CFCF and CCOT CFCF, respectively. Moreover, we re-run the CCOT configuration that the authors use for their ECCV submission <ref type="bibr" target="#b20">[16]</ref>, since the results might change on different CPUs. This configuration is named as CCOT VGG, while SAMF VGG is SAMF that employs VGG features as described earlier.</p><p>For SAMF CFCF, the default parameters of <ref type="bibr" target="#b18">[15]</ref> is used except for the learning rate, which is halved, since the utilized features (VGG and CFCF), which are more robust than handcrafted ones, perform better with relatively lower learning TABLE I: Analysis on feature type and quantity. Raw: raw image intensities, X Grad: magnitude of the horizontal gradient, Y Grad: magnitude of the vertical gradient, CFCF: learned single feature, MCFCF: learned multiple features. Mean OP: average overlap score with the threshold 0.5, Mean DP: average center location error with the threshold 20 pixels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DSST GRAY DSST GRAY GRADS</head><p>DSST <ref type="bibr" target="#b17">[14]</ref> DSST CFCF (Proposed) DSST MCFCF (Proposed)   rates. Similarly, for CCOT CFCF, the fine-tuned VGG-M network is integrated into the CCOT tracker by using its default hyper-parameters, except for the number of Conjugate Gradient iterations to compute the correlation filter. Hence, the best practice is to decrease the overfitting factor. The default iteration number of CCOT is 5, whereas we only perform 1 conjugate gradient iteration except for the first frame (which has 100 iterations in both our case and the baseline CCOT configuration). Hence, the learned features also help to double the computation speed, as the fps values are reported in <ref type="figure">Figure  9</ref> for 100 videos of OTB-2015. The fps is measured in Intel Xeon E5 2623 3.0 GHz except for the CNN feature extraction part which is performed by NVidia Tesla K40 in MatConvNet <ref type="bibr" target="#b75">[68]</ref>. For a 200 × 200 target object, the speed of the proposed tracker is around 1.7 fps. During tracking, 55% of the time is consumed for correlation filter learning, 17% and 11% of the time pass during the CNN feature extraction and the detection of the object, respectively, while the remaining part is spent for other functions such as image resizing. <ref type="figure">Figure 5</ref> and 6 present the localization error and overlap curves for OTB-2013 dataset, respectively. Moreover, <ref type="figure">Figure 7</ref> and 8 show the localization error and overlap curves for OTB-2015 dataset, respectively. In both of the datasets, the proposed features maintain its superiority over the baseline trackers SAMF and CCOT, while performing favourably or comparably against the state-of-the-art CFB trackers deepSRDCF <ref type="bibr" target="#b47">[42]</ref> and HCF <ref type="bibr" target="#b46">[41]</ref>. Per-attribute Area-Under-Curve (AUC) values are reported in <ref type="table" target="#tab_2">Table III</ref>. Except for background clutter (BC), our proposed tracker performs favorably against CCOT. When the best performing configurations of the concurrent works  <ref type="bibr" target="#b48">[43]</ref> and <ref type="bibr" target="#b49">[44]</ref> are compared against our tracker on OTB2015, a significant amount of performance increase is observed. DCFNet <ref type="bibr" target="#b49">[44]</ref> has AUC of 57.5 and CFNet <ref type="bibr" target="#b48">[43]</ref> has AUC of 58.9, whereas we achieve AUC of 67.8 out of 100.0. This is probably due to the fact that the concurrent works mainly focus on speed while we target at accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Evaluation in VOT2016 by Training on CFCF ILSVRC</head><p>As it is mentioned in the previous section, the proposed features that are integrated into CCOT has also been tested in VOT2016 challenge dataset including 60 videos. For making fair comparison between the fine-tuned VGG features for our loss function and the VGG features utilized by CCOT, VOT2016 challenge configuration of CCOT is utilized. In that configuration, first and fifth convolutional layers of VGG are employed as well as the color names of <ref type="bibr" target="#b76">[69]</ref> with 11 features and 31 HOG gradient maps in <ref type="bibr" target="#b41">[36]</ref>. <ref type="table" target="#tab_4">Table IV</ref> reports the performance results of VOT2016 challenge. Among 71 participants, we only show the top ten trackers and the proposed tracker CFCF ordered by the EAO metric unifying the robustness and accuracy of the trackers.</p><p>In <ref type="figure" target="#fig_0">Figure 10</ref>, the ranking results in <ref type="table" target="#tab_4">Table IV is</ref>   of optimization iterations is reduced to 1 from 5, bringing a significant decrease in the computation time. It should also be noted that the number of failures is decreased by 25% with respect to the CCOT. The raw accuracy performance is also improved by at least 3.5%. <ref type="table" target="#tab_5">Table V</ref> reports the per-attribute accuracy and failure values of the compared trackers for VOT2016 dataset. The proposed tracker has favorable accuracy and failure values compared to CCOT for most of the attribute types. Notably, CFCF has significantly better accuracy than the compared trackers for camera motion attribute and less failures for scale change and camera motion attributes. Regarding the comparison against the concurrent works <ref type="bibr" target="#b48">[43]</ref> and <ref type="bibr" target="#b49">[44]</ref>, knowing the fact that <ref type="bibr" target="#b49">[44]</ref> has an EAO value below 0.25 (in VOT2015) and VOT2016 annotations are more challenging, the possible EAO value of the tracker in <ref type="bibr" target="#b49">[44]</ref> would be smaller than our EAO value. Since CFNet <ref type="bibr" target="#b48">[43]</ref> exploits VOT2016 dataset as the validation dataset, we do not compare CFNet   <ref type="bibr" target="#b48">[43]</ref> and our tracker on VOT2016 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Ablation Study</head><p>We conduct a set of ablation studies on VOT2016 dataset to analyze how the proposed feature learning framework perform in different training and input configurations. During these experiments, only convolutional features are employed.</p><p>Utilization of chroma components: <ref type="table" target="#tab_5">Table VI</ref> compares the performances of our tracker configurations for the inputs with color and gray scale when the zeroth, first and fifth convolutional layer activations are utilized. Moreover, it also demonstrates the improvement obtained by fine-tuning the VGG-M network with our loss function. When the network is fine-tuned by our framework, in terms of EAO, more than 11% improvement is achieved over the use of pre-trained network while increasing accuracy by 2% and decreasing the number of failures by 25%. If the input is grayscale image, the tracking performance decreases, however, it achieves a close EAO (0.26) to the top ten trackers among VOT2016 challenge participants. Moreover, fine-tuning VGG-M network improves EAO value by 12% over the one trained on ImageNet <ref type="bibr" target="#b14">[12]</ref>.</p><p>Performance analyses of different layers: In this part, the tracking performances of different layers of our finetuned network model (VGG-M) are investigated. For this purpose, convolutional layers are individually tested on VOT2016 dataset in terms of accuracy, robustness (failures) and EAO. Then, the best performing higher level (among (conv-3, conv-4 and conv5) convolutional layer and lower level convolutional  Conv-2 layer has the best EAO, accuracy and robustness values. Among the higher convolutional layers, conv-5 performs better than conv-4. Hence, the combination of conv-2 and conv-5 performs favorably against conv-2 or conv-5.</p><p>Training from scratch: We analyze the impact of the size of the training set on the tracking performance. For this purpose, the first four convolutional layers of VGG-M are utilized. The final and the fifth layer of the model is the convolutional layer with 32 feature maps. For the tracking application, these 32 feature maps and color channels of the input image are employed in CCOT implementation. In <ref type="table" target="#tab_2">Table VIII</ref>, the tracking performance on VOT2016 dataset improves in terms of average number of failures as the number of training samples are increased from 500 to 200K. The randomly initialized network performs favorably against the one trained with 500 samples. One reason for the performance degradation by training a very small amount of data is overfitting. Remarkably, training 200K samples significantly improves the EAO value and average failures over the random network by 10%, and these values are comparable against the participating trackers of VOT2016 which have average ranking even though the tracker configuration adopts 35 feature maps (32 from conv-5 and 3 from color channels).</p><p>Failure cases: <ref type="figure" target="#fig_0">Figure 11</ref> illustrates some failure cases of the proposed tracker CFCF with respect to CCOT <ref type="bibr" target="#b20">[16]</ref> when both trackers utilize convolutional, HOG gradient maps and color names. The sequences are selected according to the case  when both of them fail at the same frame or do not fail. Some failure cases show up in particular sequences though the proposed features improve the tracking performance over the ones extracted from the pre-trained model. Some possible reasons might be the amount of difference between challenges of VOT2016 and the training set CFCF ILSVRC (the sequence leaves), abrupt aspect ratio change (the sequence motocross2 and gymnastics1), and object deformation (the sequence fish4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION AND FUTURE WORK</head><p>In this study, we address the feature learning problem for correlation filter based visual tracking task. For this purpose, a novel and generic framework is proposed to train any deep and fully convolutional network. By exploiting the correlation theorem, an efficient backpropagation formulation is presented to train any fully convolutional network by using stochastic gradient descent algorithm. The introduced feature learning method is trained on the frames generated by utilizing VOT2015 and ILSVRC Video datasets. The learned models have been integrated into the state-of-the-art correlation filter based trackers to show the validity of the proposed technique. In benchmark tracking datasets, favorable performance is achieved against the state-of-the-art tracking methods. Notably, the top performing tracker of VOT2016 challenge has been improved by at least 18% in terms of the expected average overlap metric. The proposed methodology can be adopted to custom deep network designs.</p><formula xml:id="formula_26">APPENDIX A DERIVATION OF dL dh k AND dL dx l</formula><p>By the Correlation Theorem in (2), L is:</p><formula xml:id="formula_27">L = n l i h l [i]x l [i + n] − g[n] 2<label>(20)</label></formula><p>The partial derivative for a particular component of h k [m] is:</p><formula xml:id="formula_28">∂L ∂h k [m] = n l i h l [i]x l [i + n] − g[n] ∂ l i h l [i]x l [i + n] ∂h k [m]<label>(21)</label></formula><formula xml:id="formula_29">∂L ∂h k [m] = n l i h l [i]x l [i + n] − g[n] x k [m + n]<label>(22)</label></formula><p>If we utilize the error signal e[n] = l i</p><formula xml:id="formula_30">h l [i]x l [i + n] − g[n]</formula><p>defined in <ref type="bibr" target="#b15">(13)</ref>, the derivatives will have better interpretation for the sake of both the time and frequency domain. By substituting this error to <ref type="bibr" target="#b26">(22)</ref>, the derivative signal will have an efficient calculation as follows by using <ref type="formula" target="#formula_1">(2)</ref> In this part, it is analyzed that the correlation quality of a layer behaves analogous to the layer above it if some assumptions on the additive appearance noise hold. This noise can be perceived as the appearance difference between the template x and the test patch z. Convolutional layers have a set of 2-D feature maps. To obtain another convolutional layer on top of the previous one, they are summed with a set of weight parameters. For this purpose, X is 2-D DFT of a single feature map obtained from a network in a certain layer, e.g. l th layer, for the training example x. Similarly, z is the test patch with the centered object and the corresponding correlation filter for x is</p><formula xml:id="formula_31">H = X Ĝ * X * X + γ ,<label>(25)</label></formula><p>where γ is the regularization parameter, andĜ is the DFT of the desired responseĝ for the template x with a peak in its center location. If the localized test sample z has the feature map in DFT domain as Z = X + µ with µ being the additive noise due to the appearance change of the object, then the resulting correlation error turns out to be:</p><formula xml:id="formula_32">E single = H * Z −Ĝ = X * Ĝ X * X + γ (X + µ) −Ĝ ≈ µX * Ĝ X * X + γ<label>(26)</label></formula><p>If the convolutional kernel at level l−1 is assumed to be 1×1 with their values fixed to 1 and there exists only two feature maps, then we can split X as X = X 1 + X 2 by ignoring the bias terms. In this case, the feature map of the test example Z will be split as Z = Z 1 + Z 2 , where Z 1 = X 1 + µ 1 and Z 2 = X 2 + µ 2 . The µ 1 and µ 2 are the individual additive noises of the feature maps. The two correlation filters are:</p><formula xml:id="formula_33">H 1 = X 1 Ĝ * X * 1 X 1 + X * 2 X 2 + γ H 2 = X 2 Ĝ * X * 1 X 1 + X * 2 X 2 + γ<label>(27)</label></formula><p>The correlation of z and h yields:</p><formula xml:id="formula_34">E multi = H * 1 Z 1 + H * 2 Z 2 −Ĝ = X * 1 Ĝ X * 1 X 1 + X * 2 X 2 + γ (X 1 + µ 1 )+ X * 2 Ĝ X * 1 X 1 + X * 2 X 2 + γ (X 2 + µ 2 ) −Ĝ<label>(28)</label></formula><p>By neglecting the effect of γ value, the error is reduced to</p><formula xml:id="formula_35">E multi = µ 1 X * 1 Ĝ + µ 2 X * 2 Ĝ X * 1 X 1 + X * 2 X 2 + γ<label>(29)</label></formula><p>To make a similarity between the <ref type="formula" target="#formula_1">(26)</ref> and <ref type="formula" target="#formula_1">(29)</ref>, the X is replaced with X 1 + X 2 and µ = µ 1 + µ 2 in <ref type="bibr" target="#b31">(26)</ref>. Moreover, all of the terms are copied in <ref type="bibr" target="#b34">(29)</ref>. Finally, we obtain the following error for single and multiple channels:</p><formula xml:id="formula_36">E multi = µ 1 X * 1 Ĝ + µ 2 X * 2 Ĝ + µ 1 X * 1 Ĝ + µ 2 X * 2 Ĝ X * 1 X 1 + X * 2 X 2 + γ + X * 1 X 1 + X * 2 X 2 + γ E single = µ 1 X * 1 Ĝ + µ 2 X * 2 Ĝ + µ 1 X * 2 Ĝ + µ 2 X * 1 Ĝ X * 1 X 1 + X * 2 X 2 + X * 1 X 2 + X * 2 X 1 + γ<label>(30)</label></formula><p>In <ref type="formula" target="#formula_1">(29)</ref> and <ref type="formula" target="#formula_2">(30)</ref>, both errors are proportional to µ 1 and µ 2 . Hence, if the sum of these two variables (i.e. µ) decreases, µ 1 and µ 2 have also tendency to decrease. This derivation can be extended to more than two feature maps, where the same assumption could hold. If the two correlation response errors in two consecutive layers are almost the same, one can argue that the mitigation of the appearance noise in one of the layers is likely to reduce the correlation response error in another one. Hence, training a fully convolutional model to reduce its correlation error with respect to the top layer will eventually increase the correlation quality in the lower layers. The experimental results have clearly shown that this analysis is practically valid for most scenarios. Erhan Gundogdu received the B.Sc., M.Sc. and Ph.D. degrees in Electrical and Electronics Engineering Dept. from Middle East Technical University (METU), Ankara, Turkey, in 2010, 2012 and 2017, respectively. He was a Research Assistant at Electrical and Electronics Eng. Dept. at METU, Ankara, from 2010 to 2012, and is currently a Research Engineer in Aselsan Research Center, Ankara. His current research interests include visual tracking, object recognition, and detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Method overview: A CNN model is trained to improve the correlation quality such that when the correlation filter h θ and the observation x θ (both depend on the parameters of the CNN model) are circularly correlated, the resulting correlation output will be improved. This can be achieved by training the model with appropriate set of xraw and yraw pairs which differ in the appearance and translation of the object with respect to the center of the patch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>minimized under the l 2 regularization of the correlation filters. There exists a closed form solution in the frequency domain for one training example, i.e. t = 1:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 :</head><label>2</label><figDesc>The custom architectures which are trained on the CFCF VOT2015.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>during fine-tuning VGG-M network on CFCF ILSVRC dataset. During training, 51% of the time is spent for calculating the gradient terms of the loss function ( dL dx l and dL dy l ), whereas 14% and 34% of the time are spent for forward and backward propagation of the convolutional parts of the model, respectively. The remaining time is consumed by other stages such as updating the weights (3.5%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 :</head><label>3</label><figDesc>The drop of the loss in<ref type="bibr" target="#b11">(9)</ref> and the localization error during the training epochs for fine-tuning VGG-M network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 :</head><label>4</label><figDesc>One-Pass-Evaluation (OPE) curves. Left: Overlap precision (OP) and right: center localization error (CLE).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 5 :Fig. 6 :</head><label>56</label><figDesc>OTB-2013 localization error curves OTB-2013 overlap curves</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 :Fig. 8 :Fig. 9 :</head><label>789</label><figDesc>OTB-2015 localization error curves OTB-2015 overlap curves Speed comparison between CCOT (baseline) and the proposed tracker CFCF on OTB-2015 sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>pictured within a 2-D plot. As the figure shows, the proposed tracker outperforms all of the existing participants. Moreover, the proposed features significantly improves the top tracker CCOT by 18.7% in terms of EAO. On the other hand, the number</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) The sequence leaves (b) The sequence fish4 (c) The sequence gymnastics1 (d) The sequence motocross2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 :</head><label>11</label><figDesc>Failure cases of our approach in VOT2016 dataset. Red and blue: indicate CFCF (ours) and CCOT, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. Gundogdu is with the ASELSAN Research Center, Intelligent Data Analytics Research Program Department, 06370 Ankara, Turkey, and also with the Department of Electrical and Electronics Engineering, Middle East Technical University, 06800 Ankara, Turkey (e-mail: egundogdu87@gmail.com). A. A. Alatan is with the Department of Electrical and Electronics Engineering, Middle East Technical University, 06800 Ankara, Turkey, and also with the Center for Image Analysis (OGAM), Middle East Technical University, 06800 Ankara, Turkey (e-mail: alatan@metu.edu.tr). /www.ieee.org/publications standards/publications/rights/index.html for more information. Citation of this paper: E. Gundogdu and A. A. Alatan, "Good Features to Correlate for Visual Tracking," in IEEE Transactions on Image Processing, vol. 27, no. 5, pp.</figDesc><table><row><cell cols="2">1057-7149</cell><cell>c</cell><cell>2018</cell><cell cols="2">IEEE.</cell><cell cols="2">Personal</cell><cell>use</cell><cell>is</cell><cell>permitted,</cell></row><row><cell>but</cell><cell cols="4">republication/redistribution</cell><cell cols="2">requires</cell><cell cols="2">IEEE</cell><cell>permission.</cell><cell>See</cell></row><row><cell cols="2">http:/2526-2540,</cell><cell>May</cell><cell>2018.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>doi: 10.1109/TIP.2018.2806280, URL: http://ieeexplore.ieee.org/document/8291524</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE II :</head><label>II</label><figDesc>AUC values for 11 attributes of the 40 sequences<ref type="bibr" target="#b0">[1]</ref>.</figDesc><table><row><cell></cell><cell>IV</cell><cell cols="2">SV OCC DEF MB FM IPR OPR OV</cell><cell>BC</cell><cell>LR Avg.</cell></row><row><cell cols="3">MCFCF CCOT (ours) 0.63 0.66 0.63</cell><cell cols="2">0.70 0.69 0.63 0.64 0.64 0.58 0.65 0.62 0.67</cell></row><row><cell>CCOT [16]</cell><cell cols="2">0.70 0.70 0.70</cell><cell cols="2">0.70 0.76 0.71 0.69 0.69 0.76 0.68 0.75 0.71</cell></row><row><cell>deepSRDCF [42]</cell><cell cols="2">0.62 0.64 0.63</cell><cell cols="2">0.68 0.69 0.64 0.65 0.65 0.64 0.65 0.44 0.67</cell></row><row><cell>SiamFC [47]</cell><cell cols="2">0.57 0.61 0.63</cell><cell cols="2">0.60 0.60 0.60 0.65 0.63 0.65 0.60 0.63 0.65</cell></row><row><cell>HOG CCOT</cell><cell cols="2">0.55 0.58 0.61</cell><cell cols="2">0.65 0.64 0.57 0.59 0.59 0.53 0.61 0.53 0.63</cell></row><row><cell cols="3">MCFCF DSST (ours) 0.56 0.56 0.55</cell><cell cols="2">0.58 0.53 0.50 0.57 0.55 0.47 0.55 0.56 0.58</cell></row><row><cell>CFCF DSST (ours)</cell><cell cols="2">0.56 0.56 0.54</cell><cell cols="2">0.55 0.50 0.47 0.56 0.54 0.50 0.52 0.50 0.56</cell></row><row><cell>DSST [14]</cell><cell cols="2">0.56 0.56 0.53</cell><cell cols="2">0.52 0.52 0.49 0.58 0.53 0.47 0.52 0.51 0.56</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE III :</head><label>III</label><figDesc>AUC values for 11 attributes for 100 sequences of<ref type="bibr" target="#b2">[2]</ref>.</figDesc><table><row><cell></cell><cell>IV</cell><cell cols="2">SV OCC DEF MB FM IPR OPR OV</cell><cell>BC</cell><cell>LR</cell></row><row><cell cols="3">CCOT CFCF (ours) 70.0 66.1 67.2</cell><cell cols="2">61.4 71.7 67.8 64.4 65.3 66.0 65.2 60.5</cell></row><row><cell>CCOT VGG [16]</cell><cell cols="2">67.9 65.5 66.0</cell><cell cols="2">60.7 69.7 67.1 62.1 64.5 63.0 66.0 58.8</cell></row><row><cell>SAMF CFCF ours</cell><cell cols="2">55.5 52.6 55.7</cell><cell cols="2">50.2 58.0 53.1 56.3 55.6 51.4 59.4 47.6</cell></row><row><cell>SAMF VGG</cell><cell cols="2">56.8 51.1 54.5</cell><cell cols="2">49.6 56.7 53.5 54.0 54.5 55.1 56.4 44.4</cell></row><row><cell>deepSRDCF [42]</cell><cell cols="2">62.4 60.7 60.3</cell><cell cols="2">56.7 64.2 62.8 58.9 60.7 55.3 62.7 47.5</cell></row><row><cell>HCF [41]</cell><cell cols="2">54.1 48.5 52.6</cell><cell cols="2">53.0 58.5 57.0 55.9 53.4 47.4 58.5 43.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE IV :</head><label>IV</label><figDesc>VOT2016 performance results. Unlike OTB experiments using only convolutional layers, CCOT and our tracker CFCF employ convolutional features, color names, and HOG orientation maps (c.f. Section V-G for the performance comparison with only convolutional layers).</figDesc><table><row><cell>Trackers</cell><cell>EAO</cell><cell>Acc. Rank</cell><cell>Rob. Rank</cell><cell>Acc. Raw</cell><cell>Fail. Raw</cell></row><row><cell>CFCF</cell><cell>0.3903</cell><cell>1.98</cell><cell>2.27</cell><cell>0.54</cell><cell>0.63</cell></row><row><cell>CCOT [16]</cell><cell>0.3310</cell><cell>2.55</cell><cell>2.95</cell><cell>0.52</cell><cell>0.85</cell></row><row><cell>TCNN [61]</cell><cell>0.3249</cell><cell>1.97</cell><cell>3.92</cell><cell>0.54</cell><cell>0.96</cell></row><row><cell>SSAT [6], [10]</cell><cell>0.3207</cell><cell>1.62</cell><cell>3.80</cell><cell>0.57</cell><cell>1.04</cell></row><row><cell>MLDF [10]</cell><cell>0.3106</cell><cell>3.70</cell><cell>2.82</cell><cell>0.48</cell><cell>0.83</cell></row><row><cell>Staple [39]</cell><cell>0.2952</cell><cell>2.57</cell><cell>4.83</cell><cell>0.54</cell><cell>1.35</cell></row><row><cell>DDC [10]</cell><cell>0.2929</cell><cell>2.27</cell><cell>4.62</cell><cell>0.53</cell><cell>1.23</cell></row><row><cell>EBT [70]</cell><cell>0.2913</cell><cell>5.07</cell><cell>2.88</cell><cell>0.4</cell><cell>0.90</cell></row><row><cell>SRBT [10]</cell><cell>0.2904</cell><cell>3.73</cell><cell>4.47</cell><cell>0.50</cell><cell>0.125</cell></row><row><cell>STAPLEp [10]</cell><cell>0.2862</cell><cell>2.03</cell><cell>4.42</cell><cell>0.55</cell><cell>1.32</cell></row><row><cell>DNT [10]</cell><cell>0.2783</cell><cell>3.03</cell><cell>4.47</cell><cell>0.50</cell><cell>1.18</cell></row><row><cell cols="6">Fig. 10: Accuracy-Robustness Ranking plot. Closeness to the</cell></row><row><cell cols="5">top right indicates good tracking performance.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE V :</head><label>V</label><figDesc>Per-attribute results of Accuracy/Failures on VOT2016. Bold indicates the best.</figDesc><table><row><cell></cell><cell>cam. mot.</cell><cell>ill. ch.</cell><cell>mot. ch.</cell><cell>occ.</cell><cell>scal. ch.</cell></row><row><cell>CFCF</cell><cell>0.57/17.0</cell><cell>0.64/2.0</cell><cell>0.50/15.0</cell><cell>0.47/15.0</cell><cell>0.53/6.0</cell></row><row><cell>CCOT [16]</cell><cell>0.56/24.0</cell><cell>0.65/2.0</cell><cell>0.47/20.0</cell><cell>0.44/14.0</cell><cell>0.50/13.0</cell></row><row><cell>TCNN [61]</cell><cell>0.55/27.9</cell><cell>0.64/3.1</cell><cell>0.52/22.1</cell><cell>0.51/15.3</cell><cell>0.51/14.9</cell></row><row><cell>SSAT [6], [10]</cell><cell>0.57/34.1</cell><cell>0.67/2.3</cell><cell>0.54/21.7</cell><cell>0.51/23.7</cell><cell>0.55/15.1</cell></row><row><cell>MLDF [10]</cell><cell>0.51/22.0</cell><cell>0.58/2.0</cell><cell>0.45/19.0</cell><cell>0.41/17.0</cell><cell>0.44/7.0</cell></row><row><cell>Staple [39]</cell><cell>0.55/34</cell><cell>0.71/7.0</cell><cell>0.51/35.0</cell><cell>0.43/24.0</cell><cell>0.51/15.0</cell></row><row><cell>DDC [10]</cell><cell>0.56/27.0</cell><cell>0.59/5.0</cell><cell>0.52/24.0</cell><cell>0.45/18.0</cell><cell>0.49/14.0</cell></row><row><cell>EBT [70]</cell><cell>0.49/20.0</cell><cell>0.41/3.0</cell><cell>0.44/19.0</cell><cell>0.37/17.0</cell><cell>0.36/11.0</cell></row><row><cell>SRBT [10]</cell><cell>0.49/33.0</cell><cell>0.44/1.0</cell><cell>0.46/24.0</cell><cell>0.43/20.0</cell><cell>0.43/13.0</cell></row><row><cell>STAPLEp [10]</cell><cell>0.56/35.0</cell><cell>0.69/6.0</cell><cell>0.51/33.0</cell><cell>0.44/23.0</cell><cell>0.53/17.0</cell></row><row><cell>DNT [10]</cell><cell>0.52/31.8</cell><cell>0.53/2.0</cell><cell>0.49/21.1</cell><cell>0.44/20.1</cell><cell>0.48/11.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE VI</head><label>VI</label><figDesc></figDesc><table><row><cell cols="5">: The effect of fine-tuning VGG-M [11] on the</cell></row><row><cell cols="5">tracking performance (VOT2016) for grayscale and color</cell></row><row><cell cols="5">inputs. Pre-trained means VGG-M trained on ImageNet [12]</cell></row><row><cell cols="5">dataset for classification task, and fine-tuned means Fine-</cell></row><row><cell cols="5">tuning the pre-trained VGG-M on CFCF ILSVRC dataset for</cell></row><row><cell cols="5">the proposed correlation filter based tracking loss function.</cell></row><row><cell>Feature Type</cell><cell>Input Type</cell><cell>Acc.</cell><cell>Fail.</cell><cell>EAO</cell></row><row><cell>Fine-tuned</cell><cell>RGB</cell><cell>0.53</cell><cell>0.75</cell><cell>0.3398</cell></row><row><cell>Pre-trained</cell><cell>RGB</cell><cell>0.52</cell><cell>1.00</cell><cell>0.3050</cell></row><row><cell>Fine-tuned</cell><cell>Grayscale</cell><cell>0.52</cell><cell>1.32</cell><cell>0.2638</cell></row><row><cell>Pre-trained</cell><cell>Grayscale</cell><cell>0.51</cell><cell>1.47</cell><cell>0.2354</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE VII :</head><label>VII</label><figDesc>Comparison between different convolutional layers of the learned network (VGG-M) by the proposed loss function on VOT2016.</figDesc><table><row><cell># of samples</cell><cell>Accuracy</cell><cell>Failures</cell><cell>EAO</cell></row><row><cell>conv-1</cell><cell>0.52</cell><cell>0.97</cell><cell>0.3062</cell></row><row><cell>conv-2</cell><cell>0.53</cell><cell>0.90</cell><cell>0.3187</cell></row><row><cell>conv-3</cell><cell>0.50</cell><cell>1.10</cell><cell>0.2719</cell></row><row><cell>conv-4</cell><cell>0.50</cell><cell>0.98</cell><cell>0.2895</cell></row><row><cell>conv-5</cell><cell>0.50</cell><cell>1.02</cell><cell>0.2994</cell></row><row><cell>conv-2+conv-5</cell><cell>0.54</cell><cell>0.82</cell><cell>0.3364</cell></row><row><cell>conv-1+conv-5</cell><cell>0.53</cell><cell>0.87</cell><cell>0.3269</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE VIII :</head><label>VIII</label><figDesc>The effect of training the first four convolutional layers of VGG-M<ref type="bibr" target="#b13">[11]</ref> with the augmented convolutional layer with 32 feature maps from scratch on the tracking performance (VOT2016) is reported for varying amount of training data (CFCF ILSVRC).Table VIIcompares EAO, accuracy and robustness values of different layers and some combinations.</figDesc><table><row><cell># of samples</cell><cell>Accuracy</cell><cell>Failures</cell><cell>EAO</cell></row><row><cell>None</cell><cell>0.47</cell><cell>2.38</cell><cell>0.1707</cell></row><row><cell>0.5K</cell><cell>0.47</cell><cell>2.45</cell><cell>0.1667</cell></row><row><cell>5K</cell><cell>0.48</cell><cell>2.32</cell><cell>0.1749</cell></row><row><cell>50K</cell><cell>0.47</cell><cell>2.22</cell><cell>0.1780</cell></row><row><cell>200K</cell><cell>0.47</cell><cell>2.05</cell><cell>0.1883</cell></row><row><cell cols="4">layer (among conv-1 and conv-2) are merged to obtain a</cell></row><row><cell>boosted performance.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>A. Aydın Alatan (SM12) was born in Ankara, Turkey, in 1968.He  received the B.Sc. degree from Middle East Technical University, Ankara, in 1990, the M.Sc. and D.I.C. degrees from the Imperial College of Science, Medicine and Technology, London, U.K., in 1992, and the Ph.D. degree from Bilkent University, Ankara, in 1997, in electrical engineering. He was a Post-Doctoral Research Associate with the Center for Image Processing Research, Rensselaer Polytechnic Institute, Troy, NY, USA, from 1997 to 1998, and the New Jersey Center for Multimedia Research, New Jersey Institute of Technology, Newark, NJ, USA, from 1998 to 2000. In 2000, he joined the faculty of the Department of Electrical and Electronics Engineering at Middle East Technical University. His current research interests include content-based video analysis, data hiding, robust image/video transmission over mobile channels and packet networks, image/video compression, video segmentation, 3D scene analysis, object recognition, detection and tracking.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The source code and the presented results are publicly available. Please check https://github.com/egundogdu/CFCF</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For the convenience of the derivations, θ and the subscript i representing the index of the training example are dropped from the variables.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Online object tracking: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2411" to="2418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<ptr target="http://dblp.uni-trier.de/db/conf/cvpr/cvpr2013.html#WuLY13" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object tracking benchmark</title>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1834" to="1848" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real time robust l1 tracker using accelerated proximal gradient approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, IEEE Conference on</title>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="page" from="1830" to="1837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Robust and fast collaborative tracking with two stage sparse optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kulikowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ECCV, ser. Lecture Notes in Computer Science</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heidelberg</forename><surname>Springer Berlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">6314</biblScope>
			<biblScope unit="page" from="624" to="637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<idno type="DOI">10.1007/978-3-642-15561-1_45</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-642-15561-145" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Struck: Structured output tracking with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="263" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deeptrack: Learning discriminative feature representations by convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BMVC</title>
		<meeting>the BMVC</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on CVPR</title>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="2544" to="2550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2016 challenge results</title>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="http://dx.doi.org/10.1007/s11263-015-0816-y" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Accurate scale estimation for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A scale adaptive kernel correlation filter tracker with feature integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="254" to="265" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<idno type="DOI">10.1007/978-3-319-16181-5_18</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-16181-518" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<idno>I-511-I-518</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001</title>
		<meeting>the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiresolution gray-scale and rotation invariant texture classification with local binary patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ojala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maenpaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="971" to="987" />
			<date type="published" when="2002-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visual tracking with online multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on CVPR</title>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="983" to="990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Real-time tracking via on-line boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.20.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="6" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Part-based visual tracking with online latent structural learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013-06" />
			<biblScope unit="page" from="2363" to="2370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Object tracking via dual linear structured svm and explicit feature map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Incremental learning for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R.-S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-007-0075-7</idno>
		<ptr target="http://dx.doi.org/10.1007/s11263-007-0075-7" />
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="125" to="141" />
			<date type="published" when="2008-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Kernel-based object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="564" to="575" />
			<date type="published" when="2003-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<idno type="DOI">10.1109/TPAMI.2003.1195991</idno>
		<ptr target="http://dx.doi.org/10.1109/TPAMI.2003.1195991" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust visual tracking using l1 minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICCV</title>
		<imprint>
			<date type="published" when="2009-09" />
			<biblScope unit="page" from="1436" to="1443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Minimum error bounded efficient l1 tracker with occlusion detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Blasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Robust visual tracking via structured multi-task sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="367" to="383" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Robust visual tracking via consistent low-rank sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="171" to="190" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visual tracking via online nonnegative matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="374" to="383" />
			<date type="published" when="2014-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Exploiting the circulant structure of tracking-by-detection with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ECCV</title>
		<meeting>the ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">High-speed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Enable scale and aspect ratio adaptability in visual tracking with detection proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the BMVC</title>
		<meeting>the BMVC</meeting>
		<imprint>
			<date type="published" when="2015-09" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="185" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Scalable kernel correlation filter with sparse feature integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Solis Montero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laganiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Real-time part-based visual tracking via adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Correlation filters with limited boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<idno>abs/1403.7876</idno>
		<ptr target="http://arxiv.org/abs/1403.7876" />
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4310" to="4318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Discriminative correlation filter with channel and spatial reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojír</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<idno>abs/1611.08461</idno>
		<ptr target="http://arxiv.org/abs/1611.08461" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Target Response Adaptation for Correlation Filter Tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-46466-4_25</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-46466-425" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="419" to="433" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Staple: Complementary learners for real-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Golodetz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Miksik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="1401" to="1409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Context-aware correlation filter tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="3074" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Convolutional features for correlation filter based visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="621" to="629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06036</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dcfnet: Discriminant correlation filters network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<idno>abs/1704.04057</idno>
		<ptr target="http://arxiv.org/abs/1704.04057" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.158</idno>
		<ptr target="http://dx.doi.org/10.1109/CVPR.2016.158" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="1420" to="1429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning to track at 100 FPS with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<idno type="DOI">10.1007/978-3-319-46448-0_45</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-319-46448-045" />
		<title level="m">European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="749" to="765" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09549</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/journals/cogsci/cogsci14.html#Elman90" />
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Recurrently target-attending tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Sanet: Structure-aware network for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno>abs/1611.06878</idno>
		<ptr target="http://arxiv.org/abs/1611.06878" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">RATM: recurrent attentive tracking model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<idno>abs/1510.08660</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<ptr target="http://arxiv.org/abs/1510.08660" />
		<title level="m">Available</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">First step toward model-free, anonymous object tracking with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno>abs/1511.06425</idno>
		<ptr target="http://arxiv.org/abs/1511.06425" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2014 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
		<ptr target="http://www.votchallenge.net/vot2014/program.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE ECCV Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ensembles of correlation filters for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tokola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bolme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on WACV</title>
		<imprint>
			<date type="published" when="2015-01" />
			<biblScope unit="page" from="935" to="942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Reliable patch trackers: Robust visual tracking by exploiting reliable patches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on CVPR</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">MEEM: robust tracking via multiple experts using entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision (ECCV)</title>
		<meeting>of the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Tracking by sampling trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="1195" to="1202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Co-tracking using semisupervised support vector machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Robust object tracking via sparsity-based collaborative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhong Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ming-Hsuan</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2354409.2354886" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on CVPR, ser. CVPR &apos;12</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1838" to="1845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Online tracking and reacquisition using co-trained generative and discriminative trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-540-88688-4_50</idno>
		<ptr target="http://dx.doi.org/10.1007/978-3-540-88688-450" />
	</analytic>
	<monogr>
		<title level="m">IEEE ECCV</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Modeling and propagating cnns in a tree structure for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<idno>abs/1608.07242</idno>
		<ptr target="http://arxiv.org/abs/1608.07242" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<ptr target="http://arxiv.org/abs/1409.1556" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Nus-pro: A new visual tracking challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="335" to="349" />
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Visual tracking: An experimental survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2013.230</idno>
		<ptr target="http://dx.doi.org/10.1109/TPAMI.2013.230" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1442" to="1468" />
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1505.00853</idno>
		<ptr target="http://arxiv.org/abs/1505.00853" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE ICCV</title>
		<imprint>
			<date type="published" when="2015-12" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Matconvnet -convolutinoal neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Adaptive color attributes for real-time visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">V D</forename><surname>Weijer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014-06" />
			<biblScope unit="page" from="1090" to="1097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Beyond local search: Tracking objects everywhere with instance-specific proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="943" to="951" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
