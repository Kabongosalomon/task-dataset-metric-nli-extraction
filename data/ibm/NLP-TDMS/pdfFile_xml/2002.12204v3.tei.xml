<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Commonsense R-CNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tan</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
							<email>jianqiang.jqh@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Damo Academy</orgName>
								<orgName type="institution">Alibaba Group</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@ntu.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
							<email>qianrusun@smu.edu.sg</email>
							<affiliation key="aff3">
								<orgName type="institution">Singapore Management University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Commonsense R-CNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel unsupervised feature representation learning method, Visual Commonsense Region-based Con-</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>"On the contrary, Watson, you can see everything. You fail, however, to reason from what you see."</p><p>-Sherlock Holmes, The Adventure of the Blue Carbuncle Today's computer vision systems are good at telling us "what" (e.g., classification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b30">31]</ref>, segmentation <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39]</ref>) and "where" (e.g., detection <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b37">38]</ref>, tracking <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b33">34]</ref>), yet bad at knowing "why", e.g., why is it dog? Note that the "why" here does not merely mean by asking for visual reasons -attributes like furry and four-legged -that are already well-addressed by machines; beyond, it also means by asking for high-level commonsense reasons -such as dog barks <ref type="bibr" target="#b16">[17]</ref> -that are still elusive, even for us human philosophers <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b57">58]</ref>, not to mention for machines. A dog is holding a frisbee.</p><p>A dog is jumping up into the air to catch a frisbee. <ref type="figure" target="#fig_6">Figure 1</ref>. Examples of "cognitive errors" in image captioning and VQA due to the dataset bias. The ratio ./. denotes the co-occurrence% in ground-truth text (captioning: captions, VQA: questions). By comparing with the Faster R-CNN <ref type="bibr" target="#b53">[54]</ref> based features <ref type="bibr">[2]</ref>, our VC R-CNN features can correct the errors, e.g., more accurate visual relationships and visual attentions, by being more commonsense awareness.</p><p>It is not hard to spot the "cognitive errors" committed by machines due to the lack of common sense. As shown in <ref type="figure" target="#fig_6">Figure 1</ref>, by using only the visual features, e.g., the prevailing Faster R-CNN <ref type="bibr" target="#b53">[54]</ref> based Up-Down <ref type="bibr">[2]</ref>, machine usually fails to describe the exact visual relationships (the captioning example), or, even if the prediction is correct, the underlying visual attention is not reasonable (the VQA example). Previous works blame this for dataset bias without further justification <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr">7]</ref>, e.g., the large concept co-occurrence gap in <ref type="figure" target="#fig_6">Figure 1</ref>; but here we take a closer look at it by appreciating the difference between the "visual" and "commonsense" features. As the "visual" only tells "what"/"where" about person or leg per se, it is just a more descriptive symbol than its correspondent English word; when there is bias, e.g., there are more person than leg regions co-occur with the word "ski", the visual attention is thus more likely to focus on the person region. On the other hand, if we could use the "commonsense" features, the action of "ski" can focuses on the leg region because of the common sense: we ski with legs.</p><p>We are certainly not the first to believe that visual features should include more commonsense knowledge, rather  <ref type="figure">Figure 2</ref>. The illustration of why P (Y |do(X)) learns common sense while P (Y |X) does not. Thanks to intervention, P (Y |do(X)) can "borrow" objects from other images and "put" them into the local image, to perform further justifications if X truly causes Y regardless of the unobserved confounders, and thus alleviate the observational bias. than just visual appearances. There is a trend in our community towards weakly-supervised learning features from large-scale vision-language corpus <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61]</ref>. However, despite the major challenge in trading off between annotation cost and noisy multimodal pairs, common sense is not always recorded in text due to the reporting bias <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b36">37]</ref>, e.g., most may say "people walking on road" but few will point out "people walking with legs". In fact, we humans naturally learn common sense in an unsupervised fashion by exploring the physical world, and we wish that machines can also imitate in this way.</p><p>A successful example is the unsupervised learning of word vectors in our sister NLP community <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b50">51]</ref>: a word representation X is learned by predicting its contextual word Y , i.e., P (Y |X) in a neighborhood window. However, its counterpart in our own community, such as learning by predicting surrounding objects or parts <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">43]</ref>, is far from effective in down-stream tasks. The reason is that the commonsense knowledge, in the form of language sentences, has already been recorded in discourse; in contrast, once an image has been taken, the explicit knowledge why objects are contextualized will never be observed, so the true common sense that causes the existence of objects X and Y might be confounded by the spurious observational bias, e.g., if keyboard and mouse are more often observed with table than any other objects, the underlying common sense that keyboard and mouse are parts of computer will be wrongly attributed to table.</p><p>Intrigued, we perform a toy MS-COCO <ref type="bibr" target="#b35">[36]</ref> experiment with ground-truth object labels -by using a mental apparatus, intervention, that makes us human <ref type="bibr" target="#b49">[50]</ref> -to screen out the existence of confounders and then eliminate their effect. We compare the difference between association P (Y |X) and causal intervention P (Y |do(X)) <ref type="bibr" target="#b48">[49]</ref>. Before we formally introduce do in Section 3.1, you can intuitively understand it as the following deliberate experiment illustrated in  <ref type="figure">Figure 3</ref>. The sensible difference between the likelihood before (i.e.,P (Y |X)) and after intervention (i.e., P (Y |do(X))) in MS-COCO. The object is represented by the 80 ground-truth class labels. Only 20 pairs are visualized to avoid clutter.</p><p>"put" them around X and Y , then 3) test if X still causes the existence of Y given Z. The "borrow" and "put" is the spirit of intervention, implying that the chance of Z is only dependent on us (probably subject to a prior), but independent on X or Y . By doing so, as shown in <ref type="figure">Figure 3</ref>, P (sink|do(dryer)) is lower because the most common restroom context such as towel is forced to be seen as fair as others. Therefore, by using P (Y |do(X)) as the learning objective, the bias from the context will be alleviated.</p><p>More intrigued, P (person|do(toilet)) is higher. Indeed, person and toilet co-occur rarely due to privacy. However, human's seeing is fundamentally different from machine's because our instinct is to seek the causality behind any association <ref type="bibr" target="#b49">[50]</ref> -and here comes the common sense. As opposed to the passive observation P (Y |X): "How likely I see person if I see toilet", we keep asking "Why does seeing toilet eventually cause seeing person?" by using P (Y |do(X)). Thanks to intervention, we can increase P (Y |do(X)) by "borrowing" non-local context that might not be even in this image, for the example in <ref type="figure">Figure 2</ref>, objects usable by person such as chair and handbag -though less common in the restroom context -will be still fairly "borrowed" and "put" in the image together with the common sink. We will revisit this example formally in Section 3.1.</p><p>So far, we are ready to present our unsupervised region feature learning method: Visual Commonsense R-CNN (VC R-CNN), as illustrated in <ref type="figure" target="#fig_8">Figure 4</ref>, which uses Regionbased Convolutional Neural Network (R-CNN) <ref type="bibr" target="#b53">[54]</ref> as the visual backbone, and the causal intervention as the training objective. Besides its novel learning fashion, we also design a novel algorithm for the do-operation, which is an effective approximation for the imaginative intervention (cf. Section 3.2). The delivery of VC R-CNN is a region feature extractor for any region proposal, and thus it is fundamental and ready-to-use for many high-level vision tasks such as Image Captioning <ref type="bibr" target="#b66">[67]</ref>, VQA <ref type="bibr">[3]</ref>  <ref type="figure" target="#fig_8">Figure 4</ref>. The overview of VC R-CNN. Any R-CNN backbone (e.g., Faster R-CNN <ref type="bibr" target="#b53">[54]</ref>) can be used to extract regions of interest (RoI) on the feature map. Each RoI is then fed into two sibling branches: a Self Predictor to predict its own class, e.g., x c , and a Context Predictor to predict its context labels, e.g., y c , with our Do calculus. The architecture is trained with a multi-task loss.</p><p>shows significant and consistent improvements over strong baselines -the prevailing methods in each task. Unlike the recent "Bert-like" methods <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b59">60]</ref> that require huge GPU computing resource for pre-training features and finetuning tasks, VC R-CNN is light and non-intrusive. By "light", we mean that it is just as fast and memory-efficient as Faster R-CNN <ref type="bibr" target="#b53">[54]</ref>; by "non-intrusive", we mean that re-writing the task network is not needed, all you need is numpy.concatenate and then ready to roll.</p><p>We apologize humbly to disclaim that VC R-CNN provides a philosophically correct definition of "visual common sense". We only attempt to step towards a computational definition in two intuitive folds: 1) common: unsupervised learning from the observed objects, and 2) sensemaking: pursuing the causalities hidden in the observed objects. VC R-CNN not only re-thinks the conventional likelihood-based learning in our CV community, but also provides a promising direction -causal inference [50]via practical experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Multimodal Feature Learning. With the recent success of pre-training language models (LM) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b50">51]</ref> in NLP, several approaches <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b8">9]</ref> seek weakly-supervised learning from large, unlabelled multi-modal data to encode visual-semantic knowledge. However, all these methods suffer from the reporting bias <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b36">37]</ref> of language and the great memory cost for downstream fine-tuning. In contrast, our VC R-CNN is unsupervised learning only from images and the learned feature can be simply concatenated to the original representations. Un-/Self-supervised Visual Feature Learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr">76]</ref>. They aim to learn visual features through an elaborated proxy task such as denoising autoencoders <ref type="bibr">[6,</ref><ref type="bibr" target="#b65">66]</ref>, context &amp; rotation prediction <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref> and data augmentation <ref type="bibr" target="#b32">[33]</ref>. The context prediction is learned from correlation while image rotation and augmentation can be regarded as applying the random controlled trial <ref type="bibr" target="#b49">[50]</ref>, which is active and non-observational (physical); by contrast, our VC R-CNN learns from the observational causal inference that is passive and observational (imaginative). Visual Common Sense. Previous methods mainly fall into two folds: 1) learning from images with commonsense knowledge bases <ref type="bibr" target="#b64">[65,</ref><ref type="bibr">73,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr">77]</ref> and 2) learning actions from videos <ref type="bibr" target="#b18">[19]</ref>. However, the first one limits the common sense to the human-annotated knowledge, while the latter is essentially, again, learning from correlation. Causality in Vision. There has been a growing amount of efforts in marrying complementary strengths of deep learning and causal reasoning <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b47">48]</ref> and have been explored in several contexts, including image classification <ref type="bibr">[8,</ref><ref type="bibr" target="#b39">40]</ref>, reinforcement learning <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr">5]</ref> and adversarial learning <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b25">26]</ref>. Lately, we are aware of some contemporary works on visual causality such as visual dialog <ref type="bibr" target="#b51">[52]</ref>, image captioning <ref type="bibr" target="#b71">[72]</ref> and scene graph generation <ref type="bibr" target="#b61">[62]</ref>. Different from their task-specific causal inference, VC R-CNN offers a generic feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Sense-making by Intervention</head><p>We detail the core technical contribution in VC R-CNN: causal intervention and its implementation. As shown in <ref type="figure" target="#fig_3">Figure 5</ref> (left), our visual world exists many confounders z ∈ Z that affects (or causes) either X or Y , leading to spurious correlations by only learning from the likelihood P (Y |X). To see this, by using Bayes rule:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Causal Intervention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Do-expression</head><formula xml:id="formula_0">P (Y |X) = z P (Y |X, z) P (z|X),<label>(1)</label></formula><p>where the confounder Z introduces the observational bias via P (z|X). For example, as recorded in <ref type="figure" target="#fig_11">Figure 6</ref>, when P (z=sink|X=toilet) is large while P (z=chair|X=toilet) is small, most of the likelihood sum in Eq. (1) will be credited to P (Y =person|X=toilet,z=sink), other than P (Y =person|X=toilet,z=chair), so, the prediction from toilet to person will be eventually focused on sink rather than toilet itself, e.g., the learned features of a region toilet are merely its surrounding sink-like features.</p><p>As illustrated in <ref type="figure" target="#fig_3">Figure 5</ref> (right), if we intervene X, e.g., do(X=toilet), the causal link between Z and X is cutoff. By applying the Bayes rule on the new graph, we have: <ref type="figure" target="#fig_11">Figure 6</ref>. A case study of the differences between P (z|Toilet and P (z) from MS-COCO ground-truth object labels. Only 29 labels of Z are shown to avoid clutter.</p><formula xml:id="formula_1">P (Y |do(X)) = z P (Y |X, z) P (z).<label>(2)</label></formula><p>Compared to Eq. (1), z is no longer affected by X, and thus the intervention deliberately forces X to incorporate every z fairly, subject to its prior P (z), into the prediction of Y . <ref type="figure" target="#fig_11">Figure 6</ref> shows the gap between the prior P (z) and P (z|toilet), z ∈ Z is the set of MS-COCO labels. We can use this figure to clearly explain the two interesting key results by performing intervention. Please note that P (Y |X, z) remains the same in both Eq. (1) and Eq. (2), Please recall <ref type="figure">Figure 3</ref> for the sensible difference between P (Y |X) and P (Y |do(X)).</p><p>First, P (person|do(toilet))&gt;P (person|toilet) is probably because the number of classes z such that P (z|toilet)&gt;P (z) is smaller than those such that of P (z|toilet) &lt; P (z), i.e., the left grey area is smaller than the right grey area in <ref type="figure" target="#fig_11">Figure 6</ref>, making Eq. (1) smaller than Eq. (2). Second, we can see that z making P (z) &lt; P (z|X) is mainly from the common restroom context such as sink, bottle, and toothbrush. Therefore, by using intervention P (Y |do(X)) as the feature learning objective, we can adjust between "common" and "sense-making", thus alleviate the observational bias. <ref type="figure" target="#fig_5">Figure 7</ref>(a) visualizes the features extracted from MS-COCO images by using the proposed VC R-CNN. Promisingly, compared to P (Y |X) (left), P (Y |do(X)) (right) successfully discovers some sensible common sense. For example, before intervention, window and leg features in red box are close due to the street view observational bias, e.g., people walking on street with window buildings; after intervention, they are clearly separated. Interestingly, VC R-CNN leg features are closer to head while window features are closer to wall. Furthermore, <ref type="figure" target="#fig_5">Figure 7</ref>(b) shows the features of ski, snow and leg on same MS-COCO images via Up-Down (left) and our VC R-CNN (right). We can see the ski feature of our VC R-CNN is reasonably closer to leg and snow than Up-Down. Interestingly, VC R-CNN merges into sub-clusters (dashed boxes), implying that the common sense is actually multifacet and varies from context to context.   X → Y or Y → X? We want to further clarify that both two causal directions between X and Y can be meaningful and indispensable with do calculus. For X → Y , we want to learn the visual commonsense about X (e.g., toilet) that causes the existence of Y (e.g., person), and vice versa.</p><p>Only objects are confounders? No, some confounders are unobserved and beyond objects in visual commonsense learning, e.g., color, attributes, and the nuanced scene contexts induced by them; however, in unsupervised learning, we can only exploit the objects. Fortunately, this is reasonable: 1) we can consider the objects as the partially observed children of the unobserved confounder <ref type="bibr" target="#b14">[15]</ref>; 2) we propose the implementation below to approximate the contexts, e.g., in <ref type="figure">Figure 8</ref>, Stop sign may be the child of the confounder "transportation", and Toaster and Refrigerator may contribute to "kitchen".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Proposed Implementation</head><p>To implement the theoretical and imaginative intervention in Eq. (2), we propose the proxy task of predicting the local context labels of Y 's RoI. For the confounder set Z, since we can hardly collect all confounders in real world, we approximate it to a fixed confounder dictionary Z = [z 1 , ..., z N ] in the shape of N × d matrix for practical use, where N is the category size in dataset (e.g., 80 in MS-COCO) and d is the feature dimension of RoI. Each entry z i is the averaged RoI feature of the i-th category samples in dataset. The feature is pre-trained by Faster R-CNN.</p><p>Specifically, given X's RoI feature x and its contextual Y 's RoI whose class label is y c , Eq. (2) can be implemented as z P (y c |x, z) P (z). The last layer of the network for label prediction is the Softmax layer: P (y c |x, z) = Softmax(f y (x, z)), where f y (·) calculates the logits for N categories, and the subscript y denotes that f (·) is parameterized by Y 's RoI feature y, motivated by the intuition that the prediction for y c should be characterized by Y . In Sink : Sink : Dinning <ref type="table">Table   Toaster</ref>   <ref type="figure">Figure 8</ref>. The visualizations of the top 3 confounders given RoI feature x (red box) and y (green box), while numbers denote the attention weight. We can see that our model can recognize reasonable confounders z, e.g., the common context (yellow boxes). summary, the implementation is defined as:</p><formula xml:id="formula_2">P (Y |do(X)) := E z [Softmax(f y (x, z))].<label>(3)</label></formula><p>Note that E z requires expensive sampling. Normalized Weighted Geometric Mean (NWGM). We apply NWGM [69] to approximate the above expectation. In a nutshell, NWGM 2 effeciently moves the outer expectation into the Softmax as:</p><formula xml:id="formula_3">E z [Softmax(f y (x, z))] NWGM ≈ Softmax(E z [f y (x, z)]). (4)</formula><p>In this paper, we use the linear model</p><formula xml:id="formula_4">f y (x, z) = W 1 x + W 2 · g y (z), where W 1 , W 2 ∈ R N ×d</formula><p>denote the fully connected layer. Then the Eq. (4) can be derived as:</p><formula xml:id="formula_5">E z [f y (x, z)] = W 1 x + W 2 · E z [g y (z)].<label>(5)</label></formula><p>Note that the above approximation is reasonable, because the effect on Y comes from both X and confounder Z (cf. the right <ref type="figure" target="#fig_3">Figure 5</ref>).</p><formula xml:id="formula_6">Computing E z [g y (z)]</formula><p>. Specifically, given y and confounder dictionary Z, we firstly calculate the attention vector a = Softmax(q T K/ √ σ), followed by the broadcasting operation to get matrix A = [a; ...; a] of the same shape as Z, where [; ] denotes broadcasting along the row. q = W 3 y, K = W 4 Z T . W 3 , W 4 map each vector to the common subspace and σ denotes the first dimension of W 3 , W 4 as a constant scaling factor. Then we can have</p><formula xml:id="formula_7">E z [g y (z)] = z [A Z]P (z),</formula><p>where P (z) denotes the prior statistic probability and is the element-wise product. <ref type="figure">Figure 8</ref> visualizes the top 3 confounders ranked by the soft attention weights. Note that they are the cancer in learning "sense-making" features from P (Y |X). Neural Causation Coefficient (NCC). Due to the fact that the causality from the confounders as the category averaged features are not yet verified, that is, Z may contain colliders (or v-structure) <ref type="bibr" target="#b48">[49]</ref> causing spurious correlations when intervention. To this end, we apply NCC <ref type="bibr" target="#b39">[40]</ref> to remove possible colliders from Z. Given x and z, N CC (x → z) outputs the relative causality intensity from x to z. Then we discard the training samples with strong collider causal intensities above a threshold. <ref type="bibr">2</ref> The detailed derivation about NWGM can be found in the Supp..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">VC R-CNN</head><p>Architecture. <ref type="figure" target="#fig_8">Figure 4</ref> illustrates the VC R-CNN architecture. VC R-CNN takes an image as input and generates feature map from a CNN backbone (e.g., ResNet101 <ref type="bibr" target="#b22">[23]</ref>). Then, unlike Faster R-CNN <ref type="bibr" target="#b53">[54]</ref>, we discard the Region Proposal Network (RPN). The ground-truth bounding boxes are directly utilized to extract the object level representation with the RoIAlign layer. Finally, each two RoI features x and y eventually branch into two sibling predictors: Self Predictor with a fully connected layer to estimate each object class, while Context Predictor with the approximated do-calculus in Eq. (3) to predict the context label. Training Objectives. The Self-Predictor outputs a discrete probability distribution p = (p <ref type="bibr">[1]</ref>, ..., p[N ]) over N categories (note that we do not have the "background" class). The loss can be defined as</p><formula xml:id="formula_8">L self (p, x c ) = −log(p[x c ])</formula><p>, where x c is the ground-truth class of RoI X. The Context Predictor loss L cxt is defined for each two RoI feature vectors. Considering X as the center object while Y i is one of the K context objects with ground-truth label y c i , the loss is</p><formula xml:id="formula_9">L cxt (p i , y c i ) = −log(p i [y c i ]), where p i is calculated by p i = P (Y i |do(X)) in Eq. (3) and p i = (p i [1], ..., p i [N ])</formula><p>is the probability over N categories. Finally, the overall mulit-task loss for each RoI X is:</p><formula xml:id="formula_10">L (X) = L self (p, x c ) + 1 K i L cxt (p i , y c i ). (6)</formula><p>Feature Extractor. We consider VC R-CNN as a visual commonsense feature extractor for any region proposal.</p><p>Then the extracted features are directly concatenated to the original visual feature utilized in any downstream tasks. It is worth noting that we do NOT recommend early concatenations for some models that contain a self-attention architecture such as AoANet <ref type="bibr" target="#b24">[25]</ref>. The reasons are two-fold. First, as the computation of these models are expensive, early concatenation significantly slows down the training. Second, which is more crucial, the self-attention essentially and implicitly applies P (Y |X), which contradicts to causal intervention. We will detail this finding in Section 5.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>We used the two following datasets for unsupervised learning VC R-CNN. MS-COCO Detection <ref type="bibr" target="#b35">[36]</ref>. It is a popular benchmark dataset for classification, detection and segmentation in our community. It contains 82,783, 40,504 and 40,775 images for training, validation and testing respectively with 80 annotated classes. Since there are 5K images from downstream image captioning task which can be also found in MS-COCO validation split, we removed those in training. Moreover, recall that our VC R-CNN relies on the context prediction task, thus, we discarded images with only one annotated bounding box. Open Images <ref type="bibr" target="#b31">[32]</ref>. We also used a much larger dataset called Open Images, a huge collection containing 16M bounding boxes across 1.9M images, making it the largest object detection dataset. We chose images with more than three annotations from the official training set, results in about 1.07 million images consisting of 500 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>We trained our VC R-CNN on 4 Nvidia 1080Ti GPUs with a total batch size of 8 images for 220K iterations (each mini-batch has 2 images per GPU). The learning rate was set to 0.0005 which was decreased by 10 at 160K and 200K iteration. ResNet-101 was set to the image feature extraction backbone. We used SGD as the optimizer with weight decay of 0.0001 and momentum of 0.9 following <ref type="bibr" target="#b53">[54]</ref>. To construct the confounder dictionary Z, we first employed the pre-trained official ResNet-101 model on Faster R-CNN with ground-truth boxes as the input to extract the RoI features for each object. For training on Open Images, we first trained a vanilla Faster R-CNN model. Then Z is built by making average on RoIs of the same class and is fixed during the whole training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparative Designs</head><p>To evaluate the effectiveness of our VC R-CNN feature (VC), we present three representative vision-and-language downstream tasks in our experiment. For each task, a classic model and a state-of-the-art model were both performed for comprehensive comparisons. For each method,   times. The evaluation metrics consist of three pre-type accuracies (i.e., "Yes/No", "Number" and "Other"). Visual Commonsense Reasoning (VCR). In VCR, given a challenging question about an image, machines need to present two sub-tasks: answer correctly (Q→A) and provide a rationale justifying its answer (QA→R). The VCR dataset [75] contains over 212K (training), 26K (validation) and 25K (testing) derived from 110K movie scenes. The model was evaluated in terms of 4-choice accuracy and the random guess accuracy on each sub-task is 25%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Results and Analysis</head><p>Results on Image Captioning. We compared our VC representation with ablative features on two representative approaches: Up-Down <ref type="bibr">[2]</ref> and AoANet <ref type="bibr" target="#b24">[25]</ref>. For Up-Down model shown in <ref type="table" target="#tab_3">Table 1</ref>, we can observe that with our +VC trained on MS-COCO, the model can even outperform current SOTA method AoANet over most of the metrics. However, only utilizing the pure VC feature (i.e., Only VC) would hurt the model performance. The reason can be obvious. Even for human it is insufficient to merely know the common sense that "apple is edible" for specific tasks, we also need visual features containing objects and attributes (e.g., "what color is the apple") which are encoded by previous representations. When comparing +VC with the +Det and +Cor without intervention, results also show absolute gains over all metrics, which demonstrates the effectiveness of our proposed causal intervention in representation learning. AoANet <ref type="bibr" target="#b24">[25]</ref> proposed an "Attention on Attention" module on feature encoder and caption decoder for refining with the self-attention mechanism. In our experiment, we discarded the AoA refining encoder (i.e., AoANet † ) rather than using full AoANet since the self-attentive operation on feature can be viewed as an indiscriminate correlation against our do-expression. From <ref type="table" target="#tab_3">Table 1</ref> we can observe that our +VC with AoANet † achieves a new SOTA performance. We also evaluated our feature on the online COCO test server in <ref type="table" target="#tab_4">Table 2</ref>. We can find our model also achieves the best single-model scores across all metrics outperforming previous methods significantly. Moreover, since the existing metrics fall short to the dataset bias, we also applied a new metric CHAIR <ref type="bibr" target="#b54">[55]</ref> to measure the object hallucination (e.g., "hallucinate" objects not in image). The lower is better. As shown in <ref type="table">Table 3</ref>, we can see that our VC feature performs the best on both standard and CHAIR metrics, thanks to our proposed intervention that can encode the visual commonsense knowledge. Results on VQA. In <ref type="table">Table 4</ref>, we applied our VC feature on classical Up-Down <ref type="bibr">[2]</ref> and recent state-of-the-art method MCAN [74]. From the results, our proposed +VC outperforms all the other ablative representations on three answer types, achieving the state-of-the-art performance. However, compared to the image captioning, the gains on VQA with our VC feature are less significant. The potential reason lies in the limited ability of the current question understanding, which cannot be resolved by "visual" common sense. Table 5 reports the single model performance of various models on both test-dev and test-standard sets. Although our VC feature is limited by the question understanding, we still receive the absolute gains by just feature concatenation compared to previous methods with complicated module stack, which only achieves a slight improvement. Results on VCR. We present two representative methods R2C [75] and ViLBERT <ref type="bibr" target="#b40">[41]</ref> in this emerging task on the validation set. Note that as the R2C applies the ResNet backbone for residual feature extraction, here for fair comparison we switched it to the uniform bottom-up features. Moreover, for ViLBERT, since our VC features were not involved in the pretraining process on Conceptual Captions, here we utilized the ViLBERT † <ref type="bibr" target="#b40">[41]</ref> rather than the full ViL-BERT model. From the comparison with ablative visual Ski:0.21</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A man standing on skis on a snow covered slope. A man standing on skis in the snow.</head><p>Ski pole:0.20 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A plate of food with a bowl of pasta. A plate of food on the table. A bowl of soup sitting on the table. A pot of soup with broccoli and a spoon.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A group of benches sitting on a bench. A book sitting on top of a wooden bench.</head><p>Sidewalk:0.11 Bench:0.13 <ref type="figure">Figure 9</ref>. Qualitative examples of utilizing our VC feature (right) compared with using Obj feature (left). Boxes in images denote the attention region labeled with name and attention weight. Three rows represent Image Captioning, VQA and VCR task respectively.  <ref type="table">Table 7</ref>. Ablation studies of our proposed intervention trained on MS-COCO and evaluated with CIDEr-D (captioning) and Accuracy (VQA) on Karpathy testset and VQA2.0 validation set.</p><p>representations in <ref type="table">Table 6</ref>, our +VC feature still shows the superior performances similar to the above two tasks.</p><p>Results on Open Images. To evaluate the transfer ability and flexibility of the learned visual commonsense feature, we also performed our proposed VC R-CNN on a large image detection collection. The results can be referred to <ref type="table" target="#tab_3">Table 1&amp;4&amp;6</ref>. We can see that the performances are extremely close to the VC feature trained on MS-COCO, indicating the stability of our learned semantically meaningful representation. Moreover, while performing VCR with the dataset of movie clip, which has quite diverse distributions compared to the captioning and VQA built on MS-COCO, our VC R-CNN trained on Open Images achieves the reasonable better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Qualitative Analysis</head><p>We visualize several examples with our VC feature and previous Up-Down feature <ref type="bibr">[2]</ref> for each task in <ref type="figure">Figure 9</ref>. Any other settings except for feature kept the same. We can observe that with our VC, models can choose more precise, reasonable attention area and explicable better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Ablation Study</head><p>To evaluate our proposed intervention implementation, we carry out different settings for each module in our VC R-CNN and report results on captioning and VQA in Table 7. E z [z] denotes utilizing statistical P (z) by counting from the dataset without attention. Random Dictionary denotes initializing the confounder dictionary by randomization rather than the average RoI feature, while the Context Dictionary encodes contexts in each image as a dynamic dictionary set. The default setting is the fixed confounder dictionary with our attention module and NCC, which gives the best results. We can observe that random dictionary and E z [z] would hurt the performance, which demonstrates the effectiveness of our implementation. Moreover, we can find that NCC refining just brings a little difference to the downstream task performance. The potential reason is that NCC just provides a qualitative prediction and may have deviation when applying on real-world visual feature. We will continue exploring NCC in the future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We presented a novel unsupervised feature representation learning method called VC R-CNN that can be based on any R-CNN framework, supporting a variety of highlevel tasks by using only feature concatenation. The key novelty of VC R-CNN is that the learning objective is based on causal intervention, which is fundamentally different from the conventional likelihood. Extensive experiments on benchmarks showed impressive performance boosts on almost all the strong baselines and metrics. In future, we intend to study the potential of our VC R-CNN applied in other modalities such as video and 3D point cloud. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>In this Supplementary Material, we will further detail the following aspects omitted in the main paper. The detailed code guide and VC features can be referred to https:// github.com/Wangt-CN/VC-R-CNN.</p><p>• Section A: the detailed derivation of the intervention in Section 3.1 Causal Intervention of the main paper .</p><p>• Section B: The details of our proposed implementation in Section 3.2 of the main paper.</p><p>• Section C: The details of the network architecture of our VC R-CNN in Section 4 in the main paper.</p><p>• Section D: more quantitative results of VC features concatenated on on different Faster R-CNN based representations.</p><p>• Section E: more qualitative visualizations compared our VC features with previous bottom-up representations <ref type="bibr">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Do-Expression</head><p>In our main paper, we give the do-expression Eq. (2) comparing with the Bayes rule in an intuitive way for easier understanding. In this section, we further formally explain and prove the intervention (do calculus) in causal theory which is applied in our VC R-CNN. As written in our main paper, in our visual world there may exists many "background factors" z ∈ Z, no matter known or unknown, that affect (or cause) either X or/and Y , leading to spurious correlations by only learning from the likelihood P (Y |X). To avoid the confounder as shown in <ref type="figure" target="#fig_6">Figure 1</ref>, the causal intervention (do calculus) is achieved by cutting off the effect from Z to X in the form of a graph surgery. Here for clear clarification, we use P and P m to distinguish the probabilities in the causal graph before and after surgery, respectively. Therefore, due to the definition of the Do-expression we can have:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Do-expression</head><formula xml:id="formula_11">P (Y |do(X)) = P m (Y |X). (Definition)<label>(1)</label></formula><p>Then the key to compute the causal effect lies in the observation P m , the manipulated probability, shares two essential properties with P (i.e., the original probability function that prevails in the preintervention model). First, the marginal probability P (Z = z) is invariant under the intervention, because the process determining Z is not affected by removing the arrow from Z to X, i.e., P (z) = P m (z). Second, the conditional probability P (Y |X, z) is invariant, because the process by which Y responds to X and Z remains the same, regardless of whether X changes spontaneously or by deliberate manipulation:</p><formula xml:id="formula_12">P m (Y |X, z) = P (Y |X, z). (Invariance)<label>(2)</label></formula><p>Moreover, we can also use the fact that Z and X are independent under the intervention distribution. This tell us that P m (z|X) = P m (z). Putting these considerations together, we have:</p><formula xml:id="formula_13">P (Y |do (X)) = P m (Y |X) = z P m (Y |X, z) P m (z|X) (Bayes Rule) = z P m (Y |X, z) P m (z) (Independency) = z P (Y |X, z) P (z) ,<label>(3)</label></formula><p>where P (Y |X, z) denotes the conditional probability given X and confounder z and P (z) is a prior probability of each object class. The Eq. (3) is called the adjustment formula, which computed the association between X and Y for each value z of Z, then averages over all values. This procedure is referred to as "adjusting for Z" or "controlling for Z". Then with this final expression, we can measure the casual effects between X to Y directly from the data, since it consists only of conditional probabilities.</p><p>Moreover, in the main paper to show the difference between Bayes Rule and Intervention clearly, we propose an example about person and toilet by comparing P (Z) and P (Z|toilet) on partial labels. Here in the Supplementary Material we present the integrated figure for whole 80 MS-COCO labels on both P (Z), P (Z|X) and P (Y |X, Z)P (Z),P (Y |X, Z)P (Z|X) in <ref type="figure">Figure 2</ref>   <ref type="figure">Figure 2</ref> we can see that the do intervention achieves "borrow" and "put" by applying P (Z) to replace P (Z|X), which can be also regarded as a kind of method to alleviate the previous long tail distribution (blue line).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Our Proposed Implementation</head><p>B.1. Normalized Weighted Geometric Mean.</p><p>In our main paper we just give the application of Normalized Weighted Geometric Mean (NWGM) due to the limited space, here we present the detailed derivation and reader can Probability (%) <ref type="figure">Figure 2</ref>. The case study of the differences between P (z|Toilet and P (z) from whole MS-COCO ground-truth object labels. Note that confounders that never appeared with X (i.e., Toilet) is not contained. <ref type="figure">Figure 3</ref>. The case study of the differences between P (Person|Toilet, z)P (z|Toilet) and P (Person|Toilet, z)P (z) from whole MS-COCO ground-truth object labels. Note that confounders that never appeared with X (i.e., Toilet) is not contained. also refer to the <ref type="bibr">[8]</ref>. Recall that in the main paper we have defined the RoI feature x as the X, one of its context class label y c as Y . For the confounder set Z, we denote it as a global confounder dictionary Z = [z 1 , ..., z N ] in the shape of N × d matrix for practical use, where N is the category size in dataset (e.g., 80 in MS-COCO) and d is the feature dimension of x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Probability (%)</head><p>Here we first introduce the normalized weighted geometric mean in our softmax class label prediction:</p><formula xml:id="formula_14">NWGM[f y (x, z)] = z exp(f y (x, z)) p(z) j z exp(f y (x, z)) p(z) = exp(E z [f y (x, z)]) j exp(E z [f y (x, z)]) = Softmax(E z [f y (x, z)]),<label>(4)</label></formula><p>where f y (·) calculates the logits for N categories. Note that the subscript y denotes that f (·) is parameterized by feature y, motivated by the heuristics that the context prediction task for RoI Y is characterized by its visual feature. We can see that the most ingenious operation in Eq. (4) is to change the production to the sum by putting it into the exp. Moreover, from the results in <ref type="bibr">[8,</ref><ref type="bibr">2,</ref><ref type="bibr">7]</ref>, we know <ref type="figure">z)</ref>)] under the softmax activation. Therefore Eq. (3) in the main paper can be further derived as: </p><formula xml:id="formula_15">NWGM[f y (x, z)] ≈ E z [Softmax(f y (x,</formula><formula xml:id="formula_16">P (Y |do(X)) ≈ Softmax(E z [f y (x, z)]).<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Neural Causation Coefficient (NCC)</head><p>Here we give a more detailed information about the usage of NCC and collider of our proposed implementations in the main paper. In our visual world sometimes there are no confounders in the structure like X → Z ← Y what we call "collider", as shown in <ref type="figure" target="#fig_8">Figure 4</ref>. Felix Elwert and Chris Winship <ref type="bibr">[4]</ref> have illustrated this junction using three features of Hollywood actors: Talent (X), Celebrity (Z), and Beauty (Y ). Here we are asserting that both talent and beauty contribute to an actor's success, but beauty and talent are completely unrelated to one another in the general population. In this structure making the intervention on variable Z (i.e., condition on Z) would create a spurious dependence between X and Y . The reason is that if X and Y are independent to begin with, conditioning on Z will make them dependent. For example, if we look only at famous actors (in other words, we observe the variable Celebrity = 1), we will see a negative correlation between talent and beauty: finding out that a celebrity is unattractive increases our belief that he or she is talented. This negative correlation is sometimes called collider bias or the "explain-away" effect. Therefore we cannot make the intervention as what we do before in the collider structure. For simplicity we would make a preliminary examination before training to eliminate the effect of collider in the whole dataset. We apply the neural causation inference model (NCC) <ref type="bibr">[6]</ref> to detect the strong causal effect from X → Z and Y → Z with the RoI feature directly.</p><p>NCC has partly proven <ref type="bibr">[6]</ref> to be efficient for transferring to real-world, visual cause-effect observational samples with just training on artificially constructed synthetic observational samples. Specifically, the n synthetic observational samples S i = {(x ij , y ij )} mi j=1 are drawn from an heteroscedastic additive noise model y ij = f i (x ij ) + v ij e ij for all j = 1, ..., m i , The cause terms x ij are drawn from a mixture of k i Gaussians distributions. We construct each Gaussian by sampling its mean from Gaussian(0, r i ) , its standard deviation from Gaussian(0, s i ) followed by an absolute value, and its unnormalized mixture weight from Gaussian (0, 1) followed by an absolute value. NCC samples k i from RandomInteger <ref type="bibr">[1,</ref><ref type="bibr">5]</ref> and r i ,s i from Uniform <ref type="bibr">[0,</ref><ref type="bibr">5]</ref>. NCC normalizes the mixture weights to sum to one and x ij mi j=1 to zero mean and unit variance. The noise term v ij and e ij are also sampled from Gaussian distribution and mechanism f i is a cubic hermite spline which can be referred to <ref type="bibr">[6]</ref>. Finally NCC is trained with two embedding layers and two classification layers followed by the softmax in a ternary classification task (causal, anticausal and no causation). Then while testing the model can be used to evaluate on the RoI feature vectors directly. The output NCC (x → y) ranges from (0, 1) denotes the relative causality intensity from x inferring y.</p><p>However since the NCC model just can provide a qualitative prediction and may have huge deviation when applying on real-world feature which may affects the training procedure of our VC R-CNN, in our experiment we just discard few training samples with very strong collider causal structure (i.e., X → Z ← Y ) by setting a threshold (we set 0.001 in our experiment). Moreover, we use the objectlevel RoI features extracted by the pretrained Faster R-CNN to pre-calculate the NCC score, which may also lead to a deviation since the pretrained RoI representations may not fully present the objects. From the <ref type="table">Table 7</ref> in the main paper we can also observe that NCC refining just brings a little difference to the downstream task performance. The potential reason is that our VC R-CNN can automatically learn the reasonable confounder attention during the large dataset training. We will continue exploring the usage of NCC and other causal discovery method in our future work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Network Architecture</head><p>Here we introduce the detailed network architectures of all the components of our VC R-CNN in <ref type="table" target="#tab_3">Table 1</ref>. Given an image and the feature extraction backbone, any two RoI feature vectors x and y were extracted as in <ref type="table" target="#tab_3">Table 1</ref> (1)(2). Then as the Section 3.2 The Proposed Implementation, we adopted the Scale Dot-Product Attention to refine confounders from the confounder dictionary Z as in <ref type="table" target="#tab_3">Table 1</ref> (3). A linear addition model f y (x, z) was proposed to combine the effect on Y from both X and confounder Z. Finally we made the do calculus by Self Predictor and Context Predictor in <ref type="table" target="#tab_3">Table 1</ref> (6)(7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Quantitative Results</head><p>In the experiment of our main paper, we adopted the bottom-up feature <ref type="bibr">[1]</ref> as our base feature. The bottomup feature pretrained Faster R-CNN on ImageNet <ref type="bibr">[3]</ref> and Visual Genome <ref type="bibr">[5]</ref> to propose salient object level features with attribute rather than the uniform gird of equally-sized image regions, enable attention to be calculated at the level of semantically meaningful regions and bring a huge improvement in image-and-language tasks.</p><p>Here we also concatenated our VC feature onto the vanilla image region representations based on pretrained Faster R-CNN model with ResNet-101 on MS-COCO dataset. Note that for better comparison we utilized the bounding box coordinates of the bottom-up feature to control the number and location of the boxes and then applied new feature in the Image Captioning task. Results are shown in <ref type="table" target="#tab_4">Table 2</ref>. We can also observe that concatenating with our VC feature can lead to a huge performance improvement, which demonstrates the stability of our VC feature and effectiveness of the proposed intervention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. More Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Failure Case</head><p>Failure in VC R-CNN. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, we can see that sometimes our VC R-CNN cannot make quite reasonable refinement for confounder dictionary via the Scaled  Dot-Product Attention while predicting Y given X and Z, especially when there is no obvious relation between X and Y . For example while making the intervention between dog and vase, chair and fork, the model attends to the giraffe and skateboard respectively. To tackle this limitation, the better schedule of confounder exploring, for example choosing approapiate context objects as the confounder dictionary, will be tried in our future work.</p><p>Failure in Downstream Tasks. Though we designed the intervention (do-expression) in unsupervised representation learning to prevent the cognition error and help machine learn the common sense, some attention errors still exist in downstream tasks. Here we present two examples in <ref type="figure" target="#fig_11">Figure 6</ref>. We can observe that in the VQA example (left), the model provides a reasonable but incorrect answer, while in image captioning the generated description does not cover every instance. The possible reason lies in two folds. First, the current detection technique is still limited, for example the Faster R-CNN cannot recognize the kangaroo on the stop sign. Second, we know that our VC R-CNN can find the probable and reasonable confounders from the confounder dictionary according to the given image. However, it may still fail to exploit the exact confounder (e.g., motorcycle in VQA and lamp, chair in Image Captioning) to fully eliminate the correlation bias.  From the illustration we can observe that with our VC feature, model can generate more fruitful descriptions with more accurate attention. For example, in <ref type="figure">Figure 8</ref> bottom with our VC feature, model focuses on birds and gives the accurate and fruitful descriptions: "two birds perched" rather than "a bird sitting" generated by the baseline model. Furthermore, we can also see that our VC feature can help to overcome the language bias efficiently. Other than giving the common collections, the model can generate reasonable captions according to the image content. For example in the middle of <ref type="figure">Figure 8</ref>, "cat" appearing with "bed" ("cat+bed"/"cat"=6.7%) is quite more often than "cat" with "blanket" ("cat+blanket"/"cat"=1.4%) in the training text, leading to a "hallucination" to generate "bed" without seeing the bed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3. VQA</head><p>We presented the comparison of Faster R-CNN feature (left) and our VC feature (right) in VQA in <ref type="figure">Figure 9</ref> &amp; 10 based on the Up-Down model. We can see that in VQA task the most serious problem is the incorrect attention even with the correct answer, which means the model actually NOT understand the question and make inference combining the vision and language. As we described in Introduction of the main paper, the dataset co-occurring bias may lead to the incorrect attention. For example in the middle of <ref type="figure">Figure 9</ref> the model attend to the horse rather than human since horse and person co-occur too many times. Thanks to our VC feature, the attention becomes better and more accurate with alleviating the correlation bias by our proposed intervention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A vase of flowers sitting on top of a table.</head><p>A white vase filled with purple flowers on top of a table.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vase Flower</head><p>A dog holding a frisbee in his mouth.</p><p>A dog is running with a frisbee in his mouth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dog</head><p>A couple of dog lying on the beach.</p><p>A dog lying on the beach next to the water. <ref type="table">A woman sitting at a  table with a table.</ref> A woman sitting at a table in a restaurant. A black and white cat sitting on a bed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dog Water</head><p>A black and white cat laying on a green blanket.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blanket</head><p>A plane is flying in the sky.</p><p>An airplane is flying in the sky over a tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tree</head><p>A bird sitting on top of a tree.</p><p>Two birds perched on top of a tree.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Birds</head><p>A girl standing next to a sheep.</p><p>A women petting a sheep in a field.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1</head><label></label><figDesc>https://github.com/Wangt-CN/VC-R-CNN Q: Is this person good at skiing? A: No Leg Person "person + ski" / "ski" : 8.4%"leg + ski" / "ski" : 0.2% "frisbee + dog" / "dog" : 7.9% "jumping + dog" / "dog" :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2: 1) "borrow" objects Z from other images, 2)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>The causal intervention P (Y |do(X)). Nodes denote variables and arrows denote the direct causal effects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>( a )</head><label>a</label><figDesc>Object features learned by correlation P (Y |X) and intervention P (Y |do(X)) (our VC R-CNN). Object features of Up-Down features and our VC R-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>The t-SNE visualization<ref type="bibr" target="#b41">[42]</ref> of object features trained on MS-COCO with Up-Down[2]  provided Faster R-CNN labels. Features out of the label legend are faded out to avoid clutter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 .</head><label>1</label><figDesc>The do expression P (Y |do(X)) with a graph surgery. Nodes denote variables and arrows mean the direct causal effects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>The causal graph structure of the "collider". Nodes denote variables, arrows denote the direct causal effects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 .</head><label>5</label><figDesc>The examples of the failure case about confounder finding in VC R-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>living room with lamps, a couch and a chair. Ours: A living room with a couch and a table.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 .</head><label>6</label><figDesc>The examples of the failure case in downstream tasks.E.2. Image CaptioningFigure 7 &amp; 8 exhibit visualizations of utilizing our VC feature (right) compared with using Faster R-CNN feature (i.e., bottom-up feature, left) with the classical Up-Down model in image captioning task. The boxes represent the attended regions when generating words with the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>WallFigure 7 .</head><label>7</label><figDesc>Qualitative visualizations in Image Captioning with utilizing Faster R-CNN feature (left) and our VC feature (right). Boxes in image represent the attention region when generating words with the same color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>SheepFigure 8 .Figure 9 .</head><label>89</label><figDesc>Qualitative visualizations in Image Captioning with utilizing Faster R-CNN feature (left) and our VC feature (right). Boxes in image represent the attention region when generating words with the same color. Q: Is the man wearing a scarf? the rider a child or an adult? A: Adult Person Horse Q: Is the rider a child or an adult? The qualitative results of Visual Question Answering by using the Faster R-CNN feature (left) and concatenated with our VC feature (right). Boxes denote the attended region when answering.Q: Is this woman legs stuck?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 .</head><label>10</label><figDesc>The qualitative results of Visual Question Answering by using the Faster R-CNN feature (left) and concatenated with our VC feature (right). Boxes denote the attended region when answering.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Person Dog NWGM Attention + Self Predictor Context Predictor Confounder Dictionary … FC FC RoI Featrue Conv Feature Map Do FC</head><label></label><figDesc></figDesc><table><row><cell>, and VCR [75]. Through extensive experiments in Section 5, VC R-CNN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>] 36.3 27.7 56.9 120.1 36.3 27.7 56.9 120.1 Obj 36.7 27.8 57.5 122.3 36.7 27.8 57.5 122.3 Only VC 34.5 27.1 56.5 115.2 35.1 27.2 56.6 115.7 +Det 37.5 28.0 58.3 125.9 37.4 27.9 58.2 125.7 +Cor 38.1 28.3 58.5 127.5 38.3 28.4 58.8 127.4 +VC 39.5 29.0 59.0 130.5 39.1 28.8 59.0 130.0 AoANet † Origin 3 [25] 38.9 28.9 58.8 128.4 38.9 28.9 58.8 128.4 Obj 38.1 28.4 58.2 126.0 38.1 28.4 58.2 125.9 Only VC 35.8 27.6 56.8 118.1 35.8 27.9 56.7 118.5 +Det 38.8 28.8 58.7 128.0 38.7 28.6 58.7 127.7 +Cor 38.8 28.9 58.7 128.6 38.9 28.8 58.7 128.2 +VC 39.5 29.3 59.3 131.6 39.3 29.1 59.0 131.5 SOTA AoANet [25] 38.9 29.2 58.2 129.8 38.9 29.2 58.2 129.8 The image captioning performances of representative two models with ablative features on Karpathy split. The metrics: B4, M, R and C denote BLEU@4, METEOR, ROUGE-L and CIDEr-D respectively. The grey row highlight our features in each model. AoANet † indicates the AoANet without the refine encoder. Note that the Origin and Obj share the same results in MS-COCO and Open Images since they does not contain our new trained features.</figDesc><table><row><cell cols="2">Model Feature</cell><cell>B4</cell><cell>MS-COCO M R</cell><cell>C</cell><cell>B4</cell><cell>Open Images M R</cell><cell>C</cell></row><row><cell>Up-Down</cell><cell>Origin [2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Down [2] 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5 SGAE [70] 37.8 68.7 28.1 37.0 58.2 73.1 122.7 125.5 CNM [71] 37.9 68.4 28.1 36.9 58.3 72.9 123.0 125.3 AoANet [25] 37.3 68.1 28.3 37.2 57.9 72.8 124.0 126.2 Up-Down+VC 37.8 69.1 28.5 37.6 58.2 73.3 124.1 126.2 AoANet † +VC 38.4 69.9 28.8 38.0 58.6 73.8 125.5 128.1 The performances of various single models on the online MS-COCO test server. Up-Down+VC and AoANet † +VC are the short for concatenated on [2] in Up-Down and AoANet † .</figDesc><table><row><cell>Model</cell><cell cols="2">BLEU-4</cell><cell cols="2">METEOR</cell><cell cols="2">ROUGE-L</cell><cell cols="2">CIDEr-D</cell></row><row><cell>Metric</cell><cell>c5</cell><cell>c40</cell><cell>c5</cell><cell>c40</cell><cell>c5</cell><cell>c40</cell><cell>c5</cell><cell>c40</cell></row><row><cell cols="9">Up-Model Feature CHs Chi Model Feature CHs Chi</cell></row><row><cell>Up-Down</cell><cell>Obj +Det +Cor +VC</cell><cell cols="2">12.8 8.1 12.0 7.5 11.2 7.1 10.3 6.5</cell><cell>AoANet  †</cell><cell>Obj +Det +Cor +VC</cell><cell></cell><cell cols="2">12.6 8.0 9.5 6.2 10.4 6.5 8.8 5.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Hallucination analysis<ref type="bibr" target="#b54">[55]</ref> of various models on MS-COCO Karpathy test split to measure object hallucination for image captioning. The lower, the better. The VQA task requires answering natural language questions according to the images. We evaluated the VQA model on VQA2.0<ref type="bibr" target="#b19">[20]</ref>.<ref type="bibr" target="#b41">42</ref>.8 55.8 63.2 80.3 42.8 55.8 63.2 Only VC 77.8 37.9 51.6 59.8 77.9 38.1 51.1 59.9 +Det 81.8 44.5 56.8 64.5 81.9 44.7 56.5 64.6 +Cor 81.5 44.6 57.1 64.7 81.3 44.7 57.0 64.6 +VC 82.5 46.0 57.6 65.4 82.8 45.7 57.4 65.4 MCAN Obj [74] 84.8 49.4 58.4 67.1 84.8 49.4 58.4 67.1 Only VC 80.8 40.7 48.9 60.1 81.0 40.8 49.1 60.3 +Det 84.8 49.2 58.8 67.2 84.9 49.3 58.4 67.2 +Cor 85.0 49.2 58.9 67.4 85.1 49.1 58.6 67.3 +VC 85.2 49.4 59.1 67.7 85.1 49.1 58.9 67.5 SOTA MCAN 84.8 49.4 58.4 67.1 84.8 49.4 58.4 67.1 Accuracy (%) of various ablative features on VQA2.0 validation set. Since the Obj achieves almost equal results with that in the original paper, here we just merge the two rows.</figDesc><table><row><cell>Compared with VQA1.0 [3], VQA2.0 has more question-image pairs for training (443,757) and validation (214,354),</cell></row></table><note>we used the following five ablative feature settings: 1) Obj: the features based on Faster R-CNN, we adopted the popu- lar used bottom-up feature [2]; 2) Only VC: pure VC fea- tures; 3) +Det: the features from training R-CNN with sin- gle self detection branch without Context Predictor. "+" de- notes the extracted features are concatenated with the orig- inal feature, e.g., bottom-up feature; 4) +Cor: the features from training R-CNN by predicting all context labels (i.e., correlation) without the intervention; 5) +VC: our full fea- ture with the proposed implemented intervention, concate- nated to the original feature. For fair comparisons, we re- tained all the settings and random seeds in the downstream task models. Moreover, since some downstream models may have different settings in the original papers, we also quoted their results for clear comparison. For each down- stream task, we detail the problem settings, dataset and eval- uation metrics as below. Image Captioning. Image captioning aims to generate tex- tual description of an image. We trained and evaluated on the most popular "Karpathy" split built on MS-COCO dataset, where 5K images for validation, 5K for testing, and the rest for training. The sentences were tokenized and changed to lowercase. Words appearing less than 5 times were removed and each caption was trimmed to a maximum of 16 words. Five standard metrics were applied for evaluat- ing the performances of the testing models: CIDEr-D [64], BLEU [47], METROT [4], ROUGE [35] and SPICE [1]. Visual Question Answering (VQA).and all the question-answer pairs are balanced. Before train- ing, we performed standard text pre-processing. Questions were trimed to a maximum of 14 words and candidate an- swer set was restricted to answers appearing more than 8</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>Single model accuracies (%) on VQA2.0 test-dev and test set, where Up-Down+VC and MCAN+VC are the short for Object-VC R-CNN feature in Up-Down and MCAN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table 6. Experimental results on VCR with various visual features. ViLBERT</figDesc><table><row><cell>Model Feature</cell><cell cols="4">MS-COCO Q→ A QA→ R Q→ A QA→ R Open Images</cell></row><row><cell>R2C ViLBERT  † SOTA ViLBERT  † [41] Origin [75] Obj Only VC +Det +Cor +VC Obj 3 Only VC +Det +Cor +VC</cell><cell>63.8 65.9 64.1 66.1 66.5 67.4 69.1 68.8 69.2 69.3 69.5 69.3</cell><cell>67.2 68.2 66.7 68.5 68.9 69.5 69.6 70.1 69.8 69.9 70.2 71.0</cell><cell>63.8 65.9 64.3 66.1 66.6 67.2 69.1 68.9 69.1 69.2 69.5 69.3</cell><cell>67.2 68.2 66.8 68.3 69.1 69.9 69.6 70.1 69.6 70.0 70.3 71.0</cell></row></table><note>† [41] denotes ViLBERT without pretraining process.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table : 0.19 Plate:0.39</head><label>:</label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>[ 73 ]</head><label>73</label><figDesc>Mark Yatskar, Vicente Ordonez, and Ali Farhadi. Stating the obvious: Extracting visual common sense knowledge. In NAACL, 2016. 3 [74] Zhou Yu, Jun Yu, Yuhao Cui, Dacheng Tao, and Qi Tian. Deep modular co-attention networks for visual question answering. In CVPR, 2019. 7 [75] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi.</figDesc><table><row><cell>From recognition to cognition: Visual commonsense reason-</cell></row><row><cell>ing. In CVPR, 2019. 2, 7</cell></row><row><cell>[76] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lu-</cell></row><row><cell>cas Beyer. S4l: Self-supervised semi-supervised learning. In</cell></row><row><cell>ICCV, 2019. 3</cell></row><row><cell>[77] Yuke Zhu, Alireza Fathi, and Li Fei-Fei. Reasoning about</cell></row><row><cell>object affordances in a knowledge base representation. In</cell></row><row><cell>ECCV. Springer, 2014. 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Furthermore, we use the linear model f y (x, z) = W 1 x + W 2 · g y (z), where W 1 , W 2 ∈ R N ×d denote the fully connected layer. Then the linear projection of the expectation of one variable equals to the linear projection of that and we can put E into the linear projection asSoftmax(W 1 E z [x] + W 2 · E z [g y (z)])). Since the RoI representation x remains the same, we can discard the E over That means the expectation of the outputs over all possible confounder z can be simply computed by feedforward propagation with</figDesc><table><row><cell cols="4">Index Input (1) -(2) -(3) (2), Z Scale Dot-Product Attention E z [g y (z)] (1024 × 1) Operation Output RoI feature x (1024 × 1) RoI feature y (1024 × 1) (4) (1),(3) Linear Addition Model E z [f y (x, z)] (80 × 1) (5) (1) Feature Embedding W x (80 × 1) (6) (5) Self Predictor Softmax</cell><cell>Trainable Parameters --W 3 (512 × 1024) W 4 (512 × 1024) W 1 (80 × 1024) W 2 (80 × 1024) W (80 × 1024) -</cell></row><row><cell>(7)</cell><cell>(4)</cell><cell>Context Predictor</cell><cell>Softmax</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="3">Table 1. The detailed network architecture of our VC R-CNN.</cell></row><row><cell cols="3">the expectation vector E z [g y (z)] as the input.</cell><cell></cell><cell></cell></row></table><note>x, i.e., Softmax(W 1 x + W 2 · E z [g y (z)])).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 2 .</head><label>2</label><figDesc>25.9 54.7 18.9 104.7 77.1 32.6 25.2 55.2 18.3 110.6 Obj+Det 75.4 34.4 26 55.8 19.9 108.9 77.9 33.9 25.4 56.1 19.8 114.7 Obj+Cor 75.6 34.5 26.1 55.2 19.6 108.7 78.0 34.1 25.6 56.0 19.9 115.2 Obj+VC 76.3 35.3 26.3 56.3 20.2 111.6 79.1 35.7 25.9 57.0 20.5 119.7 The image captioning performances of two models with ablative features (based on vanilla Faster R-CNN feature) on Karpathy split.</figDesc><table><row><cell>Model Feature</cell><cell>Cross-Entropy Loss B@1 B@4 M R</cell><cell>S</cell><cell>C</cell><cell cols="2">CIDEr Optimization B@1 B@4 M R</cell><cell>S</cell><cell>C</cell></row><row><cell cols="5">Up-Down 33.2 AoANet Obj 74.5 Obj 74.6 34.1 25.9 55.4 19.7 108.1 78.1 Obj+Det 75.1 33.9 26.1 55.7 19.8 109.7 78.3 Obj+Cor 75.5 34.3 26.2 55.9 20.1 110.8 78.7 Obj+VC 76.0 35.0 26.4 56.1 20.5 112.2 79.1</cell><cell cols="3">35.4 25.6 56.7 20.7 118.4 36.2 27.1 56.9 20.9 120.2 36.8 27.5 57.2 21.1 121.1 37.2 29.0 57.6 21.5 123.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Probability (%)</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Since we cannot achieve performances reported in original paper using the official code even with the help of author, here we show ours as the baseline. The original results can be found at the bottom row: SOTA.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Pot:0. <ref type="bibr" target="#b28">29</ref> Acknowledgments We would like to thank all reviewers for their constructive comments. This work was partially supported by the NTU-Alibaba JRI and the Singapore Ministry of Education (MOE) Academic Research Fund (AcRF) Tier 1 grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satanjeev</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACLW</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A meta-transfer objective for learning to disentangle causal mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasim</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olexa</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10912</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep generative stochastic networks trainable by backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Laufer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Reducing unimodal biases for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Dancette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Chalupka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2309</idno>
		<title level="m">Visual causal feature learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">El</forename><surname>Kholy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11740</idno>
		<title level="m">Uniter: Learning universal image-text representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformer-XL: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishita</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jane</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Chiappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jovana</forename><surname>Mitrovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeb</forename><surname>Kurth-Nelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08162</idno>
		<title level="m">Causal reasoning from meta-reinforcement learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Who killed the directed model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Domke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alap</forename><surname>Karapurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On multi-cause approaches to causal inference with unobserved counfounding: Two cautionary failure cases and a promising alternative</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Damour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Dynamic fusion with intra-and inter-modality attention flow for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengkai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The theory of affordances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gibson</surname></persName>
		</author>
		<idno>1977. 1</idno>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Hilldale, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The&quot; something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yash</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tejas</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Common sense concepts about motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ibrahim</forename><surname>Abou Halloun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Hestenes</surname></persName>
		</author>
		<idno>1985. 1</idno>
	</analytic>
	<monogr>
		<title level="j">American journal of physics</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Women also snowboard: Overcoming bias in captioning models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaylee</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Yong</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diviyan</forename><surname>Kalainathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Goudet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michèle</forename><surname>Sebag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.04929</idno>
		<title level="m">Structural agnostic model, causal discovery and penalized adversarial learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bilinear Attention Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin-Hwa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaehyun</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Causalgan: Learning causal implicit generative models with adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sriram</forename><surname>Vishwanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Revisiting self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ales</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luka</forename><surname>Cehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustav</forename><surname>Hager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Pflugfelder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<title level="m">The open images dataset v4. IJCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Rethinking data augmentation: Self-supervision and self-distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hankook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05872</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text summarization branches out</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Don&apos;t just listen, use your imagination: Leveraging visual common sense for non-visual tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Discovering causal signals in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Beyond categories: The visual memex model for reasoning about object relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomasz</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyosha</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Explicit bias discovery in visual question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varun</forename><surname>Manjunatha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nirat</forename><surname>Saini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Causal induction from visual observations for goal directed tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suraj</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01751</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL. Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Interpretation and identification of causal mediation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<idno>2014. 3</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The book of why: the new science of cause and effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Mackenzie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Basic Books</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Two causal principles for improving visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Overcoming language priors in visual question answering with adversarial regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aishwarya</forename><surname>Sainandan Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Object hallucination in image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaylee</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sophia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rosenfeld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Harvard University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Viske: Visual knowledge extraction and question answering by visual verification of relation phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fereshteh</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Santosh K Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">The structures of the common-sense world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning visual knowledge memory networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqi</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yurong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">LXMERT: Learning crossmodality encoder representations from transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019-11" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Generative image modeling using spatial lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning common sense through visual abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tanmay</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. ACM</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Ask me anything: Free-form visual question answering based on knowledge from external sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning to collocate neural modules for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Deconfounded image captioning: A causal retrospect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03923,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The dropout learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Baldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sadowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">210</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="78" to="122" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Endogenous selection bias: The problem of conditioning on a collider variable. Annual review of sociology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Elwert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Winship</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="31" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Discovering causal signals in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6979" to="6987" />
		</imprint>
	</monogr>
	<note>Bernhard Scholkopf, and Léon Bottou</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
