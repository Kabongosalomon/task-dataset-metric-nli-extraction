<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Gumbel-Attention for Multi-modal Machine Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengbo</forename><surname>Liu</surname></persName>
							<email>liupengbo.work@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailong</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
							<email>tjzhao@mtlab.hit.edu.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Harbin</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Gumbel-Attention for Multi-modal Machine Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-modal machine translation (MMT) improves translation quality by introducing visual information. However, the existing MMT model ignores the problem that the image will bring information irrelevant to the text, causing much noise to the model and affecting the translation quality. In this paper, we propose a novel Gumbel-Attention for multi-modal machine translation, which selects the text-related parts of the image features. Specifically, different from the previous attentionbased method, we first use a differentiable method to select the image information and automatically remove the useless parts of the image features. Through the score matrix of Gumbel-Attention and image features, the image-aware text representation is generated. And then, we independently encode the text representation and the image-aware text representation with the multi-modal encoder. Finally, the final output of the encoder is obtained through multi-modal gated fusion. Experiments and case analysis prove that our method retains the image features related to the text, and the remaining parts help the MMT model generates better translations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-modal machine translation (MMT) is a novel research field of machine translation, which not only considers text information but also uses other modal information (mostly visual modal information) to improve translation effects. Under the influence of visual modal information, the translation result will be more accurate, since the contextual representation of the fusion of visual information will reduce ambiguity.</p><p>Recent research explores various methods based on the seq2seq network for MMT. transform and make the image features as one of the steps in the encoder as text, in order to make it possible to attend to both the text and the image while decoding.  uses global image features to initialize the hidden state of the encoder/decoder.  adopts the mechanism of visual attention to jointly optimize the shared visual language three people flying a kite on a small hill at dusk. drei personen lassen in der dämmerung auf einem kleinen hügel einen drachen steigen. three people flying a kite on a small hill at dusk <ref type="figure">Figure 1</ref>: An example of useless information in the Multi30k dataset. The above one is the original picture, and the bottom is the remaining part after masking the parts irrelevant to the text in the picture. embedding and model, which links visual semantics with corresponding text semantics. In Zero-resource Machine Translation, images are used to generate bilingual descriptions to complete the translation model, thereby solving the problem of lack of data .</p><p>Transformer <ref type="bibr" target="#b7">[Vaswani et al., 2017]</ref> introduces a multihead self-attention mechanism, which can capture relationships between words in a sentence. There have been many improved MMT models based on the Transformer recently. <ref type="bibr" target="#b2">[Ive et al., 2019]</ref> propose a translate-and-refine method based on deliberation networks and structured visual information on Transformer, where images are only used by a second stage decoder. To solve the problem of relative importance among different modalities, ] models text modal and vision modal from the perspective of a graph network.</p><p>The above methods integrate multi-modal information in multi-modal machine translation, and has made great progress. Despite their success, the current studies did not exploit how to automatically select the information that is valuable to the text in the image mixed with noise. The noise in the image will not help understand the context and affect the performance of the model. As shown in <ref type="figure">Figure 1</ref>, more than two-thirds of the original image is masked, and the remaining part can still fully express the semantics of the text. Even if we only keep the area around the entities in the picture (such as "three people", "kites"), we can explain all the content in the text. Namely, most of the content in the image is not re-lated to the text in some scenarios. We believe that selecting the necessary parts of the image and reducing the noise of visual information during the training process will help the model to capture image information related to the text. That is to say, a selecting method is essential.</p><p>In this paper, we propose a novel attention mechanism using <ref type="bibr">Gumbel-Sigmoid [Geng et al., 2020]</ref> to automatically select meaningful information in the image, called Gumebl-Attention. The selected part of the image can help the model understand the context. Through this method, an imageaware text representation can be obtained, and then we use both image-aware text representation and text representation in the encoder. In summary, our major contributions are as follows:</p><p>• We propose a novel Gumbel-Attention MMT model. Different from the conventional attention mechanism, our Gumbel-Attention is a selecting method in multimodal scenarios. To the best of our knowledge, this is the first attempt to perform denoising with sampling method during training among multimodal machine translation models. • We also design a loss function to constrain the visual representation, expecting the image-aware text representation to be semantically similar to the text representation. It further ensures that parts irrelevant to the text in the image are removed. • Our method achieves or nears the state-of-the-art performance on the three test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>Our model is based on the structure of Transformer and incorporates visual modal information. In this section, we will elaborate our proposed Gumbel-Attention MMT model. The visual information related to text is selected and integrated into the model by Gumbel-Attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Text-only Transformer Model</head><p>Given the source language sentence X, we firstly use the encoder to get the representation of X. Next, the decoder will generate the target language sentence Y based on the representation of source language. Input. We convert the input sentence into the combination of the position encoding and word embedding of each word. The word embedding is obtained by random initialization, and position encoding is calculated by sine and cosine functions. The sum E input of word embedding and position encoding will be input of the model.</p><p>Encoder. The encoder consists of N enc layers, and each layer contains two sub-layers: multi-head self-attention and position-wise fully connected feed-forward network. We employ residual connection and layer normalization between each layer. The output of the encoder H s is contextual representation of the source language, and the dimensions of H s are the same as the input E input of the encoder.</p><p>Decoder. Similarly, the decoder consists of N dec layers. In addition to the two sub-layers included in the encoder, the decoder has a third sub-layer that performs multi-head attention on the output of the encoder. After obtaining the output  E output of the decoder, we can generate words through full connection layer and softmax operations. In our work, the main improvements are concentrated in the encoder of text-only Transformer, and we will keep the original decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gumbel-Attention</head><p>Compared with Text-only Transformer Model, the main improvement of our model is to propose the Gumbel-Attention mechanism that can denoise image information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Vanilla Multi-head Attention</head><p>Multi-head Attention mainly consists of scaled dot-product attention. Q, K, V represent the query, key and value, respectively. The output is calculated as:</p><formula xml:id="formula_0">Attention(Q, K, V ) = softmax QK T √ d k V<label>(1)</label></formula><p>Besides, the multi-head attention mechanism concatenates a series of basic attention with different parameters to enhance the performance of the model.</p><formula xml:id="formula_1">MultiHead(Q, K, V ) = Concat (head 1 , . . . , head h ) W O where head i = Attention QW Q i , KW K i , V W V i (2) where W Q i ∈ R d model ×d k , W K i ∈ R d model ×d k , W V i ∈ R d model ×dv and W O ∈ R hdv×d model are parameter matrices.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gumbel-Attention</head><p>In the encoder block of multi-modal machine translation, we propose Gumbel-Attention to reduce the interference of irrelevant information in the image and map image and text to the same semantic space. More specifically, our main purpose is to select the regions in the image that are really helpful for the text semantics, instead of directly using the entire picture as in the previous work.</p><p>Softmax and Selecting. Selecting relevant content in the picture is a question of selecting a few elements in some candidate sets. The usual approach is to normalize them using the softmax function first and then select the candidate elements according to the probability. This is also a common method for classification tasks. However, the selecting of the image is an intermediate step of the entire model in this task, which should be differentiable and can be backpropagated.</p><p>Gumbel-Max trick and Gumbel-Softmax. To draw samples z from a categorical distribution with class probabilities π,  provides Gumbel-Max method:</p><formula xml:id="formula_2">z = one-hot arg max i [g i + log π i ]<label>(3)</label></formula><p>where g i = − log (− log (u i )) , u i ∼ Uniform(0, 1), called Gumbel(0, 1) distribution. This method is equivalent to sampling by probability, but it is still not differentiable. <ref type="bibr" target="#b3">[Jang et al., 2017]</ref> approximates argmax with softmax and introduces Gumbel-Softmax:</p><formula xml:id="formula_3">y i = exp ((log (π i ) + g i ) /τ ) k j=1 exp ((log (π j ) + g j ) /τ ) for i = 1, . . . , k</formula><p>(4) where τ ∈ (0, ∞) is a hyperparameter controlling the distribution tendency of sampling results. When τ is smaller(such as 0.1), the sampling result tends to be closer to a real one-hot vector. In contrast, the sampling result will be more similar in each dimension when τ becomes larger. <ref type="bibr">Gumbel-Sigmoid. [Geng et al., 2020]</ref> proposed Gumbel-Sigmoid to select a subset of elements from all elements helping contribute to the meaning of the sentence by paying more attention to content words. The implement of Gumebl-Sigmoid is similar to Gumbel-Softmax by adding Gumbel noise in the sigmoid function:</p><formula xml:id="formula_4">Gumbel-Sigmoid (E s ) = sigmoid ((E s + G − G ) /τ ) = exp ((E s + G ) /τ ) exp ((E s + G ) /τ ) + exp (G /τ )<label>(5)</label></formula><p>where the input E s is a matrix, and G , G are two independent noises called Gumbel noise. we can obtain differentiable sample by Gumbel-Sigmoid. Similarly, in the local features of the image, a similar method can also be used to select whether each feature is retained.</p><p>Gumbel-Attention. To map the vision information into the semantic space of the text, a simple method is using attention mechanism. In our work, we introduce an improved method for attention mechanism using Gumbel-Sigmoid, called Gumbel-Attention. The Gumbel-Attention mechanism a young man performing a trick on his skateboard .</p><p>visual modal text modal <ref type="figure">Figure 3</ref>: A simple description of Gumbel-Attention. In our attention mechanism, only the region associated with the current word in the image is selected to participate in the score matrix calculation.</p><p>can be regarded as a selecting of the part of the image that is related to the text, which is a differentiable, discretely distributed approximate sampling. The selecting for each element in the score matrix in the attention is as follows:</p><formula xml:id="formula_5">α ij = Gumbel-Sigmoid    x text i W Q x image j W K T √ d model    (6) where x text i is the i th text features, x image j is the j th image spatial features, d model is dimension of model, and W Q ∈ R dtext×d model , W K ∈ R dimage×d model is trainable parameter matrices.</formula><p>The score-matrix consists of the sampling results of each word and all regional features of the image. Then, we will get image-aware text representation using score matrix:</p><formula xml:id="formula_6">v i = n j=1α ij x text j W V<label>(7)</label></formula><p>v i is i th image-aware text representation which calculate the weighted sum and only use the image features related to the current word. <ref type="figure">Figure 3</ref> provides an easy-to-understand explanation of the principle of this novel attention mechanism. Gumbel-Attention in matrix form is as follows:</p><formula xml:id="formula_7">Gumbel-Attention(Q, K, V ) = Gumbel-Sigmoid QK T √ d k V<label>(8)</label></formula><p>To enhance the selecting accuracy of Gumbel-Attention, we also use multiple heads to improve ability of Gumbel-Attention to filter image features, just like the attention in vanilla transformer:</p><formula xml:id="formula_8">MultiHead-Gumbel-Attention(Q, K, V ) = Concat (Gumbel-Attention 1 , . . . , Gumbel-Attention h ) W O<label>(9)</label></formula><p>Besides, similar to Gumbel-Sigmoid, regions with a probability higher than the threshold are selected as output in the inference stage. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-modal Encoder and Gated Fusion</head><p>Multi-modal Gated Encoder. We use two independent transformer encoder to encode text representation and imageaware text representation respectively, and fuse them together after obtaining their respective outputs:</p><formula xml:id="formula_9">h image i = TransformerEncoder(v i ) h text j = TransformerEncoder(x text j )<label>(10)</label></formula><p>Multi-modal Gated Fusion. To merge the output of the two modals, we introduce a gating mechanism to control the fusion of h image and h text refering to recent work :</p><formula xml:id="formula_10">λ = sigmoid W h image + U h text H = h text + λh image<label>(11)</label></formula><p>where W and U are trainable parameters. The final output H is directly fed to the vanilla transformer decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Multi-modal Similarity Loss</head><p>We propose a loss function to make the text and image expression more similar, which is also a supplement to image denoising:</p><formula xml:id="formula_11">loss sim = max(0, 1 − cosine(h image , h text ) − margin)<label>(12)</label></formula><p>where margin is a hyperparameter that controls the degree of similarity slack 1 , and we use cosine similarity to measure the similarity of two vector. The loss function of the entire model is as follows:</p><formula xml:id="formula_12">loss = (x,y)∈D − log p(y | x) + α loss sim<label>(13)</label></formula><p>where x, y represent source language sentence and target language sentence respectively, α is a trainable parameter to control the proportion of loss sim , and D is the corpus. 1 margin should be a number from -1 to 1, 0-0.5 is suggested. In our experiment, we choose 0.3 as the value of margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>Following previous work, we conduct experiments on the widely used Multil30k  dataset to investigate the effectiveness of our proposed model, which is the largest existing manual labeling dataset for multi-modal machine translation. Each picture in the Multil30k dataset is equipped with an English description, and the description is manually translated into German and French. The dataset contains 29,000 instances for training, 1,014 instances for validation, and 1,000 instances for testing(Test2016). We also evaluate our model on the WMT17 test set(Test2017) and the MSCOCO test, which contain 1,000 and 461 instances respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Setup</head><p>Our model is implemented based on OpenNMT-py toolbox <ref type="bibr" target="#b4">[Klein et al., 2017]</ref>. Due to the small size of the training corpus, our encoder and decoder only use 4 layers, the number of attention heads is 4, and the input and output dimensions are both 128. We adopt Adam with β 1 = 0.9, β 2 = 0.98 to optimeze our model.</p><p>We used official script 2 of Multi30k to preprocess the data, containing tokenization and the byte pair encoding(BPE) <ref type="bibr" target="#b6">[Sennrich et al., 2016]</ref>. Spatial features are extracted from VGG19 network as the visual representation. The feature dimension is 7 × 7 × 512 , which represents the local information in the image. The text representation is the randomly initialized word embedding . Finally, we evaluate the translation quality using BLEU <ref type="bibr" target="#b5">[Papineni et al., 2002]</ref> and ME-TEOR [Denkowski and Lavie, 2011]. <ref type="table">Table 1</ref> shows the results of all method on three test set. Our Gumbel-Attention MMT model is better than most existing models, except for Trg-mul  in meteor. One possible reason is that the result of Trg-mul comes from the system on the latest technology WMT2017 test set, which has been selected based on METEOR. An obvious finding is that our model surpasses the text-only Transformer by above 1.5 bleu points. In fact, Gumbel-Attention based method has 2 https://github.com/Multi30k/dataset/tree/master/scripts  only a minor modifications on the vanilla Transformer, which proves the effectiveness of our model. Moreover, we draw two important conclusions: First, compared with Multi-modal Transformer , the main improvement of our method is the Gumbel-Sigmoid operation in the score matrix and the addition of the loss function. The results of Test2016 show that this method of information selection based on Gumbel-Sigmoid is indeed effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>Second, our method is simpler while achieving better results compared with the transformer-based two-stage model Deliberation networks <ref type="bibr" target="#b2">[Ive et al., 2019]</ref>. This shows that our method is not only effective, but the model structure is also as simple as possible compared to other methods, which is conducive to reproducibility and easy to apply to other tasks.</p><p>Overall, our model maintains the best or near best performance on the three test sets. Therefore, we reconfirmed the effectiveness and universality of Gumbel-Attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Which Layer Applies Gumbel-Attention?</head><p>To explore which layer of the encoder is more suitable for applying Gumbel-Attention, we apply Gumbel-Attention before the 1st to 4th layer respectively. For instance, if Gumbel-Attention is applied on the 3rd layer, in the first two layers we use a transformer encoder to encode image features, and then interact with the text representation through Gumbel-Attention to get image-aware text representation. Then in the last two layers, we continue to encode the obtained imageaware text representation to get the final output. The comparison results are shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>The conclusion we got is similar to that of : applying the Gumbel-Attention before 1st layer achieves the best performance. This conclusion is intuitive: we need to select and encode valuable visual information as early as possible. This also shows that the multi-modal Gumbel-Sigmoid selecting method is very similar to the pure text Gumbel-Sigmoid selecting method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Ablation Study</head><p>To investigate the effectiveness of different modules in Gumbel-Attention MMT, we further compare our method  with the variants in <ref type="table" target="#tab_3">Table 2</ref>. replace with vanilla-attention. In this variant, we replace Gumebl-Attention with vanilla-attention, which performs a weighted sum of similarity between text and image information instead of selecting. This method can also make reasonable use of picture information, so the performance is improved compared with text-only baseline. Additionally, Gumbel-Attention model and vanilla-attention-based model have a gap of more than 1 bleu on Test2016, which demonstrates the influence of image noise on the MMT task.</p><p>replace with random images. With reference to Multimodal Transformer , we conducted random images replacement experiments. The result of  indicates the model with random images performs even worse than the text-only model. However, different from the previous conclusion, the result of row 4 shows that using random images to replace the original images in the Gumbel-Attention model is still better than text-only. This suggests that the random image as a regularization item improves the model effect, similar to the effect of random noise on the model .</p><p>shared parameters. In this variant, we encode text representation and image-aware text representation with the shared encoder, which can reduce a large number of parameters in the model. Although there is a decline compared with the original model, it is still an improvement over the text-only Transformer. Through the method of sharing parameters, we can obtain the gain brought by the image information with a smaller training cost.</p><p>w/o multi-modal gated fusion. Instead of multi-modal gated fusion, we directly sum the outputs of two independent encoders. The result in row 6 shows the multi-modal features fusion method based on the gate is a more suitable method than direct addition.</p><p>w/o multi-modal similarity loss We delete the multimodal similarity loss in the loss function. The results of row 7 show that compared with the original model, the performance does not drop much. This shows that image-aware representation is semantically close to text representation even without additional loss function constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Case Study and Error Analysis</head><p>In this section, we discuss an error in translation and analyze the advantages of our method over other methods. we display the best translations of the four cases generated by different models(including text-only Transformer, multi-modal src.</p><p>a boy hangs out of a passing taxi cab window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ref.</head><p>ein junge hängt aus dem fenster eines vorbeifahrenden taxis.</p><p>Text-only Transformer ein junge hängt aus einem taxi an.</p><p>Multi-modal Transformer ein junge hängt aus einem sanitärcontainer.</p><p>Vanilla-Attention ein junge hängt an einem geschäft aus dem fenster.</p><p>Gumbel-Attention ein junge hängt aus einem taxi an einem taxi.  <ref type="figure" target="#fig_3">Figure 5</ref>. Because the word "vorbeifahren" in the reference translation only appears twice in the training set, and the training set is much maller than other machine translation datasets, it is difficult to translate the accurate meaning. It can be seen from <ref type="figure" target="#fig_3">Figure 5</ref> that none of these four models accurately translate all meanings of the source text, including our method. But in terms of the degree of error and the severity of the error, the gap between the four models is very large.</p><p>Different from some images in the Multi30k dataset, the objects in the image are very cluttered. The taxi and the boy are in the middle position, and the upper part of the picture is the shops, passing vehicles and pedestrians. Intuitively, the extra objects in the image will bring noise to the model and cause erroneous translation results.</p><p>In the translation results of the multi-modal Transformer model, the word "sanitärcontainer" appeared, which is not related to the semantic information of text and pictures, and even "sanitärcontainer" is not in the original Multi30k training vocabulary. The fundamental reason is that  use the back-translation method to expand the data set, which brings in vocabulary outside the domain. However, if the back-translation method is not used, the result of this model is "ein junge hängt aus dem fenster eines busses und fährt aus dem fenster.", which is farther away from the correct result.</p><p>The word "geschäft" appeared in the results of the vanillaattention model. This word caused the translation error of the whole sentence. In the text-only machine translation model (such as the text-only transformer in the line 3), it is almost difficult to make this error, because "geschäft" and the source language text have a large semantic difference. The underlying reason is that the shop in the picture affected the imageaware text representation, which in turn mislead the model.</p><p>In the translation of our Gumbel-Attention model, not all words are completely translated. However, due to the noise reduction capability of Gumbel-Attention, compared with other multi-modal translation models, our translation results do not introduce noisy words and contains more information than the translation result of the text-only model. This case shows that Gumbel-Attention can filter out irrelevant parts of the image and provide visual-modal auxiliary information for the text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future work</head><p>In this paper, we have proposed Gumbel-Attention for multimodal machine translation, which aims to reduce irrelevant noise in images by differentiable method to select the image information. Our experiment results shows that we have achieved state-of-the-art results on Multi30k test set, and it demonstrates the effectiveness of our model. We also introduced multi-modal similarity loss to further restrict image representation and text representation to be more similar. In the ablation study, we proved that even if the original picture is replaced with a random picture, the translation result will be improved due to the effect of regularization. This is different from the previous conclusion . Moreover, we give an example of a translation that does not perform well in all models. The example proves that our method can improve the quality of translation without introducing noise from irrelevant objects in the image.</p><p>In future work, we will study the noise filtering of image information in the decoder. We are also very interested in the principle of the association mechanism between these two modals, and look forward to doing some explanatory work to analyze which stage of the image plays a role in multi-modal machine translation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Multi-modal Gumbel-Attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>the position of applying Gumbel-Attention on the model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The result of applying Gumbel-Attention before different layers, "after layer 4" means to use Gumbel-Attention to get imageaware text representation after the respective encoding of the image and text.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Translation cases of different models, red indicates parts that are seriously inconsistent with semantics, and blue indicates parts that are consistent with semantics Transformer, vanilla-attention-based model and our Gumbel-Attention model) as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Ablation study of our model on Test2016</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">From words to sentences: A progressive learning approach for zero-resource machine translation with visual pivots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop ; Caglayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI</title>
		<editor>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan</editor>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI<address><addrLine>Stella Frank</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2986" to="2995" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Statistical theory of extreme values and some practical applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Emil Julius</forename><surname>Gumbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Gumbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation</meeting>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="639" to="645" />
		</imprint>
		<respStmt>
			<orgName>Association for Computer Linguistics</orgName>
		</respStmt>
	</monogr>
	<note>Attention-based multimodal neural machine translation</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distilling translations with visual awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Ive</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6525" to="6538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Opennmt: Open-source toolkit for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="67" to="72" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Papineni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural machine translation of rare words with subword units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computer Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computer Linguistics</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A visual attention grounding neural model for multimodal machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaowei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Mingyang Zhou, Runxiang Cheng, Yong Jae Lee, and Zhou Yu</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3643" to="3653" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">From words to sentences: A progressive learning approach for zero-resource machine translation with visual pivots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bishop ; Caglayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI</title>
		<editor>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Omar Zaidan</editor>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI<address><addrLine>Stella Frank</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2986" to="2995" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention-based multimodal neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Gumbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Emil Julius Gumbel. Statistical theory of extreme values and some practical applications. NBS Applied Mathematics Series</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1954" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6525" to="6538" />
		</imprint>
	</monogr>
	<note>5th International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computer Linguistics</title>
		<editor>Rico Sennrich, Barry Haddow, and Alexandra Birch</editor>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computer Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A visual attention grounding neural model for multimodal machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaowei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>Mingyang Zhou, Runxiang Cheng, Yong Jae Lee, and Zhou Yu</editor>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Brussels</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3643" to="3653" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
