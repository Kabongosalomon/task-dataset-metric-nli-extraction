<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with Minimal Resources</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2020-07-15">15 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianhui</forename><surname>Wu</surname></persName>
							<email>wuqianhui@tsinghua.org.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist) Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijia</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxin</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist) School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">BÃ¶rje</forename><forename type="middle">F</forename><surname>Karlsson</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biqing</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Beijing National Research Center for Information Science and Technology (BNRist) Department of Automation</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with Minimal Resources</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-15">15 Jul 2020</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For languages with no annotated resources, transferring knowledge from rich-resource languages is an effective solution for named entity recognition (NER). While all existing methods directly transfer from source-learned model to a target language, in this paper, we propose to fine-tune the learned model with a few similar examples given a test case, which could benefit the prediction by leveraging the structural and semantic information conveyed in such similar examples.</p><p>To this end, we present a meta-learning algorithm to find a good model parameter initialization that could fast adapt to the given test case and propose to construct multiple pseudo-NER tasks for meta-training by computing sentence similarities. To further improve the model's generalization ability across different languages, we introduce a masking scheme and augment the loss function with an additional maximum term during meta-training. We conduct extensive experiments on cross-lingual named entity recognition with minimal resources over five target languages. The results show that our approach significantly outperforms existing state-of-the-art methods across the board.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Named entity recognition (NER) is the task of locating and classifying text spans into pre-defined categories such as locations, organizations, etc. It is a fundamental component in many downstream tasks. Most state-of-the-art NER systems employ neural architectures <ref type="bibr" target="#b10">(Huang, Xu, and Yu 2015;</ref><ref type="bibr" target="#b14">Lample et al. 2016;</ref><ref type="bibr" target="#b17">Ma and Hovy 2016;</ref><ref type="bibr" target="#b3">Chiu and Nichols 2016;</ref><ref type="bibr" target="#b23">Peters et al. 2017;</ref>, and thus, depend on a large amount of manually annotated data, which prevents their adaptation to low-resource languages due to the high annotation cost. An effective solution to this problem, which we refer to as cross-lingual named entity recognition, is transferring knowledge from a high-resource source language with abundant annotated data to a low-resource target language with limited or even no annotated data.</p><p>In this paper, we attempt to address the extreme scenario of cross-lingual transfer with minimal resources, where there is only one source language with rich labeled data while no labeled data is available in target languages. To tackle this problem, some approaches convert the crosslingual NER task into a monolingual NER task by performing annotation projection using bilingual parallel text and word alignment information <ref type="bibr" target="#b22">(Ni, Dinu, and Florian 2017)</ref>. To eliminate the requirement of parallel texts, some methods propose to translate the labeled data of the source language at the phrase/word level, which inherently provides alignment information for label projection <ref type="bibr" target="#b18">(Mayhew, Tsai, and Roth 2017;</ref><ref type="bibr" target="#b38">Xie et al. 2018)</ref>. Instead of generating labeled data in target languages, other works explore language-independent features and perform crosslingual NER in a direct-transfer manner, where the model trained on the labeled data of the source language is directly tested on target languages <ref type="bibr" target="#b32">(Tsai, Mayhew, and Roth 2016;</ref><ref type="bibr" target="#b22">Ni, Dinu, and Florian 2017)</ref>. Among these methods, cross-lingual word representations are the most prevalent language-independent features. For example, the multilingual version of BERT <ref type="bibr" target="#b4">(Devlin et al. 2019)</ref> utilizes Word-Piece modeling strategy to project word embeddings of different languages into a shared space and achieved state-ofthe-art performance <ref type="bibr" target="#b36">(Wu and Dredze 2019)</ref>. In this paper, we leverage the multilingual BERT <ref type="bibr" target="#b4">(Devlin et al. 2019</ref>) as a base model to produce cross-lingual word representations.</p><p>While all existing direct transfer based methods straightly evaluate the source-trained model on target languages, we hold the idea that the source-trained model can be further effectively improved. Indeed, recent developments in learning cross-lingual sentence representations suggest that any sentence can be encoded into a shared space by building universal cross-lingual encoders <ref type="bibr" target="#b36">(Wu and Dredze 2019;</ref><ref type="bibr" target="#b13">Lample and Conneau 2019)</ref>. By simply calculating cosine similarity between sentences in different languages with multilingual BERT <ref type="bibr" target="#b4">(Devlin et al. 2019)</ref>, we find that it is possible to retrieve a few source examples that are quite similar to a given target example in structure or semantics, as shown in <ref type="table" target="#tab_1">Table 1</ref>. In Example #1, both sentences have a structure of "Location -Date", while in Example #2, both sentences are about people talking about sports. Intuitively, reviewing the structural and semantic information conveyed by similar examples might benefit prediction. Therefore, given a test example in a target language, we pro-#1 Ginebra <ref type="bibr">[B-LOC]</ref> , 23 may ( EFECOM <ref type="bibr">[B-ORG]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>#2</head><p>Flores <ref type="bibr">[B-PER]</ref> afirmÃ³: "conÃ©l intentaremos ganar en velocidad, que es una de las mejores virtudes que tiene este equipo." (Flores said: "With him, we will try to win faster, which is one of the best advantages of this team.") "Things fell in for us," said Sorrento <ref type="bibr">[B-PER]</ref>, who has six career grand slams and hit the ninth of the season for the Mariners <ref type="bibr">[B-ORG]</ref> . pose to first retrieve a small set of similar examples from the source language, and then, use these retrieved examples to fine-tune the model before testing. However, if the retrieved similar set is too large, too much noise will be introduced via relatively distant examples. And thus to avoid misleading the model with distant examples, the cardinality of a similar group is typically small. In such a scenario, the model is expected to achieve higher performance on a test example after only one or a few fine-tuning steps using the limited-size set of retrieved examples. This inspires us to apply meta-learning, which aims to learn a model that facilitates fast adaptation to new tasks with a minimal amount of training examples <ref type="bibr" target="#b0">(Andrychowicz et al. 2016;</ref><ref type="bibr">Vinyals et al. 2016;</ref><ref type="bibr" target="#b5">Finn, Abbeel, and Levine 2017)</ref>.</p><p>In this paper, we follow the recently proposed modelagnostic meta-learning approach <ref type="bibr" target="#b5">(Finn, Abbeel, and Levine 2017)</ref> and extend it to the cross-lingual NER task with minimal resources, where no labeled data is provided in target languages. We construct a set of pseudo-meta-NER tasks using the labeled data from the source language and propose a meta-learning algorithm to find a good model parameter initialization that could fast adapt to new tasks. When it comes to the adaptation phase, we regard each test example as a new task, build a pseudo training set for it, and fine-tune the meta-trained model before testing.</p><p>When adapting meta-learning to cross-lingual NER, we notice that most mispredictions occur on language-specific infrequent entities. It is known that an NER system makes predictions through word features of an entity itself, and the syntactic or semantic information of its context. However, most entities are generally of low frequency in the training corpora of the base model, and thus, entity representations across different languages are not well-aligned in the shared embedding space. That is, for the prediction of a lowfrequency entity, over-dependence on its own features will inhibit the model transferring across languages. Therefore, we introduce a masking scheme on named entities during meta-training to weaken the dependence on entities and promote the prediction through contextual information. Meanwhile, considering that the commonly used average loss over all tokens treats each token equally though some tokens may be more difficult to learn and easier to be mispredict, we add a maximum term to the original loss function, which makes the model focus more on such tokens and thus reduce mispredictions, so that the meta-knowledge of these mispredictions will not be transferred to target languages.</p><p>To summarize our contributions: â¢ We propose a model-agnostic meta-learning-based approach to tackle cross-lingual NER with minimal resources. To our best knowledge, this is the first successful attempt in adapting meta-learning to NER. â¢ We propose a masking scheme on named entities and augment the loss function with an additional maximum term during meta-training, to facilitate the model's ability to generalize across different languages. â¢ We evaluate our approach over 5 target languages, i.e., Spanish, Dutch, German, French, and Chinese. We show that the proposed approach significantly outperforms existing state-of-the-art methods across the board.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-lingual NER with Minimal Resources</head><p>There are two major branches of work in cross-lingual NER with minimal resources: methods based on annotation projection and methods based on direct transfer. One of the typical approaches in the annotation projection category is to take bilingual parallel corpora, annotate the source side, and project the annotations to the target using learned word alignment information <ref type="bibr" target="#b22">(Ni, Dinu, and Florian 2017)</ref>. However, these methods depend on parallel texts, as well as annotations in at least one side, which is unavailable in many cases. To eliminate the requirement of parallel data, some approaches first translate source-language labeled data at the word/phrase level, and then directly copy labels across languages <ref type="bibr" target="#b38">(Xie et al. 2018;</ref><ref type="bibr" target="#b18">Mayhew, Tsai, and Roth 2017)</ref>. Yet, this might bring in too much noise due to sense ambiguity and word order differences. Differently, most approaches based on direct transfer leverage language-independent features to train a model on the source language and then directly apply it on target languages. Cross-lingual word embeddings are the most widely used ones of such features <ref type="bibr" target="#b22">(Ni, Dinu, and Florian 2017;</ref><ref type="bibr" target="#b4">Devlin et al. 2019)</ref>, while other approaches also introduces word clusters <ref type="bibr" target="#b28">(TÃ¤ckstrÃ¶m, McDonald, and Uszkoreit 2012)</ref> and Wikifier <ref type="bibr" target="#b32">(Tsai, Mayhew, and Roth 2016)</ref> as crosslingual features.</p><p>In this paper, we use a contextual cross-lingual word embedding <ref type="bibr" target="#b4">(Devlin et al. 2019)</ref> as the language-independent feature. Rather than directly transferring from the sourcelearned model to target, we propose to fine-tune the model by converting the minimal-resource cross-lingual transfer problem into a low-resource learning problem, and furthermore, present an enhanced meta-learning algorithm to tackle it. To our best knowledge, we are the first to extend the idea of meta-learning to cross-lingual NER with minimal resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Learning</head><p>Meta-learning has a long history <ref type="bibr" target="#b20">(Naik and Mammone 1992)</ref> and emerged recently as a way to fast adapt to new tasks with very limited data. It has been applied to various tasks such as image classification <ref type="bibr" target="#b12">(Koch, Zemel, and Salakhutdinov 2015;</ref><ref type="bibr" target="#b26">Ravi and Larochelle 2017)</ref>, neural machine translation <ref type="bibr" target="#b6">(Gu et al. 2018</ref>), text generation <ref type="bibr" target="#b25">Qian and Yu 2019)</ref>, and reinforcement learning <ref type="bibr" target="#b5">(Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b15">Li et al. 2018)</ref> There are three categories of meta-learning algorithms: learning a metric space which can be used to compare lowresource examples with rich-resource examples <ref type="bibr">(Vinyals et al. 2016;</ref><ref type="bibr" target="#b27">Sung et al. 2018)</ref>, learning an optimizer to update the parameters of a model <ref type="bibr" target="#b0">(Andrychowicz et al. 2016;</ref><ref type="bibr" target="#b2">Chen et al. 2018)</ref>, and learning a good parameter initialization of a model <ref type="bibr" target="#b5">(Finn, Abbeel, and Levine 2017;</ref><ref type="bibr" target="#b19">Mi et al. 2019)</ref>.</p><p>Our approach falls into the last category. We extend the idea of model-agnostic meta-learning (MAML) <ref type="bibr" target="#b5">(Finn, Abbeel, and Levine 2017)</ref> to the cross-lingual NER with minimal resources by constructing multiple pseudo-meta-NER tasks. Furthermore, we employ a masking scheme and enhance the loss function with an additional maximum item during meta-training to improve the model's ability to transfer across languages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head><p>Named Entity Recognition is proposed as a sequence labeling problem. Given a sequence with L tokens</p><formula xml:id="formula_0">x = {x i } L i=1 , an NER system is expected to produce a label sequence y = {y i } L i=1</formula><p>, where x i is the i-th token and y i is the corresponding label of x i . Denote the labeled training data of a source language as D S train and the test data of a target language as D T test . Minimal-resource cross-lingual NER aims to train a model M with D S train and it is expected that the model will perform well on D T test .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Base Model</head><p>In this section, we give a brief introduction to multilingual BERT <ref type="bibr" target="#b4">(Devlin et al. 2019</ref>) (mBERT), which we leverage as the base model in our approach, since it produces an effective cross-lingual word representation. To ease the explanation, we start with BERT <ref type="bibr" target="#b4">(Devlin et al. 2019)</ref> here.</p><p>BERT is a language model learned with the Transformer encoder <ref type="bibr" target="#b33">(Vaswani et al. 2017</ref>). It reads the input sequence at once and learns via two strategies, i.e., masked language modeling and next sentence prediction.</p><p>mBERT follows the same model architecture and training procedure as BERT except that it is pre-trained on concatenated Wikipedia data of 104 languages. For tokenization, mBERT utilizes WordPiece embeddings <ref type="bibr" target="#b37">(Wu et al. 2016)</ref> with a 110k shared vocabulary to facilitate embedding space alignment across different languages.</p><p>Following <ref type="bibr" target="#b4">(Devlin et al. 2019</ref>) and (Wu and Dredze 2019), we address cross-lingual NER by adding a linear classification layer with softmax upon the pre-trained mBERT, which can be formulated as:</p><formula xml:id="formula_1">h = mBERT(x),<label>(1)</label></formula><formula xml:id="formula_2">y l = softmax(W h l + b),<label>(2)</label></formula><p>where x is structured as {x 0 , x 1 , ..., x L , x L+1 }. x 0 = [CLS] and x L+1 = [SEP] are two special tokens as in <ref type="bibr" target="#b4">(Devlin et al. 2019)</ref>. h = {h l } L l=1 and h l denotes the output of the pretrained mBERT that corresponds to the input token x l .Å· l denotes the predicted probability distribution for x l . W and b are trainable parameters.</p><p>The learning loss w.r.t. x is modeled as the cross-entropy of the predicted label distribution and the ground-truth one for each token:</p><formula xml:id="formula_3">L(Î¸) = â 1 L L l=1 CrossEntropy(y l ,Å· l )<label>(3)</label></formula><p>where y l is a one-hot vector of the ground-truth label for the l-th input token x l . And the total loss for learning is the summation of losses on all training examples. It should be noted that, if a word is split into several subwords after tokenization, only the label of the first subword is considered.</p><p>Enhanced Meta-Learning for Cross-Lingual NER with Minimal Resources</p><p>In this section, we elaborate on the proposed approach. First, we clarify how to construct multiple pseudo-meta-NER tasks with the labeled data of the source language. Then, we describe the meta-training algorithm of our approach. Next, we illustrate the proposed masking mechanism and the augmented loss involved in the meta-training phase. Finally, we show how to adapt the meta-learned model to test examples of target languages. The whole procedure of our algorithm is summarized in Algorithm 1.</p><p>Pseudo-Meta-NER Tasks In a typical meta-learning scenario, a model is trained on a set of tasks in the meta-training phase, such that the trained model can quickly adapt to new tasks using only a small number of examples. Thus to tackle the minimal-resource cross-lingual NER via meta-learning, we first construct a set of pseudo-meta-NER tasks using the labeled data of the source language.</p><formula xml:id="formula_4">Assuming there are N examples in D S train = {x (i) } N i=1</formula><p>. We take each x (i) as the test set D Ti test of an individual meta task T i , and create a pseudo training set D Ti train for it by retrieving the most similar examples of x (i) from D S train . The pseudo-meta-NER tasks T i can be denoted as:</p><formula xml:id="formula_5">T i = (D Ti train , D Ti test ), i â 1, 2, ..., N .</formula><p>(4) Specifically, we first compute the sentence representation r (i) for each x (i) , i â {1, 2, ..., N }:</p><formula xml:id="formula_6">r (i) = f (x (i) )</formula><p>(5) where f (Â·) could be any function that is able to produce cross-lingual sentence representations. Here, we employ the multilingual BERT <ref type="bibr" target="#b4">(Devlin et al. 2019</ref>) and use the the final hidden vector corresponding to the first input token ([CLS]) as the sentence representation.</p><p>Then, we construct D Ti train by selecting top-K similar examples from D S train \ x (i) . The metric used to measure the similarity between x (i) and x (m) is:</p><formula xml:id="formula_7">s(x (i) , x (m) ) = r (i) Â· r (m) r (i) r (m) ,<label>(6)</label></formula><p>Algorithm for all T i do 7:</p><p>Update Î¸ â² i = U n (Î¸; Î±).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Compute end for 20: end procedure where m â {1, 2, ..., N } and m = i.</p><formula xml:id="formula_8">g i = â Î¸ â² i L D T i test (Î¸ â² i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Meta-Training</head><p>In the meta-training phase, we train a model M by repeatedly simulating the adaptation phase, where the meta-trained model is fine-tuned with a minimal amount of training data of a new task and then tested on the test data.</p><p>Specifically, given the created pseudo-meta-NER tasks {T i } N i=1 and a model M Î¸ parameterized by Î¸, we first randomly sample a task T i to derive new model parameters Î¸ â² via n gradient updates on the original model parameters Î¸, which we refer to as inner-update:</p><formula xml:id="formula_9">Î¸ â² i = U n (Î¸; Î±)<label>(7)</label></formula><p>where U n is the operator that performs gradient descent n times with the learning rate Î± to minimize the loss L D T i train computed on D Ti train . For example, when applying a single gradient update,</p><formula xml:id="formula_10">Î¸ â² i = Î¸ â Î±â Î¸ L D T i train (Î¸)<label>(8)</label></formula><p>We then evaluate the updated parameters Î¸ â² i on D Ti test and further update the meta model M Î¸ by minimizing the loss L D T i test (Î¸ â² i ) with respect to Î¸, which is referred to as meta-update. When aggregating multiple pseudo-meta-NER tasks, the meta-objective is:</p><formula xml:id="formula_11">min Î¸ i L D T i test (Î¸ â² i )<label>(9)</label></formula><p>Take a single gradient update with the learning rate Î², the meta-update can be formulated as:</p><formula xml:id="formula_12">Î¸ â Î¸ â Î² i â Î¸ L D T i test (Î¸ â² i ) = Î¸ â Î² i g i (10)</formula><p>where g i is the meta-gradient on task T i , which can be expanded to:</p><formula xml:id="formula_13">g i = â Î¸ L D T i test (Î¸ â² i ) = â Î¸ â² i L D T i test (Î¸ â² i )â Î¸ (Î¸ â² i )<label>(11)</label></formula><p>In Equation 11, â Î¸ (Î¸ â² i ) is the Jacobian matrix of the update operation U n that will introduce higher order gradient. To reduce computational cost, we use a first-order approximation by replacing the Jacobian â Î¸ (Î¸ â² i ) with the identity matrix as in <ref type="bibr" target="#b5">(Finn, Abbeel, and Levine 2017)</ref>. Therefore, g i can be computed as:</p><formula xml:id="formula_14">g i = â Î¸ â² i L D T i test (Î¸ â² i )<label>(12)</label></formula><p>Compared with the common training scheme, the metalearned model is more sensitive to the changes among different tasks, which can promote the learning of the common internal representations rather than the distinctive features of the source language training data D S train . When coming to the adaptation phase, the model could be more sensitive to the features of new tasks, and hence only one or a few fine-tune epochs on a minimal amount of data can make rapid progress without overfitting <ref type="bibr" target="#b5">(Finn, Abbeel, and Levine 2017)</ref>.</p><p>Masking on Named Entities For cross-lingual NER with minimal resources, the alignments of the entity representations in the shared space are particularly important as this task focuses on understanding entities across languages. However, compared with commonly used words, most entities are of low frequency in the pre-training corpora of the base model. As a result, the learned entity representations across languages are not well-aligned in the shared space.</p><p>In order to reduce the dependence on target entity representations and encourage the model to predict through context information, we employ the [MASK] token as introduced in <ref type="bibr" target="#b4">(Devlin et al. 2019)</ref> to mask entities at the token level in each training example, i.e., each token inside an entity is randomly masked with a given probability. Then, the masked examples are fed as input data for the model. Note that we re-perform the masking scheme at the beginning of each training epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max Loss</head><p>In Equation 3, the loss for each token is uniformly weighted so that all tokens contribute equally when training the model. Nonetheless, this will result in insufficient learning for those tokens with relatively higher losses. In order to force the model to put more effort in learning from such tokens, we modify the loss function as:</p><formula xml:id="formula_15">L(Î¸) = â 1 L L i=1 CrossEntropy(y i ,Å· i ) â Î» max iâ{1,2,...,L} CrossEntropy(y i ,Å· i )<label>(13)</label></formula><p>where Î» â¥ 0 is a weighting factor. In this way, the potential mispredictions of the high-loss tokens would probably be corrected during meta-training. The benefit of such correction is that the meta-knowledge about the mispredictions, which is also going to be transferred to target tasks, would be reduced, so that the model could achieve better performance after transferring. In summary, the L D T i train and L D T i test in Meta-Training of Algorithm 1 are with the masking scheme and the max loss.</p><p>Adaptation When it comes to the adaptation phase, i.e., applying M Î¸ * on target languages, we take each test example x (j) â D T test as the test set D Tj test of a target task T j . We then construct a pseudo training set D  <ref type="bibr">i.e., x (j)</ref> . It should be noted that in the adaptation phase, we do not perform the masking scheme to avoid information loss of target entities. Besides, since the size of the pseudo training set is very small, we employ the loss function as in Equation 3 rather than Equation 13 to prevent over-adjusting on uncertain or mispredicted tokens. In fact, when using Equation 13 for adaptation, the model could achieve slightly better performance in some cases but also get worse performance in others due to the mentioned over-adjustment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we evaluate our enhanced meta-learning approach for cross-lingual NER with minimal resources and compare our approach to current state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We conduct experiments on four benchmark datasets: <ref type="bibr">CoNLL-2002</ref><ref type="bibr">Spanish and Dutch NER (Tjong Kim Sang 2002</ref><ref type="bibr">), CoNLL-2003</ref><ref type="bibr">English and German NER (Tjong Kim Sang and De Meulder 2003</ref>, Europeana Newspapers French NER <ref type="bibr" target="#b21">(Neudecker 2016)</ref>, and MSRA Chinese NER <ref type="bibr" target="#b1">(Cao et al. 2018)</ref>. <ref type="table" target="#tab_6">Table 2</ref> shows the statistics of all datasets.</p><p>â¢ CoNLL-2002/2003 is annotated with four entity types:</p><p>PER, LOC, ORG, and MISC. All datasets are split into a training set, a development set (testa) and a test set (testb). â¢ Europeana Newspapers is annotated with three types:</p><p>PER, LOC, and ORG. We randomly sample 10% of sentences from the whole data to build a test set. â¢ MSRA is also annotated with three types: PER, LOC, and ORG. Since gold word segmentation is not provided in the test set, we use word segmentation from .</p><p>For all experiments, we use English as the source language and the others as target languages, i.e., the model M is trained on the training set of English data and evaluated on the test sets of each other language. When transferring to  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We implement our approach with PyTorch 1.0.1. We use the cased multilingual BERT BASE with 12 Transformer blocks, 768 hidden units, 12 self-attention heads, GELU activations <ref type="bibr" target="#b8">(Hendrycks and Gimpel 2016)</ref>, a dropout rate of 0.1 and learned positional embeddings. We employ WordPiece embeddings <ref type="bibr" target="#b37">(Wu et al. 2016)</ref> to split a word into subwords, which are then directly fed into the model without any other pre-processing. We empirically select the hyper-parameters and utilize them in all experiments. Specifically, for sequence length, we employ a sliding window with a maximum length of 128. When the sequence length is larger than 128, the last 64 subwords of the first window are kept as the context for the subsequent window. Following , we select K = 2 similar examples for both pseudo NER task construction and the adaptation phase. The mask ratio is set to 0.2, Î» in Equation 13 is set to 2.0, update steps n in Equation 7 is set to 2, the number of sampled pseudo-NER tasks used for one meta-update is set to 32, and the maximum meta-update steps is set to 3 Ã 10 3 . Following (Wu and Dredze 2019), we freeze the parameters of the embedding layer and the bottom three layers of the base model. According to the suggestions of model hyper parameters in <ref type="bibr" target="#b4">(Devlin et al. 2019)</ref>, for the optimizers of both inner-update and meta-update, we use Adam <ref type="bibr" target="#b11">(Kingma and Ba 2015)</ref> with learning rate of Î±, Î² = 3e â 5, while for gradient updates during adaptation, we set the learning rate Î³ to 1e-5. Following (Tjong Kim Sang 2002), we use the phrase level F1-score as the evaluation metric. To reduce the model bias, we carry out 5 runs and report the average performance. <ref type="table" target="#tab_8">Table 3</ref> presents our results on transferring from English to five other languages, alongside results from previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Performance Comparison</head><p>The results show that our approach significantly outperforms the previous state-of-the-art methods across the board, with relative improvements on F1-score compared to the base model ranging from 1.09% for Dutch to 8.67% for French (with average improvement of 3.21%), which demonstrates es nl de fr zh Average <ref type="bibr" target="#b28">TÃ¤ckstrÃ¶m, McDonald, and Uszkoreit (2012)</ref>    the effectiveness of the proposed enhanced meta-learning algorithm.</p><p>Particularly, compared with the base model, our approach achieves particularly significant improvement on German and French, which can be attributed to our model's stronger ability to predict through context information. In English, proper nouns of LOCATION, PERSON, etc. often begin with a capital letter while most general nouns do not. As a result, without effective extraction of context information, the base model tends to mislabel capitalized terms for general nouns as entities, and such phenomenon is especially serious when adapting the model to French and German, where capitalization rules differ from English for general nouns, titles, etc. or due to noise in datasets. In contrast, our approach is more robust in such cases due to the introduction of the masking scheme and the max loss, which facilitates the model to label general nouns as non-entities based more on context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We propose several strategies to enhance the base model, including the meta-training and adaptation procedure, the masking scheme, and the augmented loss. In this section, we conduct ablation study experiments to investigate the influence of these factors. <ref type="table" target="#tab_9">Table 4</ref> shows the results.</p><p>â¢ Ours w/o max loss, which removes the additional maximum term in the loss function. The performance in terms of F1-score decreases by 1.35 on average. We conjecture that, without the maximum term, the meta-knowledge from mispredictions is transferred to target tasks along with the meta-model, which hurts performance. â¢ Ours w/o masking, which wipes out the masking scheme during the meta-training phase. This causes a performance drop across all languages, with a maximum drop of 1.18 F1-score in Spanish. That further demonstrates the necessity of predicting through contextual information. â¢ Ours w/o max loss/masking, which cuts out both the masking scheme and the max loss at once. In this case, our approach degrades into the base model trained with merely model-agnostic meta-learning. This results in a performance drop of 1.46 F1-score on average, indicating that both the masking scheme and the max loss do bring enhancement to meta-learning for cross-lingual NER with minimal resources. â¢ Ours w/o meta-train/max loss/masking, which further eliminates the meta-training and the adaptation phase from Ours w/o max loss/masking. In that case, our approach degenerates into the base model.From <ref type="table" target="#tab_9">Table 4</ref>, we can see that this will lead to a significant and consistent performance drop on all five target languages, which demonstrates the effectiveness of meta-leaning employed in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Case Study</head><p>We give a case study to analyze the quality of the results produced by our approach and the base model. <ref type="table" target="#tab_11">Table 5</ref> demonstrates that our approach has a stronger ability to transfer semantic information.</p><p>In example #1, the base model fails to identity "SecretarÃ­a General" as ORG, probably because its most similar phrase "secretary general" in the English dataset is usually labeled as non-entities. However, our approach can recognize it according to the learned semantic information "a PER was selected to replace another PER at the head of an ORG". Similarly, in example #2, the base model incorrectly labels "Edmond Thieffrylaan" as PER. We suspect that this is because Edmond appears as a part of a person name "Jim Edmond" in the English training data. Surprisingly, the proposed approach labels it as LOC correctly according to the context "clean up the playground on LOC". Moreover, the baseline   <ref type="table">Table 6</ref>: Low resource cross-lingual NER results, where x% denotes the percentage of labeled training data in target languages used in the adaptation phase. CL: Cross-lingual transfer using a shared character embedding layer <ref type="bibr" target="#b39">(Yang, Salakhutdinov, and Cohen 2017)</ref>. ML: The multi-lingual framework as in <ref type="bibr" target="#b16">Lin et al. (2018)</ref>. MLMT: The multi-lingual multi-task framework as in .</p><p>model mispredicts the labels of "Krauses" in Example #3 and "å¥¥çº³è¥¿æ¯" in Example #4, two unseen entities in the English training data, while our approach gives the right prediction on the basis of context information. Considering the limited space, we provide more cases in <ref type="table" target="#tab_1">Table S1</ref> of the supplementary material.</p><p>Discussion: Extend to Low-Resource Cross-Lingual NER Here, we extend the proposed approach to the task of lowresource cross-lingual NER. To simulate a low-resource setting, we use randomly sampled subsets of the training data of a target language. Compared with minimal-resource crosslingual transfer, we take the same meta-training procedure. For the adaptation phase, we directly use the entire subsets to fine-tune the meta-learned model for efficiency, and then test on the test data of the target language. We compare our meta-learning based approach with other multi-lingual and multi-task based approaches. For the results not reported in <ref type="bibr" target="#b39">(Yang, Salakhutdinov, and Cohen 2017)</ref> and <ref type="bibr" target="#b16">(Lin et al. 2018)</ref>, we re-implement their methods based on the open-source github repositories 2,3,4 . As presented in <ref type="table">Table 6</ref>, our approach significantly outperforms other approaches across all target languages with different percentage of labeled data. Compared with the base model, there is an average improvement of 1.47 F1-score. We also study the factor analysis of the enhanced meta-learning algorithm under low-resource setting. One can refer to <ref type="table" target="#tab_6">Table S2</ref> of the supplementary material for details due to the limited space. Similarly, removing any factor in our proposed approach will lead to a performance drop, which further demonstrates that our approach is reasonable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose an enhanced meta-learning algorithm for cross-lingual NER with minimal resources, considering that the model could achieve better results after a few fine-tuning steps over a very limited set of structurally/semantically similar examples from the source language. To this end, we propose to construct multiple pseudo-NER tasks for meta-training by computing sentence similarities. Moreover, in order to improve the model's capability to transfer across different languages, we present a masking scheme and augment the loss function with an additional maximum term during meta-training. Experiments on five target languages show that the proposed approach leads to new state-of-the-art results with a relative F1-score improvement of up to 8.76%. We also extend the approach to low-resource cross-lingual NER, and it also achieves stateof-the-art results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>) . PRESS DIGEST -Israel [B-LOC] -Aug 25 .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Examples of similar sentence pairs in structure (#1)</cell></row><row><cell>or semantics (#2), where WHITE ( GREY ) highlights the</cell></row><row><cell>Spanish (retrieved English) examples.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>1</head><label></label><figDesc>Enhanced Meta-Learning for Cross-Lingual NER with Minimal Resources 1: procedure META-TRAINING(D S train , Î±, Î²) 2:Construct T = {T i } with D S train .Initialize with the pre-trained base model M Î¸ .Sample a batch of source tasks T i from T .</figDesc><table><row><cell>4:</cell><cell>while not done do</cell></row><row><cell>5:</cell><cell></cell></row></table><note>3:6:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>). being the final updated Î¸ 13: end procedure14: procedure ADAPTATION(M Î¸ * , D S train , D T test , Î³)</figDesc><table><row><cell>9:</cell><cell>end for</cell><cell></cell></row><row><cell>10: 11:</cell><cell>Update Î¸ â Î¸ â Î² i g i . end while</cell><cell></cell></row><row><cell cols="2">12: return M Î¸  15: for all x (j) â D T test do 16: D Tj test = x (j) and construct D</cell><cell cols="2">Tj train with D S train .</cell></row><row><cell>17:</cell><cell cols="2">train UpdateÎ¸ D T j</cell><cell>.</cell></row><row><cell>18:</cell><cell>Label D</cell><cell></cell></row></table><note>* with Î¸*j = Î¸* â Î³â Î¸* LTj test , i.e., x (j) , using MÎ¸j .19:</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Tj train for each T j by retrieving top-K similar examples of x (j) from the source language training data D S train using the metric in Equation 6. Subsequently, we fine-tune the meta-learned model M Î¸ * with the pseudo training set D</figDesc><table><row><cell>Tj train as in Equation 3 via one</cell></row><row><cell>gradient update, and then use the fine-tuned model to predict</cell></row><row><cell>labels for the test set D</cell></row></table><note>Tj test ,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Dataset statistics. French and Chinese, we relabel the MISC entities in English training data into non-entities for meta-training as there is no MISC in the French and Chinese test sets. Following (Wu and Dredze 2019), we use the BIO labeling scheme.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>Results of cross-lingual NER with minimal resources 1 .</figDesc><table><row><cell></cell><cell>es</cell><cell>nl</cell><cell>de</cell><cell>fr</cell><cell>zh</cell><cell>Average</cell></row><row><cell>Ours</cell><cell>76.75</cell><cell>80.44</cell><cell>73.16</cell><cell>55.30</cell><cell>77.89</cell><cell>72.71</cell></row><row><cell>Ours w/o max loss</cell><cell cols="6">76.05 (-0.70) 79.50 (-0.94) 71.84 (-1.32) 52.64 (-2.66) 76.77 (-1.12) 71.36 (-1.35)</cell></row><row><cell>Ours w/o masking</cell><cell cols="6">75.57 (-1.18) 80.38 (-0.06) 72.76 (-0.40) 54.29 (-1.01) 77.79 (-0.10) 72.16 (-0.55)</cell></row><row><cell>Ours w/o max loss/masking</cell><cell cols="6">75.33 (-1.42) 80.13 (-0.31) 71.49 (-1.67) 52.79 (-2.51) 76.50 (-1.39) 71.25 (-1.46)</cell></row><row><cell cols="7">Ours w/o meta-train/max loss/masking 74.59 (-2.16) 79.57 (-0.87) 70.79 (-2.37) 50.89 (-4.41) 76.42 (-1.47) 70.45 (-2.26) (i.e., Base Model)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on cross-lingual NER with minimal resources.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>#1Base Model: ... [PER Fidalgo] fue elegido para sustituir a [PER Antonio GutiÃ©rrez] al frente de la SecretarÃ­a General .Spanish Ours: ... [PER Fidalgo] fue elegido para sustituir a [PER Antonio GutiÃ©rrez] al frente de la [ORG SecretarÃ­a General] . Translation in English: ... Fidalgo was elected to replace Antonio Gutierrez at the head of the General Secretariat. #2 Base Model: ... hebben politie het speelplein [LOC Redoute] aan de [PER Edmond Thieffrylaan] schoongeveegd van junkies. Dutch Ours: ... hebben politie het speelplein [LOC Redoute] aan de [LOC Edmond Thieffrylaan] schoongeveegd van junkies. Translation in English: ... the police have cleaned up the Redoute playground on Edmond Thieffrylaan from junkies. Kurzfristig wurde der VorgÃ¤nger [PER Krauses] , [PER Peter WÃ¼nsch] , zurÃ¼ckgeholt. Translation in English: Not long ago, Krauses' predecessor, Peter WÃ¼nsch, was taken back. #4 Base Model: å¨ [LOC å¸è] ï¼ äººä»¬ å¸¸ç§° ä» ä¸º " å° å¥¥çº³è¥¿æ¯ " ã Chinese Ours: å¨ [LOC å¸è] ï¼ äººä»¬ å¸¸ç§° ä» ä¸º " å° [PER å¥¥çº³è¥¿æ¯] " ã Translation in English: In Greece, he is often called "Little Onassis".</figDesc><table><row><cell>#3</cell><cell>Base Model: Kurzfristig wurde der VorgÃ¤nger [ORG Krauses] , [PER Peter WÃ¼nsch] , zurÃ¼ckgeholt.</cell></row><row><cell>German</cell><cell>Ours:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Case study of cross-lingual NER with minimal resources. The GREEN ( RED ) highlight indicates a correct (incorrect) label.</figDesc><table><row><cell cols="2">data systems</cell><cell>es</cell><cell>nl</cell><cell>de</cell><cell>fr</cell><cell>zh Average</cell></row><row><cell></cell><cell>CL</cell><cell cols="2">60.43 54.77</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ML</cell><cell cols="2">62.72 63.57</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>1%</cell><cell cols="3">MLMT 68.33 66.73</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="6">Base Model 76.83 80.90 73.22 60.51 79.72 74.24</cell></row><row><cell></cell><cell>Ours</cell><cell cols="5">78.59 82.72 75.12 61.71 80.34 75.70</cell></row><row><cell></cell><cell>CL</cell><cell cols="2">66.45 61.91</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ML</cell><cell cols="2">71.22 70.62</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>2%</cell><cell cols="3">MLMT 72.59 70.92</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="6">Base Model 77.28 81.54 74.31 64.43 81.86 75.88</cell></row><row><cell></cell><cell>Ours</cell><cell cols="5">79.54 83.07 75.64 65.79 82.58 77.32</cell></row><row><cell></cell><cell>CL</cell><cell cols="2">69.63 70.29</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>ML</cell><cell cols="2">76.15 76.52</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>5%</cell><cell cols="3">MLMT 77.13 77.03</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="6">Base Model 78.74 82.00 75.48 67.73 85.11 77.81</cell></row><row><cell></cell><cell>Ours</cell><cell cols="5">80.32 83.72 77.70 69.19 85.67 79.32</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The zh results reported in (Wu and Dredze 2019) used a dataset not specified in the paper, so we don't list them here.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/kimiyoung/transfer 3 https://github.com/limteng-rpi/mlmt 4 We re-implement only Spanish and Dutch as the original repositories only provide aligned word embeddings for these two languages.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to learn by gradient descent by gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrychowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shillingford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3981" to="3989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adversarial transfer learning for Chinese named entity recognition with self-attention mechanism</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="182" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Meta multitask learning for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Named entity recognition with bidirectional LSTM-CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nichols</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">O</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meta-learning for low-resource neural machine translation</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno>abs/1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language to structured query generation via meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="732" to="738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Cross-lingual language model pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<idno>abs/1901.07291</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A multilingual multi-task architecture for low-resource sequence labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="799" to="809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional lstm-cnns-crf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cheap translation for cross-lingual named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2536" to="2545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Metalearning for low-resource natural language generation in task-oriented dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Faltings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Meta-neural networks that learn by learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mammone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCNN</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An open corpus for named entity recognition in historic newspapers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neudecker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly supervised cross-lingual named entity recognition via effective annotation and representation projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Florian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1470" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence tagging with bidirectional language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Power</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1756" to="1765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep contextualized word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Domain adaptive dialog generation via meta learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-lingual word clusters for direct transfer of linguistic structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>TÃ¤ckstrÃ¶m</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="477" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2003 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong</forename><surname>Kim Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Meulder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="142" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong</forename><surname>Kim Sang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">F</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
	<note>Introduction to the</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<imprint/>
	</monogr>
	<note>In CoNLL</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Cross-lingual named entity recognition via wikification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-T</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mayhew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Å</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Beto, bentz, becas: The surprising cross-lingual effectiveness of BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dredze</surname></persName>
		</author>
		<idno>abs/1904.09077</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Neural cross-lingual named entity recognition with minimal resources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="369" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Transfer learning for sequence tagging with hierarchical recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Chinese NER using lattice LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1554" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
