<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lane Detection and Classification using Cascaded CNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Pizzati</surname></persName>
							<email>fabio.pizzati2@unibo.it</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Bologna</orgName>
								<address>
									<addrLine>Viale Risorgimento 2</addrLine>
									<postCode>40136</postCode>
									<settlement>Bologna</settlement>
									<region>BO</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Allodi</surname></persName>
							<email>marco.allodi1@studenti.unipr.it</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Parma</orgName>
								<address>
									<addrLine>Via delle Scienze, 181 / a</addrLine>
									<postCode>43124</postCode>
									<settlement>Parma</settlement>
									<region>PR</region>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Barrera</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Universidad</orgName>
								<address>
									<addrLine>Carlos III de Madrid, Av. de la Universidad, 30</addrLine>
									<postCode>28911</postCode>
									<settlement>Leganés, Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>García</surname></persName>
							<email>fegarcia@ing.uc3m.es</email>
							<affiliation key="aff2">
								<orgName type="institution">Universidad</orgName>
								<address>
									<addrLine>Carlos III de Madrid, Av. de la Universidad, 30</addrLine>
									<postCode>28911</postCode>
									<settlement>Leganés, Madrid</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lane Detection and Classification using Cascaded CNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Lane boundary detection · Lane boundary classification · Deep learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lane detection is extremely important for autonomous vehicles. For this reason, many approaches use lane boundary information to locate the vehicle inside the street, or to integrate GPS-based localization. As many other computer vision based tasks, convolutional neural networks (CNNs) represent the state-of-the-art technology to indentify lane boundaries. However, the position of the lane boundaries w.r.t. the vehicle may not suffice for a reliable positioning, as for path planning or localization information regarding lane types may also be needed. In this work, we present an end-to-end system for lane boundary identification, clustering and classification, based on two cascaded neural networks, that runs in real-time. To build the system, 14336 lane boundaries instances of the TuSimple dataset for lane detection have been labelled using 8 different classes. Our dataset and the code for inference are available online.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In autonomous driving, a deep understanding of the surrounding environment is vital to safely drive the vehicle. For this reason, a precise interpretation of the visual signals is necessary to identify all the components essential for navigation. One of them of particular importance is lane boundary position, which is needed to avoid collisions with other vehicles, and to localize the vehicle inside the street. Besides easing localization, lane detection is employed in many ADAS for lane departure warning and lane keeping assist. As many others computer vision based tasks, lanes boundaries detection accuracy has been significantly improved after the introduction of deep learning. The majority of recent systems, indeed, use convolutional neural networks to process sensorial data and</p><p>The GPU used has been donated by the NVIDIA corporation. arXiv:1907.01294v2 [cs.CV] 17 Jul 2019 infer high-level information relative to lanes. Some of them process LiDAR data to exploit differences in lane markings reflectivity <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. However, LiDARs are extremely expensive, thus not always available on a vehicle. On the other hand, cameras are cheaper, and they make it possible to exploit chromatic differences on the road surface. Among the deep learning based lane detection approaches that rely solely on visual data, there is significant interest in lane marking detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. In <ref type="bibr" target="#b3">[4]</ref>, a modified version of Faster R-CNN <ref type="bibr" target="#b7">[8]</ref> is used to identify road patches that belong to lane markings. Many of those patches are then joined to obtain a complete representation of the marking. However, the system runs at approximately 4 frames per seconds, so it is not suitable for real-time elaboration, that is often a hard requirement for high-speed driving. In other works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref> lane markings detection and classification is achieved using fully-convolutional networks <ref type="bibr" target="#b9">[10]</ref>, and this enables more complex path planning tasks, where lane changes could be considered. Nonetheless, a comprehensive understanding of lane boundaries may be needed for path planning, so it may be necessary to join the detected markings with post processing algorithms. An alternative approach is to directly identify the boundaries, in order to reduce post-processing times. In <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, fully-convolutional networks are used to obtain a pixelwise representation of lane boundaries. A slightly different approach is proposed in <ref type="bibr" target="#b14">[15]</ref>, where a CNN is used to estimate polylines points, in order to solve fragmentation issues that often occurr in segmentation networks. They classify the obtained boundaries, but only in terms of position w.r.t. the ego vehicle, so no information regarding the lane boundary type (e.g. dashed, continuous) is extracted. Similarly to <ref type="bibr" target="#b14">[15]</ref>, Ghafoorian et al. <ref type="bibr" target="#b15">[16]</ref> exploit adversarial training to reduce fragmentation. In <ref type="bibr" target="#b16">[17]</ref>, an end-to-end approach is proposed, where lane boundary parameters are directly estimated by a CNN. As lane boundaries position, also lane boundaries types could be exploited to achieve a high grade of scene understanding. For example, knowing if a lane is dashed is indispensable for a lane change. Nonetheless, there is little interest in literature for simultaneous lane boundary identification and classification using deep learning. This could be caused by the lack of datasets that contains both information. For this reason, we extended a lane detection dataset with lane class annotations. Then, we developed a novel approach, based on the concatenation of multiple neural networks, that is used to perform lane boundary instance segmentation and classification, in an end-to-end deep learning based fashion. Our system satisfy real-time constraints on a NVIDIA Titan Xp GPU. Code for inference and pretrained models are available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our method is composed by two main sections, as presented in <ref type="figure" target="#fig_0">figure 1</ref>. As a first step, we train a CNN for lane boundary instance segmentation. Then, we extract a descriptor for each detected lane boundary and process it with a second CNN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Instance Segmentation</head><p>As discussed in section 1, several state-of-the-art approaches employ pixelwise classifications in order to differentiate pixels belonging to lane boundaries and background. In our case, different approaches are possible, so several design guidelines have been defined. First of all, we train the CNN to recognize lane boundaries, rather than lane markings. Doing this, it is indeed avoidable to group different lane markings in a lane boundary, considerably saving processing times. For similar reasons, we perform instance segmentation on lane boundaries instead of semantic segmentation. In this way, it is possible to distinguish different lane boundaries without relying on clustering algorithms. The state-of-theart network for instance segmentation is Mask R-CNN <ref type="bibr" target="#b17">[18]</ref>. However, two-step networks like Mask R-CNN are tipically not amenable to use in real-time application, as they are typically slower than single-step ones. Furthermore, they are usually employed to detect objects easily enclosable in rectangular boxes. Being lane boundaries appeareance heavily influcenced by the perspective effects, other kinds of architectures should be preferred. Taking into account the previous assumptions, a fully-convolutional network has been trained. We choose ERFNet <ref type="bibr" target="#b18">[19]</ref> as our baseline model, as it is the model that, at the time of writing, has the best performances among real-time networks in the Cityscapes Dataset benchmark for semantic segmentation. As in all deeplearning based approaches, large amounts of images and annotations are crucial to achieve correct predictions and to avoid overfitting. For this reason, the TuSimple dataset for lane detection has been used. It is composed by 6408 1280×720 images, divided in 3626 for training, and 2782 for testing. 410 images extracted from the training set have been used as validation set during training. The main peculiarity in the TuSimple dataset is that entire lane boundaries are annotated, rather than lane markings. This makes the TuSimple dataset ideal for our needs. The lane boundaries are represented as polylines. In order to avoid clustering via post-processing, it is possible to make use of the loss function presented in <ref type="bibr" target="#b19">[20]</ref>, that is based on the Kullback-Leibler divergence minimizaton between the output probability distributions associated to pixels belonging to the same lane boundary instance. Please note that we do not address an unlimited number of possible instances for lane boundaries, as we decided to detect only the ego-lane boundaries, and the ones of the lanes on the sides of the egolane. Considering that two boundaries are shared for different lanes, we set a fixed maximum number of detected boundaries to 4. However, directly training the network with <ref type="bibr" target="#b19">[20]</ref> ultimately leads to gradient explosion and loss divergence. For this reason, the curriculum learning strategy <ref type="bibr" target="#b20">[21]</ref> has been used to achieve convergence. In fact, in a first step a binary cross entropy loss has been used to train the network to distinguish between points belonging to a generic lane boundary and background. The resulting model is fine-tuned using <ref type="bibr" target="#b19">[20]</ref> as loss function. The network has been trained using the images in the dataset resized at 512 × 256 resolution, for 150 epochs. This resolution leaded to satisfying results, while keeping the computational cost low. In order to represent the ground truth data as images, the polylines in the dataset have been projected with a fixed width of 5px on semantic maps of size 512 × 256. We use the Adam optimizer, with learning rate 5 · 10 −4 , and polynomial learning rate decay with exponent set to 0.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Classification</head><p>To the best of our knowledge, there are currently no publicly available datasets where entire lane boundaries with class-related information are annotated. For this reason, all the lanes in the TuSimple dataset have been manually classified using 8 different classes: single white continuous, double white continuous, single yellow continuous, double yellow continuous, dashed, double-dashed, Botts' dots and unknown. The obtained annotations are available online at https://github.com/fabvio/TuSimple-lane-classes.</p><p>Associating a class to each lane boundary detected could be addressed in several different ways. One possibility that has been considered in an early stage of the development was branching the instance segmentation network, to perform a pixel-wise classification of lane boundaries with dedicated convolutional layers, then fuse the outputs of the two branches. This approach has been discharged as it is memory intensive, because it requires two decoders in the same network, and it may generate inconsistencies between the detection of the two branches, that should be solved using post-processing algorithms. For example, there could be pixels classified as background from the instance segmentation branch, but classified as lane from the other branch. For this reason, we perform a classification for each lane boundary with another CNN, associating the detected boundaries to the ground truth. A problem with this approach is that each lane boundary is constituted by a different number of points in the input image. For this reason, it is difficult to extract a representation of them that is position-indipendent w.r.t. the ego vehicle. This may be essential to achieve a correct classification. Thus, we extract a descriptor for each boundary, sampling a fixed number of points from the input image which belong to the detected lane boundary. The points extracted in this way are then ordered following their index in the original image, and arranged in squared images, that are processed by the second neural network. In this way, a spatially normalized compact representation of lane boundaries is obtained, while preserving information given by visual clues such as lane markings. Furthermore, using this approach we are able to perform lane boundary instance segmentation and classification with only two inferences, in an end-to-end fashion. In fact, the descriptors of different lane boundaries detected could be grouped in batches and classified simultaneously. Examples of descriptors are shown in image 2. The architecture we use for this task is derived from H-Net <ref type="bibr" target="#b10">[11]</ref>. A detailed description of its structure is given in image 3. We trained this network separately from the first one. To do that, the TuSimple dataset has been processed by the instance segmentation network. Each detected lane boundary is then compared with the ground truth, and it is associated to the corresponding class if the average distance between the detected points and the ground truth is under a threshold. This is needed to filter false positives generated by the first network. In fact, only lane boundaries that are effectively in the training set have a ground truth class, while others detected by the CNN should be excluded. As a result, we obtain a set of {descriptor, class} objects that can be used to train the classification neural network. This has been trained with the same hyperparameters of the instance segmentation network. Examples of extracted descriptors are shown in image 2. Code for inference, descriptor extraction and pretrained models are publicly available at https://github.com/fabvio/Cascade-LD. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>In order to validate our method, we evaluate the performances of both networks separately. For lane boundary instance segmentation, the evaluation formula for the TuSimple benchmark is presented in equation 1. In it, C i and S i are the number of correctly detected points and ground truth points in image i, respectively. A point is defined as correctly detected if it has a distance w.r.t. a ground truth point under 20 pixels. Additionally, false positive and false negative lane boundaries are evaluated. Given that our detected lane boundaries have width over 1 pixel, we average the x coordinates of the detected pixels for a given row, to obtain a single value. In 2, F pred refers to the number of erroneously detected lanes, while N pred is the total number of detected lanes. In 3, M pred is the total number of unidentified lanes, and N gt is the total number of lane boundaries annotated.</p><formula xml:id="formula_0">accuracy = i C i i S i (1) F P = F pred N pred (2) F N = M pred N gt<label>(3)</label></formula><p>We compare our instance segmentation network with the top-three approaches in the TuSimple benchmark for lane detection. We do not evaluate lanes that are composed than less of three points, in order to filter false positives. Results are presented in table 1. Inference times are evaluated on 512 × 256 images. Our network is slightly less accurate than the others. However, taking into account the computational times reduction, we found this tradeoff acceptable. For classification, two different experiments are performed. In a first phase, we train the network to distinguish between two different classes: dashed and continuous. To do that, the single white continuous, double white continuous, single yellow continuous, double yellow continuous classes are mapped to the continuous class. On the other hand, dashed, Botts' dots and double-dashed are equally labelled as dashed. Unknown descriptors are ignored. In this way, it is possible to distinguish between lane boundaries that may or may not be crossed. In the second experiment, we treat the double-dashed class as indipendent. Doing this, we could identify also the boundaries of lanes that may be crossed only in specific conditions, as highway entry or exit. An ablation study regarding the descriptor size has been performed. We evaluate classification performances on the validation set, as the test set labels for lane boundaries are not publicly available. Results are reported in table 2. Inference times for the classification network are around 1ms.  As it is visible, it is possible to achieve better performances increasing the descriptor spatial resolution. However, this leads to a major occupation of GPU RAM. On the other hand, our results demonstrate that it is possible to achieve satifying accuracies with only 256 points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusions</head><p>In this work, we presented a novel approach to lane boundary identification and classification in a end-to-end deep learning fashion. With our method, it is possible to achieve high accuracy in both tasks, in real-time. We formalized a descriptor extraction strategy that is useful when it is needed to combine instance segmentation and classification without relying on two-step detection networks. Furthermore, we performed an ablation study on the descriptor size, in order to define the tradeoff between detection accuracy and needed GPU RAM.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>System overview</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Descriptors of different sizes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Classification Network. Output channels are listed below each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Qualitative results on the test set. From top to bottom: original image, instance segmentation, classification. For instance segmentation, different colors represent different boundaries. For classification, green represents dashed lanes, yellow double-dashed, red continuous.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>TuSimple Lane detection metrics results and comparison.</figDesc><table><row><cell>Method</cell><cell>Accuracy</cell><cell>FP</cell><cell>FN</cell><cell>FPS</cell></row><row><cell>Xingang Pan [12]</cell><cell>96.53</cell><cell cols="2">6.17 1.80</cell><cell>5.31</cell></row><row><cell>Yen-Chang Hsu [20]</cell><cell>96.50</cell><cell>8.51</cell><cell>2.69</cell><cell>55.55</cell></row><row><cell>Davy Neven [11]</cell><cell>96.40</cell><cell cols="2">23.65 2.76</cell><cell>52.63</cell></row><row><cell>Ours</cell><cell>95.24</cell><cell cols="3">11.97 6.20 58.93</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on descriptor size.</figDesc><table><row><cell cols="3">Descriptor size Acc. (two classes) Acc. (three classes)</cell></row><row><cell>256 × 256</cell><cell>0.9698</cell><cell>0.9600</cell></row><row><cell>128 × 128</cell><cell>0.9596</cell><cell>0.9600</cell></row><row><cell>64 × 64</cell><cell>0.9519</cell><cell>0.9443</cell></row><row><cell>32 × 32</cell><cell>0.9527</cell><cell>0.9436</cell></row><row><cell>16 × 16</cell><cell>0.9359</cell><cell>0.9203</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Fast lidar-based road detection using fully convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Svensson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wahde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ieee intelligent vehicles symposium (iv)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep multi-sensor lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mattyus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Homayounfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Lakshmikanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Efficient road lane marking detection with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1809.03994</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lane marking detection via deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gelernter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep neural network for structural prediction and lane detection in traffic scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Vpgnet: Vanishing point guided network for lane and road marking detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bailo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Traffic lane detection using fully convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">APSIPA ASC</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Real-time road surface and semantic lane estimation using deep features. Signal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kidono</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Video Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1133" to="1140" />
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Towards end-to-end lane detection: an instance segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="286" to="291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial as deep: Spatial cnn for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">32nd AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Geometric constrained joint lane segmentation and lane boundary detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page" from="486" to="502" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end ego lane estimation based on sequential transfer learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE CVPR Workshops</title>
		<meeting>the IEEE CVPR Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chougule</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Koznek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ismail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schulze</surname></persName>
		</author>
		<title level="m">Reliable Multilane Detection and Classification by Utilizing CNN as a Regression Network</title>
		<meeting><address><addrLine>Munich, Germany; Proceedings, Part V</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">El-gan: embedding loss driven generative adversarial networks for lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nugteren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Baka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Booij</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">End-to-end lane detection through differentiable least-squares fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00293</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Erfnet: Efficient residual factorized convnet for real-time semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Romera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Bergasa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arroyo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on ITS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning to cluster for proposal-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
