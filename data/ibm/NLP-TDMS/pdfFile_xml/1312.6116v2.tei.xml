<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Deep Neural Networks with Probabilistic Maxout Units</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<postCode>79110</postCode>
									<settlement>Freiburg im Breisgau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
							<email>riedmiller@cs.uni-freiburg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Freiburg</orgName>
								<address>
									<postCode>79110</postCode>
									<settlement>Freiburg im Breisgau</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Deep Neural Networks with Probabilistic Maxout Units</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a probabilistic variant of the recently introduced maxout unit. The success of deep neural networks utilizing maxout can partly be attributed to favorable performance under dropout, when compared to rectified linear units. It however also depends on the fact that each maxout unit performs a pooling operation over a group of linear transformations and is thus partially invariant to changes in its input. Starting from this observation we ask the question: Can the desirable properties of maxout units be preserved while improving their invariance properties ? We argue that our probabilistic maxout (probout) units successfully achieve this balance. We quantitatively verify this claim and report classification performance matching or exceeding the current state of the art on three challenging image classification benchmarks (CIFAR-10, CIFAR-100 and SVHN).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Regularization of large neural networks through stochastic model averaging was recently shown to be an effective tool against overfitting in supervised classification tasks. Dropout <ref type="bibr" target="#b0">[1]</ref> was the first of these stochastic methods which led to improved performance on several benchmarks ranging from small to large scale classification problems <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. The idea behind dropout is to randomly drop the activation of each unit within the network with a probability of 50%. This can be seen as an extreme form of bagging in which parameters are shared among models, and the number of trained models is exponential in the number of these model parameters. During testing an approximation is used to average over this large number of models without instantiating each of them. When combined with efficient parallel implementations this procedure opened the possibility to train large neural networks with millions of parameters via back-propagation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> .</p><p>Inspired by this success a number of other stochastic regularization techniques were recently developed. This includes the work on dropconnect <ref type="bibr" target="#b3">[4]</ref>, a generalization of dropout, in which connections between units rather than their activation are dropped at random. Adaptive dropout <ref type="bibr" target="#b4">[5]</ref> is a recently introduced variant of dropout in which the stochastic regularization is performed through a binary belief network that is learned alongside the neural network to decrease the information content of its hidden units. Stochastic pooling <ref type="bibr" target="#b5">[6]</ref> is a technique applicable to convolutional networks in which the pooling operation is replaced with a sampling procedure.</p><p>Instead of changing the regularizer the authors in <ref type="bibr" target="#b6">[7]</ref> searched for an activation function for which dropout performs well. As a result they introduced the maxout unit, which can be seen as a generalization of rectified linear units (ReLUs) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, that is especially suited for the model averaging performed by dropout. The success of maxout can partly be attributed to the fact that maxout aids the optimization procedure by partially preventing units from becoming inactive; an artifact caused by the thresholding performed by the rectified linear unit. Additionally, similar to ReLUs, they are 1 arXiv:1312.6116v2 [stat.ML] <ref type="bibr" target="#b18">19</ref> Feb 2014 piecewise linear and -in contrast to e.g. sigmoid units -typically do not saturate, which makes networks containing maxout units easier to optimize.</p><p>We argue that an equally important property of the maxout unit however is that its activation function can be seen as performing a pooling operation over a subspace of k linear feature mappings (in the following referred to as subspace pooling). As a result of this subspace pooling operation each maxout unit is partially invariant to changes within its input. A natural question arising from this observation is thus whether it could be beneficial to replace the maximum operation used in maxout units with other pooling operations, such as L2 pooling. The utility of different subspace pooling operations has already been explored in the context of unsupervised learning where e.g. L2-pooling is known give rise to interesting invariances <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. While work on generalizing maxout by replacing the max-operation with general Lp-pooling exists <ref type="bibr" target="#b12">[13]</ref>, a deviation from the standard maximum operation comes at the price of discarding some of the desirable properties of the maxout unit. For example abandoning piecewise linearity, restricting units to positive values and the introduction of saturation regimes, which potentially worsen the accuracy of the approximate model averaging performed by dropout.</p><p>Based on these observations we propose a stochastic generalization of the maxout unit that preserves its desirable properties while improving the subspace pooling operation of each unit. As an additional benefit when training a neural network using our proposed probabilistic maxout units the gradient of the training error is more evenly distributed among the linear feature mappings of each unit. In contrast, a maxout network helps gradient flow through each of the maxout units but not through their k linear feature mappings. Compared to maxout our probabilistic units thus learn to better utilize their full k-dimensional subspace. We evaluate the classification performance of a model consisting of these units and show that it matches the state of the art performance on three challenging classification benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Description</head><p>Before defining the probabilistic maxout unit we briefly review the notation used in the following for defining deep neural network models. We adopt the standard feed-forward neural network formulation in which given an input x and desired output y (a class label) the network realizes a function computing a C-dimensional vector o -where C is the number of classes -predicting the desired output. The prediction is computed by first sequentially mapping the input to a hierarchy of N hidden layers h <ref type="bibr" target="#b0">(1)</ref> , . . . , h (N ) . Each unit h (l) i within hidden layer l ∈ [1, N ] in the hierarchy realizes a function h</p><formula xml:id="formula_0">(l) i (v; w (l) i , b (l)</formula><p>i ) mapping its inputs v (given either as the input x or the output of the previous layer h (l−1) ) to an activation using weight and bias parameters w i . Finally the prediction is computed based on the last layer output h N . This prediction is realized using a softmax layer o = sof tmax(W N +1 h (N ) + b N +1 ) with weights W N +1 and bias b N +1 . All parameters θ = {W (1) , b <ref type="bibr" target="#b0">(1)</ref> , . . . , W (N +1) , b (N +1) } are then learned by minimizing the cross entropy loss between output probabilities o and label y :</p><formula xml:id="formula_1">L(o, y; x) = − C i=1 y i log(o i ) + (1 − y i )log(1 − o i ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Probabilistic Maxout Units</head><p>The maxout unit was recently introduced in <ref type="bibr" target="#b6">[7]</ref> and can be formalized as follows: Given the units input v ∈ R d (either the activation from the previous layer or the input vector) the activation of a maxout unit is computed by first computing k linear feature mappings z ∈ R k where</p><formula xml:id="formula_2">z i = w i v + b i ,<label>(1)</label></formula><p>and k is the number of linear sub-units combined by one maxout unit. Afterwards the output h maxout of the maxout hidden unit is given as the maximum over the k feature mappings:</p><formula xml:id="formula_3">h maxout (v) = max[z 1 , . . . , z k ].<label>(2)</label></formula><p>When formalized like this it becomes clear that (in contrast to conventional activation functions) the maxout unit can be interpreted as performing a pooling operation over a k-dimensional subspace of linear units [z 1 , . . . , z k ] each representing one transformation of the input v. This is similar to spatial max-pooling which is commonly employed in convolutional neural networks. However, unlike in  <ref type="figure">Figure 1</ref>: Schematic of different pooling operations. a) An exemplary input image taken from the ImageNet dataset together with the depiction of a spatial pooling region (cyan) as well as the input to one maxout / probout unit (marked in magenta). b) Spatial max-pooling proceeds by computing the maximum of one filter response at the four different positions from a). c) Maxout computes a pooled response of two linear filter mappings applied to one input patch. d) The activation of a probout unit is computed by sampling one of the linear responses according to their probability. spatial pooling the maxout unit pools over a subspace of k different linear transformations applied to the same input v. In contrast to this, spatial max-pooling of linear feature maps would compute a pooling over one linear transformation applied to k different inputs. A schematic of the difference between several pooling operations is given in <ref type="figure">Fig. 1</ref> .</p><p>As such maxout is thus more similar to the subspace pooling operations used for example in topographic ICA <ref type="bibr" target="#b9">[10]</ref> which is known to result in partial invariance to changes within its input. On the basis of this observation we propose a stochastic generalization of the maxout unit that preserves its desirable properties while improving gradient propagation among the k linear feature mappings as well as the invariance properties of each unit. In the following we call these generalized units probout units since they are a direct probabilistic generalization of maxout.</p><p>We derive the probout unit activation function from the maxout formulation by replacing the maximum operation in Eq. (2) with a probabilistic sampling procedure. More specifically we assume a Boltzmann distribution over the k linear feature mappings and sample the activation h(v) from the activation of the corresponding subspace units. To this end we first define a probability for each of the k linear units in the subspace as:</p><formula xml:id="formula_4">p i = e λzi k j=1 e λzj ,<label>(3)</label></formula><p>where λ is a hyperparameter (referred to as an inverse temperature parameter) controlling the variance of the distribution. The activation h probout (x) is then sampled as</p><formula xml:id="formula_5">h probout (v) = z i , where i ∼ M ultinomial{p 1 , . . . , p k }.<label>(4)</label></formula><p>Comparing Eq. (4) to Eq. (2) we see that both, are not bounded from above or below and their activation is always given as one of the linear feature mappings within their subspace. The probout unit hence preserves most of the properties of the maxout unit, only replacing the sub-unit selection mechanism.</p><p>We can further see that Eq. (4) reduces to the maxout activation for λ → ∞. For other values of λ the probout unit will behave similarly to maxout when the activation of one linear unit in the subspace dominates. However, if the activation of multiple linear units differs only slightly they will be selected with almost equal probability. Futhermore, each active linear unit will have a chance to be selected. The sampling approach therefore ensures that gradient flows through each of the k linear subspace units of a given probout unit for some examples (given that λ is sufficiently small). We hence argue that probout units can learn to better utilize their full k-dimensional subspace.</p><p>In practice we want to combine the probout units described by Eq. (4) with dropout for regularizing the learned model. To achieve this we directly include dropout in the probabilistic sampling step by 3 re-defining the probabilities as:p</p><formula xml:id="formula_6">0 = 0.5 (5) p i = e λzi 2 · k j=1 e λzj .<label>(6)</label></formula><p>Consequently, we sample the probout activation function including dropoutĥ probout (v) aŝ</p><formula xml:id="formula_7">h probout (v) = 0 if i = 0 z i else , where i ∼ M ultinomial{p 0 ,p 1 , . . . ,p k }.<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Relation to other pooling operations</head><p>The idea of using a stochastic pooling operation has been explored in the context of spatial pooling within the machine learning literature before. Among this work the approach most similar to ours is <ref type="bibr" target="#b13">[14]</ref>. There the authors introduced a probabilistic pooling approach in order to derive a convolutional deep believe network (DBN). They also use a Boltzmann distribution based on unit activations to calculate a sampling probability. The main difference between their work and ours is that they calculate the probability of sampling one unit at different spatial locations whereas we calculate the probability of sampling a unit among k units forming a subspace at one spatial location. Another difference is that we forward propagate the sampled activation z i whereas they use the calculated probability to activate a binary stochastic unit.</p><p>Another approach closely related to our work is the stochastic pooling presented in <ref type="bibr" target="#b5">[6]</ref>. Their stochastic pooling operation samples the activation of a pooling unit p i proportionally to the activation a of a rectified linear unit <ref type="bibr" target="#b7">[8]</ref> computed at different spatial positions. This is similar to Eq. (4) in the sense that the activation is sampled from a set of different activations. Similar to <ref type="bibr" target="#b13">[14]</ref> it however differs in that the sampling is performed over spatial locations rather than activations of different units.</p><p>It should be noted that our work also bears some resemblance to recent work on training stochastic units, embedded in an autoencoder network, via back-propagation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. In contrast to their work, which aims at using stochastic neurons to train a generative model, we embrace stochasticity in the subspace pooling operation as an effective means to regularize a discriminative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Inference</head><p>At test time we need to account for the stochastic nature of a neural network containing probout units. During a forward pass through the network the value of each probout unit is sampled from one of k values according to their probability. The output of such a forward pass thus always represents only one of k M different instantiations of the trained probout network; where M is the number of probout units in the network. When combined with dropout the number of possible instantiations increases to (k + 1) M . Evaluating all possible models at test time is therefore clearly infeasible. The Dropout formulation from <ref type="bibr" target="#b0">[1]</ref> deals with this large amount of possible models by removing dropout at test time and halving the weights of each unit. If the network consists of only one softmax layer then this modified network performs exact model averaging <ref type="bibr" target="#b0">[1]</ref>. For general models this computation is merely an approximation of the true model average which, however, performs well in practice for both deep ReLU networks <ref type="bibr" target="#b1">[2]</ref> and the maxout model <ref type="bibr" target="#b6">[7]</ref>.</p><p>We adopt the same procedure of halving the weights for removing the influence of dropout at testtime and rescale the probabilities such that k i=1p i = 1 andp 0 = 0, effectively replacing the sampling from Eq .(7) with Eq. (4). We further observe that from the k M models remaining after removing dropout only few models will be instantiated with high probability. We therefore resort to sampling a small number of outputs o from the networks softmax layer and average their values. An evaluation of the exact effect of this model averaging can be found in Section 3.1.1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Evaluation</head><p>We evaluate our method on three different image classification datasets (CIFAR-10, CIFAR-100 and SVHN) comparing it against the basic maxout model as well as the current state of the art on all datasets. All experiments were performed using an implementation based on Theano and the pylearn2 library <ref type="bibr" target="#b16">[17]</ref> using the fast convoltion code of <ref type="bibr" target="#b1">[2]</ref>. We use mini-batch stochastic gradient descent with a batch size of 100. For each of the datasets we start with the same network used in <ref type="bibr" target="#b6">[7]</ref> -retaining all of their hyperparameter choices -to ensure comparability between results. We replace the maxout units in the network with probout units and choose one λ (l) via crossvalidation for each layer l in a preliminary experiment on CIFAR-10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experiments on CIFAR-10</head><p>We begin our experiments with the CIFAR-10 <ref type="bibr" target="#b17">[18]</ref> dataset. It consists of 50, 000 training images and 10, 000 test images that are grouped into 10 categories. Each of these images is of size 32 × 32 pixels and contains 3 color channels. Maxout is known to yield good performance on this dataset, making it an ideal starting point for evaluating the difference between maxout and probout units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Effect of replacing maxout with probout units</head><p>We conducted a preliminary experiment to evaluate the effect of the probout parameters λ (l) on the performance and compare it to the standard maxout model. For this purpose we use a five layer model consisting of three convolutional layers with 48, 128 and 128 probout units respectively which pool over 2 linear units each. The penultimate layer then consists of 240 probout units pooling over a subspace of 5 linear units. The final layer is a standard softmax layer mapping from the 240 units in the penultimate layer to the 10 classes of CIFAR-10. The receptive fields of units in the convolutional layers are 8, 8 and 5 respectively. Additionally, spatial max-pooling is performed after each convolutional layer with pooling size of 4 × 4, 4 × 4 and 2 × 2 using a stride of 2 in all layers. We split the CIFAR-10 training data retaining the first 40000 samples for training and using the last 10000 samples as a validation set.</p><p>We start our evaluation by using probout units everywhere in the network and cross-validate the choice of the inverse-temperature parameters λ (l) ∈ {0.1, 0.5, 1, 2, 3, 4} keeping all other hyperparameters fixed. We find that annealing the λ (l) parameter during training to a lower value improved performance for all λ (l) &gt; 0.5 and hence linearly decrease λ (l) to a value that is 0.9 lower than the initial λ in these cases. As shown in <ref type="figure">Fig. 3a</ref> the best classification performance is achieved when λ is set to allow higher variance sampling for the first two layers, specifically when λ (1) = 1 and λ (2) = 2. For the third as well as the fully connected layer we observe a performance increase when λ (3) is chosen as λ (3) = 3 and λ (4) = 4, meaning that the sampling procedure selects the maximum value with high probability. This indicates that the probabilistic sampling is most effective in lower layers. We verified this by replacing the probout units in the last two layers with maxout units which did not significantly decrease classification accuracy.</p><p>We hypothesize that increasing the probability of sampling a non maximal linear unit in the subspace pulls the units in the subspace closer together and forces the network to become "more invariant" to changes within this subspace. This is a property that is desired in lower layers but might turn to be detrimental in higher layers where the model averaging effect of maxout is more important than achieving invariance. Here sampling units with non-maximal activation could result in unwanted correlation between the "submodels". To qualitatively verify this claim we plot the first layer linear filters learned using probout units alongside the filters learned by a model consisting only of maxout units in <ref type="figure" target="#fig_2">Fig. 2</ref>. When inspecting the filters we can see that many of the filters belonging to one subspace formed by a probout unit seem to be transformed versions of each other, with some of then resembling "quadrature pairs" of filters. Among the linear filters learned by the maxout model some also appear to encode invariance to local transformations. Most of the filters contained in a subspace however are seemingly unrelated. To support this observation empirically we probed for changes in the feature vectors of different layers (extracted from both maxout and probout models) when they are applied to translated and rotated images from the validation set. Similar to <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3]</ref> we calculate the normalized Euclidean distance between feature vectors extracted from an unchanged image and a transformed version. We then plot these distances for several exemplary images as well as the mean over 100 randomly sampled images. The result of this experiment is given in <ref type="figure" target="#fig_4">Fig. 4</ref>, showing that introducing probout units into the network has a moderate positive effect on both invariance to translation and rotations.</p><p>Finally, we evaluate the computational cost of the model averaging procedure described in Section 2.3 at test time. As depicted in <ref type="figure">Fig. 3b</ref> the classification error for the probout model decreases with more model evaluations saturating when a moderate amount of 50 evaluations is reached. Conversely, using sampling at test time in conjunction with the standard maxout model significantly decreases performance. This indicates that the maxout model is highly optimized for the maximum responses and cannot deal with the noise introduced through the sampling procedure. We additionally also tried to replace the model averaging mechanism with cheaper approximations. Replacing the sampling in the probout units with a maximum operation at test time resulted in a decrease in performance, reaching 14.13%. We also tried to use probability weighting during testing <ref type="bibr" target="#b5">[6]</ref> which however performed even worse, achieving 15.21%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Evaluation of Classification Performance</head><p>As the next step, we evaluate the performance of our model on the full CIFAR-10 benchmark. We follow the same protocol as in <ref type="bibr" target="#b6">[7]</ref> to train the probout model. That is, we first preprocess all images by applying contrast normalization followed by ZCA whitening. We then train our model using the first 40000 examples from the training set using the last 10000 examples as a validation set. Training then proceeds until the validation error stops decreasing. We then retrain the model on the complete training set for the same amount of epochs it took to reach the best validation error.</p><p>To comply with the experiments in <ref type="bibr" target="#b6">[7]</ref> we used a larger version of the model from Section 3.1.1 in all experiments. Compared to the preliminary experiment the size of the convolutional layers was 6 <ref type="table">Table 1</ref>: Classification error of different models on the CIFAR-10 dataset. <ref type="bibr">METHOD</ref> ERROR CONV. NET + SPEARMINT <ref type="bibr" target="#b19">[20]</ref> 14.98 % CONV. NET + MAXOUT <ref type="bibr" target="#b6">[7]</ref> 11.69 % CONV. NET + PROBOUT 11.35 % 12 × CONV. NET + DROPCONNECT <ref type="bibr" target="#b3">[4]</ref> 9.32 % CONV. NET + MAXOUT <ref type="bibr" target="#b6">[7]</ref> 9.38 % CONV. NET + PROBOUT 9.39 % increased to 96, 192 and 192 units respectively. The size of the fully connected layer was increased to 500 probout units pooling over a 5 dimensional subspace.</p><p>The top half of <ref type="table">Table 1</ref> shows the result of training this model as well as other recent results. We achieve an error of 11.35%, slightly better than -but statistically tied to -the previous state of the art given by the maxout model. We also evaluated the performance of this model when the training data is augmented with additional transformed training examples. For this purpose we train our model using the original training images as well as add randomly translated and horizontally flipped versions of the images. The bottom half of <ref type="table">Table 1</ref> shows a comparison of different results for training on CIFAR-10 with additional data augmentation. Using this augmentation process we achieve a classification error of 9.39%, matching, but not outperforming the maxout result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CIFAR-100</head><p>The images contained in the CIFAR-100 dataset <ref type="bibr" target="#b17">[18]</ref> are -just as the CIFAR-10 images -taken from a subset of the 10-million images database. The dataset contains 50, 000 training and 10, 000 test examples of size 32 × 32 pixels each. The dataset is hence similar to CIFAR-10 in both size and image content. It, however, differs from CIFAR-10 in its label distribution. Concretely, CIFAR-100 contains images of 100 classes grouped into 20 "super-classes". The training data therefore contains 500 training images per class -10 times less examples per class than in CIFAR-10 -which are accompanied by 100 examples in the test-set.</p><p>We do not make use of the 20 super-classes and train a model using a similar setup to the experiments we carried out on CIFAR-10. Specifically, we use the same preprocessing and training procedure (determining the amount of epochs using a validation set and then retraining the model on the complete data). The same network as in Section 3.1.1 was used for this experiment (adapted to classify 100 classes). Again, this is the same architecture used in <ref type="bibr" target="#b6">[7]</ref> thus ensuring comparability between results. During testing we use 50 model evaluations to average over the sampled probout units.</p><p>The result of this experiment is given in <ref type="table" target="#tab_0">Table 2</ref>. In agreement with the CIFAR-10 results our model performs marginally better than the maxout model (by 0.45% 1 ). As also shown in the table the current best method on CIFAR-100 achieves a classification error of 36.85% <ref type="bibr" target="#b20">[21]</ref>, using a larger convolutional neural network together with a tree-based prior on the classes formed by utilizing the super-classes. A similar performance increase could potentially be achieved by combining their tree-based prior with our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SVHN</head><p>The street view house numbers dataset <ref type="bibr" target="#b21">[22]</ref> is a collection of images depicting digits which were obtained from google street view images. The dataset comes in two variants of which we restrict ourselves to the one containing cropped 32 × 32 pixel images. Similar to the well known MNIST dataset <ref type="bibr" target="#b22">[23]</ref> the task for this dataset is to classify each image as one of 10 digits in the range from 0 to 9. The task is considerably more difficult than MNIST since the images are cropped out of natural image data. The images thus contain color information and show significant contrast vari- METHOD ERROR RECEPTIVE FIELD LEARNING <ref type="bibr" target="#b23">[24]</ref> 45.17 % LEARNED POOLING <ref type="bibr" target="#b24">[25]</ref> 43.71 % CONV. NET + STOCHASTIC POOLING <ref type="bibr" target="#b5">[6]</ref> 42.51 % CONV. NET + DROPOUT + TREE <ref type="bibr" target="#b20">[21]</ref> 36.85 % CONV. NET + MAXOUT <ref type="bibr" target="#b6">[7]</ref> 38  <ref type="table" target="#tab_2">Table 3</ref> . This includes the current best result with data augmentation which was obtained using a generalization of dropout in conjunction with a large network containing rectified linear units <ref type="bibr" target="#b3">[4]</ref>.  <ref type="bibr" target="#b25">[26]</ref> 2.78 % CONV. NET + MAXOUT <ref type="bibr" target="#b6">[7]</ref> 2.47 % CONV. NET + PROBOUT 2.39 % CONV. NET + DROPOUT <ref type="bibr" target="#b25">[26]</ref> 2.68 % 5 × CONV. NET + DROPCONNECT <ref type="bibr" target="#b3">[4]</ref> 1.93 %</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We presented a probabilistic version of the recently introduced maxout unit. A model built using these units was shown to yield competitive performance on three challenging datasets (CIFAR-10, CIFAR-100, SVHN). As it stands, replacing maxout units with probout units is computationally expensive at test time. This problem could be diminished by developing an approximate inference scheme similar to <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> which we see as an interesting possibility for future work.</p><p>We see our approach as part of a larger body of work on exploring the utility of learning "complex cell like" units which can give rise to interesting invariances in neural networks. While this paradigm has extensively been studied in unsupervised learning it is less explored in the supervised scenario. We believe that work towards building activation functions incorporating such invariance properties, while at the same time designed for use with efficient model averaging techniques such as dropout, is a worthwhile endeavor for advancing the field. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of pairs of first layer linear filters learned by the maxout model (left) as well as the probout model (right). In contrast to the maxout filters the filter pairs learned by the probout model appear to mostly be transformed versions of each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>5 0 4 λFigure 3 :</head><label>543</label><figDesc>(l) Class. Error (validation set) l = 2 (with λ (1) = 1) l = 1 (with λ (2) = 2) (a) Validation of the λ (l) parameter for layers l ∈ [1, 2] on CIFAR-10. We plot the error on the validation set after training (using 50 model evaluations). When evaluating the choice of λ (1) (red curve) the second parameter fixed λ (2) = 2. Likewise, for the experiments regarding λ (2) (blue curve) λ (1) = 1. (b) Evolution of the classification error and standard deviation on the CIFAR-10 dataset for a changing number E of model evaluations. We average the activation o ∈ R C of the softmax layer over all E evaluations and compute the predicted class labelŷ as the maximum y = arg max i∈{1,...,C} o i . The standard deviation is computed over 10 runs of E model evaluations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Analysis of the impact of vertical translation and rotation on features extracted from a maxout and probout network. We plot the distance between normalized feature vectors extracted on transformed images and the original, unchanged, image. The distances for the probout model are plotted using thick lines. The distances for the maxout model are depicted using dashed lines. (a,b) 4 exemplary images undergoing different vertical translations and rotations respectively. (c,d) Euclidean distance between feature vectors from the original 4 images depicted in (a,b) and transformed images for Layer 1 (convolutional) and Layer 4 (fully connected) respectively. (e,f) Euclidean distance between feature vectors from the original 4 images and transformed versions for Layer 2 (convolutional) and Layer 4 (fully connected) respectively. (g,h) Mean Euclidean distance between feature vectors extracted from 100 randomly selected images and their transformed versions for different layers in the network.10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 :</head><label>2</label><figDesc>Classification error of different models on the CIFAR-100 dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The training and test set contain 73, 257 and 20, 032 labeled examples respectively. In addition to this data there is an "extra" set of 531, 131 labeled digits which are somewhat less difficult to differentiate and can be used as additional training data. As in<ref type="bibr" target="#b6">[7]</ref> we build a validation set by selecting 400 examples per class from the training and 200 examples per class from the extra dataset. We conflate all remaining training images to a large set of 598, 388 images which we use for training.The model trained for this task consists of three convolutional layers containing 64, 128 and 128 units respectively, pooling over a 2 dimensional subspace. These are followed by a fully connected and a softmax layer of which the fully connected layer contains 400 units pooling over a 5 dimensional subspace. This yields a classification error of 2.39% (using 50 model evaluations at testtime), matching the current state of the art for a model trained on SVHN without data augmentation achieved by the maxout model (2.47%). A comparison to other results can be found in</figDesc><table><row><cell></cell><cell>.57 %</cell></row><row><cell>CONV. NET + PROBOUT</cell><cell>38.14 %</cell></row><row><cell cols="2">ation. Furthermore, although centered on one digit, several images contain multiple visible digits,</cell></row><row><cell>complicating the classification task.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Classification error of different models on the SVHN dataset. The top half shows a comparison of our result with the current state of the art achieved without data augmentation. The bottom half gives the best performance achieved with data augmentation as additional reference.</figDesc><table><row><cell>METHOD</cell><cell>ERROR</cell></row><row><cell cols="2">CONV. NET + STOCHASTIC POOLING [6] 2.80 %</cell></row><row><cell>CONV. NET + DROPOUT</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">While we were writing this manuscript it came to our attention that the experiments on CIFAR-100 in<ref type="bibr" target="#b6">[7]</ref> were carried out using a different preprocessing than mentioned in the original paper. To ensure that this does not substantially effect our comparison we ran their experiment using the same preprocessing used in our experiments. This resulted in a slightly improved classification error of 38.50%.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors want to thank Alexey Dosovistkiy for helpful discussions and comments, as well as Thomas Brox for generously providing additional computing resources.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky Ilya Sutskever Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<idno>arxiv:cs/1207.0580v3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno>arxiv:cs/1311.2901v3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Adaptive dropout for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR): Workshop track</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS 2011</title>
		<imprint>
			<date type="published" when="2011-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jarmo</forename><surname>Hurri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvrinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<title level="m">Natural Image Statistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Slow, decorrelated features for pretraining complex cell-like networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">S</forename><surname>Bergstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 22</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning of invariant features via simulated fixations in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghuo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS 2012)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learned-norm pooling for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu Yoshua Bengio Caglar Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno>arxiv:stat/1311.1780v3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Estimating or propagating gradients through stochastic neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ric</forename><surname>Thibodeau-Laufer</surname></persName>
		</author>
		<title level="m">Deep generative stochastic networks trainable by backprop</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<idno>arxiv:stat/1308.4214</idno>
		<title level="m">Pylearn2: a machine learning research library</title>
		<editor>Pascal Lamblin Vincent Dumoulin Mehdi Mirza Razvan Pascanu James Bergstra Frdric Bastien Yoshua Bengio Ian J. Goodfellow, David Warde-Farley</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning invariant features through topographic filter maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc&amp;apos;aurelio</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Conference on Computer Vision and Pattern Recognition</title>
		<meeting>International Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><forename type="middle">Prescott</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative transfer learning with tree-based priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2094" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Beyond spatial pyramids: Receptive field learning for pooled image features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learnable pooling regions for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR): Workshop track</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Improving neural networks with dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Master&apos;s thesis</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
