<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PERF-Net: Pose Empowered RGB-Flow Net</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinxiao</forename><surname>Li</surname></persName>
							<email>yinxiao@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
							<email>jonathanhuang@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PERF-Net: Pose Empowered RGB-Flow Net</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, many works in the video action recognition literature have shown that two stream models (combining spatial and temporal input streams) are necessary for achieving state of the art performance. In this paper we show the benefits of including yet another stream based on human pose estimated from each frame -specifically by rendering pose on input RGB frames. At first blush, this additional stream may seem redundant given that human pose is fully determined by RGB pixel values -however we show (perhaps surprisingly) that this simple and flexible addition can provide complementary gains. Using this insight, we then propose a new model, which we dub PERF-Net (short for Pose Empowered RGB-Flow Net), which combines this new pose stream with the standard RGB and flow based input streams via distillation techniques and show that our model outperforms the state-of-the-art by a large margin in a number of human action recognition datasets while not requiring flow or pose to be explicitly computed at inference time.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Human pose is intuitively intimately linked to human centric activity recognition. For example, by localizing the two legs from a human in a collection of frames, one is often able to easily recognize actions such as jumping, walking or sitting. As such, the idea of using pose explicitly as a cue for activity recognition tasks is one that has been explored in a number of works in the computer vision literature, including <ref type="bibr" target="#b3">(Choutas et al. 2018;</ref><ref type="bibr" target="#b4">Crasto et al. 2019;</ref><ref type="bibr" target="#b26">Qiu et al. 2019;</ref><ref type="bibr" target="#b2">Chéron, Laptev, and Schmid 2015;</ref><ref type="bibr" target="#b42">Yao et al. 2011)</ref>. In this paper we revisit this conceptually simple idea of using pose as a cue for activity recognition using modern large scale datasets and models. Specifically, we exploit pose in activity recognition using 3D CNNs, which in recent years have been a dominant architecture in the subfield due to the rise of massive scale video datasets such as Kinetics <ref type="bibr" target="#b14">(Kay et al. 2017;</ref><ref type="bibr" target="#b1">Carreira and Zisserman 2017)</ref>.</p><p>To achieve state-of-the-art results on Kinetics, many recent works that rely on 3D CNNs <ref type="bibr" target="#b34">(Taylor et al. 2010;</ref><ref type="bibr" target="#b35">Tran et al. 2015)</ref> have found it necessary to rely on a two-stream approach (Simonyan and Zisserman 2014) that combines Flow modalities. The top row shows input multi-modality data. The middle row shows the response maps from the networks using Grad-CAM <ref type="bibr" target="#b28">(Selvaraju et al. 2017)</ref>. Note that the response maps are overlaid on RGB and pose images for better visualization. The bottom row shows the model predictions on each of the modalities. Our proposed pose modality focuses the attention on the entire human body, providing a useful complementary cue to the standard RGB and Flow modalities, here allowing for our model to correctly predict the "sit-up" action. spatial and temporal input stream using late fusion. Concretely, this has typically referred to models trained independently to do activity recognition on (1) a sequence of RGB images and (2) a sequence of optical flow fields (or other motion representation) and fusing the results of both models via ensembling.</p><p>In addition to this two-stream framework, we propose to add a third input stream based on human pose. Unlike the two-stream approach which is (very) loosely based on the two-stream hypothesis of the human visual system <ref type="bibr" target="#b10">(Goodale, Milner et al. 1992)</ref>, our approach takes no specific inspiration from biology -instead we rely on the natural intuition that since action datasets tend to be human centric, if we had explicit pose cues, it would often be much more straightforward to infer action from pose compared to directly from raw pixels or flow. As an example, consider Figure 1 which visualizes our models results on a person performing a sit-up using the three possible input modalities, RGB, pose and flow. However, this sit-up is more specifi-cally a barbell sit-up on a decline bench which is easily confused for bench press due to strong cues from the appearance and motion of the barbell. As such, the pose modality offers a complementary signal allowing our model to infer the correct activity.</p><p>How to provide the pose cues properly requires care however -using pose alone as an input stream is intuitively not enough, as recognition often requires contextual cues (e.g. from props, objects that the human is interacting with, etc). Instead, as the pose stream, we render pose via exaggerated colored lines on top of each corresponding RGB frame, which allows us to benefit from both a clear pose based signal as well as contextual cues from surrounding appearance. We demonstrate via ablations that this choice to superimpose pose with the corresponding RGB frame is critical for good results.</p><p>A reasonable question to ask is: why is pose not simply a redundant input stream? After all, it is fully determined by RGB values -and even more redundant given that we render poses on top of the RGB frames. So even though pose is intuitively connected to activity recognition, what additional specific benefit is pose bringing in our setting?</p><p>We have a few answers. First, by using an off-the-shelf pose estimation algorithm that was trained on the COCO dataset <ref type="bibr" target="#b19">(Lin et al. 2014)</ref>, we are injecting additional semantic knowledge that the model can leverage. Second, we note that optical flow is also fully determined by the sequence of RGB inputs. And as with flow, we show that models using the pose stream are quantitatively different (better) than simply ensembling with a second RGB-only model. In very recent work, <ref type="bibr" target="#b31">Stroud et al. (2020)</ref> showed that the benefits of the temporal stream could be captured by an RGB-only model via distillation training, thus obviating the need for redundant input streams at inference time.</p><p>Taking inspiration from Stroud et al. s flow based results <ref type="bibr" target="#b31">(Stroud et al. 2020)</ref>, we similarly apply distillation techniques to our problem with both pose and flow. Combining this with a novel self-gating based architecture, we are able to obtain a state-of-the-art RGB-only model that requires us to compute neither flow nor pose. We dub this model the Pose Empowered RGB-Flow Net (or PERF-Net).</p><p>To summarize, our contributions are as follows. • We demonstrate strong evidence that pose is an important modality for video action recognition and can provide a complementary input stream to the standard RGB and Flow streams. • We propose PERF-Net, an approach that leverages RGB, Flow and Pose input streams in a multi-teacher distillation setting to train an RGB-only model with state of the art performance on the challenging Kinetics-600 dataset. • We study the impact of using different representations of the human pose input stream. We propose a context-aware human pose rendering which can bridge the gap between pose information and RGB within a collection of frames. • We perform detailed analysis on the response of networks from different input streams (RGB, Flow, and Pose). Our qualitative results show that when trained on our Pose stream, our model sometimes attends to different regions of a frame compared to RGB or Flow, allowing this third stream to offer complementary cues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work Fusion of multiple modalities</head><p>In contrast to image data, videos are multi-modal. How to best utilize this special characteristic of video data has been a long-standing topic in the video understanding research community. One of the standard approaches, introduced by Simonyan and Zisserman (2014), captures the complementary information from appearance and motion by averaging predictions from two separately trained 2D CNNs, one from RGB frames and the other from stacked optical flow frames. Following <ref type="bibr" target="#b29">(Simonyan and Zisserman 2014)</ref>, Feichtenhofer, Pinz, and Zisserman (2016) investigated the optimal locations within CNNs to combine the two streams. A more recent trend has been to train a 3D ConvNet to directly model temporal patterns without relying explicitly on optical flow. This is easier said than done, as <ref type="bibr" target="#b1">Carreira and Zisserman (2017)</ref> showed that performance (of their 3D convolutional architecture, I3D) could be greatly improved by including an optical flow stream. However there have been some promising approaches; <ref type="bibr" target="#b8">Feichtenhofer et al. (2019)</ref> recently proposed a two-stream architecture where both streams take RGB frames as inputs, but extracted at different frame rates. Unlike the late fusion approach taken by two-stream I3D models, the fusion in <ref type="bibr" target="#b8">Feichtenhofer et al. (2019)</ref> is implemented as lateral connections at different layers of the network. <ref type="bibr" target="#b27">Ryoo et al. (2019)</ref> adapted the Evolution algorithm to search such lateral connections in a multistream architecture. In addition to different frame rates of RGB streams, they also include optical flow as an additional stream of input.</p><p>In addition to optical flow, human pose is another input modality that has been widely studied for understanding videos involving human activities. <ref type="bibr" target="#b2">Chéron, Laptev, and Schmid (2015)</ref> showed that training RGB and flow streams on the patches centered at human joint locations can improve over the global approach. In addition to RGB and flow frames, <ref type="bibr" target="#b44">Zolfaghari et al. (2017)</ref> proposed a new modality using human body part segmentation results from an existing network. Another novelty from their work is that multistream fusion is done sequentially through a Markov chain. Unlike their work, our study focuses on how best to represent human pose as an input stream for a 3d CNN. Our experiments highlight the importance of this issue, and we show that a naive representation of human pose actually degrades the final ensemble performance. More generally we run our experiments on the large scale Kinetics dataset which are properly able to leverage the expressiveness of 3D CNNs leading to stronger results and "clearer" ablation signals throughout the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distillation between modalities</head><p>While achieving state-of-the-art performance, multi-stream models are computationally more expensive. For example, the computation of optical flow could be more expensive than ConvNet inference. Distillation <ref type="bibr" target="#b0">Buciluȃ, Caruana, and Niculescu-Mizil (2006)</ref>; <ref type="bibr" target="#b12">Hinton, Vinyals, and Dean (2015)</ref> is a technique to transfer the knowledge of a complex teacher model to a smaller student model by optimizing the student model to mimic the behavior of the teacher. Recently, researchers have adapted this idea to multi-modal model training. <ref type="bibr" target="#b43">Zhang et al. (2016)</ref> used a teacher model trained on optical flow to guide a student CNN whose input is motion vectors, which can be directly obtained from compressed videos. <ref type="bibr" target="#b21">Luo et al. (2018)</ref> proposed a graph distillation approach to address the modality discrepancy between the source and target domain. Our study is most similar to recent works <ref type="bibr" target="#b31">Stroud et al. (2020)</ref>; <ref type="bibr" target="#b4">Crasto et al. (2019)</ref> which distill the flow stream into the RGB stream (e.g., flow stream is the teacher while RGB stream is the student). Besides the flow stream, our experiments show the benefits of using multiple teachers, e.g., flow and human pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose Empowered RGB-Flow Nets (PERF-Net)</head><p>In this section we describe our main contribution, the Pose Empowered RGB-Flow Nets (or PERF-Net) approach. We begin by constructing a model that predicts actions based on pose information. Specifically we describe how we represent pose and how our pose representations can be fed to a 3D CNN. The final goal is to fuse the predictions that we can obtain via this pose stream with predictions from RGB and flow streams. The standard approach of applying "late fusion to combine disparate input streams is accurate but very slow since it requires multiple runs through the 3d convnet architecture. Instead, in the PERF-Net setting, we propose to use multi-teacher distillation to train a final model that takes RGB inputs at test time, but can benefit all three modalities (RGB, Flow, Pose) at training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose representation</head><p>By pose information we refer to human body joint positions (as is typical in the literature) which we first estimate from each RGB frame using an off-the-shelf pose estimation model and then feed to a 3D CNN as a sequence of frames. For pose estimation we use the PoseNet approach <ref type="bibr" target="#b15">(Kendall, Grimes, and Cipolla 2015;</ref><ref type="bibr" target="#b22">Papandreou et al. 2017)</ref> with ResNet backbones which is pre-trained on the COCO dataset <ref type="bibr" target="#b19">(Lin et al. 2014</ref>) and produces 17 estimated pose keypoints for each detected human in a frame. We note that the success of our model does not depend on our specific choice of pose estimation approach. Additionally, we have not specifically tuned the pose model with respect to the final performance of PERF-Net. We also note that in our datasets, such as Kinetics-600, human poses are not available in many samples.</p><p>How specifically to render pose as a frame (which can then be sent as input to a convolutional network) is a more important design decision. Our approach is to render pose via colored lines (using a different color for each limb to allow the model to more easily distinguish between the limbs). The simplest approach (similar to that taken by <ref type="bibr" target="#b44">Zolfaghari et al. 2017)</ref> is to simply render the estimated pose on a black background. However using pose information alone in this way is intuitively not enough, as activity recognition often <ref type="figure">Figure 2</ref>: A few different human pose rendering effects that have been explored. Column A uses 6 different colors to represent poses, where the top row is rendered using the same thickness of the segments and bottom row uses ratio-aware thickness of the segments. Column B and C explore two different rendering markers, points and segments with 13 different colors. The top row in column B and C uses a black background. Both column B and C also add ratio-aware radius or thickness while rendering the poses.</p><formula xml:id="formula_0">(C) (B) (A)</formula><p>requires contextual cues -for example, having a golf club in the frame is highly indicative of the action. So instead we render the pose of each human on top of each corresponding RGB frame, which as we show in experiments, can have a sizeable impact on performance. We experiment with three additional variations of the rendering scheme:</p><p>• Dots vs. bars: we render joint locations with filled circles instead of limbs with line segments.</p><p>• Fine vs Coarse-grained coloring: in our coarse-grained setting we use 6 colors for the joints, assigning a unique color to the left arm, right arm, body, head, left leg, and right leg. In our fine-grained setting, each limb gets its own color (e.g., left forearm vs left upper arm).</p><p>• Uniform vs. ratio-aware line thickness: in the former setting, we render lines with a uniform width; whereas in the latter setting, we set line thickness proportional to the size of the corresponding person detection's bounding box. <ref type="figure">Figure 2</ref> shows example of these pose rendering variants. As we show in the next section, using the fine-grained coloring scheme and using ratio-aware line thicknesses can lead to improved results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone architecture</head><p>We now describe our backbone architecture which is based on a 3D version of ResNet50 where some of the convolution kernels have been "inflated (specifically described by <ref type="bibr" target="#b40">Wang et al. (2018)</ref> with a few key modifications. First, we remove all max pooling operations in the temporal dimension. We find that applying temporal downsampling in any layer degrades the performance. Second, we add a feature gating module <ref type="bibr" target="#b41">(Xie et al. 2018</ref>) after each residual block. Feature gating is a self-attention mechanism that re-weights the channels based on context (i.e., the feature map averaged over time and space). We also explored adding feature gating modules after every residual cell which achieved similar results, so we decided to keep the former configuration given</p><formula xml:id="formula_1">Block Output sizes T × S 2 × C input 64 × 224 2 × 3 conv 1 5 × 7 2 64 × 112 2 × 64 stride 1 × 2 2 pool 1 1 × 3 2 64 × 56 2 × 64 stride 1 × 2 2 res 2   3 × 1 2 1 × 3 2 1 × 1 2   × 3 64 × 56 2 × 256 feature gating res 3   t i × 1 2 1 × 3 2 1 × 1 2   × 4 64 × 28 2 × 512 feature gating res 4   t i × 1 2 1 × 3 2 1 × 1 2   × 6 64 × 14 2 × 1024 feature gating res 5   t i × 1 2 1 × 3 2 1 × 1 2   × 3 64 × 7 2 × 2048</formula><p>feature gating <ref type="table">Table 1</ref>: ResNet50-G architecture used in our experiments. The kernel dimensions are T × S 2 where T is the temporal kernel size and S is the spatial size. The strides are denoted as temporal stride × spatial stride 2 . For res3, res4, and res5 blocks the temporal convolution only applies at every other cell. E.g., ti = 3 when i is an odd number and ti = 1 when i is even.</p><p>that it is more computationally efficient. These two modifications (no temporal downsampling, feature gating) can significantly improve the final performance and ablation studies can be found in the supplementary materials. In our experiments, we denote this modified ResNet50 as ResNet50-G and a detailed description of the backbone can be found in <ref type="table">Table 1</ref>. Note however that our methodology for using pose as an input stream does not depend specifically on the choice of backbone, and indeed we also demonstrate results using the recent S3D-G backbone <ref type="bibr" target="#b41">(Xie et al. 2018</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-stream fusion via distillation</head><p>Much as flow is used as a complementary signal to RGB input streams in typical action recognition papers, the intention of our pose model is to be used as a complementary signal to both RGB and flow. We now turn to how to combine these multiple streams (RGB, flow, pose) into a single model that takes RGB as its only input. Specifically we assume now that we have trained 3 models based on RGB, flow and pose respectively. The goal of our distillation approach will be to train an RGB-only model that requires much less computation compared to running all three models separately while capturing their complementary strengths. Our approach is inspired by the D3D model of <ref type="bibr" target="#b31">Stroud et al. (2020)</ref>, an RGB-only model which captures the benefits of having a temporal stream by using distillation techniques. Specifically, Stroud et al. trained a student model which takes a spatial (RGB-only) stream as input to do action recognition, adding an additional distillation loss which compares against the output of a teacher model that was trained on temporal stream inputs.</p><p>We apply a natural extension of the D3D approach to allow it to handle multiple distillation losses (corresponding  to multiple non-spatial input streams). The total loss that we jointly minimize encourages our PERF-Net RGB-only student model to mimic logits from each teacher network while simultaneously minimizing the loss from groundtruth labels via backpropagation, and can be written as follows:</p><formula xml:id="formula_2">L = L c (S ) + N i M SE(T i , S )<label>(1)</label></formula><p>where S denotes logits from student network and T i i denotes the logits from the ith teacher network. We use mean squared loss (applied to logits of student and teacher models) as the distillation loss. <ref type="figure" target="#fig_1">Figure 3</ref> shows the structure of our multi-teacher distillation framework.</p><p>Note that our loss function is distinct from the natural alternative of training the student to directly mimic the standard late fusion model (by regression towards the sum of all teacher-produced logits, referred as unified loss). In experiments we show that our approach achieves significantly better performance (See <ref type="table" target="#tab_7">Table 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Training details. Our ResNet50-G models are trained on Google TPUs (v3) <ref type="bibr" target="#b17">(Kumar et al. 2019)</ref> using Momentum SGD with weight decay 0.0001 and momentum 0.9. We construct each batch using 2048 clips on 256 TPU cores, yielding a per-core batch size of 8. In order to fit 8 clips in TPU memory, we use mixed precision training with bfloat16 type in all our TPU training runs <ref type="bibr" target="#b39">(Wang and Kanwar 2019)</ref>. We train our ResNet50-G models on Kinetics-600 with random initialization ("from scratch"). We also experimented with initializing from an inflated (Carreira and Zisserman 2017) ImageNet <ref type="bibr" target="#b5">(Deng et al. 2009</ref>) pre-trained model but this turns out to be unnecessary in our setup. We train using a linear learning rate warm-up for the first 2k steps increasing from 0 to a base learning rate of 1.6, then use a cosine annealed learning rate <ref type="bibr" target="#b20">(Loshchilov and Hutter 2016)</ref> for 20K steps.</p><p>Our S3D-G models are trained on 51 GPUs with a percore batch size of 6 clips (so the total mini-batch size is 306).</p><p>All S3D-G models are initialized using inflation (Carreira and Zisserman 2017) with a pre-trained Inception <ref type="bibr" target="#b32">(Szegedy et al. 2015</ref>) model on ImageNet <ref type="bibr" target="#b5">(Deng et al. 2009</ref>).</p><p>All models are trained on 64 consecutive frames (at 25 FPS) from the original videos and those clips are randomly cropped from the original sequence. For each frame in the clip, we first resize the video to have a shorter side equaling to 256, and randomly crop a 224 × 224 region as the input to the networks. For UCF-101 and HMDB-51, we use random crops of 224 × 298 as inputs. Random mirroring, contrast, and brightness are also applied as data augmentation. Finally, to extract flow, we use the TV-L1 approach (Snchez Prez, Meinhardt-Llopis, and Facciolo 2013). Inference. Unlike previous work <ref type="bibr" target="#b8">Feichtenhofer et al. 2019)</ref>, we use a single central crop of the video to evaluate our models' performance. The crop size is set to 250×256×256×3 for Kinetics-600, 128×224×298× 3 for UCF-101, and 64×224×298×3 for HMDB-51, (input shapes follow the frames× height× width×channels convention). For sequences that do not have sufficiently many frames, we pad by duplicating the first or the last frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What is the best representation for pose?</head><p>Our first question is which pose rendering methods achieve the best performance ( <ref type="figure">Figure 2)</ref>? We first take the approach of rendering pose on a black background, which as shown in <ref type="table" target="#tab_2">Table 2</ref> yields an accuracy much lower than the other approaches. We argue that the reason is because there are quite a few action training examples that are missing more than 50% of the human body; thus pose cannot be determined in such frames. Instead, pose rendered on top of the RGB frames not only provides rich context beyond the pose itself, but also learns useful signals on the frames without pose.</p><p>We also experiment with dot and bar rendering markers and notice that bars yield slightly better results. We believe that this is because bars provides more geometric information about joint connections.</p><p>We also see that the fine-grained coloring scheme with ratio-aware rendering achieves the highest accuracy. This outcome is intuitive for the following reasons. First, finegrained pose rendering can provide detailed body joint relations such as fore-arm vs. upper-arm. Actions like pull-ups, hug, and throw can benefit from such joint relations. Second, with the ratio-aware line thickness, the pose itself provides information about relative distances which can serve as useful hints about group actions, e.g. playing games.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Is pose complementary to RGB?</head><p>We demonstrate that pose offers a complementary signal to the RGB (and Flow) streams. In order to demonstrate the value-add of Pose, we use the standard late-fusion approach to combining multiple streams (so as to not have potential confounding effects from distillation, which requires a more complex training setup).</p><p>Kinetics-600. In this section we focus on the the Kinetics-600 dataset <ref type="bibr" target="#b1">(Carreira and Zisserman 2017)</ref>, a large-scale, high-quality dataset containing YouTube video URLs with  dataset with markers: dot or bar, and ratio-aware marker size. The pose model is trained to validate performance. We also evaluate the approach of rendering on a black background, but since many training frames have no detected pose the performance of this naïve approach tends to be very low.  To test our multi-fusion framework, we employ S3D-G and ResNet3D-G backbones. Here, the "G" refers to the usage of selfgating. The first block shows results using S3D-G (pretrained with Imagenet) as the backbone. The second block shows results on ResNet3D-G as the backbone. Pose(BB) refers to the model trained with pose rendered on black background in <ref type="table" target="#tab_2">Table 2</ref>. Among all settings, combination of all three modalities outperform other combinations. We can also clearly see that with the pose added in the multi-stream fusion settings, the overall performance can be boosted, thus providing a complementary signal. a diverse range of human focused actions. The dataset consists of approximately 500k video clips, and covers 600 human action classes with at least 600 video clips for each type of action. Each clip is at least 10 seconds and is labeled with one single class. The actions cover a broad range of classes including human-object interactions such as playing instruments, working out, as well as human-human interactions such as sword fighting, shaking hands, and hugging.</p><p>Late multi-stream fusion. In the standard "late-fusion" approach, we run models independently on multiple streams, combining their predicted logits at the end through simple addition (see <ref type="bibr" target="#b9">Feichtenhofer, Pinz, and Zisserman (2016)</ref> for details). <ref type="table" target="#tab_4">Table 3</ref> shows a comparison of standard latefusion (across different combinations of the three streams, RGB, Flow and Pose) among our two backbone models (ResNet50-G and S3D-G).</p><p>For both S3D-G and ResNet50-G backbones, we can see that by incorporating additional modalities, we can always achieve performance gains. Adding flow or pose to the existing RGB stream yields similar improvements. Since flow</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Backbone</head><p>Top-1 Top-5 GFLOPs I3D (Carreira and Zisserman 2017) Inception 71.9 90.1 544 StNet-IRv2 RGB  InceptionResNet-V2 79.0 -440 P3D two-stream <ref type="bibr" target="#b25">(Qiu, Yao, and Mei 2017)</ref> ResNet152 80.9 94.9 -SlowFast R101+NL <ref type="bibr" target="#b8">(Feichtenhofer et al. 2019</ref>   and pose are somewhat independent modalities, by adding both of them to the RGB stream, we also observe "stacking" of the performance gains. Most importantly, we see that adding the pose stream always yields benefits (independent of backbone network and independent of whether we are already using a flow stream). One might wonder if the benefits of adding a pose stream come simply from the ensembling effect of two models -to show that this is not the case, we show that ensembling two RGB-only models (RGB+RGB in <ref type="table" target="#tab_4">Table 3</ref>) does not lead to measurable improvements. Additionally, we show an example of ensembling an RGB model with a model trained with pose rendered on a black background, which does not show measurable improvements either.</p><p>Visualization and explanation. <ref type="figure" target="#fig_2">Figure 4</ref> shows 9 examples of RGB, pose, and flow, as well as the corresponding response map from a layer from block5 in Resnet3D. The main purpose of this figure is to show the performance of the individual models trained on each modality.</p><p>The first row shows three sets of examples where the pose model is correct, and the RGB and Flow models are incorrect. For example, the leftmost example depicts an arm wrestling action. The pose response map responds most on the hands region of the frame where the wrestling happens. The response heatmap can be treated as an attention area in a tube of action sequences. For such actions, flow is not informative as there is little motion. Moveover, the RGB response could be distracted by elements in the background. However, pose can provide clear signal to the hand-to-hand interaction. The middle example shows a person performing a situp at a gym. It is difficult to classify this action correctly by focusing on the barbell regions of the image, as the RGB and flow model do. Instead, pose drives the model to "look at" the entire body configuration which allows the model to decide that it is a situp and not bench press, etc. The rightmost example shows a baby climbing a ladder. The pose stream focuses the attention on the legs where the climbing action happens, providing a useful complementary cue to the standard RGB and Flow modalities.</p><p>The second row comprises three examples where all modalities make the prediction correctly. From the response map, we can tell the three modalities mostly focus on similar locations among the video frames. For the leftmost example (playing polo), pose helps to focus more on the entire group of players, where the other two modalities put more weight on the right-most player. By looking at the original video clip, the motion of the right-most player is the largest, which is likely why RGB and flow give more weight to this player. The middle example shows a pillow fight where the pose modality response is greater on the pillow region. The pose model may learn additional information from the interaction of the two persons by looking at the pose and arm orientation, etc. The rightmost example shows swording where the pose stream focus more on the left-side acting player.</p><p>The third row shows three examples without any pose detected. There are quite a few frames in Kinetics-600 and other datasets where no pose is available. In such cases, since the RGB is still available via the pose stream, our pose based model can still learn reasonably good responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distilling down to PERF-Net</head><p>As discussed in Section , distillation can effectively incorporate multiple modalities with no additional cost to the complexity of the final model. In <ref type="table" target="#tab_7">Table 5</ref>, we show the results of multi-teacher distillation, which can jointly optimize over multiple input modalities. The advantage of the distillation is that our model size can remain the same while leveraging knowledge distilled from other modalities. Taking RGB as an example, after distilling on flow and pose using separate losses, the performance can be improved beyond single modality training -thus our final RGB-only model (a.k.a. PERF-Net) achieves 82.0 top-1 accuracy on Kinetics-600, which outperforms the state-of-the-art work by a noticeable margin with 2x less computational cost. <ref type="table" target="#tab_6">Table 4</ref> shows a comparison between PERF-Nets and other state-of-the-art single-stream works. Boosted by pose, PERF-Net outperforms all models at Top-1 and Top-5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Will distilled checkpoint transfer well?</head><p>We select two human action datasets for transfer learning experiments initialized using checkpoints on Kinetics-600 with distillation. However, during fine-tuning, we use only the classification loss, but not distillation For both of the datasets, we show that PERF-Net achieves the state-of-theart performance among single stream models. P3D <ref type="bibr" target="#b25">(Qiu, Yao, and Mei 2017)</ref> 88.6 -C3D <ref type="bibr" target="#b35">(Tran et al. 2015)</ref> 82.3 51.6 Res3D <ref type="bibr" target="#b36">(Tran et al. 2017)</ref> 85.8 54.9 TSM <ref type="bibr" target="#b18">(Lin, Gan, and Han 2019)</ref> 95.9 73.5 I3D <ref type="bibr" target="#b1">(Carreira and Zisserman 2017)</ref> 95.6 74.8 R(2+1)D <ref type="bibr" target="#b37">(Tran et al. 2018)</ref> 96.8 74.5 S3D-G <ref type="bibr" target="#b41">(Xie et al. 2018)</ref> 96.8 75.9 HATNet <ref type="bibr" target="#b6">(Diba et al. 2019)</ref> 97.7 76.2 MARS+RGB+Flow <ref type="bibr" target="#b4">(Crasto et al. 2019)</ref> 97.8 80.9 Two-stream I3D <ref type="bibr" target="#b1">(Carreira and Zisserman 2017)</ref> 98.0 80.9 EvaNet-top individual   HMDB-51 HMDB-51 <ref type="bibr" target="#b16">(Kuehne et al. 2011</ref>) contains 6849 clips divided into 51 action categories, each containing a minimum of 101 clips for each category. We apply the same pose detection and rendering method to the HMDB-51 dataset. We finetune S3D-G model pre-trained on Kinetics-600 for 30 epochs and report the accuracy by averaging the results from 3 splits. <ref type="table" target="#tab_9">Table 6</ref> shows the averaged perfor-mance of our PERF-Net models. Even though the PERF-Net is slightly lower than EvaNet-ensemble , it is better than the EvaNet top individual which is a single stream model. At time of submission, we outperform the current best on the leaderboard using single stream model (HMDB-51-Leaderboard 2020).</p><p>UCF-101 UCF-101 <ref type="bibr" target="#b30">(Soomro, Zamir, and Shah 2012)</ref> is an action recognition data set of 13,320 realistic action videos, collected from YouTube, with 101 action categories. Similar to HMDB51, in <ref type="table" target="#tab_9">Table 6</ref>, we also report the accuracy by averaging over the 3 dataset splits. Similarly, our proposed model achieves the state-of-the-art at time of submission on the leaderboard (UCF101-Leaderboard 2020).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>We have presented an empirical study of the effects of different pose rendering methods and how to effectively incorporate it into a video recognition model to benefit human action recognition. We have shown strong evidence that, with the human pose modality and the proposed rendering method, by using a simple fusion method, the model can outperform the state-of-the-art performance. We hope such pose modality can be further studied to extended to other domains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visualizations of models trained on RGB, Pose, and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The distillation framework is composed of two pieces: student network and teacher network(s). The input modality can be any representations, such as RGB, flow, or pose. The losses are computed on each of the logits from the corresponding teacher networks (separate loss). Additionally, we experimented with the loss computed on the summation of logits (1, 2, ... N) from all teacher networks and added to the regression loss (unified loss). We show separate loss outperforms unified loss in the experimental section.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Nine Grad-CAM visualizations (Selvaraju et al. 2017) of our ResNet50-G model. Each row contains three examples. For each example, the top row contains the original RGB, pose overlay, and Flow frames and the bottom row are the normalized response maps from RGB, pose, and flow streams, respectively. ROW1: arm wrestling, situp action, ladder climbing. ROW2: playing polo, pillow fight, swording. ROW3: unboxing, weaving basket, napkin folding. The top two rows show examples with pose detected. The bottom row shows three actions without any pose. Model UCF-101 HMDB-51</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Pose stream results using Resnet50-G on Kinetics-600</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Late Multi-Stream Fusion Results on Kinetics-600.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison with the state-of-the-art on Kinetics-600.</figDesc><table><row><cell cols="3">Backbone Student Teacher(s)</cell><cell cols="3">Top-1 Top-5 pretrain</cell></row><row><cell></cell><cell>RGB</cell><cell>Flow</cell><cell>78.3</cell><cell>94.3</cell><cell>-</cell></row><row><cell>S3D-G</cell><cell>RGB</cell><cell>Pose</cell><cell>78.4</cell><cell>94.2</cell><cell>-</cell></row><row><cell></cell><cell>RGB</cell><cell>Flow+Pose (SL)</cell><cell>78.9</cell><cell>94.6</cell><cell>-</cell></row><row><cell></cell><cell>RGB</cell><cell>Flow</cell><cell>80.6</cell><cell>94.6</cell><cell>-</cell></row><row><cell></cell><cell>RGB</cell><cell>Pose</cell><cell>80.4</cell><cell>94.7</cell><cell>-</cell></row><row><cell>Resnet50-G</cell><cell>RGB</cell><cell>Flow+Pose (UL)</cell><cell>80.7</cell><cell>95.3</cell><cell>-</cell></row><row><cell></cell><cell>RGB</cell><cell>Flow+Pose (SL)</cell><cell>82.0</cell><cell>95.7</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Results on Kinetics-600 distillation. SL stands for separate loss, and UL stands for unified loss.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>Comparison with state-of-the-art on UCF-101 and HMDB-51. The backbone of the PERF-Net here is S3D-G.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buciluȃ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="535" to="541" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">P-cnn: Posebased cnn features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chéron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3218" to="3226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">PoTion: Pose MoTion Representation for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2018.00734</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7024" to="7033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">MARS: Motion-augmented RGB stream for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crasto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7882" to="7891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Large Scale Holistic Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fayyaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">X3D: Expanding Architectures for Efficient Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="200" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Separate visual pathways for perception and action</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Goodale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Milner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Exploiting Spatial-Temporal Modelling and Multi-Modal Fusion for Human Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Action Recognition In Videos on HMDB-51</title>
		<idno>HMDB-51-Leaderboard. 2020</idno>
		<ptr target="https://paperswithcode.com/sota/action-recognition-in-videos-on-hmdb-51" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">PoseNet: A Convolutional Network for Real-Time 6-DOF Camera Relocalization. 2938-2946</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.336</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">HMDB51: A Large Video Database for Human Motion Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2011.6126543</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision 2556-2563</title>
		<meeting>the IEEE International Conference on Computer Vision 2556-2563</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bitorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09756</idno>
		<title level="m">Scale MLPerf-0.6 models on Google TPU-v3 Pods</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TSM: Temporal Shift Module for Efficient Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Graph distillation for action detection with privileged modalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carlos Niebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="166" to="183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards Accurate Multi-person Pose Estimation in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno>1063-6919. doi:10.1109/ CVPR.2017.395</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3711" to="3719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evolving Space-Time Neural Architectures for Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Representation Flow for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning Spatio-Temporal Representation with Pseudo-3D Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2017.590</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning Spatio-Temporal Representation with Local and Global Diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13209</idno>
		<title level="m">Assemblenet: Searching for multi-stream neural connectivity in video architectures</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
		<ptr target="http://arxiv.org/abs/1212" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">D3d: Distilled 3d networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stroud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="625" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">TV-L1 Optical Flow Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snchez Prez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Meinhardt-Llopis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Facciolo</surname></persName>
		</author>
		<idno type="DOI">10.5201/ipol.2013.26</idno>
	</analytic>
	<monogr>
		<title level="j">Image Processing On Line</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="137" to="150" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="140" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.05038</idno>
		<title level="m">Convnet architecture search for spatiotemporal feature learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Leaderboard: Action Recognition In Videos on UCF101</title>
		<idno>UCF101-Leaderboard. 2020</idno>
		<ptr target="https://paperswithcode.com/sota/action-recognition-in-videos-on-ucf101" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">BFloat16: the secret to high performance on cloud TPUs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kanwar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Google Cloud Blog</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nonlocal neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="305" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Does Human Action Recognition Benefit from Pose Estimation?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.25.67</idno>
		<idno>67.1-67.11. BMVA Press. ISBN 1-901725-43</idno>
		<ptr target="Http://dx.doi.org/10.5244/C.25.67" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Real-time action recognition with enhanced motion vector CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Chained multi-stream networks exploiting pose, motion, and appearance for action classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2904" to="2913" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
