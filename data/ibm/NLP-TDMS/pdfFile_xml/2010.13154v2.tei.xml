<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ATTENTION IS ALL YOU NEED IN SPEECH SEPARATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cem</forename><surname>Subakan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mila-Quebec AI Institute</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirco</forename><surname>Ravanelli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mila-Quebec AI Institute</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuele</forename><surname>Cornell</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Università Politecnica delle Marche</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirko</forename><surname>Bronzi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Mila-Quebec AI Institute</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Zhong</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Rochester</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ATTENTION IS ALL YOU NEED IN SPEECH SEPARATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-speech separation</term>
					<term>source separation</term>
					<term>trans- former</term>
					<term>attention</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning. RNNs, however, are inherently sequential models that do not allow parallelization of their computations. Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations with a multi-head attention mechanism.</p><p>In this paper, we propose the SepFormer, a novel RNN-free Transformer-based neural network for speech separation. The Sep-Former learns short and long-term dependencies with a multi-scale approach that employs transformers. The proposed model achieves state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It reaches an SI-SNRi of 22.3 dB on WSJ0-2mix and an SI-SNRi of 19.5 dB on WSJ0-3mix. The SepFormer inherits the parallelization advantages of Transformers and achieves a competitive performance even when downsampling the encoded representation by a factor of 8. It is thus significantly faster and it is less memory-demanding than the latest speech separation systems with comparable performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>RNNs are a crucial component of modern audio processing systems and they are used in many different domains, including speech recognition, synthesis, enhancement, and separation, just to name a few. Especially when coupled with multiplicative gate mechanisms (like LSTM <ref type="bibr" target="#b0">[1]</ref> and GRU <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>), their recurrent connections are essential to learn long-term dependencies and properly manage speech contexts. Nevertheless, the inherently sequential nature of RNNs impairs an effective parallelization of the computations. This bottleneck is particularly evident when processing large datasets with long sequences. On the other hand, Transformers <ref type="bibr" target="#b3">[4]</ref> completely avoid this bottleneck by eliminating recurrence and replacing it with a fully attention-based mechanism. By attending to the whole sequence at once, a direct connection can be established between distant elements allowing Transformers to learn long-term dependencies more easily <ref type="bibr" target="#b4">[5]</ref>. For this reason, Transformers are gaining considerable popularity for speech processing and recently showed competitive performance in speech recognition <ref type="bibr" target="#b5">[6]</ref>, synthesis <ref type="bibr" target="#b6">[7]</ref>, enhancement <ref type="bibr" target="#b7">[8]</ref>, diarization <ref type="bibr" target="#b8">[9]</ref>, as well as speaker recognition <ref type="bibr" target="#b9">[10]</ref>.</p><p>Little research has been done so far on Transformer-based models for monaural audio source separation. The field has been revolutionized by the adoption of deep learning techniques <ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>, and with recent works <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref> achieving impressive results by adopting an end-to-end approach. Most of the current speech separation techniques <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> require effective modeling of long input  <ref type="figure">Fig. 1</ref>. The high-level description of our system: The encoder block estimates a learned-representation for the input signal, while the masking network estimates optimal masks to separate the sources present in the mixtures. The decoder finally reconstructs the estimated sources in the time domain using the masks provided by the masking network.</p><p>sequences to perform well. Current systems rely, in large part, on the learned-domain masking strategy popularized by Conv-TasNet <ref type="bibr" target="#b14">[15]</ref>. In this framework, an overcomplete set of analysis and synthesis filters is learned directly from the data, and separation is performed by estimating a mask for each source in this learned-domain. Building on this, Dual-Path RNN (DPRNN) <ref type="bibr" target="#b16">[17]</ref> has demonstrated that better long-term modeling is crucial to improve the separation performance. This is achieved by splitting the input sequence into multiple chunks that are processed locally and globally with different RNNs. Nevertheless, due to the use of RNNs, DPRNN still suffers from the aforementioned limitations of recurrent connections, especially regarding the global processing step. An attempt to integrate transformers into the speech separation pipeline has been recently done in <ref type="bibr" target="#b21">[22]</ref> where the proposed Dual-Path Transformer Network (DPTNet) is shown to outperform the standard DPRNN. Such an architecture, however, still embeds an RNN, effectively negating the parallelization capability of pure-attention models.</p><p>In this paper, we propose a novel model called SepFormer (Separation Transformer), which is mainly composed of multi-head attention and feed-forward layers. We adopt the dual-path framework introduced by DPRNN and we replace RNNs with a multiscale pipeline composed of transformers that learn both short and long-term dependencies. The dual-path framework enables to mitigate the quadratic complexity of transformers, as transformers in the dual-path framework process smaller chunks.</p><p>To the best of our knowledge, this is the first work showing that we can obtain state-of-the-art performance in separation with an RNN-free Transformer-based architecture. The SepFormer achieves an SI-SNRi of 22.3 dB on the standard WSJ0-2mix dataset. It also achieves the SOTA performance of 19.5 dB SI-SNRi on the WSJ0-3mix dataset. The SepFormer not only processes all the time steps in parallel but also achieves competitive performance when downsampling the encoded representation by a factor of 8. This makes the proposed architecture significantly faster and less memory demand-arXiv:2010.13154v2 [eess.AS] 8 Mar 2021</p><p>ing than the latest RNN-based separation models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">THE MODEL</head><p>The proposed model is based on the learned-domain masking approach <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> and employs an encoder, a decoder, and a masking network, as shown in <ref type="figure">Figure 1</ref>. The encoder is fully convolutional, while the masking network employs two Transformers embedded inside the dual-path processing block proposed in <ref type="bibr" target="#b16">[17]</ref>. The decoder finally reconstructs the separated signals in the time domain by using the masks predicted by the masking network. To foster reproducibility, the SepFormer will be made available within the SpeechBrain toolkit 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Encoder</head><p>The encoder takes in the time-domain mixture-signal x ∈ R T as input, which contains audio from multiple speakers. It learns an STFT-like representation h ∈ R F ×T using a single convolutional layer: h = ReLU(conv1d(x)).</p><p>(1)</p><p>As we will describe in Sec. 4, the stride factor of this convolution impacts significantly on the performance, speed, and memory of the model. As in <ref type="bibr" target="#b14">[15]</ref>, the encoded input h is normalized with layer normalization <ref type="bibr" target="#b23">[24]</ref> and processed by a linear layer (with dimensionality F ). We then create overlapping chunks of size C by chopping up h on the time axis with an overlap factor of 50%. We denote the output of the chunking operation with h ∈ R F ×C×N c , where C is the length of each chunk, and N c is the resulting number of chunks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Masking Network</head><p>The representation h feeds the SepFormer block, which is the main component of the masking network. This block, which will be described in detail in Sec. 2.3, employs a pipeline composed of two transformers able to learn short and long-term dependencies.</p><p>The output of the SepFormer h ∈ R F ×C×N c is processed by PReLU activations followed by a linear layer. We denote the output of this module h ∈ R (F ×N s)×C×N c , where N s is the number of speakers. Afterwards we apply the overlap-add scheme described in <ref type="bibr" target="#b16">[17]</ref> and obtain h ∈ R F ×N s×T . We pass this representation through two feed-forward layers and a ReLU activation at the end to finally obtain the mask m k for each of the speakers. The SepFormer block is designed to model both short and longterm dependencies with the dual-scale approach of DPRNNs <ref type="bibr" target="#b16">[17]</ref>. In our model, the transformer block which models the short-term dependencies is named IntraTransformer (IntraT), and the block for longer-term dependencies is named InterTransformer (InterT). In-traT processes the second dimension of h , and thus acts on each chunk independently, modeling the short-term dependencies within 1 speechbrain.github.io/ each chunk. Next, we permute the last two dimensions (which we denote with P), and the InterT is applied to model the transitions across chunks. This scheme enables effective modelling of longterm dependencies across the chunks. The overall transformation of the SepFormer is therefore defined as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">SepFormer Block</head><formula xml:id="formula_0">h = finter(P(fintra(h ))),<label>(2)</label></formula><p>where we denote the IntraT and InterT with finter(.), and fintra(.), respectively. The overall SepFormer block is repeated N times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">Intra and Inter Transformers</head><p>Figure 2 (Bottom) shows the architecture of the Transformers used for both the IntraT and InterT blocks. It closely resembles the original one defined in <ref type="bibr" target="#b3">[4]</ref>. We use the variable z to denote the input to the Transformer. First of all, sinusoidal positional encoding e is added to the input z, such that,</p><formula xml:id="formula_1">z = z + e.<label>(3)</label></formula><p>Positional encoding injects information on the order of the various elements composing the sequence, thus improving the separation performance. We follow the positional encoding definition in <ref type="bibr" target="#b3">[4]</ref>. We then apply multiple Transformer layers. Inside each Transformer layer g(.), we first apply layer normalization, followed by multi-head attention (MHA):</p><formula xml:id="formula_2">z = MultiHeadAttention(LayerNorm(z )).<label>(4)</label></formula><p>As proposed in <ref type="bibr" target="#b3">[4]</ref>, each attention head computes the scaled dotproduct attention between all the elements of the sequence. The Transformer finally employs a feed-forward network (FFW), which is applied to each position independently:</p><formula xml:id="formula_3">z = FeedForward(LayerNorm(z + z )) + z + z . (5)</formula><p>The overall transformer block is therefore defined as follows:</p><formula xml:id="formula_4">f (z) = g K (z + e) + z,<label>(6)</label></formula><p>where g K (.) denotes K layers of transformer layer g(.). We use K = N intra layers for the IntraT, and K = N inter layers for the InterT. As shown in <ref type="figure" target="#fig_1">Figure 2</ref> (Bottom) and Eq. <ref type="formula" target="#formula_4">(6)</ref>, we add residual connections across the transformer layers, and across the transformer architecture to improve gradient backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Decoder</head><p>The decoder simply uses a transposed convolution layer, with the same stride and kernel size of the encoder. The input to the decoder is the element-wise multiplication between the mask m k of the source k and the output of the encoder h. The transformation of the decoder can therefore be expressed as follows:</p><formula xml:id="formula_5">s k = conv1d-transpose(m k * h),<label>(7)</label></formula><p>where s k ∈ R T denotes the separated source k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTAL SETUP</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset</head><p>We use the popular WSJ0-2mix and WSJ0-3mix datasets <ref type="bibr" target="#b10">[11]</ref> for source separation, where mixtures of two speakers and three speakers are created by randomly mixing utterances in the WSJ0 corpus. The relative levels for the sources are sampled uniformly between 0 dB to 5 dB. Respectively, 30, 10, 5 hours of speech is used for training, validation, and test. The training and test sets are created with different sets of speakers. The waveforms are sampled at 8 kHz.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture and Training Details</head><p>The encoder is based on 256 convolutional filters with a kernel size of 16 samples and a stride factor of 8 samples. The decoder uses the same kernel size and the stride factors of the encoder. In our best models, the SepFormer masking network processes chunks of size C = 250 with a 50 % overlap between them and employs 8 layers of transformers in both IntraT and InterT. The IntraT-InterT dual-path processing pipeline is repeated N = 2 times. We used 8 parallel attention heads, and 1024-dimensional positional feed-forward networks within each Transformer layer. The model has a total of 26 million parameters.</p><p>We explored the use of dynamic mixing (DM) data augmentation <ref type="bibr" target="#b22">[23]</ref> which consists in on-the-fly creation of new mixtures from single speaker sources. In this work we expanded this powerful technique by applying also speed perturbation on the sources before mixing them. The speed randomly changes between 95 % slow-down and 105 % speed-up.</p><p>We used the Adam algorithm <ref type="bibr" target="#b24">[25]</ref> as optimizer, with a learning rate of 15e −5 . After epoch 65 (after epoch 100 with DM), the learning rate is annealed by halving it if we do not observe any improvement of the validation performance for 3 successive epochs (5 epoch for DM). Gradient clipping is employed to limit the L2 norm of the gradients to 5. During training, we used a batch size of 1, and used the scale-invariant signal-to-noise Ratio (SI-SNR) <ref type="bibr" target="#b25">[26]</ref> via utterance-level permutation invariant loss <ref type="bibr" target="#b12">[13]</ref>, with clipping at 30dB <ref type="bibr" target="#b22">[23]</ref>. We used automatic mixed-precision to speed up training. The system is trained for a maximum of 200 epochs. Each epoch takes approximately 1.5 hours on a single NVIDIA V100 GPU with 32 GB of memory. <ref type="table" target="#tab_1">Table 1</ref> compares the performance achieved by the proposed Sep-Former with the best results reported in the literature on the WSJ0-2mix dataset. The SepFormer achieves an SI-SNR improvement (SI-SNRi) of 22.3 dB and a Signal-to-Distortion Ratio <ref type="bibr" target="#b29">[30]</ref> (SDRi) improvement of 22.4 dB on the test-set with dynamic mixing. When using dynamic mixing, the proposed architecture achieves state-ofthe-art performance. The SepFormer outperforms previous systems without using dynamic mixing except Wavesplit, which uses speaker identity as additional information.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Results on WSJ0-2mix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Hereafter we study the effect of various hyperparameters and data augmentation on the performance of the SepFormer using WSJ0-2mix dataset. The results are summarized in <ref type="table" target="#tab_2">Table 2</ref>. The reported performance in this table is calculated on the validation set. We observe that the number of InterT and IntraT blocks has an important impact on the performance. The best results are achieved with 8 layers for both blocks replicated two times. We also would like to point out that a respectable performance of 19.2 dB is obtained even when we use a single layer transformer for the Inter-  Transformer. This suggests that the IntraTransformer, and thus local processing, has a greater influence on the performance. It also emerges that positional encoding is helpful (e.g. see lines 3 and 5 of <ref type="table" target="#tab_2">Table 2</ref>). A similar outcome has been observed in <ref type="bibr" target="#b30">[31]</ref> for speech enhancement. As for the number of attention heads, we observe a slight performance difference between 8 and 16 heads. Finally, it can be observed that dynamic mixing helps the performance significantly. <ref type="table" target="#tab_3">Table 3</ref> showcases the best performing models on the WSJ0-3mix dataset. SepFormer obtains the state-of-the-art performance with an SI-SNRi of 19.5 dB and an SDRi of 19.7 dB. We used here the best architecture found for the WSJ0-2mix dataset. The only difference is that the decoder has now three outputs. It is worth noting that on this corpus the SepFormer outperforms all previously proposed systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on WSJ0-3mix</head><p>Our results on WSJ0-2mix and WSJ0-3mix show that it is possible to achieve state-of-the-art performance in separation with an RNN-free Transformer-based model. The big advantage of Sep-Former over RNN-based systems like <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref> is the possibility to parallelize the computations over different time steps. This leads to faster training and inference, as described in the following section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Speed and Memory Comparison</head><p>We now compare the training and inference speed of our model with DPRNN <ref type="bibr" target="#b16">[17]</ref> and DPTNet <ref type="bibr" target="#b21">[22]</ref>. <ref type="figure" target="#fig_4">Figure 3 (left)</ref> shows the training curves of the aforementioned models on the WSJ0-2mix dataset. We plot the performance achieved on the validation set in the first 48 hours of training versus the wall-clock time. For a fair comparison, we used the same machine with the same GPU (a single NVIDIA V100-32GB) for all the models. Moreover, all the systems are trained with a batch size of 1 and employ automatic mixed precision. We observe that the SepFormer is faster than DPRNN and DPTNeT. <ref type="figure" target="#fig_4">Figure 3</ref> (left), highlights that SepFormer reaches above 17dB levels only after a full day of training, whereas the DPRNN model requires two days of training to achieve the same level of performance. <ref type="figure" target="#fig_4">Figure 3</ref> (middle&amp;right) compares the average computation time (in ms) and the total memory allocation (in GB) during inference when single precision is used. We analyze the speed of our best model for both WSJ0-2Mix and WSJ0-3Mix datasets. We compare our models against DP-RNN, DPTNeT, and Wavesplit. All the models are stored in the same NVIDIA RTX8000-48GB GPU and we performed this analysis using the PyTorch profiler <ref type="bibr" target="#b31">[32]</ref>. For Wavesplit we used the implementation in <ref type="bibr" target="#b32">[33]</ref>.</p><p>From this analysis, it emerges that the SepFormer is not only faster but also less memory demanding than DPTNet, DPRNN, and Wavesplit. We observed the same behavior using the CPU for inference also. Such a level of computational efficiency is achieved even though the proposed SepFormer employs more parameters than the other RNN-based methods (see <ref type="table" target="#tab_1">Table 1</ref>). This is not only due to the superior parallelization capabilities of the proposed model, but also because the best performance is achieved with a stride factor of 8 samples, against a stride of 1 for DPRNN and DPTNet. Increasing the stride of the encoder results in downsampling the input sequence, and therefore the model processes less data. In <ref type="bibr" target="#b16">[17]</ref>, the authors showed that the DPRNN performance degrades when increasing the stride factor. The SepFormer, instead, reaches competitive results even with a relatively large stride, leading to the aforementioned speed and memory advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSIONS</head><p>In this paper, we proposed a novel neural model for speech separation called SepFormer (Separation Transformer). The SepFormer is an RNN-free architecture that employs a masking network composed of transformers only. The masking network learns both short and long-term dependencies using a multi-scale approach. Our results, reported on the WSJ0-2mix and WSJ0-3mix datasets, highlight that we can reach state-of-the-art performances in source separation without using RNNs in the network design. This way, computations over different time-steps can be parallelized. Moreover, our model achieves a competitive performance even when subsampling the encoded representation by a factor of 8. These two properties lead to a significant speed-up at training/inference time and a drastic reduction of memory usage, especially when compared to recent models such as DPRNN, DPTNet, and Wavesplit. As future work, we would like to explore different transformer architectures that could potentially further improve performance, speed, and memory usage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 (</head><label>2</label><figDesc>top) shows the detailed architecture of the masking network (Masking Net). The masking network is fed by the encoded representations h ∈ R F ×T and estimates a mask {m1, . . . , mNs} for each of the N s speakers in the mixture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 (</head><label>2</label><figDesc>Middle) shows the architecture of the SepFormer block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>(Top) The overall architecture proposed for the masking network. (Middle) The SepFormer Block. (Bottom) The transformer architecture f (.) that is used both in the IntraTransformer block and in the InterTransformer block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>(Left) The traning curves of SepFormer, DPRNN, and DPTNeT on the WSJ0-2mix dataset. (Middle &amp; Right) The comparison of forward-pass speed and memory usage in the GPU on inputs ranging 1-5 seconds long sampled at 8kHz.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Best results on the WSJ0-2mix dataset (test-set). DM stands for dynamic mixing.</figDesc><table><row><cell>Model</cell><cell cols="4">SI-SNRi SDRi # Param Stride</cell></row><row><cell>Tasnet [27]</cell><cell>10.8</cell><cell>11.1</cell><cell>n.a</cell><cell>20</cell></row><row><cell>SignPredictionNet [28]</cell><cell>15.3</cell><cell>15.6</cell><cell>55.2M</cell><cell>8</cell></row><row><cell>ConvTasnet [15]</cell><cell>15.3</cell><cell>15.6</cell><cell>5.1M</cell><cell>10</cell></row><row><cell>Two-Step CTN [29]</cell><cell>16.1</cell><cell>n.a.</cell><cell>8.6M</cell><cell>10</cell></row><row><cell>DeepCASA [18]</cell><cell>17.7</cell><cell>18.0</cell><cell>12.8M</cell><cell>1</cell></row><row><cell>FurcaNeXt [19]</cell><cell>n.a.</cell><cell>18.4</cell><cell>51.4M</cell><cell>n.a.</cell></row><row><cell>DualPathRNN [17]</cell><cell>18.8</cell><cell>19.0</cell><cell>2.6M</cell><cell>1</cell></row><row><cell>sudo rm -rf [21]</cell><cell>18.9</cell><cell>n.a.</cell><cell>2.6M</cell><cell>10</cell></row><row><cell>VSUNOS [20]</cell><cell>20.1</cell><cell>20.4</cell><cell>7.5M</cell><cell>2</cell></row><row><cell>DPTNet* [22]</cell><cell>20.2</cell><cell>20.6</cell><cell>2.6M</cell><cell>1</cell></row><row><cell>Wavesplit** [23]</cell><cell>21.0</cell><cell>21.2</cell><cell>29M</cell><cell>1</cell></row><row><cell>Wavesplit** + DM [23]</cell><cell>22.2</cell><cell>22.3</cell><cell>29M</cell><cell>1</cell></row><row><cell>SepFormer</cell><cell>20.4</cell><cell>20.5</cell><cell>26M</cell><cell>8</cell></row><row><cell>SepFormer + DM</cell><cell>22.3</cell><cell>22.4</cell><cell>26M</cell><cell>8</cell></row><row><cell cols="4">*only SI-SNR and SDR (without improvement) are reported.</cell><cell></cell></row><row><cell cols="2">**uses speaker-ids as additional info.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Ablation of the SepFormer on WSJ0-2Mix (validation set).</figDesc><table><row><cell cols="4">SI-SNRi N N intra N inter</cell><cell cols="4"># Heads DFF PosEnc DM</cell></row><row><cell>22.3</cell><cell>2</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>1024</cell><cell>Yes</cell><cell>Yes</cell></row><row><cell>20.5</cell><cell>2</cell><cell>8</cell><cell>8</cell><cell>8</cell><cell>1024</cell><cell>Yes</cell><cell>No</cell></row><row><cell>20.4</cell><cell>2</cell><cell>4</cell><cell>4</cell><cell>16</cell><cell>2048</cell><cell>Yes</cell><cell>No</cell></row><row><cell>20.2</cell><cell>2</cell><cell>4</cell><cell>4</cell><cell>8</cell><cell>2048</cell><cell>Yes</cell><cell>No</cell></row><row><cell>19.9</cell><cell>2</cell><cell>4</cell><cell>4</cell><cell>8</cell><cell>2048</cell><cell>Yes</cell><cell>No</cell></row><row><cell>19.8</cell><cell>3</cell><cell>4</cell><cell>4</cell><cell>8</cell><cell>2048</cell><cell>Yes</cell><cell>No</cell></row><row><cell>19.4</cell><cell>2</cell><cell>4</cell><cell>4</cell><cell>8</cell><cell>2048</cell><cell>No</cell><cell>No</cell></row><row><cell>19.2</cell><cell>2</cell><cell>4</cell><cell>1</cell><cell>8</cell><cell>2048</cell><cell>Yes</cell><cell>No</cell></row><row><cell>19.1</cell><cell>2</cell><cell>3</cell><cell>3</cell><cell>8</cell><cell>2048</cell><cell>Yes</cell><cell>No</cell></row><row><cell>19.0</cell><cell>2</cell><cell>3</cell><cell>3</cell><cell>8</cell><cell>2048</cell><cell>No</cell><cell>No</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Best results on the WSJ0-3mix dataset.</figDesc><table><row><cell>Model</cell><cell cols="3">SI-SNRi SDRi # Param</cell></row><row><cell>ConvTasnet [15]</cell><cell>12.7</cell><cell>13.1</cell><cell>5.1M</cell></row><row><cell>DualPathRNN [17]</cell><cell>14.7</cell><cell>n.a</cell><cell>2.6M</cell></row><row><cell>VSUNOS [20]</cell><cell>16.9</cell><cell>n.a</cell><cell>7.5M</cell></row><row><cell>Wavesplit [23]</cell><cell>17.3</cell><cell>17.6</cell><cell>29M</cell></row><row><cell>Wavesplit [23] + DM</cell><cell>17.8</cell><cell>18.1</cell><cell>29M</cell></row><row><cell>Sepformer</cell><cell>17.6</cell><cell>17.9</cell><cell>26M</cell></row><row><cell>Sepformer + DM</cell><cell>19.5</cell><cell>19.7</cell><cell>26M</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the properties of neural machine translation: Encoder-decoder approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SSST</title>
		<meeting>of SSST</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="103" to="111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Light gated recurrent units for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Brakel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omologo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Emerging Topics in Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="92" to="102" />
			<date type="published" when="2018-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno>abs/1706.03762</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Untangling tradeoffs between recurrence and selfattention in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kerg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kanuparthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goyette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lajoie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A comparative study on transformer vs rnn in speech applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Inaguma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Someki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E Y</forename><surname>Soplin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ASRU</title>
		<meeting>of ASRU</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="449" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural speech synthesis with transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of AAAI</title>
		<meeting>of AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6706" to="6713" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">T-gsa: Transformer with gaussian-weighted self-attention for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6649" to="6653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative neural clustering for speaker diarisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Kreyssig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Woodland</surname></persName>
		</author>
		<idno>abs/1910.09703</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">End-to-end multi-speaker speech recognition with transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6134" to="6138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Permutation invariant training of deep models for speaker-independent multitalker speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="241" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kolbaek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1901" to="1913" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">End-to-end source separation with adaptive front-ends</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrikant</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonah</forename><surname>Casebeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paris</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACSSC</title>
		<meeting>of ACSSC</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<title level="m">Conv-TasNet: Surpassing Ideal Time-Frequency Magnitude Masking for Speech Separation</title>
		<imprint>
			<date type="published" when="2019-08" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1256" to="1266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep learning for monoaural source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1562" to="1566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dual-path rnn: efficient long sequence modeling for time-domain single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yoshioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="46" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Divide and conquer: A deep casa approach to talker-independent monaural speaker separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on audio, speech, and language processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">12</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Furcanext: End-to-end monaural speech separation with dynamic gated dilated temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MultiMedia Modeling</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="653" to="665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Voice separation with an unknown number of multiple speakers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Adi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7164" to="7175" />
		</imprint>
	</monogr>
	<note type="report_type">ICML</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sudo rm -rf: Efficient networks for universal audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MLSP</title>
		<imprint>
			<biblScope unit="page" from="1" to="6" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dual-Path Transformer Network: Direct Context-Aware Modeling for End-to-End Monaural Speech Separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2642" to="2646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Wavesplit: End-to-end speech separation by speaker clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zeghidour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08933</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sdrhalf-baked or well done?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="626" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">TasNet: time-domain audio separation network for real-time, single-channel speech separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<idno>abs/1711.00541</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning based phase reconstruction for speaker separation: A trigonometric perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhong-Qiu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deliang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="71" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-step sound source separation: Training on learned latent targets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkataramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Subakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Smaragdis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Performance measurement in blind audio source separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Févotte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on audio, speech, and language processing</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1462" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">T-gsa: Transformer with gaussian-weighted self-attention for speech enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Khamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICASSP</title>
		<meeting>of ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6649" to="6653" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Profiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="https://pytorch.org/tutorials/recipes/recipes/profiler.html" />
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2020" to="2030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Asteroid: the PyTorch-based audio source separation toolkit for researchers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pariente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cornell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sivasankaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzinis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Heitkaemper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olvera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-R</forename><surname>Stöter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Martın-Doñas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ditter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Deleforge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of Interspeech</title>
		<meeting>of Interspeech</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2637" to="2641" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
