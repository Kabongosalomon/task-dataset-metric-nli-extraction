<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Competitive Multi-scale Convolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibin</forename><surname>Liao</surname></persName>
							<email>zhibin.liao@adelaide.edu.au</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<email>gustavo.carneiro@adelaide.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">ARC Centre of Excellence for Robotic Vision</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Competitive Multi-scale Convolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we introduce a new deep convolutional neural network (ConvNet) module that promotes competition among a set of multi-scale convolutional filters. This new module is inspired by the inception module, where we replace the original collaborative pooling stage (consisting of a concatenation of the multi-scale filter outputs) by a competitive pooling represented by a maxout activation unit. This extension has the following two objectives: 1) the selection of the maximum response among the multi-scale filters prevents filter co-adaptation and allows the formation of multiple sub-networks within the same model, which has been shown to facilitate the training of complex learning problems; and 2) the maxout unit reduces the dimensionality of the outputs from the multi-scale filters. We show that the use of our proposed module in typical deep ConvNets produces classification results that are either better than or comparable to the state of the art on the following benchmark datasets: MNIST, CIFAR-10, CIFAR-100 and SVHN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The use of competitive activation units in deep convolutional neural networks (ConvNets) is generally understood as a way of building one network by the combination of multiple sub-networks, where each one is capable of solving a simpler task when compared to the complexity of the original problem involving the whole dataset <ref type="bibr" target="#b21">[22]</ref>. Similar ideas have been explored in the past using multi-layer perceptron models <ref type="bibr" target="#b5">[6]</ref>, but there is a resurgence in the use of competitive activation units in deep ConvNets <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref>. For instance, rectified linear unit (ReLU) <ref type="bibr" target="#b0">[1]</ref> promotes a competition between the input sum (usually computed from the output of convolutional layers) and a fixed value of 0, while maxout <ref type="bibr" target="#b3">[4]</ref> and local winner-take-all (LWTA) <ref type="bibr" target="#b22">[23]</ref> explore an explicit competition among the input units. As shown by Srivastava et al. <ref type="bibr" target="#b21">[22]</ref>, these competitive activation units allow the formation of sub-networks that respond similarly to similar input patterns, which facilitates training <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23]</ref>  and generally produces superior classification results <ref type="bibr" target="#b21">[22]</ref>.</p><p>In this paper, we introduce a new module for deep Con-vNets composed of several multi-scale convolutional filters that are joined by a maxout activation unit, which promotes competition among these filters. Our idea has been inspired by the recently proposed inception module <ref type="bibr" target="#b23">[24]</ref>, which currently produces state-of-the-art results on the ILSVRC 2014 classification and detection challenges <ref type="bibr" target="#b16">[17]</ref>. The gist of our proposal is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>, where we have the data in the input layer filtered in parallel by a set of multi-scale convolutional filters <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>. Then the output of each scale of the convolutional layer passes through a batch normalisation unit (BNU) <ref type="bibr" target="#b4">[5]</ref> that weights the importance of each scale and also pre-conditions the model (note that the pre-conditioning ability of BNUs in ConvNets containing piece-wise linear activation units has recently been empirically shown in <ref type="bibr" target="#b10">[11]</ref>). Finally, the multi-scale filter outputs, weighted by BNU, are joined with a maxout unit <ref type="bibr" target="#b3">[4]</ref> that reduces the dimensionality of the joint filter outputs and promotes competition among the multi-scale filters, which prevents filter co-adaptation and allows the formation of multiple sub-networks. We show that the introduction of our proposal module in a typical deep ConvNet produces the best results in the field for the benchmark datasets CIFAR-10 <ref type="bibr" target="#b6">[7]</ref>, CIFAR-100 <ref type="bibr" target="#b6">[7]</ref>, and street view house number (SVHN) <ref type="bibr" target="#b15">[16]</ref>, while producing competitive results for MNIST <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Literature Review</head><p>One of the main reasons behind the outstanding performance of deep ConvNets is attributed to the use of competitive activation units in the form of piece-wise linear functions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>, such as ReLU <ref type="bibr" target="#b0">[1]</ref>, maxout <ref type="bibr" target="#b3">[4]</ref> and LWTA <ref type="bibr" target="#b22">[23]</ref> (see <ref type="figure" target="#fig_1">Fig. 2</ref>). In general, these activation functions enable the formation of sub-networks that respond consistently to similar input patterns <ref type="bibr" target="#b21">[22]</ref>, dividing the input data points (and more generally the training space) into regions <ref type="bibr" target="#b13">[14]</ref>, where classifiers and regressors can be learned more effectively given that the sub-problems in each of these regions is simpler than the one involving the whole training set. In addition, the joint training of the subnetworks present in such deep ConvNets represents a useful regularization method <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23]</ref>. In practice, ReLU allows the division of the input space into two regions, but maxout and LWTA can divide the space in as many regions as the number of inputs, so for this reason, the latter two functions can estimate exponentially complex functions more effectively because of the larger number of sub-networks that are jointly trained. An important aspect about deep Con-vNets with competitive activation units is the fact that the use of batch normalization units (BNU) helps not only with respect to the convergence rate <ref type="bibr" target="#b4">[5]</ref>, but also with the preconditioning of the model by promoting an even distribution of the input data points, which results in the maximization of the number of the regions (and respective sub-networks) produced by the piece-wise linear activation functions <ref type="bibr" target="#b10">[11]</ref>. Furthermore, training ConvNets with competitive activation units <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22]</ref> usually involves the use of dropout <ref type="bibr" target="#b19">[20]</ref> that consists of a regularization method that prevents filter coadaptation <ref type="bibr" target="#b19">[20]</ref>, which is a particularly important issue in such models, because filter co-adaptation can lead to a severe reduction in the number of the sub-networks that can be formed during training. ReLU <ref type="bibr" target="#b0">[1]</ref> (a) is active when the input is bigger than 0, LWTA <ref type="bibr" target="#b22">[23]</ref> (b) activates only the node that has the maximum value (setting to zero the other ones), and maxout <ref type="bibr" target="#b3">[4]</ref> (c) has only one output containing the maximum value from the input. This figure was adapted from <ref type="figure" target="#fig_0">Fig.1</ref> of <ref type="bibr" target="#b21">[22]</ref>.</p><p>Another aspect of the current research on deep ConvNets is the idea of making the network deeper, which has been shown to improve classification results <ref type="bibr" target="#b2">[3]</ref>. However, one of the main ideas being studied in the field is how to increase the depth of a ConvNet without necessarily increasing the complexity of the model parameter space <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24]</ref>. For the Szegedy et al.'s model <ref type="bibr" target="#b23">[24]</ref>, this is achieved with the use of 1 × 1 convolutional filters <ref type="bibr" target="#b11">[12]</ref> that are placed before each local filter present in the inception module in order to reduce the input dimensionality of the filter. In Simonyan et al.'s approach <ref type="bibr" target="#b18">[19]</ref>, the idea is to use a large number of layers with convolutional filters of very small size (e.g., 3 × 3). In this work, we restrict the complexity of the deep ConvNet with the use of maxout activation units, which selects only one of the input nodes, as shown in <ref type="figure" target="#fig_1">Fig, 2</ref>.</p><p>Finally, multi-scale filters in deep ConvNets is another important implementation that is increasingly being explored by several researchers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b26">27]</ref>. Essentially, multiscale filtering follows a neuroscience model <ref type="bibr" target="#b17">[18]</ref> that suggests that the input image data should be processed at several scales and then pooled together, so that the deeper processing stages can become robust to scale changes <ref type="bibr" target="#b23">[24]</ref>. We explore this idea in our proposal, as depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>, but we also argue (and show some evidence) that the multi-scale nature of the filters can prevent their co-adaptation during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Assume that an image is represented by x : Ω → R, where Ω denotes the image lattice, and that an image patch of size (2k − 1) × (2k − 1) (for k ∈ {1, 2, ..., K}) centred at position i ∈ Ω is represented by x i±(k−1) . The models being proposed in this paper follow the structure of the the NIN model <ref type="bibr" target="#b11">[12]</ref>, and is in general defined as follows:</p><formula xml:id="formula_0">f (x, θ f ) = f out • f L • ... • f 2 • f 1 (x),<label>(1)</label></formula><p>where • denotes the composition operator, θ f represents all the ConvNet parameters (i.e., weights and biases), f out (.) denotes an averaging pooling unit followed by a softmax activation function <ref type="bibr" target="#b11">[12]</ref>, and the network has blocks represented by l ∈ {1, ..., L}, with each block containing a composition of N l modules with f l (x) = f</p><formula xml:id="formula_1">(N l ) l • ... • f (2) l • f (1) l (x)</formula><p>. Each module f (n) l (.) at a particular position i ∈ Ω of the input data for block l is defined by</p><formula xml:id="formula_2">f (n) l (x i ) = σ γ 1 W 1 x i + β 1 , γ 3 W 3 x i±1 + β 3 , ..., γ 2k−1 W 2k−1 x i±(k−1) + β 2k−1 , γ p W 1 p 3×3 (x i±1 ) + β p .<label>(2)</label></formula><p>where σ(.) represents the maxout activation function <ref type="bibr" target="#b3">[4]</ref>, the convolutional filters of the module are represented by the weight matrices W 2k−1 for k ∈ {1, ..., K l } (i.e., filters of size 2k − 1 × 2k − 1 × #f ilters, with #f ilters denoting the number of 2-D filters present in W), which means that each module n in block l has K l different filter sizes and #f ilters different filters, γ and β represent the batch normalization scaling and shifting parameters <ref type="bibr" target="#b4">[5]</ref>, and p 3×3 (x i±1 ) represents a max pooling operator on the 3 × 3 subset of the input data for layer l centred at i ∈ Ω, i.e. x i±1 .</p><p>Using the ConvNet module defined in (2), our proposed models differ mainly in the presence or absence of the node with the max-pooling operator within the module (i.e., the node represented by γ p W 1 p 3×3 (x i±1 ) + β p ). When the module does not contain such node, it is called Competitive Multi-scale Convolution (see <ref type="figure">Fig. 3</ref>-(a)), but when the module has the max-pooling node, then we call it Competitive Inception (see <ref type="figure">Fig. 3</ref>-(b)) because of its similarity to the original inception module <ref type="bibr" target="#b23">[24]</ref>. The original inception module is also implemented for comparison purposes (see <ref type="figure">Fig. 3</ref>-(c)), and we call this model the Inception Style, which is similar to (1) and (2) but with the following differences: 1) the function σ(.) in (2) denotes the concatenation of the input parameters; 2) a 1 × 1 convolution is applied to the input x before a second round of convolutions with filter sizes larger than or equal to 3 × 3; and 3) a ReLU activation function <ref type="bibr" target="#b0">[1]</ref> is present after each convolutional layer.</p><p>An overview of all models with the structural parameters is displayed in <ref type="figure">Fig. 3</ref>. Note that all models are inspired by NIN <ref type="bibr" target="#b11">[12]</ref>, GoogLeNet <ref type="bibr" target="#b23">[24]</ref>, and MIM <ref type="bibr" target="#b10">[11]</ref>. In particular, we replace the original 5 × 5 convolutional layers of MIM by multi-scale filters of sizes 1×1, 3×3, 5×5, and 7×7. For the inception style model, we ensure that the number of output units in each module is the same as for the competitive inception and competitive multi-scale convolution, and we also use a 3 × 3 max-pooling path in each module, as used in the original inception module <ref type="bibr" target="#b23">[24]</ref>. Another important point is that in general, when designing the inception style network, we follow the suggestion by Szegedy et al. <ref type="bibr" target="#b23">[24]</ref> and include a relatively larger number of 3 × 3 and 5 × 5 filters in each module, compared to filters of other sizes (e.g., 1 × 1 and 7 × 7). An important distinction between the original GoogLeNet <ref type="bibr" target="#b23">[24]</ref> and the inception style network in <ref type="figure">Fig. 3</ref>-(c) is the fact that we replace the fully connected layer in the last layer by a single 3 × 3 convolution node in the last module, followed by an average pooling and a softmax unit, similarly to the NIN model <ref type="bibr" target="#b11">[12]</ref>. We propose this mod-ification to limit the number of training parameters (with the removal of the fully connected layer) and to avoid the concatenation of the nodes from different paths (i.e., maxpooling, 1 × 1 convolution filter, and etc.) into a number of channels that is equal to the number of classes (i.e., each channel is averaged into a single node, which is used by a single softmax unit), where the concatenation would imply that some of the paths would be directly linked to a subset of the classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Competitive Multi-scale Convolution Prevent</head><p>Filter Co-adaptation</p><p>The main reason being explored in the field to justify the use of competitive activation units <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23]</ref> is the fact that they build a network formed by multiple underlying sub-networks <ref type="bibr" target="#b21">[22]</ref>. More clearly, given that these activation units consist of piece-wise linear functions, it has been shown that the composition of several layers containing such units, divide the input space in a number of regions that is exponentially proportional to the number of network layers <ref type="bibr" target="#b13">[14]</ref>, where sub-networks will be trained with the samples that fall into one of these regions, and as a result become specialised to the problem in that particular region <ref type="bibr" target="#b21">[22]</ref>, where overfitting can be avoided because these sub-networks must share their parameters with one another <ref type="bibr" target="#b21">[22]</ref>. It is worth noting that these regions can only be formed if the underlying convolutional filters do not coadapt, otherwise all input training samples will fall into only one region of the competitive unit, which degenerates into a simple linear transform, preventing the formation of the sub-networks.</p><p>A straightforward solution to avoid such co-adaptation can be achieved by limiting the number of training samples in a mini-batch during stochastic gradient descent. These small batches allow the generation of "noisy" gradient directions during training that can activate different maxout gates, so that the different linear pieces of the activation unit can be fitted, allowing the formation of an exponentially large number of regions. However, the drawback of this approach lies in the determination of the "right" number of samples per mini-batch. A mini-batch size that is too small leads to poor convergence, and if it is too large, then it may not allow the formation of many sub-networks. Recently, Liao and Carneiro <ref type="bibr" target="#b10">[11]</ref> propose a solution to this problem based on the use of BNU <ref type="bibr" target="#b4">[5]</ref> that distributes the training samples evenly over the regions formed by the competitive unit, allowing the training to use different sets of training points for each region of the competitive unit, resulting in the formation of an exponential number of sub-networks. However, there is still a potential problem with that approach <ref type="bibr" target="#b10">[11]</ref>, which is that the underlying convolutional filters are trained using feature spaces of the same size (i.e., the underlying filters are of fixed size), which can induce the filters to co-adapt and converge to similar regions of the feature space, also preventing the formation of the subnetworks. <ref type="figure">Figure 3</ref>. The proposed competitive multi-scale convolution (a) and competitive inception (b) networks, together with the reference inception style network (c). In these three models, we ensure that the output of each layer has the same number of units. Also note that: the inception style model uses ReLU <ref type="bibr" target="#b14">[15]</ref> after all convolutional layers, the number of filters per convolutional node is represented by the number in brackets, and these models assume a 10-class classification problem.</p><p>The competitive multi-scale convolution module proposed in this paper represents a way to fix the issue introduced above <ref type="bibr" target="#b10">[11]</ref>. Specifically, the different sizes of the convolutional filters within a competitive unit force the feature spaces of the filters to be different from each other, reducing the chances that these filters will converge to similar regions of the feature space. For instance, say you have two filters of sizes 3 × 3 and 5 × 5 being joined by a competitive unit, so this means that the former filter will have a 9-dimensional space, while the latter filter will have 16 additional dimensions for a total of 25 dimensions, where these new dimensions will allow the training process for the 5 × 5 filter to have a significantly larger feature space (i.e., for these two filters to converge to similar values, the additional 16 dimensions will have to be pushed towards zero and the remaining 9 dimensions to converge to the same values as the 3 × 3 filter). In other words, the different filter sizes within a competitive unit imposes a soft constraint that the filters must converge to different values, avoiding the co-adaptation issue. In some sense, this idea is similar to DropConnect <ref type="bibr" target="#b25">[26]</ref>, which, during training, drops to zero the weights of randomly picked network connections with the goal of training regularization. Nevertheless, the underlying filters will have the same size, which promotes coadaptation even with random connections being dropped to zero. Compared with DropConnect that stochastically drops filter connections during training, our approach deterministically drops the border connections of a 7 × 7 filter (e.g., a 5 × 5 filter is a 7 × 7 filter with the 24 border connections dropped to zero, and a 3 × 3 filter is a 7 × 7 filter with the 40 border connections forced to zero -see <ref type="figure">Fig. 5</ref>). We show in the experiments that our approach is more effective than DropConnect at the task of preventing filter co-adaptation within competitive units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We quantitatively measure the performance of our proposed models Competitive Multi-scale Convolution and Competitive Inception on four computer vision/machine learning benchmark datasets: CIFAR-10 <ref type="bibr" target="#b6">[7]</ref>, CIFAR-100 <ref type="bibr" target="#b6">[7]</ref>, MNIST <ref type="bibr" target="#b7">[8]</ref> and SVHN <ref type="bibr" target="#b15">[16]</ref>. We first describe the experimental setup, then using CIFAR-10 and MNIST, we show a quantitative analysis (in terms of classification error, number of model parameters and train/test time) of the two proposed models, the Inception Style model presented in Sec. 3, and two additional versions of the proposed models that justify the use of multi-scale filters, explained in Sec. 3.1. Finally, we compare the performance of the proposed Competitive Multi-scale Convolution and Competitive Inception with respect to the current state of the art in the four benchmark datasets mentioned above.</p><p>The CIFAR-10 <ref type="bibr" target="#b6">[7]</ref> dataset contains 60000 images of 10 commonly seen object categories (e.g., animals, vehicles, etc.), where 50000 images are used for training and the rest 10000 for testing, and all 10 categories have equal volume of training and test images. The images of CIFAR-10 consist of 32 × 32-pixel RGB images, where the objects are well-centered in the middle of the image. The CIFAR-100 <ref type="bibr" target="#b6">[7]</ref> dataset extends CIFAR-10 by increasing the number of categories to 100, whereas the total number of images remains the same, so the CIFAR-100 dataset is considered as a harder classification problem than CIFAR-10 since it contains 10 times less images per class and 10 times more categories. The well-known MNIST <ref type="bibr" target="#b7">[8]</ref> dataset contains 28 × 28 grayscale images comprising 10 handwritten digits (from 0 to 9), where the dataset is divided into 60000 images for training and 10000 for testing, but note that the number of images per digit is not uniformly distributed. Finally, the Street View House Number (SVHN) <ref type="bibr" target="#b15">[16]</ref> is also a digit classification benchmark dataset that contains 600000 32 × 32 RGB images of printed digits (from 0 to 9) cropped from pictures of house number plates. The cropped images is centered in the digit of interest, but nearby digits and other distractors are kept in the image. SVHN has three sets: training, testing sets and a extra set with 530000 images that are less difficult and can be used for helping with the training process. We do not use data augmentation in any of the experiments, and we only compare our results with other methods that do not use data augmentation.</p><p>In all these benchmark datasets we minimize the softmax loss function present in the last layer of each model for the respective classification in each dataset, and we report the results as the proportion of misclassified test images, which is the standard way of comparing algorithms in these benchmark datasets. The reported results are generated with the models trained using an initial learning rate of 0.1 and following a multi-step decay to a final learning rate of 0.001 (in 80 epochs for CIFAR-10 and CIFAR-100, 50 epochs for MNIST, and 40 epochs for SVHN). The stopping criterion is determined by the convergence observed in the error on the validation set. The mini-batch size for CIFAR-10, CIFAR-100, and MNIST datasets is 100, and 128 for SVHN dataset. The momentum and weight decay are set to standard values 0.9 and 0.0005, respectively. For each result reported, we compute the mean and standard deviation of the test error from five separately trained models, where for each model, we use the same training set and parameters (e.g., the learning rate sequence, momentum, etc.), and we change only the random initialization of the filter weights and randomly shuffle the training samples.</p><p>We use the GPU-accelerated ConvNet library MatCon-vNet <ref type="bibr" target="#b24">[25]</ref> to perform the experiments specified in this paper. Our experimental environment is a desktop PC equipped with i7-4770 CPU, 24G memory and a 12G GTX TITAN X graphic card. Using this machine, we report the mean training and testing times of our models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Design Choices</head><p>In this section, we show the results from several experiments that show the design choices for our models, where we provide comparisons in terms of their test errors, the number of parameters involved in the training process and the training and testing times. <ref type="table" target="#tab_0">Tables 1 and 2</ref> show the results on CIFAR-10 and MNIST for the models Competitive Multi-scale Convolution, Competitive Inception, and Inception Style models, in addition to other models explained below. Note that all models in <ref type="table" target="#tab_0">Tables 1 and 2</ref> are constrained to have the same numbers of input channels and output channels in each module, and all networks contain three blocks <ref type="bibr" target="#b11">[12]</ref>, each with three modules (so there is a total of nine modules in each network), as shown in <ref type="figure">Fig. 3</ref>.</p><p>We argue that the multi-scale nature of the filters within the competitive module is important to avoid the coadaptation issue explained in Sec. 3.1. We assess this importance by comparing both the number of parameters and the test error results between the proposed models and the model Competitive Single-scale Convolution, which has basically the same architecture as the Competitive Multiscale Convolution model represented in <ref type="figure">Fig. 3-(a)</ref>, but with the following changes: the first two blocks contain four sets of 7×7 filters in the first module, and in the second and third modules, two sets of 3 × 3 filters; and the third block has three filters of size 5×5 in the first module, followed by two modules with two 3×3 filters. Notice that this configuration implies that we replace the multi-scale filters by the filter of the largest size of the module in each node, which is a configuration similar to the recently proposed MIM model <ref type="bibr" target="#b10">[11]</ref>. The configuration for the Competitive Single-scale Convolution has around two times more parameters than the Competitive Multi-scale Convolution model and takes longer to train, as displayed in <ref type="table" target="#tab_0">Tables 1 and 2</ref> Another important point that we test in this section is the relevance of dropping connections in a deterministic or stochastic manner when training the competitive convolution modules. Recall that the one of the questions posed in Sec. 3.1 is if the deterministic masking provided by our proposed Competitive Multi-scale Convolution module is more effective at avoiding filter co-adaptation than the stochastic masking provided by DropConnect <ref type="bibr" target="#b25">[26]</ref>. We run a quantitative analysis of the Competitive DropConnect Singlescale Convolution, where we take the Competitive Singlescale Convolution proposed before and randomly drop connections using a rate, which is computed such that it has on average the same number of parameters to learn in each round of training as the Competitive Multi-scale Convolution, but notice that the Competitive DropConnect Singlescale Convolution has in fact the same number of parameters as the Competitive Single-scale Convolution. Using <ref type="figure">Fig. 5</ref>, we see that the DropConnect rate is 0.57 for the    <ref type="figure">Fig. 3</ref>. The results in <ref type="table" target="#tab_0">Tables 1 and 2</ref> show that it has around two times more parameters, takes longer to train and performs significantly worse than the Competitive Multi-scale Convolution model.</p><p>Finally, the reported training and testing times in Tables 1 and 2 show a clear relation between the number of model parameters and those times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with the State of the Art</head><p>We now show the performances of the proposed Competitive Multi-scale and Competitive Inception Convolution models on CIFAR-10, CIFAR-100, MNIST and SVHN, and compare them with the current state of the art in the field, <ref type="figure">Figure 5</ref>. The Competitive Multi-scale Convolution module has filters of size 1 × 1, 3 × 3, 5 × 5, and 7 × 7, which is equivalent to having four 7 × 7 filters (with a total of 196 weights) with the masks in (a), where the number of deterministically masked out (or dropped) weights is 112. Using a DropConnect rate of 112/196 ≈ 0.57, a possible set of randomly dropped weights is shown in (b). Note that even though the proportion and number of weights dropped in (a) and (b) are the same, the deterministic or stochastic masking of the weights make a difference in the performance, as explained in the paper.</p><p>which can be listed as follows. Stochastic Pooling <ref type="bibr" target="#b27">[28]</ref> proposes a regularization based on a replacement of the deterministic pooling (e.g., max or average pooling) by a stochastic procedure, which randomly selects the activation within each pooling region according to a multinomial distribution, estimated from the activation of the pooling unit. Maxout Networks <ref type="bibr" target="#b3">[4]</ref> introduces a piece-wise linear activation unit that is used together with dropout training <ref type="bibr" target="#b19">[20]</ref> and is introduced in <ref type="figure" target="#fig_1">Fig. 2-(c)</ref>. The Network in Network (NIN) <ref type="bibr" target="#b11">[12]</ref> model consists of the introduction of multilayer perceptrons as activation functions to be placed between convolution layers, and the replacement of a final fully connected layer by average pooling, where the number of output channels represent the final number of classes in the classification problem. Deeply-supervised nets <ref type="bibr" target="#b8">[9]</ref> introduce explicit training objectives to all hidden layers, in addition to the back-propagated errors from the last softmax layer. The use of a recurrent structure that replaces the purely feed-forward structure in ConvNets is explored by the model RCNN <ref type="bibr" target="#b9">[10]</ref>. An extension of the NIN model based on the use of maxout activation function instead of the multilayer perceptron is introduced in the MIM model <ref type="bibr" target="#b10">[11]</ref>, which also shows that the use of batch normalization units are crucial for allowing an effective training of several single-scale filters that are joined by maxout units. Finally, the Tree based Priors <ref type="bibr" target="#b20">[21]</ref> model proposes a training method for classes with few samples, using a generative prior that is learned from the data and shared between related classes during the model learning.</p><p>The comparison on CIFAR-10 <ref type="bibr" target="#b6">[7]</ref> dataset is shown in Tab. 3, where results are sorted based on the performance of each method, and the results of our proposed methods are highlighted. The results on CIFAR-100 <ref type="bibr" target="#b6">[7]</ref> dataset are displayed in Tab.4. <ref type="table">Table 5</ref> shows the results on MNIST <ref type="bibr" target="#b7">[8]</ref>, where it is worth reporting that the best result (over the five trained models) produced by our Competitive Multi-scale Convolution model is a test error of 0.29%, which is bet-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Error (mean ± standard deviation) Competitive Multi-scale Convolution 6.87 ± 0.05% Competitive Inception 7.13 ± 0.31% MIM <ref type="bibr" target="#b10">[11]</ref> 8.52 ± 0.20% RCNN-160 <ref type="bibr" target="#b9">[10]</ref> 8.69% Deeply-supervised nets <ref type="bibr" target="#b8">[9]</ref> 9.69% Network in Network <ref type="bibr" target="#b11">[12]</ref> 10.41% Maxout Networks <ref type="bibr" target="#b3">[4]</ref> 11.68% Stochastic Pooling <ref type="bibr" target="#b27">[28]</ref> 15.13% <ref type="table">Table 3</ref>. Comparison in terms of classification error between our proposed models (highlighted) and the state-of-the-art methods on CIFAR-10 <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Error (mean ± standard deviation) Competitive Multi-scale Convolution 27.56 ± 0.49% Competitive Inception</p><p>28.17 ± 0.25% MIM <ref type="bibr" target="#b10">[11]</ref> 29.20 ± 0.20% RCNN-160 <ref type="bibr" target="#b9">[10]</ref> 31.75% Deeply-supervised nets <ref type="bibr" target="#b8">[9]</ref> 34.57% Network in Network <ref type="bibr" target="#b11">[12]</ref> 35.68% Tree based Priors <ref type="bibr" target="#b20">[21]</ref> 36.85% Maxout Networks <ref type="bibr" target="#b3">[4]</ref> 38.57% Stochastic Pooling <ref type="bibr" target="#b27">[28]</ref> 42.51% <ref type="table">Table 4</ref>. Comparison in terms of classification error between our proposed models (highlighted) and the state-of-the-art methods on CIFAR-100 <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Error (mean ± standard deviation) RCNN-96 <ref type="bibr" target="#b9">[10]</ref> 0.31% Competitive Multi-scale Convolution 0.33 ± 0.04% MIM <ref type="bibr" target="#b10">[11]</ref> 0.35 ± 0.03% Deeply-supervised nets <ref type="bibr" target="#b8">[9]</ref> 0.39% Competitive Inception 0.40 ± 0.02% Network in Network <ref type="bibr" target="#b11">[12]</ref> 0.45% Conv. Maxout+Dropout <ref type="bibr" target="#b3">[4]</ref> 0.47% Stochastic Pooling <ref type="bibr" target="#b27">[28]</ref> 0.47% <ref type="table">Table 5</ref>. Comparison in terms of classification error between our proposed models (highlighted) and the state-of-the-art methods on MNIST <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Test Error (mean ± standard deviation) Competitive Multi-scale Convolution 1.76 ± 0.07% RCNN-192 <ref type="bibr" target="#b9">[10]</ref> 1.77% Competitive Inception Convolution 1.82 ± 0.05% Deeply-supervised nets <ref type="bibr" target="#b8">[9]</ref> 1.92% Drop-connect <ref type="bibr" target="#b25">[26]</ref> 1.94% MIM <ref type="bibr" target="#b10">[11]</ref> 1.97 ± 0.08% Network in Network <ref type="bibr" target="#b11">[12]</ref> 2.35% Conv. Maxout+Dropout <ref type="bibr" target="#b3">[4]</ref> 2.47% Stochastic Pooling <ref type="bibr" target="#b27">[28]</ref> 2.80% <ref type="table">Table 6</ref>. Comparison in terms of classification error between our proposed models (highlighted) and the state-of-the-art methods on SVHN <ref type="bibr" target="#b15">[16]</ref>.</p><p>ter than the single result from Liang and Hu <ref type="bibr" target="#b9">[10]</ref>. Finally, the comparison on SVHN <ref type="bibr" target="#b15">[16]</ref> dataset is shown in <ref type="table">Table 6</ref>, where two out of the five models show test error results of 1.69%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion and Conclusions</head><p>In terms of the model design choices in Sec. 4.1, we can see that the proposed Competitive Multi-scale Convolution produces more accurate classification results than the proposed Competitive Inception. Given that the main differ-ence between these two models is the presence of the maxpooling path within each module, we can conclude that this path does not help with the classification accuracy of the model. The better performance of both models with respect to the Inception Style model can be attributed to the maxout unit that induces competition among the underlying filters, which helps more the classification results when compared with the collaborative nature of the Inception module. Considering model complexity, it is important to notice that the relation between the number of parameters and training and testing times is not linear, where even though the Inception Style model has 10× fewer parameters, it trains and tests 2 to 1.5× faster than the proposed Competitive Multi-scale Convolution and Competitive Inception models.</p><p>When answering the questions posed in Sec. 3.1, we assume that classification accuracy is a proxy for measuring the co-adaptation between filters within a single module, where the intuition is that if the filters joined by a maxout activation unit co-adapt and become similar to each other, a relatively small number of large regions in the input space will be formed, which results in few sub-networks to train, with each sub-network becoming less specialized to its region <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22]</ref>. We argue that the main consequence of that is a potential lower classification accuracy, depending on the complexity of the original classification problem. Using this assumption, we note from Tables 1 and 2 that the use of multi-scale filters within a competitive module is in fact important to avoid the co-adaptation of the filters, as shown by the more accurate classification results of the Multi-scale, compared to the Single-scale model. Furthermore, the use of deterministic, as opposed to stochastic, mapping also appears to be more effective in avoiding filter co-adaptation given the more accurate classification results of the former mapping. Nevertheless, the reason behind the worse performance of the stochastic mapping may be due to the fact that DropConnect has been designed for the fully connected layers only <ref type="bibr" target="#b25">[26]</ref>, while our test bed for the comparison is set in the convolutional filters. To be more specific, we think that a fully connected layer usually encapsulates hundreds to thousands of weights for inputs of similar scale of dimensions, thus a random dropping on a subset of weight elements can hardly change the distribution of the outputs pattern. However, the convolution filters are of small dimensions, and each of our maxout unit controls 4 to 5 filters at most, so such masking scheme over small weights matrix could result in "catastrophic forgetting" <ref type="bibr" target="#b12">[13]</ref> which explains why the Competitive DropConnect Single-scale Convolution performs even worse than Competitive Singlescale Convolution on CIFAR-10.</p><p>We also run an experiment that assesses whether filters of larger size within a competitive module can improve the classification accuracy at the expense of having a larger number of parameters to train. We test the inclusion of two more filters of sizes 9 × 9 and 11 × 11 in module 1 of blocks 1 and 2, and two more filter sizes 7 × 7 and 9 × 9 in module 1 of block 3 (see <ref type="figure">Fig. 3</ref>). The classification re-sult obtained is 7.36 ± 0.16% on CIFAR-10, and number of model parameters is 13.11 M. This experiment shows that increasing the number of filters of larger sizes do not necessarily help improve the classification results. An important modification that can be suggested for our proposed Competitive Multi-scale Convolution model is the replacement of the maxout by ReLU activation, where only the largest size filter of each module is kept and all other filters are removed. One can argue that such model is perhaps less complex (in terms of the number of parameters) and probably as accurate as the proposed model. However, the results we obtained with such model on CIFAR-10 show that this model has 3.28 M parameters (i.e., just slightly less complex than the proposed models, as shown in Tab. 1) and has a classification test error of 8.16 ± 0.15%, which is significantly larger than for our proposed models. On MNIST, this model has 0.81 M parameters and produces a classification error of 0.37 ± 0.05%, which also shows no advantage over the proposed models.</p><p>The comparisons with the state of the art in <ref type="table">Tables 3-6</ref> of Sec. 4.2 show that the proposed Competitive Multiscale Convolution model produces the best results in the field for three out of the four considered datasets. However, note that this comparison is not strictly fair to us because we run a five-model validation experiment (using different model initializations and different sets of mini batches for the stochastic gradient descent), which provides a more robust performance assessment of our method. In contrast, most of the methods in the field only show one single result of their performance. If we consider only the best result out of the five results in the experiment, then our Competitive Multi-scale Convolution model has the best results in all four datasets (with, for example, 0.29% on MNIST and 1.69% on SVHN). An analysis of these results also allows us to conclude that the main competitors of our approach are the MIM <ref type="bibr" target="#b10">[11]</ref> and RCNN <ref type="bibr" target="#b9">[10]</ref> models, where the MIM method is quite related to our approach, but the RCNN method follows a quite different strategy.</p><p>In this paper, we show the effectiveness of using competitive units on modules that contain multi-scale filters. We argue that the main reason of the superior classification results of our proposal, compared with the current state of the art in several benchmark datasets, lies in the following points: 1) the deterministic masking implicitly used by the multi-scale filters avoids the issue of filter co-adaptation; 2) the competitive unit that joins the underlying filters and the batch normalization units promote the formation of a large number of sub-networks that are specialized in the classification problem restricted to a small area of the input space and that are regularized by the fact that they are trained together within the same model; and 3) the maxout unit allows the reduction of the number of parameters in the model. It is important to note that such modules can be applied in several types of deep learning networks, and we plan to apply it to other types of models, such as the recurrent neural network <ref type="bibr" target="#b9">[10]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The proposed deep ConvNet modules are depicted in (a) and (b), where (a) only contains multi-scale convolutional filters within each module, while (b) contains the max-pooling path, which resembles the original inception module depicted in (c) for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Competitive activation units, where the gray nodes are the active ones, from which errors flow during backpropagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. The idea behind the use of the largest size filters within each module is based on the results obtained from the training of the batch normalisation units of the Competitive Multi-scale Convolution modules, which indicates that the highest weights (represented by γ in(2)) are placed in the largest size filters within each module, as shown inFig. 4. The classification results of the Competitive Single-scale Convolution, shown in Tables 1 and 2, demonstrate that it is consistently inferior to the Competitive Multi-scale Convolution model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Mean and standard deviation of the learned γ values in the batch normalisation unit of (2) for the Competitive Multi-scale Convolution model on CIFAR-10. This result provides an estimate of the importance placed on each filter by the training procedure. module 1 of blocks 1 and 2 specified in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Results on CIFAR-10 of the proposed models, in addition to the Competitive Single-scale Convolution and Competitive DropConnect Single-scale Convolution that test our research questions posed in Sec. 3.1.</figDesc><table><row><cell>Method</cell><cell>No. of Params</cell><cell cols="3">Test Error Train Time Test Time</cell></row><row><cell></cell><cell></cell><cell>(mean ± std dev)</cell><cell>(h)</cell><cell>(ms)</cell></row><row><cell>Competitive Multi-scale</cell><cell>4.48 M</cell><cell>6.87 ± 0.05%</cell><cell>6.4 h</cell><cell>2.7 ms</cell></row><row><cell>Convolution</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Competitive Inception</cell><cell>4.69 M</cell><cell>7.13 ± 0.31%</cell><cell>7.6 h</cell><cell>3.1 ms</cell></row><row><cell>Inception Style</cell><cell>0.61 M</cell><cell>8.50 ± 0.06%</cell><cell>3.9 h</cell><cell>1.5 ms</cell></row><row><cell>Competitive Single-scale</cell><cell>9.35 M</cell><cell>7.15 ± 0.12%</cell><cell>8.0 h</cell><cell>3.2 ms</cell></row><row><cell>Convolution</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Competitive DropConnect</cell><cell>9.35 M</cell><cell>9.12 ± 0.17%</cell><cell>7.7 h</cell><cell>3.1 ms</cell></row><row><cell>Single-scale Convolution</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>No. of Params</cell><cell cols="3">Test Error Train Time Test Time</cell></row><row><cell></cell><cell></cell><cell>(mean ± std dev)</cell><cell>(h)</cell><cell>(ms)</cell></row><row><cell>Competitive Multi-scale</cell><cell>1.13 M</cell><cell>0.33 ± 0.04%</cell><cell>1.5 h</cell><cell>0.8 ms</cell></row><row><cell>Convolution</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Competitive Inception</cell><cell>1.19 M</cell><cell>0.40 ± 0.02%</cell><cell>1.9 h</cell><cell>1.0 ms</cell></row><row><cell>Inception Style</cell><cell>0.18 M</cell><cell>0.44 ± 0.01%</cell><cell>1.4 h</cell><cell>0.7 ms</cell></row><row><cell>Competitive Single-scale</cell><cell>2.39 M</cell><cell>0.37 ± 0.03%</cell><cell>1.7 h</cell><cell>0.9 ms</cell></row><row><cell>Convolution</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Competitive DropConnect</cell><cell>2.39 M</cell><cell>0.35 ± 0.03%</cell><cell>1.6 h</cell><cell>0.9 ms</cell></row><row><cell>Single-scale Convolution</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Results on MNIST of the proposed models, in addition to the Competitive Single-scale Convolution and Competitive Drop-Connect Single-scale Convolution that test our research questions posed in Sec. 3.1.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">* This research was supported by the Australian Research Council Centre of Excellence for Robotic Vision (project number CE140100016) (a) Competitive multi-scale convolution module (b) Competitive Inception module (c) Original inception module<ref type="bibr" target="#b23">[24]</ref> </note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-scale orderless pooling of deep convolutional activation features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2014</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="392" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-digit number recognition from street view imagery using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bulatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 30th International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hinton. Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="87" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AISTATS</title>
		<meeting>AISTATS</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Recurrent convolutional neural network for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3367" to="3375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">On the importance of normalisation layers in deep learning with piecewise linear activation units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<idno>abs/1508.00330</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Network in network. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Catastrophic interference in connectionist networks: The sequential learning problem. The psychology of learning and motivation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page">92</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On the number of linear regions of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2924" to="2932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML)</title>
		<meeting>the 27th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<meeting><address><addrLine>Granada, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">2011</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Robust object recognition with cortex-like mechanisms. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bileschi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riesenhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="426" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Very deep convolutional networks for large-scale image recognition. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discriminative transfer learning with tree-based priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2094" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Understanding locally competitive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Compete to compute</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazerounian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Matconvnet -convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning (ICML)</title>
		<meeting>the 30th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<title level="m">Stochastic pooling for regularization of deep convolutional neural networks. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
