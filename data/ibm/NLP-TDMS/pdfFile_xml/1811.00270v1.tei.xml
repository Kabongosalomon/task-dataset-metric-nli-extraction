<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUBMISSION FOR IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 2018 1 Hierarchical Long Short-Term Concurrent Memory for Human Interaction Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangbo</forename><surname>Shu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">SUBMISSION FOR IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, 2018 1 Hierarchical Long Short-Term Concurrent Memory for Human Interaction Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T20:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Human interaction recognition</term>
					<term>long short-term mem- ory</term>
					<term>activity recognition</term>
					<term>deep learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we aim to address the problem of human interaction recognition in videos by exploring the long-term inter-related dynamics among multiple persons. Recently, Long Short-Term Memory (LSTM) has become a popular choice to model individual dynamic for single-person action recognition due to its ability of capturing the temporal motion information in a range. However, existing RNN models focus only on capturing the dynamics of human interaction by simply combining all dynamics of individuals or modeling them as a whole. Such models neglect the inter-related dynamics of how human interactions change over time. To this end, we propose a novel Hierarchical Long Short-Term Concurrent Memory (H-LSTCM) to model the long-term inter-related dynamics among a group of persons for recognizing the human interactions. Specifically, we first feed each person's static features into a Single-Person LSTM to learn the single-person dynamic. Subsequently, the outputs of all Single-Person LSTM units are fed into a novel Concurrent LSTM (Co-LSTM) unit, which mainly consists of multiple sub-memory units, a new cell gate and a new co-memory cell. In a Co-LSTM unit, each sub-memory unit stores individual motion information, while this Co-LSTM unit selectively integrates and stores inter-related motion information between multiple interacting persons from multiple sub-memory units via the cell gate and co-memory cell, respectively. Extensive experiments on four public datasets validate the effectiveness of the proposed H-LSTCM by comparing against baseline and state-of-the-art methods.</p><p>Index Terms-Human interaction recognition, long short-term memory, activity recognition, deep learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>H UMAN interactions (e.g., handshaking, and talking) are typical human activities that occur in public places and are attracting substantial attentions from researchers <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b3">[4]</ref>. A human interaction usually involves at least two individual motions from multiple persons, who are concurrently inter-related with each other (e.g., some persons are talking together, some persons are handshaking with each other). In most cases of human interaction, the concurrent interrelated motions between multiple persons are strongly interacting (e.g., person A kicks person B, while person B retreats back). It has been shown that the concurrent inter-related motions among multiple persons rather than single-person motions can contribute discriminative information for recognizing human interactions <ref type="bibr" target="#b4">[5]</ref>.</p><p>Two main types of solutions exist for the problem of human interaction recognition. One solution (e.g., <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>) is to extract individual motion descriptors from each interacting person and then predict the class label of an interaction by inferring the coherence between two individual motions. However, this solution, i.e., regarding human interactions as multiple single-person actions, ignores some inter-related motion information and brings in some irrelevant individual motion information. The other solution is to extract motion descriptors on interacting regions and then train an interaction recognition model <ref type="bibr" target="#b4">[5]</ref>. However, interacting regions are difficult to locate before the close interaction occurs.</p><p>Recently, due to the powerful ability to capture sequential motion information, Long Short-Term Memory (LSTM) <ref type="bibr" target="#b7">[8]</ref>, has proven to be successful at various human action recognition tasks <ref type="bibr" target="#b8">[9]</ref>- <ref type="bibr" target="#b12">[13]</ref>. Therefore, we aim to explore the long-term inter-related dynamics among a group of interacting persons by leveraging LSTM. However, existing LSTMs model human dynamics independently, and do not consider the concurrent inter-relation of dynamics among multiple persons. A straightforward way to overcome this limitation is to either 1) merge individual actions at the preprocessing stage <ref type="bibr" target="#b13">[14]</ref> (e.g., consider interacting persons as a whole); or 2) utilize several LSTMs to model the single-person dynamics of individuals and then fuse the output sequences of these LSTMs <ref type="bibr" target="#b12">[13]</ref>. However, both methods neglect the inter-related dynamics of how interactions among these persons change over time.</p><p>To this end, we propose a novel Hierarchical Long Short-Term Concurrent Memory (H-LSTCM) for human interaction (activity) recognition to model the long-term inter-related dynamics among a group of interacting persons, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. For each person, we first feed her/his static features (e.g., CNN features) into a Single-Person LSTM to learn the single-person dynamic, which describes a person's long-term motion information in a whole video clip. Then, all outputs of Single-Person LSTM units are fed into a novel Concurrent LSTM (Co-LSTM) unit, which mainly consists of multiple sub-memory units, multiple new cell gates and a new co-memory cell. In a Co-LSTM unit, multiple sub-memory units store single-person motion information from the Single-Person LSTM units. Following these sub-memory units, the cell gates allow the inter-related motion memory in sub-memory units to enter a new comemory cell, and the co-memory cell selectively integrates and stores the inter-related memory to reveal the concurrent inter-related motion information among all interacting persons. Overall, all interacting persons in each frame are jointly modeled by a Co-LSTM unit on the person bonding boxes. At the last time step, the output of Co-arXiv:1811.00270v1 [cs.CV] 1 Nov 2018 LSTM is a dynamic inter-related representation of the group activity. Extensive experiments on various datasets are conducted to evaluate the performance of H-LSTCM compared with the state-of-the-arts.</p><p>The main contributions of this work are summarized as follows:</p><p>• We propose a novel Hierarchical Concurrent Long Short-Term Concurrent Memory (H-LSTCM) to effectively address the problem of human interaction recognition with multiple persons, by learning the dynamic inter-related representations among all persons in the group crowd scenes. • We design a novel Concurrent LSTM (Co-LSTM) to aggregate the inter-related memory from individuals in collective activity scenes, by capturing the concurrently long-term inter-related dynamics among multiple persons rather than dynamics of individuals. Our preliminary Co-LSTSM method in <ref type="bibr" target="#b14">[15]</ref> with two sub-memory units can recognize only the interactions between two persons, while the proposed H-LSTCM in this paper can recognize various group activities at a larger scale, including collective activity with multiple persons (≥ 3 persons), and group activity with multiple sub-group activities. This is because Co-LSTSM learns the dynamic inter-related representation between two persons simply from the static singleperson features. Actually there is a large gap between the static singleperson features and the dynamic inter-related representation, which limits the performance of the Co-LSTSM. Thus, in H-LSTCM we bring in the single-person dynamic, which is a basic element in the group activity to describe a person's long-term motion information in a whole video clip, and reflects motion patterns caused by interactions with other persons. H-LSTCM learns dynamic inter-related representation among multiple persons in a hierarchical way, from the static to dynamic features at the single-person level first, and further to an inter-related level of group activities. Specifically, the singleperson LSTMs in H-LSTCM first learn single-person dynamics from the static single-person features. And then, an extended Co-LSTM with multiple sub-memory units in H-LSTCM learns concurrently inter-related representation among all persons based on the singleperson dynamics. Such a hierarchical strategy ensures that H-LSTCM learns more discriminative representation than Co-LSTSM for group activities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Human interaction recognition (activity recognition) aims to automatically understand the interaction performed by at least two persons <ref type="bibr" target="#b1">[2]</ref>. In the task of two persons' interaction recognition, earlier researchers have noted that several interactive attributes provide discriminative information to represent person-person interactions. For example, Kong et al. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref> regarded multiple interactive phrases as the latent mid-level feature to recognize person-person interactions from human individual actions. Consider that there exists temporal context information in a video clip, Zhang et al. <ref type="bibr" target="#b6">[7]</ref> and Liu et al. <ref type="bibr" target="#b15">[16]</ref> used a new set of spatio-temporal action attribute phrases to describe the person-person interactions in a video. However, the difference in some person-person interactions (e.g., boxing and patting) is too small to be identified via only interactive phrases. Moreover, some person-person interactions are complex, and cannot be described well by a specified number of interactive phrases.</p><p>Benefiting from the success of deep learning, some deep learning methods have been proposed to understand two persons' interaction for the last five years <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b16">[17]</ref>. For example, Wang et al. <ref type="bibr" target="#b16">[17]</ref> adopted deep context features instead of the traditional context features (e.g., <ref type="bibr" target="#b17">[18]</ref>) on the event neighborhood to recognize personperson interactions, where the size of the event neighborhood must be manually defined at the preprocessing step. One limitation of the above methods is that locating the interactive region is a challenging task before the close interaction occurs. Therefore, this work aims to design a human interaction recognition without locating the interactive regions accurately.</p><p>In a scene of multiple persons' interaction (i.e., group activity), several persons interact with each others, which makes activity recognition a complex task. Two solutions are commonly used to address the problem of group-person interaction recognition. One solution is to exploit spatial distribution of human activities and to present spatio-temporal descriptors to capture the spatial distribution of persons <ref type="bibr" target="#b18">[19]</ref>- <ref type="bibr" target="#b20">[21]</ref>. The other solution is to track all the body parts in the video, and then learn holistic representations to estimate the class of the collective activity <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>. However, the former solution requires inference of the complex spatio-relation between persons, and the latter brings in some individual action of outlier persons.</p><p>Recently, Long Short Term Memory (LSTM) has been proposed to address the problem of human interaction recognition by learning high-level dynamic representations of persons <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. This insight motivates us to employ superior LSTM models to learn high-level dynamic representations of human activity. Therefore, we propose a new Hierarchical Long Short-Term Concurrent Memory Closely related work includes Hierarchical Deep Temporal Model (HDTM) <ref type="bibr" target="#b12">[13]</ref>, Deep Structured Model (DSM) <ref type="bibr" target="#b23">[24]</ref>, and Structure Inference Machines (SIM) <ref type="bibr" target="#b24">[25]</ref>. Specifically, HDTM <ref type="bibr" target="#b12">[13]</ref> first models the individual dynamic motions by several LSTMs. Subsequently, the outputs of these LSTMs are pooled into a single vector, which is the input of a following LSTM. HDTM pools single-person dynamics into an overall dynamic representation, and dose not consider the inter-relations among persons in the group activity. DSM <ref type="bibr" target="#b23">[24]</ref> and SIM <ref type="bibr" target="#b24">[25]</ref> utilize CNN to obtain the initialized class labels of singleperson actions and group-level activity and refine the group activity class label by exploring the relations among the actions of all individuals in an iterative manner. If one person's action is closely related to the group activity and the other persons' actions, this person intensively participates in the group activity; otherwise, this person is an outlier. Since DSM and SIM target "key" persons who play crucial roles in the group activity rather than those of all persons, some sudden motion information of "outlier" persons may be lost. Compared to HDTM <ref type="bibr" target="#b12">[13]</ref>, the proposed H-LSTCM considers the inter-relations among persons via the cell gates and co-memory cell. And compared to DSM <ref type="bibr" target="#b23">[24]</ref> and SIM <ref type="bibr" target="#b24">[25]</ref>, the proposed H-LSTCM models the concurrently inter-related dynamics among all persons, which dose lose outlier yet useful persons.</p><p>Recently, some works <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref> proposed to learn the concurrently location-related representation among multiple persons for multitarget tracking. They assume that two persons who have close position are inter-related with each other. By contrast, the proposed H-LSTCM learns sematic-related representation among multiple persons by leveraging the inter-relation between the single-person dynamic at the current time step and the dynamic of the whole activity at the previous time step. Here, it is assumed that one person, whose current representation is closely related to the hidden representation of the whole activity, is likely to be more involved in this activity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARIES: LSTM-BASED ACTION RECOGNITION</head><p>Given an input video clip {xt ∈ R n |t = 1, · · · , T } with length T , where xt is the static feature at time step t. A traditional Recurrent Neural Networks (RNN) <ref type="bibr" target="#b27">[28]</ref> models the dynamics of this video clip through a sequence of hidden states. Due to the exponential decay in retaining the context information of video frames <ref type="bibr" target="#b7">[8]</ref>, RNN does not model the long-term dynamics of video sequences well. To this end, Long Short-Term Memory (LSTM) <ref type="bibr" target="#b7">[8]</ref>, a variant of RNN, provides a solution by incorporating memory units that enable the network to learn when to forget previous hidden states and when to update hidden states given new information <ref type="bibr" target="#b8">[9]</ref>.</p><p>A traditional LSTM unit <ref type="bibr" target="#b7">[8]</ref> at time step t contains an input gate it, a forget gate ft, an output gate ot) and a memory cell ct, which are expressed as follows,</p><formula xml:id="formula_0">it = σ(Wix · xt + W ih · ht−1 + bi); (1) ft = σ(W f x · xt + W f h · ht−1 + b f ); (2) ot = σ(W ox · xt + W oh · ht−1 + bo),<label>(3)</label></formula><formula xml:id="formula_1">gt = ϕ(Wgx · xt + W gh · ht−1 + bg),<label>(4)</label></formula><formula xml:id="formula_2">ct = f s t ct−1 + it gt,<label>(5)</label></formula><p>where σ(·) is a sigmoid function, denotes element-wise product, ϕ(·) is a hyperbolic tangent tanh(·), W * x and W * h are weight matrices, and b * is bias vector. Subsequently, a hidden state ht at time step t can be expressed as</p><formula xml:id="formula_3">ht = ot ϕ(ct),<label>(6)</label></formula><p>which denotes the dynamic representation of the t-th frame. All hidden states {ht|t = 1, 2, · · · , T } describe the dynamic of the video clip. Finally, the output zt ∈ R k at time step t is computed as</p><formula xml:id="formula_4">zt = ϕ(W zh · ht + bz),<label>(7)</label></formula><p>which can be transformed to a probability y t,l (l = 1, · · · , k) corresponding to the l-th class of the activity by a softmax function</p><formula xml:id="formula_5">y t,l = exp(z t,l ) k j=1 exp(zt,j) ,<label>(8)</label></formula><p>where zt,j in zt denotes the encoding of the confidence score on the j-th activity class. Generally, we set yt = [yt,1, yt,2, · · · , y t,k ] T as the predicted class label vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. HIERARCHICAL LONG SHORT-TERM CONCURRENT MEMORY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The Architecture</head><p>For human interaction recognition, each video frame contains at least two concurrent singe-person actions among multiple persons, which are inter-related in a group activity. Existing LSTM models targeting single-person actions cannot handle multiple-person interactions well. As mentioned previously, we can roughly treat all the interacting persons as a whole before training the LSTM network. However, this solution results in some individual-specific motion information. Additionally, we can model the single-person dynamics of individuals by multiple LSTM networks, and then combine (e.g., concatenate or pool) the single-person dynamics obtained by all these LSTM networks into the final representation. Since this strategy assumes that all persons in a group activity are independent of each other, some of the inter-related motion information among these persons is lost.</p><p>Recently, Deng et al. <ref type="bibr" target="#b24">[25]</ref> proposed a new Structure Inference Machines (SIM) for group activity recognition, which indicated that some persons are related to the group activity, while others are outliers. Specifically, SIM first utilizes CNN to initialize the class labels of individuals' actions and group activity. Then, the group activity class label is refined by considering the relations among the actions of all individuals in an iterative manner. Motivated by this, for a group activity with multiple persons, we also consider designing a model to capture the inter-relation among multiple persons  However, SIM targets "key" persons who play crucial roles in the group activity, some sudden motion information of "outlier" persons may be lost. Empirically, we observe that outlier persons are not irrelevant to the group activity at any time step. For example, a person suddenly spike ball at one moment in a volleyball game. Therefore, we propose a Hierarchical Long Short-Term Concurrent Memory (H-LSTCM) to capture the concurrently inter-related dynamics among all the persons rather than those of a selection of persons. Specifically, the proposed H-LSTCM first models the temporal motion information for each person via multiple Single-Person LSTMs corresponding to these persons, and then captures the inter-related dynamics among all the persons by a novel Concurrent LSTM (Co-LSTM). <ref type="figure" target="#fig_0">Figure 1</ref> shows the whole framework of the proposed H-LSTCM. The key point of H-LSTCM is to utilize multiple sub-memory units in a Concurrent LSTM (Co-LSTM) unit to selectively integrate and store the concurrently inter-related temporal information among multiple persons from the individual temporal information. <ref type="figure" target="#fig_3">Figure 2</ref> illustrates the architecture of a Co-LSTM unit of the proposed H-LSTCM at a time step. The Co-LSTM unit mainly consists of multiple specific sub-memory units (the number of units corresponds to the number of interacting persons), multiple cell gates, a common output gate and a new co-memory cell. Specifically, all sub-memory units include their respective input gates, forget gates, and memory cells. Following these sub-memory units, the cell gates allow the inter-related motion memory in the sub-memory units to enter a new co-memory cell, and the co-memory cell selectively integrates and memorizes the inter-related motion information among all the interacting persons. Overall, the stacked Co-LSTM units are recurrent in a time sequence to capture the concurrently inter-related dynamics among all interacting persons over time.</p><p>Formally, {x 1 t ∈ R n |t = 1, · · · , T }, {x 2 t ∈ R n |t = 1, · · · , T }, · · · and {x p t ∈ R n |t = 1, · · · , T } denote the sets of static features (e.g., CNN features) on the tracklets (obtained by object detector and object tracker) of p interacting persons in a video clip vn (wherein n = 1, 2, · · · , N ). For a feature set of {x s t |t = 1, · · · , T } of the s-th person, we can obtain her/his hidden state (i.e., single-person dynamic) {h s t |t = 1, · · · , T } at each time step via a Single-Person LSTM. In the s-th sub-memory unit of Co-LSTM on the top of the Single-Person LSTMs, i s t , f s t , g s t , and c s t (s = 1, 2, · · · , p) denote input gate, forget gate, input modulation gate and sub-memory cell at time step t, respectively. These components can be expressed by the following equations</p><formula xml:id="formula_6">i s t = σ(W s ix · h s t + W s ih · ht−1 + b s i ), s = 1, 2, · · · , p; (9) f s t = σ(W s f x · h s t + W s f h · ht−1 + b s f ), s = 1, 2, · · · , p;<label>(10)</label></formula><formula xml:id="formula_7">g s t = ϕ(W s gx · h s t + W s gh · ht−1 + b s g ), s = 1, 2, · · · , p;<label>(11)</label></formula><formula xml:id="formula_8">c s t = f s t c s t−1 + i s t g s t , s = 1, 2, · · · , p,<label>(12)</label></formula><p>where W s * x and W s * h are weight matrices, b * is bias vector, and the hidden state ht−1 denotes the dynamic inter-related representation of the whole activity at time step t − 1. All hidden states {ht|t = 1, 2, · · · , T } describe the inter-related dynamic of the activity scene in the video clip.</p><p>Following the s-th sub-memory unit, a new cell gate π s t aims to control the memory that enters and leaves this sub-memory unit at time step t. Like traditional gates, the cell gate π s t is activated by a nonlinear function of the input h s t and the past hidden state ht−1,</p><formula xml:id="formula_9">π s t = σ(W s πh · h s t + W πh · ht−1 + b π ), s ∈ {1, 2, · · · , p},<label>(13)</label></formula><p>where W s πh and W πh are the weight matrices, and bπ is the bias vector. Based on the consistent interactions among multiple interacting persons, all cell gates π s t (s = 1, 2, · · · , p) allow more concurrently inter-related motion information among interacting persons to enter a new co-memory cell ct, which contributes to a common hidden state ht at time step t. In this work, the co-memory cell ct can be expressed as</p><formula xml:id="formula_10">ct = p s=1 π s t c s t .<label>(14)</label></formula><p>This co-memory cell ct corresponds to an output gate ot that is related to all the inputs and the common hidden state at the previous time step, i.e.,</p><formula xml:id="formula_11">ot = σ( p s=1 W s ox h s t + W oh · ht−1 + b o ).<label>(15)</label></formula><p>Finally, the hidden state ht at time step t can be expressed as ht = ot ϕ(ct).</p><p>If we obtain ht, we can compute the probability vector yt of one human interaction by Eq <ref type="bibr" target="#b6">(7)</ref> and Eq (8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Algorithm</head><p>We employ a loss function to learn the model parameters of H-LSTCM by measuring the deviation between the ground-truth class label vectorŷt = [ŷt,1,ŷt,2, · · · ,ŷ t,k ] T and the predicted probability vector yt = [y t,l , yt,2, · · · , y t,k ] T corresponding to ht at time step t,</p><formula xml:id="formula_13">(yt, l) = − k l=1ŷ t,l logy t,l .<label>(17)</label></formula><p>When the training label of the activity frame at time step t corresponds to the target class lt (lt ∈ {1, 2, · · · , k}), elementŷ t,l t inŷt isŷ t,l t = 1, and the other elements inŷt are zero. Then, Eq. <ref type="formula" target="#formula_4">(17)</ref> can be simplified as</p><formula xml:id="formula_14">(yt, lt)= − logy t,l t ,<label>(18)</label></formula><p>where y t,l t is defined in Eq. (8). Some researchers <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b10">[11]</ref> indicated that the memory cell of LSTM at the last time step can store useful for n = 1, 2, · · · , N do 6:</p><p>Update parameters in Single-Person LSTMs via BPTT;</p><formula xml:id="formula_15">7: bz ← m · b s o − η · ∂J (Θ) ∂bz 1 ; 8: W zh ← m · b s o − η · ∂J (Θ) ∂W zh ; 9: bo ← m · b s o − η · ∂J (Θ) ∂bo ; 10: W oh ← m · W oh − η · ∂J (Θ) ∂W oh ; 11: W s ox ← m · W s ox − η · ∂J (Θ) ∂W s ox ; 12: bπ ← m · bπ − η · ∂J (Θ) ∂bπ ; 13: W s πh ← m · W s πh − η · ∂J (Θ) ∂W s πh ; 14: W πh ← m · W πh − η · ∂J (Θ) ∂W πh ; 15: b s * ← m · b s * − η · ∂J (Θ) ∂b s * , s = 1, · · · , p, and * ∈ {i, f, g}; 16: W s * x ← m · W s * x − η · ∂J (Θ) ∂W s * x ; 17: W s * h ← m · W s * h − η · ∂J (Θ) ∂W s * h . 18:</formula><p>end for 19: end for Output: Parameter set Θ. <ref type="bibr" target="#b0">1</ref> Here, m and θ are the momentum parameter and learning rate, respectively. The detailed deductions of the derivative of all the parameters can be found in Appendix A. sequence information of the whole data sequence (e.g., a video clip). That is, for a video clip of length T , if its class label l is annotated at the video level, the H-LSTCM model can be trained by minimizing the loss at time step T , i.e., (yT , l)= − log y T,l . Otherwise, if the class label l is annotated on each frame t, we can minimize the cumulative loss over the sequence, i.e., T t=1 (yt, l). In this work, given a training video clip with label l (l ∈ {1, 2, · · · , k}) at the video level, we choose the loss</p><formula xml:id="formula_16">J (Θ) = (yT , l),<label>(19)</label></formula><p>where </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENTS</head><p>In experiments, we evaluate the performance of H-LSTCM compared with the state-of-the-art methods and some baselines on four public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>The detailed descriptions of four public datasets are as follows:</p><p>• BIT dataset <ref type="bibr" target="#b5">[6]</ref>. It consists of eight classes of human interactions, i.e., bow, boxing, handshake, high-five, hug, kick, pat, and push. Each class includes 50 videos with cluttered backgrounds. Following in <ref type="bibr" target="#b0">[1]</ref>, 34 videos per class are randomly chosen as the training data and the remaining ones are used for testing.  <ref type="bibr" target="#b28">[29]</ref>. It consists of ten videos, each video containing six classes of human interactions, i.e., handshake, hug, kick, point, punch and push. After extracting the frames, we obtain 60 video clips, namely 10 video clips per class. Leave-one-out cross-validation is adopted for the experiments. • Collective Activity Dataset (CAD) <ref type="bibr" target="#b18">[19]</ref>. It contains 44 videos of five multiple-person activities, i.e., crossing, waiting, queuing, walking, and talking. Similar to <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b29">[30]</ref>, we select one-third of the video clips from each activity category to form the test set, and the rest of the video clips are used for training. The one-versus-all technique is employed for this recognition task. • Volleyball Dataset (VD) <ref type="bibr" target="#b12">[13]</ref>. It contains 55 volleyball videos with 4830 annotated frames. Each frame, there has a group-level activity class label (e.g., left pass, right pass, left set, right set, left spike, right spike, left winpoint or right winpoint). Following in <ref type="bibr" target="#b12">[13]</ref>, two-thirds of the annotated frames are used for training and the remaining ones are used for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation Details</head><p>In the preprocessing step, the bounding box (tracklet) corresponding to each person is detected and tracked over all frames by an object detector <ref type="bibr" target="#b30">[31]</ref> and an object tracker <ref type="bibr" target="#b31">[32]</ref>. Following in <ref type="bibr" target="#b8">[9]</ref>, the pretrained AlexNet model <ref type="bibr" target="#b32">[33]</ref> is employed to extract the fc6 feature (static feature) on each bounding box around one person, respectively.</p><p>For the BIT, UT, CAD and VD datasets, the length T of the time steps is set to 30, 40, 10 and 10, respectively. In the configurations of H-LSTCM on four datasets, the number of memory cell nodes of each Single-Person LSTM, the number of output nodes of each Single-Person LSTM, and the number of sub-memory cell nodes of Co-LSTM are set to 2048, 1024 and 512, respectively. We use the Torch toolbox and Caffe <ref type="bibr" target="#b33">[34]</ref> as the deep learning platform and an NVIDIA Tesla K20 GPU to run the experiments. The learning rate, momentum and decay rate are set to 0.5 × 10 −3 , 0.9 and 0.95, respectively. In experiments, the training of H-LSTCM begins to converge after approximately 500, 600, 600 and 700 epochs on the BIT, UT, CAD and VD datasets, respectively. The learning curves for training the proposed H-LSTCM on the BIT, UT, CAD and VD datasets are plotted in Appendix B of the supplemental material.</p><p>In experiments, the following four baselines are chosen:  <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results on the BIT dataset</head><p>Comparison with baselines. <ref type="table" target="#tab_1">Table I</ref> shows the recognition accuracy of the proposed H-LSTCM are better than all baseline methods. Adding the temporal information by employing LSTM (i.e., B2, B3, B4, and Co-LSTSM) improves the performance of B1 without temporal information. Specifically, Co-LSTSM achieves higher accuracy than B2, B3 and B4. It is illustrated that inter-related motion information among multiple persons is more important than the single-person motion information of individuals for recognizing human interactions. The confusion matrix of H-LSTCM is shown in Appendix C of the supplementary material.</p><p>Comparison with state-of-the-art methods. We also compare H-LSTCM with the state-of-the-art methods for human interaction recognition, i.e., hand-crafted spatio-temporal interest points <ref type="bibr" target="#b35">[36]</ref> methods of Lan et al. <ref type="bibr" target="#b19">[20]</ref>, Liu et al. <ref type="bibr" target="#b15">[16]</ref>, and Kong et al. <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, as well as the LSTM-based methods of Donahue et al. <ref type="bibr" target="#b8">[9]</ref>, and Ke et al. <ref type="bibr" target="#b13">[14]</ref>. <ref type="table" target="#tab_1">Table I</ref> lists the results of recognition accuracy, in which some results are reported in <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>. H-LSTCM performs better than the alternatives, especially the LSTM-based methods, i.e., Donahue et al. <ref type="bibr" target="#b8">[9]</ref> and Ke et al. <ref type="bibr" target="#b13">[14]</ref>. In particular, the proposed H-LSTCM has gained an approximately 9% improvement compared with the stateof-the-art LSTM-based methods (i.e., Ke et al. <ref type="bibr" target="#b13">[14]</ref> with an accuracy    <ref type="bibr" target="#b28">[29]</ref> 75.00 87.50 62.50 50.00 75.00 75.00 70.80 Yu et al. <ref type="bibr" target="#b36">[37]</ref> 100.00 65.00 100.00 85.00 75.00 75.00 83.33 <ref type="bibr">Ryoo [38]</ref> 80.00 90.00 90.00 80.00 90.00 80.00 85.00 Kong et al. <ref type="bibr" target="#b5">[6]</ref> 80.00 80.00 100.00 90.00 90.00 90.00 88.33 Kong et al. <ref type="bibr" target="#b0">[1]</ref> 100.00 90.00 100.00 80.00 90.00 90.00 91.67 Kong et al. <ref type="bibr" target="#b4">[5]</ref> 90.00 100.00 90.00 100.00 90.00 90.00 93.33 Raptis et al. <ref type="bibr" target="#b38">[39]</ref> 100.00 100.00 90.00 100.00 80.00 90.00 93.30 Shariat et al. <ref type="bibr" target="#b39">[40]</ref> ------91.57 Zhang et al. <ref type="bibr" target="#b6">[7]</ref> 100.00 100.00 100.00 90.00 90.00 90.00 95.00 Donahue et al. <ref type="bibr" target="#b8">[9]</ref> 90.00 80.00 90.00 80.00 90.00 80.00 85.00 Ke et al. <ref type="bibr" target="#b13">[14]</ref> -  <ref type="figure" target="#fig_6">Figure 3(a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Results on the UT dataset</head><p>Comparison with baselines. <ref type="table" target="#tab_1">Table II</ref> shows the recognition accuracy of the proposed H-LSTCM compared with that of baselines (including Co-LSTSM <ref type="bibr" target="#b14">[15]</ref>). It is observed that H-LSTCM performs consistently better than all the baselines. In particular, H-LSTCM and Co-LSTSM, targeting to model the inter-related dynamics rather than the individual dynamics, achieve impressive accuracy. The confusion matrix of H-LSTCM is shown in Appendix C of the supplementary material.</p><p>Comparison with state-of-the-art methods. The proposed H-LSTCM is also compared with the state-of-the-art methods, including some traditional methods (i.e., <ref type="bibr">Ryoo</ref>   et al. <ref type="bibr" target="#b6">[7]</ref>), a deep learning method (i.e., Wang et al. <ref type="bibr" target="#b16">[17]</ref>), as well as LSTM-based methods (i.e., Ke et al. <ref type="bibr" target="#b13">[14]</ref> and Donahue et al. <ref type="bibr" target="#b8">[9]</ref>). The recognition accuracy results are shown in <ref type="table" target="#tab_1">Table II</ref>. Co-LSTSM achieves satisfactory accuracy, i.e., 95%. By further extending Co-LSTSM in a hierarchical way, the proposed H-LSTCM, which first models single-person dynamics and then captures concurrently interrelated dynamics among persons, improves the recognition accuracy to 98.33%, which is the state-of-the-art performance. Some of the recognition results of H-LSTCM are shown in <ref type="figure" target="#fig_6">Figure 3(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Results on the CAD dataset</head><p>Comparison with baselines. We compare the recognition accuracy of the proposed H-LSTCM and that of all the baselines. We also regard the preliminary Co-LSTSM <ref type="bibr" target="#b14">[15]</ref> as a baseline. Since most of the group activities in the CAD dataset contain multiple interacting persons (≥ 3 persons), the original Co-LSTSM <ref type="bibr" target="#b14">[15]</ref> modeling two interacting persons cannot directly model group activity with multiple persons (≥ 3 persons). Thus, we extend the Co-LSTSM to a new version, named as Co-LSTSM + . Co-LSTSM + has multiple submemory units corresponding to multiple persons, and its architecture is similar to the Co-LSTM module of H-LSTCM in <ref type="figure" target="#fig_6">Figure 3</ref>. The recognition accuracy of the proposed H-LSTCM and all baselines is shown in <ref type="table" target="#tab_1">Table III</ref>. It is observed that H-LSTCM achieves the best performance. Furthermore, Co-LSTSM+ no longer achieves the significant performance improvements compared with B4. Here, Co-LSTSM + (i.e., the extended version of Co-LSTSM <ref type="bibr" target="#b14">[15]</ref>) cannot capture the complex inter-related dynamics among multiple persons based on the static single-person CNN features, since the collective activities in the CAD dataset are more complex than the interactions in either the BIT dataset or UT dataset. The confusion matrix of H-LSTCM is shown in Appendix C of the supplementary material.</p><p>Comparison with state-of-the-art methods. We also compare the recognition accuracy of H-LSTCM and the state-of-the-art methods, including some traditional methods (i.e., <ref type="bibr">Choi</ref>   <ref type="bibr" target="#b41">[42]</ref>, and Hajimirsadeghi et al. <ref type="bibr" target="#b42">[43]</ref>), a deep learning based method (i.e., Deng et al. <ref type="bibr" target="#b23">[24]</ref>), RNN based methods (i.e., Deng et al. <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and an LSTM based method (i.e., Ibrahim et al. <ref type="bibr" target="#b12">[13]</ref>). The results of recognition accuracy are shown in <ref type="table" target="#tab_1">Table II</ref>. H-LSTCM achieves better performance than that of the other methods. As a new exploration that leverages the variants of LSTM, the proposed H-LSTCM achieves an approximately 2% improvement compared with the most closely related work <ref type="bibr" target="#b12">[13]</ref>, which uses only </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Results on the Volleyball dataset</head><p>Comparison with baselines. In a volleyball sport, there are two sub-groups of players from two teams. The players on the same team have more interactions among themselves than with players on the other teams. We consider using two Co-LSTMs to model the inter-related dynamics among players on two teams, respectively. The new framework of H-LSTCM is shown in <ref type="figure" target="#fig_7">Figure 4</ref>. The main extension is that a concatenation operation and an LSTM layer are added on the top of the Co-LSTM layer. In this framework, each subgroup is modeled by a Co-LSTM, and then the outputs of two Co-LSTMs are concatenated into a sequence of representations, which are input into an LSTM layer. Likewise, Co-LSTSM+ (introduced in Section V-E) is also modified in this way. In the B2, we model dynamics of one team by B2, and add a concatenation operation and an LSTM layer on the top of the LSTM layer. In the baseline B4, the outputs of one team of multiple Single-Person LSTMs are pooled into a sequence of representations. The representations of the two teams are concatenated into a long representation which is then fed into an LSTM layer. The recognition accuracy of H-LSTCM and all the baselines is shown in <ref type="table" target="#tab_1">Table IV</ref>, where "lpass", "rpass", "lset", "rset", "lspike", "rspike", "lwin" and "rwin" denote left pass, right pass, left set, right set, left spike, right spike, left winpoint and right winpoint, respectively. H-LSTCM achieves the best performance over all baseline methods. It is noted that Co-LSTSM+ and B4 are comparable. These results illustrate that Co-LSTSM+ cannot learn concurrently inter-related representations between multiple persons well, when a complex pattern of group activity exists. The confusion matrix of H-LSTCM is shown in Appendix C of the supplementary material.</p><p>Comparison with state-of-the-art methods. The results of the proposed H-LSTCM and other related methods are also shown in <ref type="table" target="#tab_1">Table IV</ref>. H-LSTCM achieves the higher recognition accuracy than the state-of-the-art methods, including Ibrahim et al. <ref type="bibr" target="#b12">[13]</ref>, Shu et al. <ref type="bibr" target="#b43">[44]</ref>, Li et al. <ref type="bibr" target="#b44">[45]</ref>, and Biswas et al. <ref type="bibr" target="#b45">[46]</ref>. In particular, H-LSTCM with an accuracy of 88.4% achieves approximately 5% improvement compared with Shu et al. with an accuracy of 83.6%. This demonstrates that H-LSTCM is effective in modeling complex collective activity among a sub-group of persons. Finally, we present some recognition results of H-LSTCM in <ref type="figure" target="#fig_6">Figure 3(d)</ref>.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Evaluation on Human Interaction Prediction</head><p>We also evaluate H-LSTCM on human interaction prediction. In contrast to human interaction recognition, human interaction prediction is defined as recognizing an ongoing interaction activity before the interaction is completely executed <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b37">[38]</ref>. Due to the large variations in appearance and the evolution of scenes, human interaction prediction is a challenging task. Following the experimental setting in <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b46">[47]</ref>, a testing video clip is divided into 10 incomplete action executions by using 10 observation ratios (i.e., from 0 to 1 with a step size of 0.1), which represent the increasing amount of sequential data with time. For example, given a testing video clip of length T , an observation ratio of 0.3 denotes that the accuracy is tested with the first 0.3×T frames. When the observation ratio is 1, namely the entire video clip is used, H-LSTCM acts as a human interaction recognition model.</p><p>The baselines include Dynamic Bag-of-Words (DBoW) <ref type="bibr" target="#b37">[38]</ref>, Sparse Coding (SC) <ref type="bibr" target="#b47">[48]</ref>, Sparse Coding with Mixture of training video Segments (MSSC) <ref type="bibr" target="#b47">[48]</ref>, Multiple Temporal Scales based on SVM (MTSSVM) <ref type="bibr" target="#b48">[49]</ref>, Max-Margin Action Prediction Machine (MMAPM) <ref type="bibr" target="#b46">[47]</ref>, Long-term Recurrent Convolutional Networks (LRCN) <ref type="bibr" target="#b8">[9]</ref>, Spatial-Structural-Temporal Feature Learning (SSTFL) <ref type="bibr" target="#b13">[14]</ref> and our preliminary Co-LSTSM <ref type="bibr" target="#b14">[15]</ref>. The results of all the methods on the BIT and UT datasets with different observation ratios are listed in <ref type="figure" target="#fig_9">Figure 5</ref>(a) and <ref type="figure" target="#fig_9">Figure 5</ref>(b), respectively. Overall, H-LSTCM and Co-LSTSM outperforms all the baselines. All the interactions in the BIT dataset are the two persons interactions with simple background, and Co-LSTSM is proposed to learn the dynamic inter-related representation between two persons. Thus, the performance of H-LSTCM is comparable to Co-LSTSM for the problem of two persons interaction predication on the BIT dataset. Specifically, we can observe that: 1) the improvements of H-LSTCM and Co-LSTSM on BIT are more significant when the observation ratio is 0.6; 2) the accuracy of H-LSTCM becomes stable on both BID and UT when the observation ratio is approximately 0.8, which illustrates the end of close interaction is ending; and 3) since H-LSTSM and Co-LSTSM can accumulate the temporal interacting information, their accuracy monotonously increases with increasing video observation ratio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSIONS</head><p>In this work, on human interaction recognition, we propose a novel Hierarchical Concurrent Long Short-Term Concurrent Memory (H-LSTCM) to learn the dynamic inter-related representation among all persons from the static singe-person features in a hierarchical way. Specifically, for each person, we first feed her/his static singleperson features into a Single-Person LSTM to learn the single-person dynamic. Afterwards, the outputs of all Single-Person LSTMs unit are fed into a novel Concurrent LSTM (Co-LSTM) unit, which mainly consists of multiple sub-memory units and a new co-memory cell. In the Co-LSTM unit, each sub-memory unit stores individual motion information, while a concurrent LSTM (Long Short-Term Memory) unit selectively integrates and stores the inter-related motion information among multiple interacting persons from multiple submemory units via a new co-memory cell. The proposed method is evaluated on four public datasets and yields promising improvements over the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The framework of the proposed Hierarchical Long Short-Term Concurrent Memory (H-LSTCM) for modeling human interactions in a human interaction scene. The details of Co-LSTM unit is displayed inFigure 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(H-LSTCM) for Human Interaction Recognition. H-LSTCM adopts a hierarchical way to first model the single-person dynamics of individuals by LSTM, and then model the concurrently inter-related dynamics among all the interacting persons by a new Co-LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 2 .</head><label>2</label><figDesc>Illustration of a Concurrent LSTM (Co-LSTM) unit in H-LSTCM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Θ denotes a parameter set including all the parameters of the H-LSTCM model. The loss function of H-LSTCM can be minimized by Backpropagation Through Time (BPTT). The detailed deductions of the derivatives of all the parameters in the H-LSTCM model can be found in Appendix A of the supplemental material. The detailed training procedure of H-LSTCM is summarized in Algorithm 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of some recognition results of the proposed method on four datasets. In the BIT, UT and CAD datasets, each person is detected and tracked by a bounding box, of which the size is enlarged by a moderate scale to cover more context information. The VD dataset provides person bounding boxes. Better view in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 .</head><label>4</label><figDesc>Framework of H-LSTCM on the Volleyball activity with two subgroups of persons. A concatenation operation and a LSTM layer is added on the top of Co-LSTM. the traditional LSTM model without any change. Finally, we present some recognition results of H-LSTCM in Figure 3(c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>(b) On the UT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 5 .</head><label>5</label><figDesc>Comparisons of human interaction prediction on BIT and UT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Training for H-LSTCM Input: N video clips, Epoch, Configuration set of H-LSTCM. Initialization: Parameter set Θ, epoch ← 1. 1: Extract fc6 features of each person on the detected bounding box in each frame of each video. // Forward propagation 2: Forward propagation of Single-person LSTMs; 3: Forward propagation of Co-LSTM. // Back propagation 4: for epoch = 1, 2, · · · , Epoch do</figDesc><table><row><cell>5:</cell></row></table><note>Algorithm 1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I RECOGNITION</head><label>I</label><figDesc>ACCURACY (%) ON THE BIT DATASET.</figDesc><table><row><cell>Method</cell><cell>bow</cell><cell>boxing</cell><cell>handshake</cell><cell>high-five</cell><cell>hug</cell><cell>kick</cell><cell>pat</cell><cell>push</cell><cell>Average</cell></row><row><cell>Lan et al. [20]</cell><cell>81.25</cell><cell>75.00</cell><cell>81.25</cell><cell>87.50</cell><cell>87.50</cell><cell>81.25</cell><cell>81.25</cell><cell>81.25</cell><cell>82.03</cell></row><row><cell>Liu et al. [16]</cell><cell>100.00</cell><cell>75.00</cell><cell>81.25</cell><cell>87.50</cell><cell>93.75</cell><cell>87.50</cell><cell>75.00</cell><cell>75.00</cell><cell>84.37</cell></row><row><cell>Kong et al. [6]</cell><cell>81.25</cell><cell>81.25</cell><cell>81.25</cell><cell>93.75</cell><cell>93.75</cell><cell>81.25</cell><cell>81.25</cell><cell>87.50</cell><cell>85.16</cell></row><row><cell>Kong et al. [5]</cell><cell>87.50</cell><cell>81.25</cell><cell>87.50</cell><cell>81.25</cell><cell>87.50</cell><cell>81.25</cell><cell>87.50</cell><cell>87.50</cell><cell>85.38</cell></row><row><cell>Kong et al. [1]</cell><cell>93.75</cell><cell>87.50</cell><cell>93.75</cell><cell>93.75</cell><cell>93.75</cell><cell>87.50</cell><cell>87.50</cell><cell>87.50</cell><cell>90.63</cell></row><row><cell>Donahue et al. [9]</cell><cell>100.00</cell><cell>75.00</cell><cell>85.00</cell><cell>69.75</cell><cell>85.00</cell><cell>69.75</cell><cell>80.00</cell><cell>76.50</cell><cell>80.13</cell></row><row><cell>Ke et al. [14]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>85.20</cell></row><row><cell>B1</cell><cell>100.00</cell><cell>75.00</cell><cell>62.50</cell><cell>56.25</cell><cell>93.75</cell><cell>68.75</cell><cell>56.25</cell><cell>62.50</cell><cell>71.88</cell></row><row><cell>B2</cell><cell>100.00</cell><cell>75.00</cell><cell>84.50</cell><cell>84.50</cell><cell>88.00</cell><cell>88.00</cell><cell>70.00</cell><cell>78.00</cell><cell>83.50</cell></row><row><cell>B3</cell><cell>100.00</cell><cell>79.00</cell><cell>84.50</cell><cell>84.50</cell><cell>94.75</cell><cell>88.00</cell><cell>80.50</cell><cell>90.00</cell><cell>87.66</cell></row><row><cell>B4</cell><cell>100.00</cell><cell>82.00</cell><cell>85.75</cell><cell>84.50</cell><cell>94.75</cell><cell>88.00</cell><cell>83.00</cell><cell>90.00</cell><cell>88.50</cell></row><row><cell>Co-LSTSM [15]</cell><cell>100.00</cell><cell>90.50</cell><cell>92.50</cell><cell>92.50</cell><cell>94.75</cell><cell>88.00</cell><cell>90.50</cell><cell>94.25</cell><cell>92.88</cell></row><row><cell>H-LSTCM</cell><cell>100.00</cell><cell>92.50</cell><cell>94.75</cell><cell>95.50</cell><cell>94.75</cell><cell>89.50</cell><cell>91.00</cell><cell>94.25</cell><cell>94.03</cell></row></table><note>• UT dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>This baseline learns the single-person dynamics via multiple LSTMs, and the outputs are pooled into the other LSTM. Specifically, we first use AlexNet to extract fc6 features on person bounding boxes at each frame. Second, the fc6 features of each person are fed into each traditional LSTM network to learn the single-person hidden states. Third, the hidden states of all persons at each time step are max pooled into a single vector, which is fed into the other LSTM network, followed by a softmax. This baseline is the same as Hierarchical Deep Temporal Models</figDesc><table><row><cell>to each interacting person respectively at each time step are</cell></row><row><cell>merged into a larger bounding box. Second, fc6 features are</cell></row><row><cell>extracted by AlexNet on this "larger" bounding box at each</cell></row><row><cell>time step. Third, we use the fc6 features as inputs to train an</cell></row><row><cell>LSTM. This baseline is similar to that of Long-term Recurrent</cell></row><row><cell>Convolutional Networks [9].</cell></row></table><note>• B1: Person-box CNN. The pre-trained AlexNet is deployed on each person bounding box at each time step, where all fc6 features corresponding to each person are concatenated into a long vector. Then the concatenated features over all time steps are pooled into a single feature. All features from each video clip are trained and tested by the softmax classifier. This baseline illustrates the importance of deep features.• B2: One CNN + LSTM. This baseline treats two individual actions as a whole. First, multiple bounding boxes corresponding• B3: Multiple CNN + LSTM. This baseline models the indi- vidual dynamics of multiple persons by Multiple LSTMs. First, AlexNet is deployed on each person bounding box at each time step to extract the fc6 feature. Second, the fc6 feature extracted from each person is fed into an LSTM network to capture the individual dynamic motions, respectively. Third, we average the softmax scores output of all LSTM networks. Here, the averaged score reflects the probability of the action class. This baseline is the same as Two-Stream Convolutional Networks [35].• B4: Single-Person LSTMs + Whole LSTM.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE II RECOGNITION</head><label>II</label><figDesc>ACCURACY (%) OF DIFFERENT METHODS ON UT DATASET.</figDesc><table><row><cell>Method</cell><cell>handshake hug</cell><cell>kick</cell><cell>point punch push Average</cell></row><row><cell>Ryoo et al.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III RECOGNITION</head><label>III</label><figDesc>ACCURACY (%) OF DIFFERENT METHODS ON CAD.</figDesc><table><row><cell>Method</cell><cell cols="6">crossing waiting queuing walking talking Average</cell></row><row><cell>Choi et al. [19]</cell><cell>55.4</cell><cell>64.6</cell><cell>63.3</cell><cell>57.9</cell><cell>83.6</cell><cell>65.9</cell></row><row><cell>Lan et al. [18]</cell><cell>75</cell><cell>74</cell><cell>74</cell><cell>57</cell><cell>61</cell><cell>68.2</cell></row><row><cell>Choi et al. [3]</cell><cell>76.4</cell><cell>76.4</cell><cell>78.7</cell><cell>36.8</cell><cell>85.7</cell><cell>70.9</cell></row><row><cell>Antic et al. [41]</cell><cell>73.70</cell><cell>74.50</cell><cell>90.10</cell><cell>62.00</cell><cell>70.00</cell><cell>74.1</cell></row><row><cell>Liu et al. [16]</cell><cell>72.73</cell><cell>66.67</cell><cell>71.43</cell><cell>83.33</cell><cell>85.71</cell><cell>76.19</cell></row><row><cell>Wang et al. [4]</cell><cell>64.8</cell><cell>66.0</cell><cell>66.7</cell><cell>89.2</cell><cell>99.5</cell><cell>77.2</cell></row><row><cell>Lan et al. [20]</cell><cell>68</cell><cell>69</cell><cell>76</cell><cell>80</cell><cell>99</cell><cell>79.7</cell></row><row><cell>Choi et al. [22]</cell><cell>61.3</cell><cell>82.9</cell><cell>95.4</cell><cell>65.1</cell><cell>94.9</cell><cell>79.9</cell></row><row><cell>Kong et al. [1]</cell><cell>77.27</cell><cell>77.78</cell><cell>85.71</cell><cell>83.33</cell><cell>100</cell><cell>82.54</cell></row><row><cell>Zhou et al. [42]</cell><cell>76.83</cell><cell>74.36</cell><cell>93.76</cell><cell>87.63</cell><cell>98.16</cell><cell>82.07</cell></row><row><cell>Ibrahim et al. [13]</cell><cell>61.54</cell><cell>66.44</cell><cell>96.77</cell><cell>80.41</cell><cell>99.45</cell><cell>81.50</cell></row><row><cell>Hajimirsadeghi et al. [43]</cell><cell>72</cell><cell>75</cell><cell>92</cell><cell>70</cell><cell>99</cell><cell>81.6</cell></row><row><cell>Deng et al. [24]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>80.6</cell></row><row><cell>Deng et al. [25]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>81.2</cell></row><row><cell>B1</cell><cell>46.21</cell><cell>53.69</cell><cell>70.20</cell><cell>61.19</cell><cell>74.33</cell><cell>61.12</cell></row><row><cell>B2</cell><cell>52.38</cell><cell>54.50</cell><cell>73.89</cell><cell>61.45</cell><cell>76.35</cell><cell>63.71</cell></row><row><cell>B3</cell><cell>52.46</cell><cell>54.61</cell><cell>82.00</cell><cell>61.21</cell><cell>79.85</cell><cell>66.02</cell></row><row><cell>B4</cell><cell>62.60</cell><cell>65.25</cell><cell>90.74</cell><cell>78.33</cell><cell>95.36</cell><cell>78.46</cell></row><row><cell>Co-LSTSM +</cell><cell>65.50</cell><cell>64.85</cell><cell>94.67</cell><cell>75.33</cell><cell>95.33</cell><cell>79.14</cell></row><row><cell>H-LSTCM</cell><cell>65.50</cell><cell>68.29</cell><cell>97.90</cell><cell>87.69</cell><cell>99.35</cell><cell>83.75</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>et al. [19], Lan et al. [18], Choi et al. [3], Antic et al. [41], Liu et al. [16], Wang et al. [4], Lan et al. [20], Choi et al. [22], Kong et al. [1], Zhou et al.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE IV RECOGNITION</head><label>IV</label><figDesc>ACCURACY (%) ON VOLLEYBALL DATASET. Method lpass rpass lset rset lspike rspike lwin rwin Averae Ibrahim et al. [13] 77.9 81.4 84.5 68.8 89.4 85.6 88.2 87.4 82.9 Shu et al.</figDesc><table><row><cell>[44]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.6</cell></row><row><cell>Li et al. [45]</cell><cell cols="5">55.8 69.1 67.3 52.1 82.1</cell><cell>79.2</cell><cell>-</cell><cell>-</cell><cell>67.6</cell></row><row><cell>Biswas et al. [46]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>83.0</cell></row><row><cell>B1</cell><cell cols="5">62.8 62.1 71.4 58.7 65.1</cell><cell cols="3">76.5 63.7 61.6</cell><cell>65.2</cell></row><row><cell>B2</cell><cell cols="5">64.6 66.5 76.5 62.7 77.7</cell><cell cols="3">74.0 70.6 68.0</cell><cell>70.1</cell></row><row><cell>B3</cell><cell cols="5">74.4 77.3 81.8 69.7 88.2</cell><cell cols="3">83.7 78.6 78.0</cell><cell>79.0</cell></row><row><cell>B4</cell><cell cols="5">77.0 80.9 84.1 68.3 88.8</cell><cell cols="3">85.3 88.0 87.7</cell><cell>82.5</cell></row><row><cell>Co-LSTSM+</cell><cell cols="5">81.3 79.5 85.1 70.7 88.8</cell><cell cols="3">85.5 88.7 86.9</cell><cell>83.3</cell></row><row><cell>H-LSTCM</cell><cell cols="5">83.9 88.1 90.3 80.4 93.4</cell><cell cols="3">89.8 88.7 92.4</cell><cell>88.4</cell></row><row><cell cols="3">0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9</cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Video observation ratio</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interactive phrases: Semantic descriptions for human interaction recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1775" to="1788" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning person-person interaction in collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1905" to="1918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning context for collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A spatio-temporal crf for human interaction understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1647" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Close human interaction recognition using patchaware models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="167" to="178" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning human interaction by interactive phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatio-temporal phrases for activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Differential recurrent neural networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Veeriah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A hierarchical deep temporal model for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Spatial, structural and temporal feature learning for human interaction prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bossaid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Concurrenceaware long short-term sub-memories for person-person action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Recognizing human actions by attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical context modeling for video event recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1770" to="1782" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Retrieving actions in group contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Robinovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">What are they doing?: Collective activity classification using spatio-temporal relationship among people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shahid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Discriminative latent models for recognizing contextual group activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Robinovitch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1549" to="1562" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognition of composite human activities through context-free grammar based representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A unified framework for multi-target tracking and collective activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">A discriminative key pose sequence model for recognizing human interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranjbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCVW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep structured models for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Muralidharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Roshtkhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structure inference machines: Recurrent neural networks for analyzing relations in group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Social lstm: Human trajectory prediction in crowded spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tracking the untrackable: Learning to track multiple cues with long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A learning algorithm for continually running fully recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zipser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="270" to="280" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Spatio-temporal relationship match: Video structure comparison for recognition of complex human activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Visual recognition by counting instances: A multi-instance cardinality potential kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajimirsadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Gmcp-tracker: Global multiobject tracking using generalized minimum clique graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dehghan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VS-PETS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Real-time action recognition by spatiotemporal semantic and structural forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Human activity prediction: Early recognition of ongoing activities from streaming videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Poselet key-framing: A model for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A new adaptive segmental matching measure for human activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shariat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning latent constituents for recognition of group activities in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Antic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A generative model for recognizing mixed group activities in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-instance classification by maxmargin training of cardinality-based markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajimirsadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1839" to="1852" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Cern: Confidence-energy recurrent network for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">SBGAR: semantics based group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Chuah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Structural recurrent neural network (srnn) for group activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>WACV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Max-margin action prediction machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1844" to="1858" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barbu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narayanaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Dickinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Siskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Recognize human activities from partially observed videos,&quot; in CVPR</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A discriminative model with multiple temporal scales for action prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
