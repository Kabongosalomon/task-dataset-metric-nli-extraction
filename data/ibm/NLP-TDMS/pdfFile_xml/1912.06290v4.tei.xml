<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">META-LEARNING INITIALIZATIONS FOR IMAGE SEGMENTATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><forename type="middle">M</forename><surname>Hendryx</surname></persName>
							<email>seanmhendryx@email.arizona.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">B</forename><surname>Leach</surname></persName>
							<email>ableach@email.arizona.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Hein</surname></persName>
							<email>pauldhein@email.arizona.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><forename type="middle">T</forename><surname>Morrison</surname></persName>
							<email>claytonm@email.arizona.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Information</orgName>
								<orgName type="institution">University of Arizona Tucson</orgName>
								<address>
									<postCode>85721</postCode>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Program in Applied Mathematics University of Arizona Tucson</orgName>
								<address>
									<postCode>85721</postCode>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Information University of Arizona Tucson</orgName>
								<address>
									<postCode>85721</postCode>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Information University of Arizona Tucson</orgName>
								<address>
									<postCode>85721</postCode>
									<region>AZ</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">META-LEARNING INITIALIZATIONS FOR IMAGE SEGMENTATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We extend first-order model agnostic meta-learning algorithms (including FOMAML and Reptile)   to image segmentation, present a novel neural network architecture built for fast learning which we call EfficientLab, and leverage a formal definition of the test error of meta-learning algorithms to decrease error on out of distribution tasks. We show state of the art results on the FSS-1000 dataset by meta-training EfficientLab with FOMAML and using Bayesian optimization to infer the optimal test-time adaptation routine hyperparameters. We also construct a small benchmark dataset, FP-k, for the empirical study of how meta-learning systems perform in both few-and many-shot settings. On the FP-k dataset, we show that meta-learned initializations provide value for canonical few-shot image segmentation but their performance is quickly matched by conventional transfer learning with performance being equal beyond 10 labeled examples. Our code, meta-learned model, and the FP-k dataset are available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In recent years, there has been substantial progress in high accuracy image segmentation in the high data regime (see <ref type="bibr" target="#b0">[1]</ref> and their references). While meta-learning approaches that utilize neural network representations have made progress in few-shot image classification, reinforcement learning, and, more recently, image semantic segmentation, the training algorithms and model architectures have become increasingly specialized to the low data regime. A desirable property of a learning system is one that effectively applies knowledge gained from a few or many examples, while reducing the generalization gap when trained on little data and not being encumbered by its own learning routines when there are many examples. This property is desirable because training and maintaining multiple models is more cumbersome than training and maintaining one model. A natural question that arises is how to develop learning systems that scale from few-shot to many-shot settings while yielding competitive accuracy in both. One scalable potential approach that does not require ensembling many models nor the computational costs of relation networks, is to meta-learn an initialization such as via Model Agnostic Meta-Learning (MAML) <ref type="bibr" target="#b1">[2]</ref>.</p><p>In this work, we specifically address the problem of meta-learning initializations for deep neural networks that must produce dense, structured output, such as for the semantic segmentation of images. We ask the following questions:</p><p>1. Do first-order MAML-type algorithms extend to the higher dimensional parameter spaces, dense prediction, and skewed distributions required of semantic segmentation? 2. How sensitive is the test-time performance of gradient-based meta-learning to the hyperparameters of the update routine used to adapt the initialization to new tasks? 3. What is the number of labeled examples beyond which a conventional approach to training deep neural networks matches or outperforms the meta-learned initializations?</p><p>To address the third question, we put together a small benchmark dataset, which we call FP-k, that contains 5 tasks each with at least 420 labeled examples of image-mask pairs. In recent works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, few-shot learning approaches have become increasingly complex and specialized to the few-shot domain. Such specialization comes at an engineering cost and leaves many open questions, such as: What is the accuracy of a few-shot learning system when more labeled examples become available? After a certain number of labeled examples, will the few-shot learning system have the same accuracy as a simpler training approach such as conventional training via SGD? If so, what is the number of labeled examples beyond which a conventional approach to training deep neural networks matches or outperforms a meta-learning system? We address these questions in 5.5.</p><p>In summary, we address the above research questions as follows: We show that MAML-type algorithms do extend to few shot image segmentation, yielding state of the art results when their update routine is optimized after meta-training and when the model is regularized. Addressing question 2, we find that the meta-learned initialization's performance when being evaluated on a task is particularly sensitive to changes in the update routine's hyperparameters (see <ref type="figure" target="#fig_2">Figure  3</ref>). We show theoretically in section 3.3 and empirically in our results (see <ref type="table" target="#tab_4">Table 3b</ref>) that a single update routine used both during meta-training and meta-testing may not have optimal generalization. Finally, we address question 3 by showing that our meta-learned initializations outperform ImageNet <ref type="bibr" target="#b7">[8]</ref> and joint-trained initializations when the number of labeled examples is less than 10. Our code, meta-learned model, and the FP-k dataset are available at https://github.com/ml4ai/mliis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Learning useful models from a small number of labeled examples of a new concept has been studied for decades <ref type="bibr" target="#b8">[9]</ref> yet remains a challenging problem with no semblance of a unified solution. The advent of larger labeled datasets containing examples from many distinct concepts <ref type="bibr" target="#b9">[10]</ref> has enabled progress in the field in particular by enabling approaches that leverage the representations of nonlinear neural networks. Image segmentation is a well-suited domain for advances in few-shot learning given that the labels are particularly costly to generate <ref type="bibr" target="#b10">[11]</ref>.</p><p>Recent work in few-shot learning for image segmentation has utilized three key components: (1) model ensembling <ref type="bibr" target="#b3">[4]</ref>, (2) the relation networks of [12] 2 , and (3) late fusion of representations <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref>. The inference procedure of ensembling models with a separately trained model for each example has been shown to produce better predictions than single shot approaches but will scale linearly in time and/or space complexity (depending on the implementation) in the number of training examples, as implemented in <ref type="bibr" target="#b3">[4]</ref>. The use of multiple passes through subnetworks via iterative optimization modules was shown by <ref type="bibr" target="#b5">[6]</ref> to yield improved segmentation results but comes at the expense of additional time complexity during inference. The relation networks proposed in <ref type="bibr" target="#b11">[12]</ref> have seen increased adoption in meta-learning systems and were recently extended to the modality of dense prediction by the authors in <ref type="bibr" target="#b5">[6]</ref> and <ref type="bibr" target="#b10">[11]</ref>. While this extension of the relation networks of <ref type="bibr" target="#b11">[12]</ref> to image segmentation yield impressive results in the few-shot domain, their efficacy in scaling as more training data becomes available is untested.</p><p>Model Agnostic Meta-Learning (MAML) is a gradient-based meta-learning approach introduced in <ref type="bibr" target="#b1">[2]</ref>. First Order MAML (FOMAML) reduces the computational cost by not requiring backpropogating the meta-gradient through the inner-loop gradient and has been shown to work similarly well on classification tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>. Though learning an initialization has the potential to unify few-shot and many-shot domains, initializations learned from MAML-type algorithms have been seen to overfit in the low-shot domain when adapting sufficiently expressive models such as deep residual networks that may be more than a small number of convolutional layers 3 <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b4">5]</ref>. The Meta-SGD learning framework added additional capacity to the same network architecture used in MAML with improved generalization by meta-learning a learning rate for each parameter in the network <ref type="bibr" target="#b2">[3]</ref>, but lacks a first order approximation. In addition to possessing potential to unify few-and many-shot domains, MAML-type algorithms are intriguing in that they impose no constraints on model architecture, given that the output of the meta-learning process is simply an initialization. Futhermore, the meta-learning dynamics, which learn a temporary memory of a sampled task, are related to the older idea of fast weights <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Despite being dataset size and model architecture agnostic, MAMLtype algorithms are unproven for high dimensionality of the hypothesis spaces and the skewed distributions of image segmentation problems data <ref type="bibr" target="#b13">[14]</ref>.</p><p>In recent work, <ref type="bibr" target="#b18">[19]</ref> found that standard joint pre-training on all meta-training tasks on mini-imagenet, tiered Ima-geNet, and other few shot image classification benchmarks, with a sufficiently large network is on par with many sophisticated few-shot learning algorithms. Furthermore, implementing meta-training code comes with additional complexity. Thus it is worth testing how a vanilla training loop on the "joint" distribution of all the 760 non-test tasks compares to a meta-learned initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generalization Error in Meta-learning</head><p>In the context of image segmentation, an example from a task τ is comprised of an image x and its corresponding binary mask y, which assigns each pixel membership to the target (ex. black bear) or background class. Examples (x, y) from the domain D τ are distributed according to q τ (x, y), and we measure the loss L of predictionsŷ generated from parameters θ and a learning algorithm U . For a distribution p(τ ) over the domain of tasks T , the parameters that minimize the expected loss are</p><formula xml:id="formula_0">θ * = arg min θ E p [E qτ [L (U (θ))]]<label>(1)</label></formula><p>In practice, we only have access to a finite subset of the tasks, which we divide into the training T tr , validation T val , and test tasks T test , and instead optimize over an empirical distributionp(τ ) := p(τ |τ ∈ T tr ). For examples within each available task, we can similarly define D tr τ , D val τ , D test τ , and the corresponding empirical distribution q τ (x, y) := q τ (x, y|(x, y) ∈ D tr τ ). As a corollary to 1, the empirically optimal initialization</p><formula xml:id="formula_1">θ * = arg min θ Ep [Eq τ [L (U (θ))]]<label>(2)</label></formula><p>has a generalization gap that can then be expressed as</p><formula xml:id="formula_2">E p E qτ L U (θ * ) − Ep Eq τ L U (θ * )<label>(3)</label></formula><p>We include a proof in the supplementary material. The generalization gap between the actual and empirical error in meta-learning is twofold: from the domain of all tasks T to the sample T tr , and within that, from all examples in D τ to D tr τ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Agnostic Meta-learning</head><p>The model agnostic meta-learning (MAML) algorithm introduced in [2] uses a gradient-based update procedure U with hyperparameters ω, which applies a limited number of training steps with a few-shot subset of D tr τ to adapt a meta-learned initialization θ to each task. To be precise, U maps from an initialization θ and examples in D τ to updated parameters θ τ which parameterize a task-specific prediction function f (x; θ τ ):</p><formula xml:id="formula_3">f (x; θ τ ) = f (x; U (θ; D τ ))<label>(4)</label></formula><p>We adopt the shorthand L(U (θ)) used in <ref type="bibr" target="#b14">[15]</ref> to indicate that the loss L is computed over f (x; θ τ ) for x, y ∈ D τ :</p><formula xml:id="formula_4">L(U (θ)) := L(f (x; θ τ ), y)<label>(5)</label></formula><p>To minimize the loss incurred in the update routine, we first take the derivative with respect to the initialization</p><formula xml:id="formula_5">∂ ∂θ L(U (θ)) = U (θ) · L (U (θ))<label>(6)</label></formula><p>where the resulting term U is the derivative of a gradient based update procedure, and hence, contains second order derivatives. In first-order renditions explored in <ref type="bibr" target="#b14">[15]</ref>, FOMAML and Reptile, finite differences are used to approximate the gradient of the meta-update ∇θ. The difference between the two approximations can be summarized by how they make use of D tr τ and D val τ :</p><formula xml:id="formula_6">θ tr ← U (θ ; D tr τ , ω tr ) (7) θ val ← U (θ tr ; D val τ , ω val ) (8) θ both ← U (θ ; D tr τ ∪ D val τ , ω tr )<label>(9)</label></formula><p>Reptile trains jointly on both, while FOMAML trains on the two sets separately in sequence, favoring initializations that differ less between the splits.</p><formula xml:id="formula_7">Reptile: ∇θ ∝ θ both − θ<label>(10)</label></formula><formula xml:id="formula_8">FOMAML: ∇θ ∝ θ val − θ tr<label>(11)</label></formula><p>The gradient approximation ∇θ can then be used to optimize the initialization by stochastic gradient descent or any other gradient-based update procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Optimizing Test-Time Update Hyperparameters</head><p>As shown clearly in equations 4 and 5, the error of any function f learned or predicted from a dataset D τ depends on the learning algorithm U . This analysis motivates research question 2 in section 1, which asks how significant is the effect of the hyperparameters of U . To address this question, we leverage the flexibility to choose hyperparameters ω test when adapting to new tasks, separately from the hyperparameters ω tr used in meta-training. The optimal choice of ω test can be determined by minimizing the expected loss in eq. 1 with respect to the hyperparameters, treatingθ * and D tr τ as parameters of the update routine:</p><formula xml:id="formula_9">ω * = arg min ω Ep Eq τ L U (ω ;θ * , D tr τ )<label>(12)</label></formula><p>Empirical estimations of the optimal initializationθ * have an implicit dependence on T tr and ω tr (eq. 2), and the optimal hyperparametersω * depend on theθ * in turn (eq. 12). We call the general procedure of optimizing the update routine's hyperparameters to decrease meta-test-time error update hyperparameter optimization (UHO).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EfficientLab Architecture for Image Segmentation</head><p>To extend first-order MAML-type algorithms to more expressive models, with larger hypothesis spaces, while yielding state of the art few-shot learning results, we developed a novel neural network architecture, which we term Efficient-Lab. The top level hierarchy of the network's organization of computational layers is similar to <ref type="bibr" target="#b19">[20]</ref> with convolutional blocks that successively halve the features in spatial resolution while increasing the number of feature maps. This is followed by bilinear upsampling of features which are concatenated with features from long skip connections from the downsampling blocks in the encoding part of the network. The concatenated low and high resolution features are then fed through a novel atrous spatial pyramid pooling (ASPP) module, which we call a residual skip decoder (RSD), and finally bilinearly upsampled to the original image size.</p><p>The differences between our model and the DeepLab model are in (1) the encoder network used and (2) how the low resolution embedded features are upsampled to full resolution predictions. For the encoding subnetwork, we utilize the recently proposed EfficientNet <ref type="bibr" target="#b20">[21]</ref>. After encoding the images, the feature maps are upsampled through a parameterized number of RSD modules. The RSD computational graph of operations is shown in <ref type="figure" target="#fig_0">Figure 1</ref>. EfficientLab-3 has one RSD module at the third stage while EfficientLab-6-3 has RSD modules at the 6th and 3rd stages as shown in <ref type="figure" target="#fig_0">figure 1</ref>. The RSD module utilizes three parallel branches of a 1 × 1 convolution, 3 × 3 convolution with dilation rate = 2, and a simple average-pooling across spatial dimensions of the feature maps. The output of the three branches is concatenated and fed into a final 3 × 3 convolutional layer with 112 filters. A residual connection wraps around the convolutional layers to ease gradient flow 4 . Before the final 1 × 1 convolution that produces the unnormalized heatmap of class scores, we use a single layer of dropout with a drop rate probability = 0.2 5 . We use the standard softmax to produce the normalized predicted probabilities. shows the EfficientNet feature extractor on the left with mobile inverted bottleneck convolutional blocks (see <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref> for more details). On the right is the residual skip decoder module that we utilize in the upsampling branch of EfficientLab. The numbers suffixing EfficientLab denote the stage at which a residual skip decoder (RSD) is located, with EfficientLab-3, pictured, having an RSD with skip connections from stage 3 in the downsampling layers. EfficientLab-6-3 has RSD modules at both stages 6 and 3.</p><p>We use batch normalization layers following convolutional layers <ref type="bibr" target="#b24">[25]</ref>. We meta-learn the β and γ parameters, adapt them at test time to test tasks, and use running averages as estimates for the population mean and variance, E[x] and V ar[x], at inference time as suggested in <ref type="bibr" target="#b25">[26]</ref>. All parameters at the end of an evaluation call are reset to their preadaptation values to stop information leakage between the training and validation sets. The network is trained with the binary cross entropy minus the log of the dice score <ref type="bibr" target="#b26">[27]</ref> , which we adapt from the loss function of <ref type="bibr" target="#b27">[28]</ref>, plus an L 2 regularization on the weights:</p><formula xml:id="formula_10">L = H − log(J) + λ θ 2 2 (13)</formula><p>where H is binary cross entropy loss:</p><formula xml:id="formula_11">H = − 1 n n i=1 (y i logŷ i + (1 − y i ) log (1 −ŷ i ))<label>(14)</label></formula><p>J is the modified Dice score:</p><formula xml:id="formula_12">J = 2IoU IoU + 1<label>(15)</label></formula><p>and IoU is the intersection over union metric:</p><formula xml:id="formula_13">IoU = 1 n n i=1 y iŷi + y i +ŷ i − y iŷi +<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We  <ref type="bibr" target="#b28">[29]</ref>. We use L 2 regularization on all weights with a coefficient λ = 5e−4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">FSS-1000 Dataset</head><p>The first few-shot image segmentation dataset was the PASCAL-5 i presented in <ref type="bibr" target="#b3">[4]</ref> which reimagines the PASCAL dataset <ref type="bibr" target="#b29">[30]</ref> as a few-shot binary segmentation problem for each of the classes in the original dataset. Unfortunately, the dataset contains relatively few distinct tasks (20 excluding background and unlabeled). The idea of a meta-learning dataset for image segmentation was further developed with the recently introduced FSS-1000 dataset, which contains 1000 classes, 240 of which are dedicated to the meta-test set T test , with 10 image-mask pairs for each class <ref type="bibr" target="#b10">[11]</ref>. For each of the rows in the results table 3b, we evaluate the network on the 240 test tasks, sampling two random splits into training and testing sets for each task, yielding 480 data points per meta-learning approach for which the mean intersection over union (IoU) (eq. 16) and 95% confidence interval are reported. The FSS-1000 dataset is the focus of the empirical comparisons of network ablations and meta-learning approaches that we experiment with in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">FP-k Dataset</head><p>For investigating how the meta-learned representations integrate new information as more data becomes a available, we put together a small benchmark dataset that we call FP-k. FP-k takes 5 tasks from FSS-1000 and 5 tasks from PASCAL-5 i for the same concept <ref type="bibr" target="#b5">6</ref> . Using this dataset, we train over a range of "k"-training shots from ImageNettrained 7 , joint-trained, and our meta-learned initializations. We report the performance of our EfficientLab network meta-trained with FOMAML over a range of k examples as a benchmark which we hope will inspire future empirical research into studying how meta-learning approaches scale in accuracy and computational complexity as more labeled data become available. For all three initializations, we use UHO for estimating the hyperparameters of U for k &lt; 10.</p><p>For k ≥ 10, we use a fixed learning rate and early stopping to estimate the optimal number of iterations. These results are shown in <ref type="figure">Figure 4</ref> and discussed in 5.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Test-Time Update Hyperparameter Optimization Methodology</head><p>Generalization in meta-learning requires both the ability to learn representations for new tasks efficiently (T tr to T test ), and to select representations that are able to capture unseen test examples effectively (D tr τ to D test τ ). The approximation scheme of FOMAML addresses the latter by taking the finite difference between updates using the train and validation sets (as shown in eq. 11), favoring initializations that differ less between splits of D tr τ ∪ D val τ . In investigation of research question 2 in section 1 and to further improve generalization within task to D test τ , we tune ω after meta-learningθ * to find ω test (as shown in eq. 12). We use ω test at meta-test time when adapting the initialization to new tasks. Specifically, we use Bayesian optimization with Gaussian processes to optimize the hyperparameters ω <ref type="bibr" target="#b30">[31]</ref>. We apply this UHO procedure to estimate the optimal adaption routine's hyperparameters using 200 randomly validation tasks T val that are held our from meta-training. We specifically search over the learning rate and the number of gradient updates that are applied when adapting to a new task τ . We report results with and without optimized update hyperparameters in table 3b. We find that optimizing ω significantly improves adaptation performance on the meta-test tasks T test .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Joint-Trained Initialization</head><p>We trained EfficientLab on the "joint" distribution of T tr ∪ T val in a standard training loop, without an inner loop. Each batch contained a random sample of examples from any of the classes as is standard in SGD. The only change to the network architecture was that instead of predicting 2 channel output (foreground and background), the network was trained to predict the number of task classes plus a background class. Other than these changes, we matched meta-training hyperparameters as faithfully as possible: training for 200 epochs, batch size of 8 image-mask pairs, using a learning rate of 0.005 with a linear decay, and regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Results</head><p>We show the results of experimenting with different decoder architectures for EfficientLab in <ref type="table">Table 1</ref>. Each network topology is meta-trained with FOMAML and the same meta-training hyperparameters defined in 5. The bulk of our experiments were done with EfficientLab-3, including all FP-k experiments, though our best results were found using EfficientLab-6-3. We expect this trend of asymptotic improvements when increasing model dimensionality as also found in <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Architecture</head><p>IoU Auto-DeepLab decoder 71. <ref type="bibr" target="#b15">16</ref>  80.43 ± 0.91% Specifically, we use EfficientLab-3 to indicating that the RSD module is placed on the upsampling path at stage 3 and EfficientLab-6-3 to indicate that RSD modules are located on the upsampling path at both stages 6 and 3. <ref type="table">Table 1</ref>: EfficientLab architecture ablations. Each network is meta-trained in the same way following Section 5 and tested on the set of test tasks from FSS-1000 <ref type="bibr" target="#b10">[11]</ref>. The row "RSD at Stage 3 w/o residual" contains results of removing the short-range residual connection from our proposed RSD module. The final row is the best network we find for few-shot performance via model agnostic meta-learning. We call this network EfficientLab in reference to the encoder of EfficientNet <ref type="bibr" target="#b20">[21]</ref> and the decoder of DeepLab <ref type="bibr" target="#b19">[20]</ref>, which it is inspired by.</p><p>The results of our model with an initialization meta-learned using Reptile and FOMAML are shown in <ref type="table" target="#tab_4">Table 3b</ref>. We find that EfficientLab trained with FOMAML and importantly with an adaptation routine optimized for low out of distribution test error, regularization, and improved use of batch normalization yields state of the art results. Given that previous works have used regularization minimally or not at all during meta-training, we also conducted an ablation of removing regularization on the model. We find, unsurprisingly, that the combination of an L2 loss on the weights, with simple augmentations, and a final layer of dropout significantly increases generalization performance. After optimizing the update routine's hyperparameters, our approach sets the new state of the art for the FSS-1000 dataset.</p><p>We have included a visualization of example predictions for a small set of randomly sampled test tasks in 2. See the supplementary material for additional examples and failure cases. Importantly, we also find that the original definition of FOMAML in which the mini-datasets D tr and D val are disjoint yields worse results than sampling with some amount of overlap. We find that by sampling D tr and D val with replacement from D tr ∪ D val yields better results. This sampling procedure is denoted by FOMAML * below and can be interpreted as an interpolation between the original Reptile and FOMAML definitions put forth in <ref type="bibr" target="#b14">[15]</ref>. We suspect that this sampling strategy serves as a form of meta-regularization though further work would be required on  <ref type="table">Table 2</ref>: Training paradigms. Mean IoU scores of the EfficientLab-3 and EfficientLab-6-3 network evaluated on FSS-1000 test set of tasks for 1-shot and 5-shot learning. We report the FSS-1000 baseline from <ref type="bibr" target="#b10">[11]</ref>. Our best found model combined FOMAML * , EfficientLab-6-3, regularization, and UHO. FOMAML D tr ∩ D val = ∅ denotes the original definition of FOMAML put forth in <ref type="bibr" target="#b14">[15]</ref> in which D tr and D val are completely disjoint while FOMAML * denotes that the two mini-datasets have been sampled with replacement from D tr ∪ D val . this detail to be conclusive. Similarly, we find that meta-training with Reptile using all 10 examples in each task of FSS-1000 produces worse results than meta-training with 5 examples per task in each meta-example.</p><p>To address research question 2 in section 1, we also searched through a range of update routine learning rates, α, that were 10× less to 10× greater than the learning rate used during meta-training. As clearly shown in <ref type="figure" target="#fig_2">Figure 3</ref>, the learned representations are not robust to such large variations in the hyperparameter.</p><p>In this work we posit that a fixed update procedure that is used at test time and not conditioned on the labeled examples for a new task D tr ∈ τ is one of the major hinderances of applying MAML-type algorithms to unseen tasks. In section 3, we show that the hyperparameters ω that are used to adapt the networks weights θ to a new task D τ can be optimized. We find this analysis to be supported empirically as well. We find that: (1) the estimated optimal hyperparameters for the update routine on the validation tasks are not the same as those specified a priori during meta-training, as illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. One may expect that MAML-type algorithms would converge to a point in parameter space from which optimal minima for each of the training tasks are reachable. We find that even after 330 epochs through the training tasks, this was not the case. The best learning rate and number of iterations we found via UHO on the heldout validation set of 200 tasks T val were orders of magnitude smaller for the learning rate and orders of magnitude greater for the best number of iterations respectively. We discuss this phenomena in more depth in the supplementary material. Furthermore, we noticed that the larger the number of maximum steps that UHO was allowed to search within, the smaller the optimal learning rate that was returned. (2) Optimizing the hyperparameters after meta-training improves test-time results on unseen tasks. Furthermore, we find that meta-training from scratch (and evaluating) with the UHO-selected hyperparameters yields nearly identical results to meta-training with the initial hyper parameters learning rate = 0.005 and inner-iterations = 5. This further suggests that it may be useful to tune the hyperparameters ω after meta-training to improve the generalization performance of the gradient-based adaptation routine U .</p><p>By training our model on the FP-k dataset, we found that our meta-learned initializations reliably outperformed jointtrained, and ImageNet-trained initializations but only up to 10 training examples <ref type="bibr" target="#b7">8</ref> . As shown in <ref type="figure">Figure 4</ref>, we again  During optimization of the learning rate and the number of steps, the relationship between the learning rate and the IoU is modeled as a Gaussian process (shown in blue dashed line with 95% confidence interval). Points are colored by median of the iterations each task was trained for before stopped by early stopping.</p><p>find that the hyperparameters of U are critical for effectively adapting the parameters to new tasks. In particular, we find that a large early stopping patience is necessary for maximal performance when adapting to new tasks as learning, especially with non-meta-learned initializations, undergoes a period where out of distribution error increases. We suspect that the larger patience is required for good generalization as the number of training examples grows due to the concept of double descent noted in <ref type="bibr" target="#b32">[33]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E[IoU]</head><p>initialization meta-learned imagenet joint-trained <ref type="figure">Figure 4</ref>: Mean IoU results as a function of the training set size of our EfficientLab-3 model adapted to tasks of the FP-k dataset. Given the importance we have shown in the adaption routine's hyperparameters, we experiment with different learning rate schedules and early stopping parameters. On the left, are the results from using a step decay learning rate as in <ref type="bibr" target="#b33">[34]</ref> with early stopping patience of 100 gradient steps. In the middle and right are the results from using a fixed learning rate but with an early stopping patience of 50 and 100 gradient steps respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Future Work</head><p>In future work, it would be useful to take a more critical look at the interplay between batch normalization and metalearning. While single task deep neural networks in large data regimes apply batch normalization with a consistent pattern, different groups working in few-shot meta-learning have incorporated batch norm in markedly different ways such as by: (1) not using it at all for the meta-learning components <ref type="bibr" target="#b4">[5]</ref>, (2) not using learned β and γ parameters at all while still using estimated population means and variances during inference <ref type="bibr" target="#b5">[6]</ref>, or (3) meta-learning β and γ while only using batch statistics for the normalization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref>, or (4) meta-learning β and γ and also using population estimates of the mean and variance, as done conventionally when training deep neural networks in the large data regime, which is the approach that we adopt and find to be most useful. Another interesting question to address would be to evaluate how EfficientLab performs on more standard many-shot multi-class image segmentation problems such as the CityScapes dataset <ref type="bibr" target="#b34">[35]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this work, we showed that gradient-based first order model agnostic meta-learning algorithms extend to the high dimensionality of the hypothesis spaces and the skewed distributions of few-shot image segmentation problems, but are sensitive to the hyperparameters, ω, of the update routine, U . Furthermore, we find that the initializations that are meta-learned quickly match test time performance of traditional transfer learning approaches as more data becomes available. These results, while produced from a small number of tasks, provide important context on the value in terms of labeled data efficiency of applying FOMAML to image segmentation. Future work should investigate more critically, both empirically and theoretically, the efficacy of few-shot learning systems as more labeled data becomes available. To this end, we have reported results on a novel meta-test benchmark dataset, FP-k, which contains 5 tasks with 400 training and 20 test examples per task.</p><p>We also showed that the optimal hyperparameter configuration for the test-time update routine may not be the same configuration used during meta-learning. These findings are supported by our theoretical analyses which show that MAML-type algorithms minimize the empirical risk on the training set of a fixed update routine and the initialization θ, but do not natively guarantee that the update routine's hyperparameters are optimal. We suspect that improvements realized by relation networks <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref>, models that learn to generate parameters conditioned on the training data <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4]</ref>, and models with learned learning rates <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref> directly leverage information on how to adapt given a few-shot sample of labeled examples. We also suspect that the previous work in <ref type="bibr" target="#b15">[16]</ref> may have found MAML-type algorithms to overfit when applied to high dimensional parameter spaces due to lack of regularization and lack of an empirical risk minimization of the update routine's hyperparameters. Lastly, we hope that this work draws, what we argue is necessary, attention to the open problem of building learning systems that can unify small and large data regimes by gaining expertise and integrating new information as more data becomes available, much as people do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Meta-Training Details</head><p>For meta-training, we adapted the code in <ref type="bibr" target="#b35">[36]</ref>. We referenced the hyperparameters used in the Reptile meta-training runs for Mini-ImageNet in <ref type="bibr" target="#b14">[15]</ref>. Due to the computational cost of meta-training and the combinatorial expansion in the meta-training hyperparameter search space due to having effectively the same number of hyperparameters for both the outer and inner meta-training loops, we did not exhaustively search over all meta-training hyperparameters. We did initial experimentation on tuning the inner batch size and the number of inner-loop gradient steps by evaluating on the validation tasks T val , though found that fine-tuning these values mattered less than optimizing the test-time hyperparameters. Both Reptile and FOMAML were meta-trained with the hyperparameters shown in <ref type="table" target="#tab_4">Table 3</ref>. The only hyperparameter that we found significantly changed the results between the two approaches was the use of the "train-shots", which is the number of examples that each inner batch can sample from. We found that setting the Reptile train-shots equal to 10, which is the total number of examples per task in the FSS-1000 dataset, significantly reduced test-time performance. By decreasing the train-shots to 5, mIoU increased by ≈ 5% absolute percentage points of mean intersection over union <ref type="bibr">(IoU)</ref>. Similarly, we found that if we sampled D tr and D val with replacement, as opposed to using all 10 train-shots for FOMAML (using 5 for D tr and 5 for Dval), meta-test results increased by ≈ 5% absolute percentage points of mean IoU. We suspect that limiting the number of train shots in this way serves as a form of meta-regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Test-Time Update Hyperparameter Optimization Method Details</head><p>Motivated by the insight discussed in the preliminaries that the loss of a meta-learning algorithm with a fixed initialization θ is a function of the update routine U and its hyperparameters ω, we tune ω to maximize intersection over union (IoU) on out-of-distribution tasks T val . We call this procedure inference hyperparameter optimization (UHO). For UHO, we use Bayesian optimization (BO) with a Gaussian process (GP) estimator of the objective function which is a standard approach for optimizing expensive black box functions <ref type="bibr" target="#b30">[31]</ref>. For the joint-trained and meta-learned initializations evaluated in <ref type="table">Table 2</ref>, we experimented with tuning their learning rate by evaluating 30 parameters on each of the 200 validation tasks. The first half of the parameters were randomly from a log-uniform distribution and the second half were sampled from the posterior of the GP to maximize expected improvement in mean IoU over the validation tasks. For all evaluations, we optimize the learning rate over the interval [0.0005, 0.05].</p><p>Because the effects of the learning rate are intertwined with the number of gradient updates, we also leveraged early stopping to decrease runtime and to estimate the optimal number of gradient steps when adapting to a new task. The use of early stopping in this way is purely an runtime optimization that reduces the search space that is explored when tuning ω. We train each validation set T val task independently and record the optimal number of gradient updates for each task. We then evaluate all tasks in T val at the median number of steps returned by early stopping across tasks in T val . We could have also used the Bayesian optimization with GP prior, but early stopping has the advantage of computational efficiency. Early stopping has been deeply studied with strong empirical and theoretical evidence to support its efficacy as an efficient hyperparameter tuning algorithm <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. To reduce computational expense, we restricted the maximum number of gradient steps to 20 for each task.</p><p>For our largest model, EfficientLab-6-3, we also experimented with using BO on a larger set of hyperparameters and larger number of maximum iterations for early stopping. We search over the learning rate [0.0005, 0.05], the final layer dropout rate [0.2, 0.5], the augmentation rate [0.5, 1.0], and batch size <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b9">10]</ref> with a maximum early stopping iterations of 80.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Example predictions</head><p>We have included in <ref type="figure">Figure 5</ref> a visualization of additional, randomly sampled predictions on test examples D test from test tasks T test that were never seen during meta-training. The failure cases are particularly interesting in that they suggest that a foreground object-ness prior has been learned in the meta-learned initialization.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D FP-k Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix F Proof of Generalization Gap</head><p>The generalization gap is the difference between an estimate of the error of a function on an empirical dataset and the (typically non-computable) error over the true distribution <ref type="bibr" target="#b40">[41]</ref>. We define the generalization gap as the difference between the expected loss a model f incurs over the true distribution p and the loss measured on a datasetp</p><formula xml:id="formula_14">E p L f − Ep L f<label>(17)</label></formula><p>In meta-learning,f is learned on a distribution of examplesq τ sampled from a distribution over tasksp. Thus there is a functionf τ that is learned on eachq τ</p><formula xml:id="formula_15">E p E qτ L f τ − Ep Eq τ L f τ<label>(18)</label></formula><p>Without loss of generality, we can define an update operator U which maps from a training distribution q τ (x, y) and a parameter vector θ to a functionf τ :f</p><formula xml:id="formula_16">τ = U (q τ ; θ)<label>(19)</label></formula><p>To preserve generality, U can be any arbitrary operator that returns a functionf τ . Replacingf τ with U and dropping q τ (x, y) for brevity:</p><formula xml:id="formula_17">E p [E qτ [L (U (θ))]] − Ep [Eq τ [L (U (θ))]]<label>(20)</label></formula><p>We can, further, use a meta-learning algorithm to learn an initializationθ * that we estimate to be optimal on some dataset of tasks T</p><formula xml:id="formula_18">E p E qτ L U (θ * ) − Ep Eq τ L U (θ * ) .<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix G Analysis of Weight Updates</head><p>In this section we present empirical evidence that gradient-based mete-learning algorithms converge to a point in parameter space that is close in expectation to each task τ 's manifold of optimal solutions for τ ∈ T . This builds on the theoretical analysis in Section 5.2 of <ref type="bibr" target="#b14">[15]</ref>. First we compute the Euclidean distance between the entire EfficientLab-3 parameter vectors from an initialization θ to an updated weight vector θ τ after 5 gradient steps on 5 training examples D tr from meta-test tasks τ ∈ T test :</p><formula xml:id="formula_19">d 1 = θ − θ τ 2<label>(22)</label></formula><p>We compute this distance twice on a random train-test split of the 10 examples for all 240 FSS-1000 test tasks, yielding 480 updated weight vector samples for each of the two meta-learned and joint-trained initializations.</p><p>Initialization method E T test  Let v ∈ R n be a weight vector that is a subvector from an initialization θ and u τ ∈ R n be the updated weight vector after gradient steps on examples sampled from q τ . The subvectors v and u τ represent the weight tensors from an EfficientLab block unrolled into a vector. In <ref type="figure" target="#fig_6">Figure 7</ref>, we show the unit-normalized Euclidean distance between EfficientLab blocks, where a block is either the stem convolutional block, a mobile inverted bottleneck convolutional block <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b23">24]</ref>, or our residual skip decoder:</p><formula xml:id="formula_20">d 2 = v v 2 − u τ u τ 2 2<label>(23)</label></formula><p>We also plot the mean absolute difference to get a sense of the absolute distance traveled by individual parameters:</p><formula xml:id="formula_21">d 3 = 1 n n i=1 |v − u τ | i<label>(24)</label></formula><p>As shown in <ref type="figure" target="#fig_6">Figure 7</ref>, we find that the joint-trained initialization travels significantly further when adapted to tasks from T test , even though the same learning rate and number of gradient steps are used at test time for both initializations. This implies that stable minima that produce low error lie closer in expectation over tasks from T test to the meta-learned initialization. Evidence for this interpretation is further found in the test time metrics when evaluating with 5 gradient steps which show that the meta-learned initialization has a pixel-error rate that is 3.6 times smaller. This difference in error rate is found by comparing the joint-trained and FOMAML * methods in <ref type="table">Table 2</ref>. The results in <ref type="figure" target="#fig_6">Figure 7</ref>, show that the adaptation to new tasks for both pre-training methods is non-uniform. The largest relative changes are shown in the final layers due to changes in the directionality of the EfficientLab weight subvectors. In contrast, the largest absolute changes in individual parameter values are found in the early layers of the EfficientLab model. Both initializations demonstrate similar patterns in the distribution of weight updates as a function of block depth but the changes in weights are up to an order of magnitude higher for the joint-trained initialization for all three difference metrics we investigated. The large difference between joint-trained and meta-learned distances is also in line with recent results of <ref type="bibr" target="#b41">[42]</ref> that show that when the knowledge of a model is factorized properly, the expected gradient over parameters when adapting to new tasks will be closer to zero.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Diagram of the computations performed by the EfficientLab-3 neural network. Nodes represent functions and edges represent output tensors. Output spatial resolutions are written next to the output edge. The high level architecture</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Randomly sampled example 5-shot predictions on the test images from test tasks. Positive class prediction is overlaid in red. From left to right, top to bottom, the classes are apple icon, australian terrier, church, motorbike, flying frog, flying snakes, hover board, porcupine</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Update hyperparameter optimization. Mean IoU is shown as a function of the learning rate α and the number of gradient steps over the set of 200 validation tasks T val .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>d 1 ]</head><label>1</label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Left: Average Euclidean distance between initialization and updated weights with 95% confidence interval. Right: Distributions of Euclidean distances between initialization and updated weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Differences between weights at initialization and after 5 gradient steps with a step size of 0.005 on 5 training examples D tr from test tasks T test . The left column shows the Euclidean distance (d 1 ) between EfficientLab-3 block weight vectors before and after training on D tr . Middle column shows Euclidean distance between unit norm weight vectors (d 2 ). Right column shows the mean absolute difference (d 3 ) between individual parameters in an EfficientLab block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>∼330 epochs through the training and validation tasks T tr ∪ T val of the FSS-1000 dataset using a meta-batch size of 5, an inner loop batch size of 8, and 5 inner loop iterations. For reptile, we experiment with setting the train shots to 5 and 10. Note that when setting train shots to 5, this effectively causes the number of epochs through all individual image-pair examples to be cut in half, to ∼165 epochs through the individual examples. During training, we use stochastic gradient descent (SGD) in the inner loop with a fixed learning rate of 0.005. During training and evaluation, we apply simple augmentations to the few-shot examples including random translation, rotation, horizontal flips, additive Gaussian noise, brightness, and random eraser</figDesc><table /><note>evaluate the FOMAML and Reptile meta-learning algorithms on the FSS-1000 dataset. Model topology develop- ment and meta-training hyperparameter search was done on the held out set of validation tasks, T val , and not the final test tasks. For the final evaluations reported in Table 3b, we meta-train for 50,000 meta-iterations, which is</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Meta-training hyperparameters for Reptile and FOMAML algorithms.</figDesc><table><row><cell>Hyperparameter</cell><cell>value</cell></row><row><cell>Meta-batch size</cell><cell>5</cell></row><row><cell>Meta-steps</cell><cell>50000</cell></row><row><cell>Initial meta-learning rate</cell><cell>0.1</cell></row><row><cell>Final meta-learning rate</cell><cell>1.e − 5</cell></row><row><cell>Inner batch size</cell><cell>8</cell></row><row><cell>Inner steps</cell><cell>5</cell></row><row><cell>Inner learning rate</cell><cell>0.005</cell></row><row><cell>Final layer dropout rate</cell><cell>0.2</cell></row><row><cell>Augmentation rate</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Inference hyperparameters returned from BO with GP for initializations of the EfficientLab-3 network. All other hyperparameters were fixed to the values shown inTable 3.</figDesc><table><row><cell>Initialization</cell><cell cols="2">learning rate steps</cell></row><row><cell cols="2">Joint-trained 1-shot 8.156e − 4</cell><cell>10</cell></row><row><cell cols="2">Joint-trained 5-shot 1.364e − 3</cell><cell>17</cell></row><row><cell>FOMAML * 1-shot</cell><cell>1.734e − 3</cell><cell>8</cell></row><row><cell>FOMAML * 5-shot</cell><cell>6.951e − 3</cell><cell>12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Inference hyperparameters returned from BO with GP for the EfficientLab-6-3 network.</figDesc><table><row><cell>Initialization</cell><cell cols="5">learning rate steps dropout rate augmentation rate batch size</cell></row><row><cell>FOMAML * 5-shot</cell><cell>5e − 4</cell><cell>59</cell><cell>0.5</cell><cell>0.5</cell><cell>8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Figure 5 :</head><label>5</label><figDesc>Randomly sampled example 5-shot predictions on the test images from test tasks. Predictions were generated by EfficientLab-6-3 model meta-trained with FOMAML</figDesc><table /><note>* and evaluated with UHO-returned hyperparameters. Positive class prediction is overlaid in red. From left to right, top to bottom, the classes are abes flying fish, australian terrier, flying frog, grey whale, hoverboard, manatee, marimba, porcupine, sealion, spiderman, stingray, tunnel. The final row contains hand-picked failure cases from tasks american chameleon, marimba, motorbike, and porcupine.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6</head><label>6</label><figDesc>contains the five tasks in PASCAL-5 i that have direct analogs in FSS-1000. Each row contains the name of a task in FSS-1000 and PASCAL-5 i , respectively. We combine all examples for synonymous tasks. During evaluation, we simply randomly sample 20 test examples, and sample a training set of k examples over the range:<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref> 50, 100, 200, 400]. For more details on our training and evaluation procedures see E.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 6 :</head><label>6</label><figDesc>PASCAL-5 i tasks with FSS-1000 analog.In this section, we describe our testing protocol evaluating the initializations when adapting to test tasks from the FP-k dataset. For each tuple of (initialization, k-training shots) we randomly sample 20 examples for a test set D test for the task and train on k labeled examples D tr . We repeat this random sampling and training process 4 times for each of the 5 tasks, yielding 20 evaluation samples per (initialization, k-training shots) tuple. For all three initializations, we use UHO for estimating the hyperparameters of U for k &lt; 10. For k ≥ 10, we use a fixed learning rate equal to the value used during meta-training and early stopping to estimate the optimal number of iterations. For early stopping, we use 20% of the examples in D tr to form D val .</figDesc><table><row><cell cols="2">PASCAL-5 i Task FSS-1000 Task</cell></row><row><cell>aeroplane</cell><cell>airliner</cell></row><row><cell>bus</cell><cell>bus</cell></row><row><cell>motorbike</cell><cell>motorbike</cell></row><row><cell>potted_plant</cell><cell>potted_plant</cell></row><row><cell>tvmonitor</cell><cell>television</cell></row><row><cell>Appendix E FP-k Experimental Details</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Not to be confused with the relation networks of<ref type="bibr" target="#b12">[13]</ref>.<ref type="bibr" target="#b2">3</ref> The original MAML and Reptile convolutional neural networks (CNNs) use four convolutional layers with 32 filters each for MiniImagenet<ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Residual connections have been suggested to make the loss landscape of deep neural networks more convex<ref type="bibr" target="#b21">[22]</ref>. If this is the case, it could be especially helpful in finding low-error minima via gradient-based update routines such as those used by MAML, FOMAML, and Reptile.<ref type="bibr" target="#b4">5</ref> As described in<ref type="bibr" target="#b22">[23]</ref> and used in<ref type="bibr" target="#b20">[21]</ref> the dropout layer is applied after all batch norm layers.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">See the supplementary material for more details on the dataset construction.<ref type="bibr" target="#b6">7</ref> The encoder is trained on ImageNet, while the residual skip decoder and final layer weights are initialized in the same way as EfficientNet<ref type="bibr" target="#b20">[21]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">The examples in the PASCAL dataset are known to be more challenging than the FSS-1000 dataset<ref type="bibr" target="#b10">[11]</ref>. From visual inspection of the two datasets, it is also clear that the PASCAL dataset contains more label noise than the FSS-1000 dataset. For these reasons, the mean IoU values shown inFigure 4, which contain examples from both datasets, are not directly comparable to the results shown inTable 3b, which contain examples only from FSS-1000.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Autodeeplab: Hierarchical neural architecture search for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="82" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Meta-sgd: Learning to learn quickly for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.09835</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Irfan Essa, and Byron Boots. One-shot learning for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirreza</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shray</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03410</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Andrei A Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Sygnowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.05960</idno>
		<title level="m">Meta-learning with latent embedding optimization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fayao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5217" to="5226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Is learning the n-th thing any easier than learning the first?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="640" to="646" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fss-1000: A 1000-class dataset for few-shot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Yau Pun Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.12347</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4967" to="4976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Conditional networks for few-shot semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Rakelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alyosha</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.02999</idno>
		<title level="m">Reptile: a scalable metalearning algorithm</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using fast weights to deblur old memories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David C</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Plaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ninth annual conference of the Cognitive Science Society</title>
		<meeting>the ninth annual conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="1987" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using fast weights to attend to the recent past</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4331" to="4339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A baseline for few-shot image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pratik</forename><surname>Guneet S Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Chaudhari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02729</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="801" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing the loss landscape of neural nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6389" to="6399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Understanding the disharmony between dropout and batch normalization by variance shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2682" to="2690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">How to train your maml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09502</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Measures of the amount of ecologic association between species</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee R Dice</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ecology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="297" to="302" />
			<date type="published" when="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Satellite imagery feature detection using deep convolutional neural network: A kaggle competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Iglovikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Mushinskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Osin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.06169</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Random erasing data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Practical bayesian optimization of machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2951" to="2959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep double descent: Where bigger models and more data hurt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preetum</forename><surname>Nakkiran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Kaplun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yamini</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boaz</forename><surname>Barak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02292</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The marginal value of adaptive gradient methods in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ashia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nati</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4148" to="4158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openai</surname></persName>
		</author>
		<ptr target="https://github.com/openai/supervised-reptile" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Early stopping-but when?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lutz</forename><surname>Prechelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="55" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML workshop on unsupervised and transfer learning</title>
		<meeting>ICML workshop on unsupervised and transfer learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press Cambridge</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samet</forename><surname>Oymak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zalan</forename><surname>Fabian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05392</idno>
		<title level="m">Generalization guarantees for neural networks via harnessing the low-rank structure of the jacobian</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">Pack</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05468</idno>
		<title level="m">Generalization in deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A meta-transfer objective for learning to disentangle causal mechanisms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasim</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olexa</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.10912</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
