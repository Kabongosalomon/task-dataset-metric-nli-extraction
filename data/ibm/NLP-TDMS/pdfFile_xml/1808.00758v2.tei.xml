<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Attentional Aggregation of Deep Feature Sets for Multi-view 3D Reconstruction Keywords Robust Attention Model · Deep Learning on Sets · Multi-view 3D Reconstruction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
							<email>s.wang@hw.ac.uk</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Markham</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Trigoni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sen</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Engineering &amp; Physical Sciences, Heriot-Watt University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Attentional Aggregation of Deep Feature Sets for Multi-view 3D Reconstruction Keywords Robust Attention Model · Deep Learning on Sets · Multi-view 3D Reconstruction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of recovering an underlying 3D shape from a set of images. Existing learning based approaches usually resort to recurrent neural nets, e.g., GRU, or intuitive pooling operations, e.g., max/mean poolings, to fuse multiple deep features encoded from input images. However, GRU based approaches are unable to consistently estimate 3D shapes given different permutations of the same set of input images as the recurrent unit is permutation variant. It is also unlikely to refine the 3D shape given more images due to the long-term memory loss of GRU. Commonly used pooling approaches are limited to capturing partial information, e.g., max/mean values, ignoring other valuable features. In this paper, we present a new feedforward neural module, named AttSets, together with a dedicated training algorithm, named FASet, to attentively aggregate an arbitrarily sized deep feature set for multi-view 3D reconstruction. The AttSets module is permutation invariant, computationally efficient and flexible to implement, while the FASet algorithm enables the AttSets based network to be remarkably robust and generalize to an arbitrary number of input images. We thoroughly evaluate FASet and the properties of AttSets on multiple large public datasets. Extensive experiments show that AttSets together with FASet algorithm significantly outperforms existing aggregation approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The problem of recovering a geometric representation of the 3D world given a set of images is classically defined as multi-view 3D reconstruction in computer vision. Traditional pipelines such as Structure from Motion (SfM) <ref type="bibr" target="#b27">(Ozyesil et al., 2017)</ref> and visual Simultaneous Localization and Mapping (vSLAM) <ref type="bibr" target="#b2">(Cadena et al., 2016)</ref> typically rely on hand-crafted feature extraction and matching across multiple views to reconstruct the underlying 3D model. However, if the multiple viewpoints are separated by large baselines, it can be extremely challenging for the feature matching approach due to significant changes of appearance or self occlusions <ref type="bibr" target="#b25">(Lowe, 2004)</ref>. Furthermore, the reconstructed 3D shape is usually a sparse point cloud without geometric details.</p><p>Recently, a number of deep learning approaches, such as 3D-R2N2 <ref type="bibr" target="#b5">(Choy et al., 2016)</ref>, LSM <ref type="bibr" target="#b19">(Kar et al., 2017)</ref>, DeepMVS <ref type="bibr" target="#b13">(Huang et al., 2018)</ref> and RayNet <ref type="bibr" target="#b28">(Paschalidou et al., 2018)</ref> have been proposed to estimate the 3D dense shape from multiple images and have shown encouraging results. Both 3D-R2N2 <ref type="bibr" target="#b5">(Choy et al., 2016)</ref> and LSM <ref type="bibr" target="#b19">(Kar et al., 2017)</ref> formulate multi-view reconstruction as a sequence learning problem, and leverage recurrent neural networks (RNNs), particularly GRU, to fuse the multiple deep features extracted by a shared encoder from input images. However, there are three limitations. First, the recurrent network is permutation variant, i.e., different permutations of the input image sequence give different reconstruction results <ref type="bibr" target="#b38">(Vinyals et al., 2015)</ref>. Therefore, inconsistent 3D shapes are estimated from the same image set with different per- mutations. Second, it is difficult to capture long-term dependencies in the sequence because of the gradient vanishing or exploding <ref type="bibr" target="#b1">(Bengio et al., 1994;</ref><ref type="bibr" target="#b20">Kolen and Kremer, 2001)</ref>, so the estimated 3D shapes are unlikely to be refined even if more images are given during training and testing. Third, the RNN unit is inefficient as each element of the input sequence must be sequentially processed without parallelization <ref type="bibr" target="#b26">(Martin and Cundy, 2018)</ref>, so is time-consuming to generate the final 3D shape given a sequence of images. The recent DeepMVS <ref type="bibr" target="#b13">(Huang et al., 2018)</ref> applies max pooling to aggregate deep features across a set of unordered images for multi-view stereo reconstruction, while RayNet <ref type="bibr" target="#b28">(Paschalidou et al., 2018)</ref> adopts average pooling to aggregate the deep features corresponding to the same voxel from multiple images to recover a dense 3D model. The very recent GQN <ref type="bibr" target="#b8">(Eslami et al., 2018)</ref> uses sum pooling to aggregate an arbitrary number of orderless images for 3D scene representation. Although max, average and summation poolings do not suffer from the above limitations of RNN, they tend to be 'hard attentive', since they only capture the max/mean values or the summation without learning to attentively preserve the useful information. In addition, the above pooling based neural nets are usually optimized with a specific number of input images during training, therefore being not robust and general to a dynamic number of input images during testing. This critical issue is also observed in GQN <ref type="bibr" target="#b8">(Eslami et al., 2018)</ref>.</p><p>In this paper, we introduce a simple yet efficient attentional aggregation module, named AttSets 1 . It can be easily included in an existing multi-view 3D reconstruction network to aggregate an arbitrary number of elements of a deep feature set. Inspired by the attention mechanism which shows great success in natural language processing <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b31">Raffel and Ellis, 2016)</ref>, image captioning <ref type="bibr" target="#b40">(Xu et al., 2015)</ref>, etc., we design a feed-forward neural module that can automatically learn to aggregate each element of the input deep feature set. In particular, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, given a variable sized deep feature set, which 1 Code is available at https://github.com/Yang7879/AttSets are usually learnt view-invariant visual representations from a shared encoder <ref type="bibr" target="#b28">(Paschalidou et al., 2018)</ref>, our AttSets module firstly learns an attention activation for each latent feature through a standard neural layer (e.g., a fully connected layer, a 2D or 3D convolutional layer), after which an attention score is computed for the corresponding feature. Subsequently, the attention scores are simply multiplied by the original elements of the deep feature set, generating a set of weighted features. At last, the weighted features are summed across different elements of the deep feature set, producing a fixed size of aggregated features which are then fed into a decoder to estimate 3D shapes. Basically, this AttSets module can be seen as a natural extension of sum pooling into a "weighted" sum pooling with learnt feature-specific weights. AttSets shares similar concepts with the concurrent work <ref type="bibr" target="#b14">(Ilse et al., 2018)</ref>, but it does not require the additional gating mechanism in <ref type="bibr" target="#b14">(Ilse et al., 2018)</ref>. Notably, our simple feed-forward design allows the attention module to be separately trainable according to the property of its gradients.</p><p>In addition, we propose a new Feature-Attention Separate training (FASet) algorithm that elegantly decouples the base encoder-decoder (to learn deep features) from the AttSets module (to learn attention scores for features). This allows the AttSets module to learn desired attention scores for deep feature sets and guarantees the AttSets based neural networks to be robust and general to dynamic sized deep feature sets. Basically, in the proposed training algorithm, the base encoder-decoder neural layers are only optimized when the number of input images is 1, while the AttSets module is only optimized where there are more than 1 input images. Eventually, the whole optimized AttSets based neural network achieves superior performance with a large number of input images, while simultaneously being extremely robust and able to generalize to a small number of input images, even to a single image in the extreme case. Comparing with the widely used feedforward attention mechanisms for visual recognition <ref type="bibr" target="#b18">(Jie Hu et al., 2018;</ref><ref type="bibr" target="#b33">Rodríguez et al., 2018;</ref><ref type="bibr" target="#b24">Liu et al., 2018;</ref><ref type="bibr" target="#b34">Sarafianos et al., 2018;</ref><ref type="bibr" target="#b11">Girdhar and Ramanan, 2017)</ref>, our FASet algorithm is the first to investigate and improve the robustness of attention modules to dynamically sized input feature sets, whilst existing works are only applicable to fixed sized input data.</p><p>Overall, our novel AttSets module and FASet algorithm are distinguished from all existing aggregation approaches in three ways. 1) Compared with RNN approaches, AttSets is permutation invariant and computationally efficient. 2) Compared with the widely used pooling operations, AttSets learns to attentively select and weight important deep features, thereby being more effective to aggregate useful information for better 3D reconstruction. 3) Compared with existing visual attention mechanisms, our FASet algorithm enables the whole network to be general to variable sized sets, being more robust and suitable for realistic multi-view 3D reconstruction scenarios where the number of input images usually varies dramatically.</p><p>Our key contributions are:</p><p>-We propose an efficient feed-forward attention module, AttSets, to effectively aggregate deep feature sets. Our design allows the attention module to be separately optimizable according to the property of its gradients. -We propose a new two-stage training algorithm, FASet, to decouple the base encoder/decoder and the attention module, guaranteeing the whole network to be robust and general to an arbitrary number of input images. -We conduct extensive experiments on multiple public datasets, demonstrating consistent improvement over existing aggregation approaches for 3D object reconstruction from either single or multiple views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>(1) Multi-view 3D Reconstruction. 3D shapes can be recovered from multiple color images or depth scans. To estimate the underlying 3D shape from multiple color images, classic SfM <ref type="bibr" target="#b27">(Ozyesil et al., 2017)</ref> and vSLAM <ref type="bibr" target="#b2">(Cadena et al., 2016)</ref> algorithms firstly extract and match hand-crafted geometric features <ref type="bibr" target="#b12">(Hartley and Zisserman, 2004)</ref> and then apply bundle adjustment <ref type="bibr" target="#b37">(Triggs et al., 1999)</ref> for both shape and camera motion estimation. Ji et al. <ref type="bibr" target="#b17">(Ji et al., 2017b)</ref> use "maximizing rigidity" for reconstruction, but this requires 2D point correspondences across images. Recent deep neural net based approaches tend to recover dense 3D shapes through learnt features from multiple images and achieve compelling results. To fuse the deep features from multiple images, both 3D-R2N2 <ref type="bibr" target="#b5">(Choy et al., 2016)</ref> and LSM <ref type="bibr" target="#b19">(Kar et al., 2017)</ref> apply the recurrent unit GRU, resulting in the networks being permutation variant and inefficient for aggregating long sequence of images. Recent SilNet <ref type="bibr">(Wiles and</ref><ref type="bibr">Zisserman, 2017, 2018)</ref> and DeepMVS <ref type="bibr" target="#b13">(Huang et al., 2018)</ref> simply use max pooling to preserve the first order information of multiple images, while RayNet <ref type="bibr" target="#b28">(Paschalidou et al., 2018)</ref> applies average pooling to reserve the first moment information of multiple deep features. MVSNet <ref type="bibr" target="#b43">(Yao et al., 2018)</ref> proposes a variance-based approach to capture the second moment information for multiple feature aggregation. These pooling techniques only capture partial information, ignoring the majority of the deep features. Recent SurfaceNet <ref type="bibr" target="#b16">(Ji et al., 2017a)</ref> and SuperPixel Soup <ref type="bibr">(Kumar et al., 2017)</ref> can reconstruct 3D shapes from two images, but they are unable to process an arbitrary number of images. As for multiple depth image reconstruction, the traditional volumetric fusion method <ref type="bibr" target="#b6">(Curless and Levoy, 1996;</ref><ref type="bibr" target="#b3">Cao et al., 2018)</ref> integrates multiple viewpoint information by averaging truncated signed distance functions (TSDF). Recent learning based OctNetFusion <ref type="bibr" target="#b32">(Riegler et al., 2017)</ref> also adopts a similar strategy to integrate multiple depth information. However, this integration might result in information loss since TSDF values are averaged <ref type="bibr" target="#b32">(Riegler et al., 2017)</ref>. PSDF <ref type="bibr" target="#b7">(Dong et al., 2018)</ref> is recently proposed to learn a probabilistic distribution through Bayesian updating in order to fuse multiple depth images, but it is not straightforward to include the module into existing encoder-decoder networks.</p><p>(2) Deep Learning on Sets. In contrast to traditional approaches operating on fixed dimensional vectors or matrices, deep learning tasks defined on sets usually require learning functions to be permutation invariant and able to process an arbitrary number of elements in a set <ref type="bibr" target="#b46">(Zaheer et al., 2017)</ref>. Such problems are widespread. Zaheer et al. introduce general permutation invariant and equivariant models in <ref type="bibr" target="#b46">(Zaheer et al., 2017)</ref>, and they end up with a sum pooling for permutation invariant tasks such as population statistics estimation and point cloud classification. In the very recent GQN <ref type="bibr" target="#b8">(Eslami et al., 2018)</ref>, sum pooling is also used to aggregate an arbitrary number of orderless images for 3D scene representation. <ref type="bibr" target="#b10">Gardner et al. (Gardner et al., 2017)</ref> use average pooling to integrate an unordered deep feature set for classification task. <ref type="bibr" target="#b35">Su et al. (Su et al., 2015)</ref> use max pooling to fuse the deep feature set of multiple views for 3D shape recognition. Similarly, PointNet <ref type="bibr" target="#b30">(Qi et al., 2017)</ref> also uses max pooling to aggregate the set of features learnt from point clouds for 3D classification and segmentation. In addition, the higher-order statistics based pooling approaches are widely used for 3D object recognition from multiple images. Vanilla bilinear pooling is applied for fine-grained recognition in <ref type="bibr" target="#b22">(Lin et al., 2015)</ref> and is further improved in (Lin and Maji, 2017). Concurrently, log-covariance pooling is proposed in <ref type="bibr" target="#b15">(Ionescu et al., 2015)</ref>, and is recently generalized by harmonized bilinear pooling in . Bilinear pooling techniques are further improved in the recent work <ref type="bibr" target="#b44">(Yu and Salzmann, 2018;</ref><ref type="bibr" target="#b23">Lin et al., 2018)</ref>. However, both first-order and higher-order pooling operations ignore a majority of the information of a set. In addition, the first-order poolings do not have trainable parameters, while the higher-order poolings have only few parameters available for the network to learn. These limitations lead to the pooling based neural networks to be optimized with regards to the specific statistics of data batches during training, and therefore unable to be robust and generalize well to variable sized deep feature sets during testing.</p><p>(3) Attention Mechanism. The attention mechanism was originally proposed for natural language processing <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>. Being coupled with RNNs, it achieves compelling results in neural machine translation <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref>, image captioning <ref type="bibr" target="#b40">(Xu et al., 2015)</ref>, image question answering <ref type="bibr" target="#b42">(Yang et al., 2016)</ref>, etc. However, all these coupled attention approaches are permutation variant and computationally time-consuming. Dispensing with recurrence and convolutions entirely and solely relying on attention mechanism, Transformer (Vaswani et al., 2017) achieves superior performance in machine translation tasks. Similarly, being decoupled with RNNs, attention mechanisms are also applied for visual recognition (Jie <ref type="bibr" target="#b33">Rodríguez et al., 2018;</ref><ref type="bibr" target="#b24">Liu et al., 2018;</ref><ref type="bibr" target="#b34">Sarafianos et al., 2018;</ref><ref type="bibr" target="#b48">Zhu et al., 2018;</ref><ref type="bibr">Nakka and Salzmann, 2018;</ref><ref type="bibr" target="#b11">Girdhar and Ramanan, 2017)</ref>, semantic segmentation , long sequence learning <ref type="bibr" target="#b31">(Raffel and Ellis, 2016)</ref>, and image generation . Although the above decoupled attention modules can be used to aggregate variable sized deep feature sets, they are literally designed to operate on fixed sized features for tasks such as image recognition and generation. The robustness of attention modules regarding dynamic deep feature sets has not been investigated yet.</p><p>Compared with the original attention mechanism, our AttSets does not couple with RNNs. Instead, AttSets is a simplified feed-forward module which shares similar concepts with the concurrent work <ref type="bibr" target="#b14">(Ilse et al., 2018)</ref>. However, our AttSets is much simpler, without requiring the additional gating mechanism in <ref type="bibr" target="#b14">(Ilse et al., 2018)</ref>. Besides, we further propose a dedicated FASet algorithm, enabling the AttSets based network to be remarkably robust and general to arbitrarily sized deep sets. This algorithm is the first to investigate and improve the robustness of feed-forward attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">AttSets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>This paper considers the problem of aggregating an arbitrary number of elements of a set A into a fixed single output y. Each element of set A is a feature vector extracted from a shared encoder, and the fixed dimension output y is fed into a subsequent decoder, such that the whole network can process an arbitrary number of input elements.</p><p>Given N elements in the input deep feature set A = {x 1 , x 2 , · · · , x N }, x n ∈ R 1×D , where N is an arbitrary value, while D is fixed for a specific encoder, and the output y ∈ R 1×D , which is then fed into the subsequent decoder, our task is to design an aggregation function f with learnable weights W : y = f (A, W ), which should be permutation invariant, i.e., for any permutation π:</p><formula xml:id="formula_0">f ({x 1 , · · · , x N }, W ) = f ({x π(1) , · · · , x π(N ) }, W ) (1)</formula><p>The common pooling operations, e.g., max/mean/sum, are the simplest instantiations of function f where W ∈ ∅. However, these pooling operations are predefined to capture partial information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">AttSets Module</head><p>The basic idea of our AttSets module is to learn an attention score for each latent feature of the whole deep feature set. In this paper, each latent feature refers to each entry of an individual element of the feature set, with an individual element usually represented by a latent vector, i.e., x n . The learnt scores can be regarded as a mask that automatically selects useful latent features across the set. The selected features are then summed across multiple elements of the set.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, given a set of features A = {x 1 , x 2 , · · · , x N }, x n ∈ R 1×D , AttSets aims to fuse it into a fixed dimensional output y, where y ∈ R 1×D .</p><p>To build the AttSets module, we first feed each element of the feature set A into a shared function g which can be a standard neural layer, i.e., a linear transformation layer without any non-linear activation functions. Here we use a fully connected layer as an example, the bias term is dropped for simplicity. The output of function g is a set of learnt attention activations C = {c 1 , c 2 , · · · , c N }, where</p><formula xml:id="formula_1">c n = g(x n , W ) = x n W , (x n ∈ R 1×D , W ∈ R D×D , c n ∈ R 1×D )<label>(2)</label></formula><p>Secondly, the learnt attention activations are normalized across the N elements of the set, computing a set of attention scores S = {s 1 , s 2 , · · · , s N }. We choose sof tmax as the normalization operation, so the attention scores for the n th feature element are s n = [s 1 n , s 2 n , · · · , s d n , · · · , s D n ],</p><formula xml:id="formula_2">s d n = e c d n N j=1 e c d j , c d n , c d j are the d th entry of c n , c j .<label>(3)</label></formula><p>Thirdly, the computed attention scores S are multiplied by their corresponding original feature set A, generating a new set of deep features, denoted as weighted</p><formula xml:id="formula_3">features O = {o 1 , o 2 , · · · , o N }, where o n = x n * s n<label>(4)</label></formula><p>Lastly, the set of weighted features O are summed up across the total N elements to get a fixed size feature vector, denoted as y, where y = [y 1 , y 2 , · · · , y d , · · · , y D ],</p><formula xml:id="formula_4">y d = N n=1 o d n , o d n is the d th entry of o n .<label>(5)</label></formula><p>In the above formulation, we show how AttSets gradually aggregates a set of N feature vectors A into a single vector y, where y ∈ R 1×D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Permutation Invariance</head><p>The output of AttSets module y is permutation invariant with regard to the input deep feature set A. Here is the simple proof.</p><formula xml:id="formula_5">[y 1 , · · · y d · · · , y D ] = f ({x 1 , · · · x n · · · , x N }, W )<label>(6)</label></formula><p>In Equation 6, the d th entry of the output y is computed as follows:</p><formula xml:id="formula_6">y d = N n=1 o d n = N n=1 (x d n * s d n ) = N n=1 x d n * e c d n N j=1 e c d j = N n=1 x d n * e (xnw d ) N j=1 e (xj w d ) = N n=1 x d n * e (xnw d ) N j=1 e (xj w d ) ,<label>(7)</label></formula><p>where w d is the d th column of the weights W . In above Equation 7, both the denominator and numerator are a summation of a permutation equivariant term. Therefore the value y d , and also the full vector y, is invariant to different permutations of the deep feature set A = {x 1 , x 2 , · · · , x n , · · · , x N } <ref type="bibr" target="#b46">(Zaheer et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Implementation</head><p>In Section 3.2, we described how our AttSets aggregates an arbitrary number of vector features into a single vector, where the attention activation learning function g embeds a fully connected (f c) layer. AttSets can also be easily implemented with both 2D and 3D convolutional neural layers to aggregate both 2D and 3D deep feature sets, thus being flexible to be included into a 2D encoder/decoder or 3D encoder/decoder. Particularly, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>, to aggregate a set of 2D features, i.e., a tensor of (width × height × channels), the attention activation learning function g embeds a standard conv2d layer with a stride of (1 × 1). Similarly, to fuse a set of 3D features, i.e., a tensor of (width × height × depth × channels), the function g embeds a standard conv3d layer with a stride of (1 × 1 × 1). For the above conv2d/conv3d layer, the filter size can be 1, 3 or many. The larger the filter size, the learnt attention score is considered to be correlated with the larger local spatial area. Instead of embedding a single neural layer, the function g is also flexible to include multiple layers, but the tensor shape of the output of function g is required to be consistent with the input element x n . This guarantees each individual feature of the input set A will be associated with a learnt and unique weight. For example, a standard 2-layer or 3-layer ResNet module (He et al., 2016) could be a candidate of the function g. The more layers that g embeds, the capability of AttSets module is expected to increase accordingly.</p><p>Compared with f c enabled AttSets, the conv2d or conv3d based AttSets variants tend to have fewer learnable parameters. Note that both the conv2d and conv3d based AttSets are still permutation invariant, as the function g is shared across all elements of the deep feature set and it does not depend on the order of the elements <ref type="bibr" target="#b46">(Zaheer et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">FASet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Motivation</head><p>Our AttSets module can be included in an existing encoder-decoder multi-view 3D reconstruction network, replacing the RNN units or pooling operations. Basically, in an AttSets enabled encoder-decoder net, the encoderdecoder serves as the base architecture to learn visual features for shape estimation, while the AttSets module learns to assign different attention scores to combine those features. As such, the base network tends to have robustness and generality with regard to different input image content, while the AttSets module tends to be general regarding an arbitrary number of input images.</p><p>However, to achieve this robustness is not straightforward. The standard end-to-end joint optimization approach is unable to guarantee that the base encoderdecoder and AttSets are able to learn visual features and the corresponding scores separately, because there are no explicit feature score labels available to directly supervise the AttSets module.</p><p>Let us revisit the previous Equation 7 as follows and draw insights from it.</p><formula xml:id="formula_7">y d = N n=1 x d n * e (xnw d ) N j=1 e (xj w d )<label>(8)</label></formula><p>where N is the size of an arbitrary input set and w d are the AttSets parameters to be optimized. If N is 1, then the equation can be simplified as</p><formula xml:id="formula_8">y d = x d n (9) ∂y d ∂x d n = 1, ∂y d ∂w d = 0, N = 1<label>(10)</label></formula><p>This shows that all parameters, i.e., w d , of the AttSets module are not going to be optimized when the size of the input feature set is 1. However, if N &gt; 1, Equation 8 is unable to be simplified to Equation 9. Therefore,</p><formula xml:id="formula_9">∂y d ∂x d n = 1, ∂y d ∂w d = 0, N &gt; 1<label>(11)</label></formula><p>This shows that both the parameters of AttSets and the base encoder-decoder layers will be optimized simultaneously, if the whole network is trained in the standard end-to-end fashion.</p><p>Here arises the critical issue. When N &gt; 1, all derivatives of the parameters in the encoder are different from the derivatives when N = 1 due to the chain rule of differentiation applied backwards from ∂y d ∂x d n . Put simply, the derivatives of encoder are N -dependent. As a consequence, the encoded visual features and the learnt attention scores would be N -biased if the whole network is jointly trained. This biased network is unable to generalize to an arbitrary value of N during testing.</p><p>To illustrate the above issue, assuming the base encoder-decoder and the AttSets module are jointly trained given 5 images to reconstruct every object, the base encoder will be eventually optimized towards 5view object reconstruction during training. The trained network can indeed perform well given 5 views during testing, but it is unable to predict a satisfactory object shape given only 1 image.</p><p>To alleviate the above problem, a naive approach is to enumerate various values of N during the jointly training, such that the final optimized network can be somehow robust and general to arbitrary N during testing. However, this approach would inevitably optimize the encoder to learn the mean features of input data for varying N . The overall performance will hence not be optimal. In addition, it is impractical and also timeconsuming to enumerate all values of N during training.</p><p>Algorithm 1 Feature-Attention Separate training of an AttSets enabled network. M is batch size, N is image number.</p><p>Stage 1: for number of training iterations do</p><p>• Sample M sets of images {I 1 , · · · , I m , · · · , I M } and sample N images for each set, i.e.,</p><formula xml:id="formula_10">I m = {i 1 m , · · · , i n m , · · · , i N m }. Sample M 3D shape labels {v 1 , · · · , v m , · · · , v M }.</formula><p>• Update the base network by ascending its stochastic gradient:</p><formula xml:id="formula_11">∇ Θ base 1 M N M m=1 N n=1 [ (v n m , v m )] , wherev n m is the estimated 3D shape of single image {i n m }.</formula><p>Stage 2: for number of training iterations do • Sample M sets of images {I 1 , · · · , I m , · · · , I M } and sample N images for each set, i.e.,</p><formula xml:id="formula_12">I m = {i 1 m , · · · , i n m , · · · , i N m }. Sample M 3D shape labels {v 1 , · · · , v m , · · · , v M }.</formula><p>• Update the AttSets module by ascending its stochastic gradient:</p><formula xml:id="formula_13">∇ Θ att 1 M M m=1 [ (v m , v m )]</formula><p>, wherev m is the estimated 3D shape of the image set I m .</p><p>The gradient-based updates can use any gradient optimization algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Algorithm</head><p>To resolve the critical issue discussed in Section 4.1, we propose a Feature-Attention Separate training (FASet) algorithm that decouples the base encoderdecoder and the AttSets module, enabling the base encoder-decoder to learn robust deep features and the AttSets module to learn the desired attention scores for the feature sets.</p><p>In particular, the base encoder-decoder neural layers are only optimized when the number of input images is 1, while the AttSets module is only optimized where there are more than 1 input images. In this regard, the parameters of the base encoding layers would have consistent derivatives in the whole training stage, thus being optimized steadily. In the meantime, the AttSets module would be optimized solely based on multiple elements of learnt visual features from the shared encoder.</p><p>The trainable parameters of the base encoder-decoder are denoted as Θ base , and the trainable parameters of AttSets module are denoted as Θ att , and the loss function of the whole network is represented by which is determined by the specific supervision signal of the base network. Our FASet is shown in Algorithm 1. It can be seen that Θ base and Θ att are completely decoupled from one another, thus being separately optimized in two stages. In stage 1, the Θ base is firstly well optimized until convergence, which guarantees the base encoder-decoder is able to learn robust and general visual features. In stage 2, the Θ att is optimized to learn attention scores for individual visual features. Basically, this module learns to select and weight important deep features automatically. In FASet algorithm, once the Θ base is well optimized in stage 1, it is not necessary to train it again, since the two-stage algorithm guarantees that optimizing Θ base is agnostic to the attention module. The FASet algorithm is a crucial component to maintain the superior robustness of the AttSets module, as shown in Section 5.9. Without it, the feed-forward attention mechanism is ineffective with respect to dynamic input sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>Base Networks. To evaluate the performance and various properties of AttSets, we choose the encoderdecoders of 3D-R2N2 <ref type="bibr" target="#b5">(Choy et al., 2016)</ref> and SilNet (Wiles and Zisserman, 2017) as two base networks.</p><p>-Encoder-decoder of 3D-R2N2. The original 3D-R2N2 consists of (1) a shared ResNet-based 2D encoder which encodes a size of 127 × 127 × 3 images into 1024 dimensional latent vectors, (2) a GRU module which fuses N 1024 dimensional latent vectors into a single 4 × 4 × 4 × 128 tensor, and (3) a ResNet-based 3D decoder which decodes the single tensor into a 32×32×32 voxel grid representing the 3D shape. <ref type="figure">Figure  4</ref> shows the architecture of AttSets based multi-view 3D reconstruction network where the only difference is that the original GRU module is replaced by AttSets in the middle. This network is called Base r2n2 -AttSets. -Encoder-decoder of SilNet. The original SilNet consists of (1) a shared 2D encoder which encodes a size of 127 × 127 × 3 images together with image viewing angles into 160 dimensional latent vectors, (2) a max pooling module which aggregates N latent vectors into a single vector, and (3) a 2D decoder which estimates an object silhouette (57 × 57) from the single latent vector and a new viewing angle. Instead of being explicitly supervised by 3D shape labels, SilNet aims to implicitly learn a 3D shape representation from multiple images via the supervision of 2D silhouettes. <ref type="figure">Figure  5</ref> shows the architecture of AttSets based SilNet where the only difference is that the original max pooling is replaced by AttSets in the middle. This network is called Base silnet -AttSets.</p><p>Competing Approaches. We compare our AttSets and FASet with three groups of competing approaches. Note that all the following competing approaches are connected at the same location of the base encoderdecoder shown in the pink block of <ref type="figure">Figure 4</ref> and 5, with the same network configurations and training/testing settings.</p><p>-RNNs. The original 3D-R2N2 makes use of the GRU <ref type="bibr" target="#b5">(Choy et al., 2016;</ref><ref type="bibr" target="#b19">Kar et al., 2017)</ref> unit for feature aggregation and serves as a solid baseline. -First-order poolings. The widely used max/mean/ sum pooling operations <ref type="bibr" target="#b13">(Huang et al., 2018;</ref><ref type="bibr" target="#b28">Paschalidou et al., 2018;</ref><ref type="bibr" target="#b8">Eslami et al., 2018)</ref> are all implemented for comparison. -Higher-order poolings. We also compare with the stateof-the-art higher-order pooling approaches, including bilinear pooling (BP) <ref type="bibr" target="#b22">(Lin et al., 2015)</ref>, and the very recent MHBN  and SMSO poolings <ref type="bibr" target="#b44">(Yu and Salzmann, 2018)</ref>.</p><p>Datasets. All approaches are evaluated on four large open datasets.</p><p>-ShapeNet r2n2 Dataset <ref type="bibr" target="#b5">(Choy et al., 2016)</ref>. The released 3D-R2N2 dataset consists of 13 categories of 43, 783 common objects with synthesized RGB images from the large scale ShapeNet 3D repository <ref type="bibr" target="#b4">(Chang et al., 2015)</ref>. For each 3D object, 24 images are rendered from different viewing angles circling around each object. The train/test dataset split is 0.8 : 0.2.</p><p>-ShapeNet lsm Dataset <ref type="bibr" target="#b19">(Kar et al., 2017)</ref>. Both LSM and 3D-R2N2 datasets are generated from the same 3D ShapeNet repository <ref type="bibr" target="#b4">(Chang et al., 2015)</ref>, i.e., they have the same ground truth labels regarding the same object. However, the ShapeNet lsm dataset has totally different camera viewing angles and lighting sources for the rendered RGB images. Therefore, we use the ShapeNet lsm dataset to evaluate the robustness and generality of all approaches. All images of ShapeNet lsm dataset are resized from 224×224 to 127×127 through linear interpolation. -ModelNet40 Dataset. ModelNet40 <ref type="bibr" target="#b39">(Wu et al., 2015)</ref> consists of 12, 311 objects belonging to 40 categories. The 3D models are split into 9, 843 training samples and 2, 468 testing samples. For each 3D model, it is voxelized into a 30 × 30 × 30 shape in <ref type="bibr" target="#b29">(Qi et al., 2016)</ref>, and 12 RGB images are rendered from different viewing angles. All 3D shapes are zero-padded to be 32 × 32 × 32, and the images are linearly resized from 224 × 224 to 127 × 127 for training and testing. -Blobby Dataset (Wiles and Zisserman, 2017). It contains 11, 706 blobby objects. Each object has 5 RGB images paired with viewing angles and the corresponding silhouettes, which are generated from Cycles in Blender under different lighting sources and texture models.</p><p>Metrics. The explicit 3D voxel reconstruction performance of Base r2n2 -AttSets and the competing approaches is evaluated on three datasets: ShapeNet r2n2 , ShapeNet lsm and ModelNet40. We use the mean Intersectionover-Union (IoU) <ref type="bibr" target="#b5">(Choy et al., 2016)</ref> between predicted 3D voxel grids and their ground truth as the metric. The IoU for an individual voxel grid is formally defined as follows:</p><formula xml:id="formula_14">IoU = L i=1 I(h i &gt; p) * I(h i ) L i=1 I I(h i &gt; p) + I(h i )</formula><p>where I(·) is an indicator function, h i is the predicted value for the i th voxel,h i is the corresponding ground truth, p is the threshold for voxelization, L is the total number of voxels in a whole voxel grid. As there is no validation split in the above three datasets, to calculate the IoU scores, we independently search the optimal binarization threshold value from 0.2 ∼ 0.8 with a step 0.05 for all approaches for fair comparison. In our experiments, we found that all optimal thresholds of different approaches end up with 0.3 or 0.35.</p><p>The implicit 3D shape learning performance of Base silnet -AttSets and the competing approaches is evaluated on the Blobby dataset. The mean IoU between predicted 2D silhouettes and the ground truth is used as the metric (Wiles and Zisserman, 2017). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Evaluation on ShapeNet r2n2 Dataset</head><p>To fully evaluate the aggregation performance and robustness, we train the Base r2n2 -AttSets and its competing approaches on ShapeNet r2n2 dataset. For fair comparison, all networks (the pooling/GRU/AttSets based approaches) are trained according to the proposed two-stage training algorithm.</p><p>Training Stage 1. All networks are trained given only 1 image for each object, i.e., N = 1 in all training iterations, until convergence. Basically, this is to guarantee all networks are well optimized for the extreme case where there is only one input image.</p><p>Training Stage 2. To enable these networks to be more robust for multiple input images, all networks are further trained given more images per object. Particularly, we conduct the following five parallel groups of training experiments.</p><p>-Group 1. All networks are further trained given only 2 images for each object, i.e., N = 2 in all iterations. As to our Base r2n2 -AttSets, the well-trained encoderdecoder in previous stage 1 is frozen, and we only optimize the AttSets module according to our FASet algorithm 1. As to the competing approaches, e.g., GRU and all poolings, we turn to fine-tune the whole networks because they do not have separate parameters suitable for special training. To be specific, we use smaller learning rate (1e-5) to carefully train these networks to achieve better performance where N = 2 until convergence. -Group 2/3/4. Similarly, in these three groups of secondstage training experiments, N is set to be 8, 16, 24 separately. -Group 5. All networks are further trained until convergence, but N is uniformly and randomly sampled from <ref type="bibr">[1,</ref><ref type="bibr">24]</ref> for each object during training. In the above  Group 1/2/3/4, N is fixed for each object, while N is dynamic for each object in this Group 5.</p><p>The above experiment Groups 1/2/3/4 are designed to investigate how all competing approaches would be further optimized towards the statistics of a fixed N during training, thus resulting in different level of robustness given an arbitrary number of N during testing. By contrast, the paradigm in Group 5 aims at enumerating all possible N values during training. Therefore the overall performance might be more robust regarding an arbitrary number of input images during testing, compared with the above Group 1/2/3/4 experiments.</p><p>Testing Stage. All networks trained in above five groups of experiments are separately tested given N = <ref type="bibr">{1, 2, 3, 4, 5, 8, 12, 16, 20, 24}</ref>. The permutations of input images are the same for all different approaches for fair comparison. Note that, we do not test the networks which are only trained in Stage 1, because the AttSets module is not optimized and the corresponding Base r2n2 -AttSets is unable to generalize to multiple input images during testing. Therefore, it is meaningless to compare the performance when the network is solely trained on a single image.</p><p>Results. Tables 1 ∼ 5 show the mean IoU scores of all 13 categories for experiments of Group 1 ∼ 5, while <ref type="figure" target="#fig_0">Figures 6 ∼ 10</ref> show the trends of mean IoU changes in different Groups. <ref type="figure" target="#fig_0">Figure 11</ref> shows the estimated 3D shapes in experiment Group 5, with an increasing number of images from 1 to 5 for different approaches.</p><p>We notice that the reported IoU scores of ShapeNet data repository in original LSM <ref type="bibr" target="#b19">(Kar et al., 2017)</ref> are higher than our scores. However, the experimental settings in LSM <ref type="bibr" target="#b19">(Kar et al., 2017)</ref>    color images and different train/test splits compared with our experimental settings. Therefore the reported IoU scores in LSM are not directly comparable with ours and we do not include the results in this paper to avoid confusion. Note that, the aggregation module of LSM <ref type="bibr" target="#b19">(Kar et al., 2017)</ref>, i.e., GRU, is the same as used in 3D-R2N2 <ref type="bibr" target="#b5">(Choy et al., 2016)</ref>, and is indeed fully evaluated throughout our experiments.</p><p>To highlight the performance of single view 3D reconstruction, <ref type="table" target="#tab_2">Table 6</ref> shows the optimal per-category IoU scores for different competing approaches from experiments Group 1 ∼ 5. In addition, we also compare with the state-of-the-art dedicated single view reconstruction approaches including OGN <ref type="bibr" target="#b36">(Tatarchenko et al., 2017)</ref>, AORM  and PointSet <ref type="bibr" target="#b9">(Fan et al., 2017)</ref> in <ref type="table" target="#tab_2">Table 6</ref>. Overall, our AttSets based approach outperforms all others by a large margin for either single view or multi view reconstruction, and generates much more compelling 3D shapes.</p><p>Analysis. We investigate the results as follows:</p><p>-The GRU based approach can generate reasonable 3D shapes in all experiments Group 1 ∼ 5 given either few or multiple images during testing, but the performance saturates quickly after being given more images, e.g., 8 views, because the recurrent unit is hardly able to capture features from longer image sequences, as illustrated in <ref type="figure">Figure 9</ref> 1 . -In Group 1 ∼ 4, all pooling based approaches are able to estimate satisfactory 3D shapes when given a similar number of images as given in training, but they are unlikely to predict reasonable shapes given an arbitrary number of images. For example, in experiment Group 4, all pooling based approaches have inferior IoU scores given only few images as shown in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AttSets (Ours)</head><p>Ground Truth <ref type="figure" target="#fig_0">Fig. 12</ref> Qualitative results of multi-view reconstruction from different approaches in ShapeNet lsm testing split.  <ref type="table" target="#tab_1">Table 4</ref> and <ref type="figure">Figure 9</ref> 2 , because the pooled features from fewer images during testing are unlikely to be as general and representative as pooled features from more images during training. Therefore, those models trained on 24 images fail to generalize well to only one image during testing. -In Group 5, as shown in <ref type="table">Table 5</ref> and <ref type="figure" target="#fig_0">Figure 10</ref>, all pooling based approaches are much more robust compared with Group 1∼4, because the networks are generally optimized according to an arbitrary number of images during training. However, these networks tend to have the performance in the middle. Compared with Group 4, all approaches in Group 5 tend to have better performance when N = 1, while being worse when N = 24. Compared with Group 1, all approaches in Group 5 are likely to be better when N = 24, while being worse when N = 1. Basically, these networks tend to be optimized to learn the mean features overall.</p><p>-In all experiments Group 1 ∼ 5, all approaches tend to have better performance when given enough input images, i.e., N = 24, because more images are able to provide enough information for reconstruction. -In all experiments Group 1 ∼ 5, our AttSets based approach clearly outperforms all others in either single or multiple view 3D reconstruction and it is more robust to a variable number of input images. Our FASet algorithm completely decouples the base network to learn visual features for accurate single view reconstruction as illustrated in <ref type="figure">Figure 9</ref> 3 , while the trainable parameters of AttSets module are separately responsible for learning attention scores for better multi-view reconstruction as shown in <ref type="figure">Figure 9</ref> 4 . Therefore, the whole network does not suffer from limitations of GRU or pooling approaches, and can achieve better performance for either fewer or more image reconstruction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Evaluation on ShapeNet lsm Dataset</head><p>To further investigate how well the learnt visual features and attention scores generalize across different style of images, we use the well trained networks of previous Group 5 of Section 5.1 to test on the large ShapeNet lsm dataset. Note that, we only borrow the synthesized images from ShapeNet lsm dataset corresponding to the objects in ShapeNet r2n2 testing split. This guarantees that all the trained models have never seen either the style of LSM rendered images or the 3D object labels before. The image viewing angles from the original ShapeNet lsm dataset are not used in our experiments, since the Base r2n2 network does not require image viewing angles as input. <ref type="table" target="#tab_3">Table 7</ref> shows the mean IoU scores of all approaches, while <ref type="figure" target="#fig_0">Figure 12</ref> shows the qualitative results. Our AttSets based approach outperforms all others given either few or multiple input images. This demonstrates that our Base r2n2 -AttSets approach does not overfit the training data, but has better generality and robustness over new styles of rendered color images compared with other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation on ModelNet40 Dataset</head><p>We train the Base r2n2 -AttSets and its competing approaches on ModelNet40 dataset from scratch. For fair comparison, all networks (the pooling/GRU/AttSets based approaches) are trained according to the pro-posed FASet algorithm, which is similar to the two-stage training strategy of Section 5.1.</p><p>Training Stage 1. All networks are trained given only 1 image for each object, i.e., N = 1 in all training iterations, until convergence. This guarantees all networks are well optimized for single view 3D reconstruction.</p><p>Training Stage 2. We further conduct the following two parallel groups of training experiments to optimize the networks for multi-view reconstruction.</p><p>-Group 1. All networks are further trained given all 12 images for each object, i.e., N = 12 in all iterations, until convergence. As to our Base r2n2 -AttSets, the welltrained encoder-decoder in previous Stage 1 is frozen, and only the AttSets module is trained. All other competing approaches are fine-tuned using smaller learning rate (1e-5) in this stage. -Group 2. All networks are further trained until convergence, but N is uniformly and randomly sampled from <ref type="bibr">[1,</ref><ref type="bibr">12]</ref> for each object during training. Only the AttSets module is trained, while all other competing approaches are fine-tuned in this Stage 2.</p><p>Testing Stage. All networks trained in above two groups are separately tested given N = <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4,</ref><ref type="bibr">5,</ref><ref type="bibr">8,</ref><ref type="bibr">12]</ref>. The permutations of input images are the same for all different approaches for fair comparison.</p><p>Results <ref type="table" target="#tab_4">. Tables 8 and 9</ref> show the mean IoU scores of Groups 1 and 2 respectively, and <ref type="figure" target="#fig_0">Figure 13</ref>   for both single and multiple view 3D reconstruction, and all the results are consistent with previous experimental results on both ShapeNet r2n2 and ShapeNet lsm datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Evaluation on Blobby Dataset</head><p>In this section, we evaluate the Base silnet -AttSets and the competing approaches on the Blobby dataset. For fair comparison, the GRU module is implemented with a single fully connected layer of 160 hidden units, which has similar network capacity with our AttSets based network. All networks (the pooling/GRU/AttSets based approaches) are trained with the proposed two-stage FASet algorithm as follows: proaches are fine-tuned given 2 images per object for better performance where N =2 until convergence.</p><p>-Group 2. Similar to the above Group 1, all networks are further trained given all 4 images for each object, i.e., N =4, until convergence.</p><p>Testing Stage. All networks trained in above two groups are separately tested given N = <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4]</ref>. The permutations of input images are the same for all different networks for fair comparison.</p><p>Results. <ref type="table" target="#tab_0">Table 10</ref> and 11 show the mean IoUs of above two groups of experiments and <ref type="figure" target="#fig_0">Figure 14</ref> shows the qualitative results of Group 2. Note that, the IoUs are calculated on predicted 2D silhouettes instead of 3D voxels, so they are not numerically comparable with previous experiments on ShapeNet r2n2 , ShapeNet lsm , and ModelNet40 datasets. We do not include the IoU scores of the original SilNet (Wiles and Zisserman, 2017), because the original IoU scores are obtained from an end-toend training strategy. In this paper, we uniformly apply the proposed two-stage FASet training paradigm on all approaches for fair comparison. Our Base silnet -AttSets consistently outperforms all competing approaches for shape learning from either single or multiple views.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Qualitative Results on Real-world Images</head><p>To the best of our knowledge, there is no public realworld dataset for multi-view 3D object reconstruction. Therefore, we manually collect real world images from Amazon online shops to qualitatively demonstrate the generality of all networks which are trained on the synthetic ShapeNet r2n2 dataset in experiment Group 4 of Section 5.1, as shown in <ref type="figure" target="#fig_0">Figure 15</ref>.</p><p>In the meantime, we use these real-world images to qualitatively show the permutation invariance of different approaches. In particular, for each object, we use 6 different permutations in total for testing. As shown in <ref type="figure" target="#fig_0">Figure 16</ref>, the GRU based approach generates inconsistent 3D shapes given different image permutations. For example, the arm of a chair and the leg of a table can be reconstructed in permutation 1, but fail to be recovered in another permutation. By comparison, all other approaches are permutation invariant, as the results shown in <ref type="figure" target="#fig_0">Figure 15</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Computational Efficiency</head><p>To evaluate the computation and memory cost of AttSets, we implement Base r2n2 -AttSets and the competing approaches in Python 2.7 and Tensorflow 1.2 with CUDA 9.0 and cuDNN 7.1 as the back-end driver and library. All approaches share the same Base r2n2 network and run in the same Titan X and software environments. <ref type="table" target="#tab_0">Table 12</ref> shows the average time consumption to reconstruct a single 3D object given different number of images. Our AttSets based approach is as efficient as the pooling methods, while Base r2n2 -GRU (i.e., 3D-R2N2) takes more time when processing an increasing number of images due to the sequential computation mechanism of its GRU module.    <ref type="figure" target="#fig_0">Fig. 16</ref> Qualitative results of inconsistent 3D reconstruction from the GRU based approach. </p><formula xml:id="formula_15">GRU (P2) GRU (P3) GRU (P4) GRU (P5) GRU (P6)</formula><formula xml:id="formula_16">GRU (P2) GRU (P3) GRU (P4) GRU (P5) GRU (P6)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Comparison between Variants of AttSets</head><p>We further compare the aggregation performance of f c, conv2d and conv3d based AttSets variants which are shown in <ref type="figure" target="#fig_2">Figure 3</ref> in Section 3.4. The f c based AttSets net is the same as in Section 5.1. The conv2d based AttSets is plugged into the middle of the 2D encoder, fusing a (N, 4, 4, 256) tensor into <ref type="bibr">(1,</ref><ref type="bibr">4,</ref><ref type="bibr">4,</ref><ref type="bibr">256)</ref>, where N is an arbitrary image number. The conv3d based AttSets is plugged into the middle of the 3D decoder, integrating a <ref type="bibr">(N, 8, 8, 8, 128)</ref> tensor into <ref type="bibr">(1,</ref><ref type="bibr">8,</ref><ref type="bibr">8,</ref><ref type="bibr">8,</ref><ref type="bibr">128)</ref>. All other layers of these variants are the same. Both the conv2d and conv3d based AttSets networks are trained using the paradigm of experiment Group 4 in Section 5.1. <ref type="table" target="#tab_0">Table 13</ref> shows the mean IoU scores of three variants on ShapeNet r2n2 testing split. f c and conv3d based variants achieve similar IoU scores for either single or multi view 3D reconstruction, demonstrating the superior aggregation capability of AttSets. In the meantime, we observe that the overall performance of conv2d based AttSets net is slightly decreased compared with the other two. One possible reason is that the 2D feature set has been aggregated at the early layer of the network, resulting in features being lost early. <ref type="figure" target="#fig_0">Figure 17</ref> visualizes the learnt attention scores for a 2D feature set, i.e., <ref type="bibr">(N, 4, 4, 256)</ref> features, via the conv2d based AttSets net. To visualize 2D feature scores, we average the scores along the channel axis and then roughly trace back the spatial locations of those scores corresponding to the original input. The more visual information the input image has, the higher attention scores are learnt by AttSets for the corresponding latent features. For example, the third image has richer visual information than the first image, so its attention scores are higher. Note that, for a specific base network, there are many potential locations to plug in AttSets and it is also possible to include multiple AttSets modules into the same net. To fully evaluate these factors is out of the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Feature-wise Attention vs. Element-wise Attention</head><p>Our AttSets module is initially designed to learn unique feature-wise attention scores for the whole input deep feature set, and we demonstrate that it significantly improves the aggregation performance over dynamic feature sets in previous Section 5. <ref type="bibr">1, 5.2, 5.3 and 5.4.</ref> In this section, we further investigate the advantage of this feature-wise attentive pooling over element-wise attentional aggregation.</p><p>For element-wise attentional aggregation, the AttSets module turns to learn a single attention score for each el- ement of the feature set A = {x 1 , x 2 , · · · , x N }, followed by the sof tmax normalization and weighted summation pooling. In particular, as shown in previous <ref type="figure" target="#fig_1">Figure 2</ref>, the shared function g(x n , W ) now learns a scalar, instead of a vector, as the attention activation for each input element. Eventually, all features within the same element are weighted by a learnt common attention score. Intuitively, the original feature-wise AttSets tends to be fine-grained aggregation, while the element-wise AttSets learns to coarsely aggregate features.</p><p>Following the same training settings of experiment Group 4 in Section 5.1, we conduct another group of experiment on ShapeNet r2n2 dataset for element-wise attentional aggregation. <ref type="table" target="#tab_0">Table 14</ref> compares the mean IoU for 3D object reconstruction through feature-wise and element-wise attentional aggregation. <ref type="figure" target="#fig_0">Figure 18</ref> shows an example of the learnt attention scores and the predicted 3D shapes. As expected, the feature-wise attention mechanism clearly achieves better aggregation performance compared with the coarsely element-wise approach. As shown in <ref type="figure" target="#fig_0">Figure 18</ref>, the element-wise attention mechanism tends to focus on few images, while completely ignoring others. By comparison, the featurewise AttSets learns to fuse information from all images, thus achieving better aggregation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.9">Significance of FASet Algorithm</head><p>In this section, we investigate the impact of FASet algorithm by comparing it with the standard end-to-end joint training (JoinT). Particularly, in JoinT, all parameters Θ base and Θ att are jointly optimized with a single loss. Following the same training settings of experiment Group 4 in Section 5.1, we conduct another group of experiment on ShapeNet r2n2 dataset under the JoinT training strategy. As its IoU scores shown in <ref type="table" target="#tab_0">Table 15</ref>, the JoinT training approach tends to optimize the whole net regarding the training multi-view batches, thus being unable to generalize well for fewer images during testing. Basically, the network itself is unable to dedicate the base layers to learning visual features, while the AttSets module to learning attention scores, if it is not trained with the proposed FASet algorithm. The theoretical reason is discussed previously in Section 4.1. The FASet algorithm may also be applicable to other learning based aggregation approaches, as long as the aggregation module can be decoupled from the base encoder/decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present AttSets module and FASet training algorithm to aggregate elements of deep feature sets. AttSets together with FASet has powerful permutation invariance, computation efficiency, robustness and flexible implementation properties, along with the theory and extensive experiments to support its performance for multi-view 3D reconstruction. Both quantitative and qualitative results explicitly show that AttSets significantly outperforms other widely used aggregation approaches. Nevertheless, all of our experiments are dedicated to multi-view 3D reconstruction. It would be interesting to explore the generality of AttSets and FASet over other set-based tasks, especially the tasks which constantly take multiple elements as input.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1</head><label>1</label><figDesc>Overview of our attentional aggregation module for multi-view 3D reconstruction. A set of N images is passed through a common encoder to be a set of deep features, one element for each image. The network is trained with our FASet algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Attentional aggregation module on sets. This module learns an attention score for each individual deep feature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3</head><label>3</label><figDesc>Implementation of AttSets with fully connected layer, 2D ConvNet, and 3D ConvNet. These three variants of AttSets can be flexibly plugged into different locations of an existing encoder-decoder network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 Fig. 5</head><label>45</label><figDesc>The architecture of Base r2n2 -AttSets for multi-view 3D reconstruction network. The base encoder-decoder is the same as 3D-R2N2. The architecture of Base silnet -AttSets for multi-view 3D shape learning. The base encoder-decoder is the same as SilNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 Fig. 10</head><label>610</label><figDesc>IoUs of Group 1. IoUs of Group 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 11</head><label>11</label><figDesc>are quite different from ours in the following two aspects. 1) The original LSM requires both RGB images and the corresponding viewing angles as input, while all our experiments do not. 2) The original LSM dataset has different styles of rendered Qualitative results of multi-view reconstruction from different approaches in experiment Group 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>plane bench cabinet car chair monitor lamp speaker firearm couch table phone watercraft mean Base r2n2 r2n2 -MHBN pooling 0.528 0.451 0.742 0.812 0.471 0.487 0.386 0.677 0.548 0.637 0.515 0.674 0.546 0.578 Base r2n2 -SMSO pooling 0.572 0.521 0.763 0.833 0.541 0.548 0.433 0.704 0.581 0.682 0.566 0.721 0.581 0.623 OGN 0.587 0.481 0.729 0.816 0.483 0.502 0.398 0.637 0.593 0.646 0.536 0.702 0.632 0.596 AORM 0.605 0.498 0.715 0.757 0.532 0.524 0.415 0.623 0.618 0.679 0.547 0.738 0.552 0.600 PointSet 0.601 0.550 0.771 0.831 0.544 0.552 0.462 0.737 0.604 0.708 0.606 0.749 0.611 0.640 Base r2n2 -AttSets(Ours) 0.594 0.552 0.783 0.844 0.559 0.565 0.445 0.721 0.601 0.703 0.590 0.743 0.601 0.642</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 13</head><label>13</label><figDesc>shows qualitative results of Group 2. The Base r2n2 -AttSets surpasses all competing approaches by a large margin Qualitative results of multi-view reconstruction from different approaches in ModelNet40 testing split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 15</head><label>15</label><figDesc>Qualitative results of multi-view 3D reconstruction from real-world images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Group 1: mean IoU for multi-view reconstruction of all 13 categories in ShapeNet r2n2 testing split. All networks are firstly trained given only 1 image for each object in Stage 1. The AttSets module is further trained given 2 images per object in Stage 2, while other competing approaches are fine-tuned given 2 images per object in Stage 2. 1 view 2 views 3 views 4 views 5 views 8 views 12 views 16 views 20 views 24 views Base Group 2: mean IoU for multi-view reconstruction of all 13 categories in ShapeNet r2n2 testing split. All networks are firstly trained given only 1 image for each object in Stage 1. The AttSets module is further trained given 8 images per object in Stage 2, while other competing approaches are fine-tuned given 8 images per object in Stage 2.</figDesc><table><row><cell>r2n2 -GRU</cell><cell>0.574</cell><cell>0.608</cell><cell>0.622</cell><cell>0.629</cell><cell>0.633</cell><cell>0.639</cell><cell>0.642</cell><cell>0.642</cell><cell>0.641</cell><cell>0.640</cell></row><row><cell>Base r2n2 -max pooling</cell><cell>0.620</cell><cell>0.651</cell><cell>0.660</cell><cell>0.665</cell><cell>0.666</cell><cell>0.671</cell><cell>0.672</cell><cell>0.674</cell><cell>0.673</cell><cell>0.673</cell></row><row><cell>Base r2n2 -mean pooling</cell><cell>0.632</cell><cell>0.654</cell><cell>0.661</cell><cell>0.666</cell><cell>0.667</cell><cell>0.674</cell><cell>0.676</cell><cell>0.680</cell><cell>0.680</cell><cell>0.681</cell></row><row><cell>Base r2n2 -sum pooling</cell><cell>0.633</cell><cell>0.657</cell><cell>0.665</cell><cell>0.669</cell><cell>0.669</cell><cell>0.670</cell><cell>0.666</cell><cell>0.667</cell><cell>0.666</cell><cell>0.665</cell></row><row><cell>Base r2n2 -BP pooling</cell><cell>0.588</cell><cell>0.608</cell><cell>0.615</cell><cell>0.620</cell><cell>0.621</cell><cell>0.627</cell><cell>0.628</cell><cell>0.632</cell><cell>0.633</cell><cell>0.633</cell></row><row><cell>Base r2n2 -MHBN pooling</cell><cell>0.578</cell><cell>0.599</cell><cell>0.606</cell><cell>0.611</cell><cell>0.612</cell><cell>0.618</cell><cell>0.620</cell><cell>0.623</cell><cell>0.624</cell><cell>0.624</cell></row><row><cell>Base r2n2 -SMSO pooling</cell><cell>0.623</cell><cell>0.654</cell><cell>0.664</cell><cell>0.670</cell><cell>0.672</cell><cell>0.679</cell><cell>0.679</cell><cell>0.682</cell><cell>0.680</cell><cell>0.678</cell></row><row><cell cols="7">Base r2n2 -AttSets(Ours) 0.642 0.665 0.672 0.677 0.678 0.684</cell><cell>0.686</cell><cell>0.690</cell><cell>0.690</cell><cell>0.690</cell></row><row><cell cols="11">Table 2 1 view 2 views 3 views 4 views 5 views 8 views 12 views 16 views 20 views 24 views</cell></row><row><cell>Base r2n2 -GRU</cell><cell>0.580</cell><cell>0.616</cell><cell>0.629</cell><cell>0.637</cell><cell>0.641</cell><cell>0.649</cell><cell>0.652</cell><cell>0.652</cell><cell>0.652</cell><cell>0.652</cell></row><row><cell>Base r2n2 -max pooling</cell><cell>0.524</cell><cell>0.615</cell><cell>0.641</cell><cell>0.655</cell><cell>0.661</cell><cell>0.674</cell><cell>0.678</cell><cell>0.683</cell><cell>0.684</cell><cell>0.684</cell></row><row><cell>Base r2n2 -mean pooling</cell><cell>0.632</cell><cell>0.657</cell><cell>0.665</cell><cell>0.670</cell><cell>0.672</cell><cell>0.679</cell><cell>0.681</cell><cell>0.685</cell><cell>0.686</cell><cell>0.686</cell></row><row><cell>Base r2n2 -sum pooling</cell><cell>0.580</cell><cell>0.628</cell><cell>0.644</cell><cell>0.656</cell><cell>0.660</cell><cell>0.672</cell><cell>0.677</cell><cell>0.682</cell><cell>0.684</cell><cell>0.685</cell></row><row><cell>Base r2n2 -BP pooling</cell><cell>0.544</cell><cell>0.599</cell><cell>0.618</cell><cell>0.628</cell><cell>0.632</cell><cell>0.644</cell><cell>0.648</cell><cell>0.654</cell><cell>0.655</cell><cell>0.656</cell></row><row><cell>Base r2n2 -MHBN pooling</cell><cell>0.570</cell><cell>0.596</cell><cell>0.606</cell><cell>0.612</cell><cell>0.614</cell><cell>0.621</cell><cell>0.624</cell><cell>0.628</cell><cell>0.629</cell><cell>0.629</cell></row><row><cell>Base r2n2 -SMSO pooling</cell><cell>0.570</cell><cell>0.621</cell><cell>0.641</cell><cell>0.652</cell><cell>0.656</cell><cell>0.668</cell><cell>0.673</cell><cell>0.679</cell><cell>0.680</cell><cell>0.681</cell></row><row><cell cols="7">Base r2n2 -AttSets(Ours) 0.642 0.662 0.671 0.676 0.678 0.686</cell><cell>0.688</cell><cell>0.693</cell><cell>0.694</cell><cell>0.694</cell></row><row><cell></cell><cell cols="10">1 view 2 views 3 views 4 views 5 views 8 views 12 views 16 views 20 views 24 views</cell></row><row><cell>Base r2n2 -GRU</cell><cell>0.579</cell><cell>0.614</cell><cell>0.628</cell><cell>0.636</cell><cell>0.640</cell><cell>0.647</cell><cell>0.651</cell><cell>0.652</cell><cell>0.653</cell><cell>0.653</cell></row><row><cell>Base r2n2 -max pooling</cell><cell>0.511</cell><cell>0.604</cell><cell>0.633</cell><cell>0.649</cell><cell>0.656</cell><cell>0.671</cell><cell>0.678</cell><cell>0.684</cell><cell>0.686</cell><cell>0.686</cell></row><row><cell>Base r2n2 -mean pooling</cell><cell>0.594</cell><cell>0.637</cell><cell>0.652</cell><cell>0.662</cell><cell>0.667</cell><cell>0.677</cell><cell>0.682</cell><cell>0.687</cell><cell>0.688</cell><cell>0.689</cell></row><row><cell>Base r2n2 -sum pooling</cell><cell>0.570</cell><cell>0.629</cell><cell>0.647</cell><cell>0.657</cell><cell>0.664</cell><cell>0.678</cell><cell>0.684</cell><cell>0.690</cell><cell>0.692</cell><cell>0.692</cell></row><row><cell>Base r2n2 -BP pooling</cell><cell>0.545</cell><cell>0.593</cell><cell>0.611</cell><cell>0.621</cell><cell>0.627</cell><cell>0.637</cell><cell>0.642</cell><cell>0.647</cell><cell>0.648</cell><cell>0.649</cell></row><row><cell>Base r2n2 -MHBN pooling</cell><cell>0.570</cell><cell>0.596</cell><cell>0.606</cell><cell>0.612</cell><cell>0.614</cell><cell>0.621</cell><cell>0.624</cell><cell>0.627</cell><cell>0.628</cell><cell>0.629</cell></row><row><cell>Base r2n2 -SMSO pooling</cell><cell>0.580</cell><cell>0.627</cell><cell>0.643</cell><cell>0.652</cell><cell>0.656</cell><cell>0.668</cell><cell>0.673</cell><cell>0.679</cell><cell>0.680</cell><cell>0.681</cell></row><row><cell cols="7">Base r2n2 -AttSets(Ours) 0.642 0.660 0.668 0.673 0.676 0.683</cell><cell>0.687</cell><cell>0.691</cell><cell>0.692</cell><cell>0.693</cell></row></table><note>Table 3 Group 3: mean IoU for multi-view reconstruction of all 13 categories in ShapeNet r2n2 testing split. All networks are firstly trained given only 1 image for each object in Stage 1. The AttSets module is further trained given 16 images per object in Stage 2, while other competing approaches are fine-tuned given 16 images per object in Stage 2.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4</head><label>4</label><figDesc>Group 4: mean IoU for multi-view reconstruction of all 13 categories in ShapeNet r2n2 testing split. All networks are firstly trained given only 1 image for each object in Stage 1. The AttSets module is further trained given 24 images per object in Stage 2, while other competing approaches are fine-tuned given 24 images per object in Stage 2.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="9">1 view 2 views 3 views 4 views 5 views 8 views 12 views 16 views 20 views 24 views</cell></row><row><cell cols="3">Base r2n2 -GRU</cell><cell>0.578</cell><cell>0.613</cell><cell>0.627</cell><cell>0.635</cell><cell>0.639</cell><cell>0.647</cell><cell>0.651</cell><cell>0.653</cell><cell>0.653</cell><cell>0.654</cell></row><row><cell cols="3">Base r2n2 -max pooling</cell><cell>0.504</cell><cell>0.600</cell><cell>0.631</cell><cell>0.648</cell><cell>0.655</cell><cell>0.671</cell><cell>0.679</cell><cell>0.685</cell><cell>0.688</cell><cell>0.689</cell></row><row><cell cols="3">Base r2n2 -mean pooling</cell><cell>0.593</cell><cell>0.634</cell><cell>0.649</cell><cell>0.659</cell><cell>0.663</cell><cell>0.673</cell><cell>0.677</cell><cell>0.683</cell><cell>0.684</cell><cell>0.685</cell></row><row><cell cols="3">Base r2n2 -sum pooling</cell><cell>0.580</cell><cell>0.634</cell><cell>0.650</cell><cell>0.658</cell><cell>0.660</cell><cell>0.678</cell><cell>0.682</cell><cell>0.689</cell><cell>0.690</cell><cell>0.691</cell></row><row><cell cols="3">Base r2n2 -BP pooling</cell><cell>0.524</cell><cell>0.585</cell><cell>0.609</cell><cell>0.623</cell><cell>0.630</cell><cell>0.644</cell><cell>0.650</cell><cell>0.656</cell><cell>0.659</cell><cell>0.660</cell></row><row><cell cols="3">Base r2n2 -MHBN pooling</cell><cell>0.566</cell><cell>0.595</cell><cell>0.606</cell><cell>0.613</cell><cell>0.616</cell><cell>0.624</cell><cell>0.627</cell><cell>0.631</cell><cell>0.632</cell><cell>0.632</cell></row><row><cell cols="3">Base r2n2 -SMSO pooling</cell><cell>0.556</cell><cell>0.613</cell><cell>0.635</cell><cell>0.647</cell><cell>0.653</cell><cell>0.668</cell><cell>0.674</cell><cell>0.681</cell><cell>0.682</cell><cell>0.684</cell></row><row><cell cols="9">Base r2n2 -AttSets(Ours) 0.642 0.660 0.668 0.674 0.676 0.684</cell><cell>0.688</cell><cell>0.693</cell><cell>0.694</cell><cell>0.695</cell></row><row><cell cols="12">Table 5 Group 5: mean IoU for multi-view reconstruction of all 13 categories in ShapeNet r2n2 testing split. All networks are</cell></row><row><cell cols="12">firstly trained given only 1 image for each object in Stage 1. The AttSets module is further trained given random number of</cell></row><row><cell cols="12">images per object in Stage 2, i.e., N is uniformly sampled from [1, 24], while other competing approaches are fine-tuned given</cell></row><row><cell cols="5">random number of views per object in Stage 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="9">1 view 2 views 3 views 4 views 5 views 8 views 12 views 16 views 20 views 24 views</cell></row><row><cell cols="3">Base r2n2 -GRU</cell><cell>0.580</cell><cell>0.615</cell><cell>0.629</cell><cell>0.637</cell><cell>0.641</cell><cell>0.648</cell><cell>0.651</cell><cell>0.651</cell><cell>0.651</cell><cell>0.651</cell></row><row><cell cols="3">Base r2n2 -max pooling</cell><cell>0.601</cell><cell>0.638</cell><cell>0.652</cell><cell>0.660</cell><cell>0.663</cell><cell>0.673</cell><cell>0.677</cell><cell>0.682</cell><cell>0.683</cell><cell>0.684</cell></row><row><cell cols="3">Base r2n2 -mean pooling</cell><cell>0.598</cell><cell>0.637</cell><cell>0.652</cell><cell>0.660</cell><cell>0.664</cell><cell>0.675</cell><cell>0.679</cell><cell>0.684</cell><cell>0.685</cell><cell>0.686</cell></row><row><cell cols="3">Base r2n2 -sum pooling</cell><cell>0.587</cell><cell>0.631</cell><cell>0.646</cell><cell>0.656</cell><cell>0.660</cell><cell>0.672</cell><cell>0.678</cell><cell>0.683</cell><cell>0.684</cell><cell>0.685</cell></row><row><cell cols="3">Base r2n2 -BP pooling</cell><cell>0.582</cell><cell>0.610</cell><cell>0.620</cell><cell>0.626</cell><cell>0.628</cell><cell>0.635</cell><cell>0.638</cell><cell>0.641</cell><cell>0.642</cell><cell>0.643</cell></row><row><cell cols="3">Base r2n2 -MHBN pooling</cell><cell>0.575</cell><cell>0.599</cell><cell>0.608</cell><cell>0.613</cell><cell>0.615</cell><cell>0.622</cell><cell>0.624</cell><cell>0.628</cell><cell>0.629</cell><cell>0.629</cell></row><row><cell cols="3">Base r2n2 -SMSO pooling</cell><cell>0.580</cell><cell>0.624</cell><cell>0.641</cell><cell>0.652</cell><cell>0.657</cell><cell>0.669</cell><cell>0.674</cell><cell>0.679</cell><cell>0.681</cell><cell>0.682</cell></row><row><cell cols="9">Base r2n2 -AttSets(Ours) 0.642 0.662 0.670 0.675 0.677 0.685</cell><cell>0.688</cell><cell>0.692</cell><cell>0.693</cell><cell>0.694</cell></row><row><cell></cell><cell>0.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Testing Split</cell><cell>0.64 0.66</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ShapeNet(r2n2) on IoU Mean</cell><cell>0.54 0.56 0.58 0.6 0.62</cell><cell>1 2 3 4 5 8 12 16 20 24 GRU(3D-R2N2) Max Mean Sum BP MHBN SMSO AttSets(Ours)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>The Number of Input Images</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 6</head><label>6</label><figDesc>Per-category mean IoU for single view reconstruction on ShapeNet r2n2 testing split.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 7</head><label>7</label><figDesc>Mean IoU for multi-view reconstruction of all 13 categories from ShapeNet lsm dataset. All networks are well trained in previous experiment Group 5 of Section 5.1.</figDesc><table><row><cell></cell><cell cols="9">1 view 2 views 3 views 4 views 5 views 8 views 12 views 16 views 20 views</cell></row><row><cell>Base r2n2 -GRU</cell><cell>0.390</cell><cell>0.428</cell><cell>0.444</cell><cell>0.454</cell><cell>0.459</cell><cell>0.467</cell><cell>0.470</cell><cell>0.471</cell><cell>0.472</cell></row><row><cell>Base r2n2 -max pooling</cell><cell>0.276</cell><cell>0.388</cell><cell>0.433</cell><cell>0.459</cell><cell>0.473</cell><cell>0.497</cell><cell>0.510</cell><cell>0.515</cell><cell>0.518</cell></row><row><cell>Base r2n2 -mean pooling</cell><cell>0.365</cell><cell>0.426</cell><cell>0.452</cell><cell>0.468</cell><cell>0.477</cell><cell>0.493</cell><cell>0.503</cell><cell>0.508</cell><cell>0.511</cell></row><row><cell>Base r2n2 -sum pooling</cell><cell>0.363</cell><cell>0.421</cell><cell>0.445</cell><cell>0.459</cell><cell>0.466</cell><cell>0.481</cell><cell>0.492</cell><cell>0.499</cell><cell>0.503</cell></row><row><cell>Base r2n2 -BP pooling</cell><cell>0.359</cell><cell>0.407</cell><cell>0.426</cell><cell>0.436</cell><cell>0.442</cell><cell>0.453</cell><cell>0.459</cell><cell>0.462</cell><cell>0.463</cell></row><row><cell>Base r2n2 -MHBN pooling</cell><cell>0.365</cell><cell>0.403</cell><cell>0.418</cell><cell>0.427</cell><cell>0.431</cell><cell>0.441</cell><cell>0.446</cell><cell>0.449</cell><cell>0.450</cell></row><row><cell>Base r2n2 -SMSO pooling</cell><cell>0.364</cell><cell>0.419</cell><cell>0.445</cell><cell>0.460</cell><cell>0.469</cell><cell>0.488</cell><cell>0.500</cell><cell>0.506</cell><cell>0.510</cell></row><row><cell cols="2">Base r2n2 -AttSets(Ours) 0.404</cell><cell>0.452</cell><cell>0.475</cell><cell>0.490</cell><cell>0.498</cell><cell>0.514</cell><cell>0.522</cell><cell>0.528</cell><cell>0.531</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8</head><label>8</label><figDesc>Group 1: mean IoU for multi-view reconstruction of all 40 categories in ModelNet40 testing split. All networks are firstly trained given only 1 image for each object in Stage 1. The AttSets module is further trained given 12 images per object in Stage 2, while other competing approaches are fine-tuned given 12 images per object in Stage 2.</figDesc><table><row><cell></cell><cell>1 view</cell><cell>2 views</cell><cell>3 views</cell><cell>4 views</cell><cell>5 views</cell><cell>8 views</cell><cell>12 views</cell></row><row><cell>Base r2n2 -GRU</cell><cell>0.344</cell><cell>0.390</cell><cell>0.414</cell><cell>0.430</cell><cell>0.440</cell><cell>0.454</cell><cell>0.464</cell></row><row><cell>Base r2n2 -max pooling</cell><cell>0.393</cell><cell>0.468</cell><cell>0.490</cell><cell>0.504</cell><cell>0.511</cell><cell>0.523</cell><cell>0.525</cell></row><row><cell>Base r2n2 -mean pooling</cell><cell>0.415</cell><cell>0.464</cell><cell>0.481</cell><cell>0.495</cell><cell>0.502</cell><cell>0.515</cell><cell>0.520</cell></row><row><cell>Base r2n2 -sum pooling</cell><cell>0.332</cell><cell>0.441</cell><cell>0.473</cell><cell>0.492</cell><cell>0.500</cell><cell>0.514</cell><cell>0.520</cell></row><row><cell>Base r2n2 -BP pooling</cell><cell>0.431</cell><cell>0.466</cell><cell>0.479</cell><cell>0.492</cell><cell>0.497</cell><cell>0.509</cell><cell>0.515</cell></row><row><cell>Base r2n2 -MHBN pooling</cell><cell>0.423</cell><cell>0.462</cell><cell>0.478</cell><cell>0.491</cell><cell>0.497</cell><cell>0.509</cell><cell>0.515</cell></row><row><cell>Base r2n2 -SMSO pooling</cell><cell>0.441</cell><cell>0.476</cell><cell>0.490</cell><cell>0.500</cell><cell>0.506</cell><cell>0.517</cell><cell>0.520</cell></row><row><cell>Base r2n2 -AttSets(Ours)</cell><cell>0.487</cell><cell>0.505</cell><cell>0.511</cell><cell>0.517</cell><cell>0.521</cell><cell>0.527</cell><cell>0.529</cell></row><row><cell cols="8">Table 9 Group 2: mean IoU for multi-view reconstruction of all 40 categories in ModelNet40 testing split. All networks are</cell></row><row><cell cols="8">firstly trained given only 1 image for each object in Stage 1. The AttSets module is further trained given random number of</cell></row><row><cell cols="8">images per object in Stage 2, i.e., N is uniformly sampled from [1, 12], while other competing approaches are fine-tuned given</cell></row><row><cell cols="2">random number of views per object in Stage 2.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>1 view</cell><cell>2 views</cell><cell>3 views</cell><cell>4 views</cell><cell>5 views</cell><cell>8 views</cell><cell>12 views</cell></row><row><cell>Base r2n2 -GRU</cell><cell>0.388</cell><cell>0.421</cell><cell>0.434</cell><cell>0.440</cell><cell>0.444</cell><cell>0.449</cell><cell>0.452</cell></row><row><cell>Base r2n2 -max pooling</cell><cell>0.461</cell><cell>0.489</cell><cell>0.498</cell><cell>0.506</cell><cell>0.509</cell><cell>0.515</cell><cell>0.517</cell></row><row><cell>Base r2n2 -mean pooling</cell><cell>0.455</cell><cell>0.487</cell><cell>0.498</cell><cell>0.507</cell><cell>0.512</cell><cell>0.520</cell><cell>0.523</cell></row><row><cell>Base r2n2 -sum pooling</cell><cell>0.453</cell><cell>0.484</cell><cell>0.494</cell><cell>0.503</cell><cell>0.506</cell><cell>0.514</cell><cell>0.517</cell></row><row><cell>Base r2n2 -BP pooling</cell><cell>0.454</cell><cell>0.479</cell><cell>0.487</cell><cell>0.496</cell><cell>0.499</cell><cell>0.507</cell><cell>0.510</cell></row><row><cell>Base r2n2 -MHBN pooling</cell><cell>0.453</cell><cell>0.480</cell><cell>0.488</cell><cell>0.497</cell><cell>0.500</cell><cell>0.507</cell><cell>0.509</cell></row><row><cell>Base r2n2 -SMSO pooling</cell><cell>0.462</cell><cell>0.488</cell><cell>0.497</cell><cell>0.505</cell><cell>0.509</cell><cell>0.516</cell><cell>0.519</cell></row><row><cell>Base r2n2 -AttSets(Ours)</cell><cell>0.487</cell><cell>0.505</cell><cell>0.511</cell><cell>0.518</cell><cell>0.520</cell><cell>0.525</cell><cell>0.527</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 10</head><label>10</label><figDesc>Group 1: mean IoU for silhouettes prediction on the Blobby dataset. All networks are firstly trained given only 1 image for each object in Stage 1. The AttSets module is further trained given 2 images per object, i.e., N =2, while</figDesc><table><row><cell cols="2">other competing approaches are fine-tuned given 2 views per</cell></row><row><cell>object in Stage 2.</cell><cell></cell></row><row><cell></cell><cell>1 view 2 views 3 views 4 views</cell></row><row><cell>Base silnet -GRU</cell><cell>0.857 0.860 0.860 0.860</cell></row><row><cell>Base silnet -max pooling</cell><cell>0.922 0.923 0.924 0.924</cell></row><row><cell>Base silnet -mean pooling</cell><cell>0.920 0.922 0.923 0.924</cell></row><row><cell>Base silnet -sum pooling</cell><cell>0.913 0.918 0.917 0.916</cell></row><row><cell>Base silnet -BP pooling</cell><cell>0.908 0.912 0.914 0.914</cell></row><row><cell>Base silnet -MHBN pooling</cell><cell>0.901 0.904 0.906 0.906</cell></row><row><cell>Base silnet -SMSO pooling</cell><cell>0.860 0.865 0.865 0.865</cell></row><row><cell cols="2">Base silnet -AttSets(Ours) 0.924 0.931 0.933 0.935</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 11</head><label>11</label><figDesc>Group 2: mean IoU for silhouettes prediction on the Blobby dataset. All networks are firstly trained given only 1 image for each object in Stage 1. The AttSets module is further trained given 4 images per object, i.e., N =4, while All networks are trained given only 1 image together with the viewing angle for each object, i.e., N =1 in all training iterations, until convergence. This guarantees the performance of single view shape learning. Qualitative results of silhouettes prediction from different approaches on the Blobby dataset.</figDesc><table><row><cell cols="2">other competing approaches are fine-tuned given 4 views per</cell></row><row><cell>object in Stage 2.</cell><cell></cell></row><row><cell></cell><cell>1 view 2 views 3 views 4 views</cell></row><row><cell>Base silnet -GRU</cell><cell>0.863 0.865 0.865 0.865</cell></row><row><cell>Base silnet -max pooling</cell><cell>0.923 0.927 0.929 0.929</cell></row><row><cell>Base silnet -mean pooling</cell><cell>0.923 0.925 0.927 0.927</cell></row><row><cell>Base silnet -sum pooling</cell><cell>0.902 0.917 0.921 0.924</cell></row><row><cell>Base silnet -BP pooling</cell><cell>0.911 0.916 0.919 0.920</cell></row><row><cell>Base silnet -MHBN pooling</cell><cell>0.904 0.908 0.911 0.911</cell></row><row><cell>Base silnet -SMSO pooling</cell><cell>0.863 0.865 0.865 0.865</cell></row><row><cell cols="2">Base silnet -AttSets(Ours) 0.924 0.932 0.936 0.937</cell></row><row><cell cols="2">Training Stage 1. Training Stage 2. Another two parallel groups of</cell></row><row><cell cols="2">training experiments are conducted to further optimize</cell></row><row><cell cols="2">the networks for multi-view shape learning.</cell></row><row><cell cols="2">-Group 1. All networks are further trained given only</cell></row><row><cell cols="2">2 images for each object, i.e., N =2 in all iterations.</cell></row><row><cell cols="2">As to Base silnet -AttSets, only the AttSets module is</cell></row><row><cell cols="2">optimized with the well-trained base encoder-decoder</cell></row><row><cell cols="2">being frozen. For fair comparison, all competing ap-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>In terms of the total trainable weights, the max/mean/sum pooling based approaches have 16.66 million, while AttSets based net has 17.71 million. By contrast, the original 3D-R2N2 has 34.78 million, the BP/MHBN/SMSO have 141.57, 60.78 and 17.71 million respectively. Overall, our AttSets outperforms the recurrent unit and pooling operations without incurring notable computation and memory cost.</figDesc><table><row><cell>Input</cell><cell>GRU</cell><cell>max</cell><cell>mean</cell><cell>sum</cell><cell>BP</cell><cell>MHBN</cell><cell>SMSO</cell><cell>AttSets</cell></row><row><cell>Images</cell><cell>(3D-R2N2)</cell><cell>pooling</cell><cell>pooling</cell><cell>pooling</cell><cell>pooling</cell><cell>pooling</cell><cell>pooling</cell><cell>(Ours)</cell></row><row><cell>Input</cell><cell>GRU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Images</cell><cell>(P1)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 12</head><label>12</label><figDesc>Mean time consumption for a single object (32 3 voxel grid) estimation from different number of images (milliseconds). Learnt attention scores for deep feature sets via conv2d based AttSets.</figDesc><table><row><cell>number of input images</cell><cell>1 4</cell><cell>8</cell><cell>12 16 20 24</cell></row><row><cell>Base r2n2 -GRU</cell><cell cols="3">6.9 11.2 17.0 22.8 28.8 34.7 40.7</cell></row><row><cell>Base r2n2 -max pooling</cell><cell cols="3">6.4 10.0 15.1 20.2 25.3 30.2 35.4</cell></row><row><cell>Base r2n2 -mean pooling</cell><cell cols="3">6.3 10.1 15.1 20.1 25.3 30.3 35.5</cell></row><row><cell>Base r2n2 -sum pooling</cell><cell cols="3">6.4 10.1 15.1 20.1 25.3 30.3 35.5</cell></row><row><cell>Base r2n2 -BP pooling</cell><cell cols="3">6.5 10.5 15.6 20.5 25.7 30.6 35.8</cell></row><row><cell cols="4">Base r2n2 -MHBN pooling 6.5 10.3 15.3 20.3 25.5 30.7 35.7</cell></row><row><cell>Base r2n2 -SMSO pooling</cell><cell cols="3">6.5 10.2 15.3 20.3 25.4 30.5 35.6</cell></row><row><cell cols="4">Base r2n2 -AttSets(Ours) 7.7 11.0 16.3 21.2 26.3 31.4 36.4</cell></row><row><cell>Input Images</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Estimated</cell></row><row><cell></cell><cell></cell><cell></cell><cell>3D shape</cell></row><row><cell cols="3">Attention Scores learnt by AttSets(conv2d)</cell><cell>Ground Truth</cell></row><row><cell>Fig. 17</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 13 Table 14</head><label>1314</label><figDesc>Mean IoU of AttSets variants on all 13 categories in ShapeNet r2n2 testing split.1 view 2 views 3 views 4 views 5 views 8 views 12 views 16 views 20 views 24 viewsBase r2n2 -AttSets (conv2d) 0.642 0.648 Base r2n2 -AttSets (conv3d) 0.642 0.663 0.671 0.676 0.677 Mean IoU of all 13 categories in ShapeNet r2n2 testing split for feature-wise and element-wise attentional aggregation. 1 view 2 views 3 views 4 views 5 views 8 views 12 views 16 views 20 views 24 views Base r2n2 -AttSets (element-wise) 0.642 0.653 Mean IoU of different training algorithms on all 13 categories in ShapeNet r2n2 testing split.Fig. 18 Learnt attention scores for deep feature sets via element-wise attention and feature-wise attention AttSets.</figDesc><table><row><cell>0.651</cell><cell>0.655</cell><cell>0.657</cell><cell>0.664</cell><cell>0.668</cell><cell>0.674</cell><cell>0.675</cell><cell>0.676</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.683</cell><cell>0.685</cell><cell>0.689</cell><cell>0.690</cell><cell>0.690</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Neural Machine Translation by Jointly Learning to Align and Translate. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning Longterm Dependencies with Gradient Descent is Difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="166" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Past, Present, and Future of Simultaneous Localization and Mapping: Towards the Robust-Perception Age</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cadena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carlone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Carrillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1309" to="1332" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">F</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Hu</surname></persName>
		</author>
		<title level="m">Learning to Reconstruct High-quality 3D Shapes with Cascaded Fully Convolutional Networks. European Conference on Computer Vision pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="616" to="633" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><forename type="middle">J</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<idno>arXiv 1512.03012</idno>
		<title level="m">ShapeNet: An Information-Rich 3D Model Repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<title level="m">3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction. European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A Volumetric Method for Building Complex Models from Range Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Graphics and Interactive Techniques pp</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="page" from="303" to="312" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<title level="m">PSDF Fusion: Probabilistic Signed Distance Function for On-the-fly 3D Data Fusion and Scene Reconstruction. European Conference on Computer Vision pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="714" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural scene representation and rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Besse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garnelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ruderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Reichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">360</biblScope>
			<biblScope unit="issue">6394</biblScope>
			<biblScope unit="page" from="1204" to="1210" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Point Set Generation Network for 3D Object Reconstruction from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition pp</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="605" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kanno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selmic</surname></persName>
		</author>
		<title level="m">Classifying Unordered Feature Sets with Convolutional Deep Averaging Networks. arXiv 1709</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">3019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<title level="m">Attentional Pooling for Action Recognition. International Conference on Neural Information Processing Systems pp</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="33" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition pp</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
	<note>Multiple View Geometry in Computer Vision</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Matzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">DeepMVS: Learning Multi-view Stereopsis. IEEE Conference on Computer Vision and Pattern Recognition pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2821" to="2830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Attention-based Deep Multiple Instance Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ilse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2127" to="2136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matrix backpropagation for deep networks with structured layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vantzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision pp</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2965" to="2973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sur-faceNet: An End-to-end 3D Neural Network for Multiview Stereopsis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision pp</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2326" to="2334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Revisited: a Convex Programming Approach for Generic 3D Shape Reconstruction from Multiple Perspective Views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision pp</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="929" to="937" />
		</imprint>
	</monogr>
	<note>Maximizing rigidity</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Squeezeand-Excitation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning a Multi-View Stereo Machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems pp</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="364" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Gradient Flow in Recurrent Nets: The Difficulty of Learning Long Term Dependencies. A Field Guide to Dynamical Recurrent Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Kolen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Kremer</surname></persName>
		</author>
		<editor>Kumar S, Dai Y, Li H</editor>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="4649" to="4657" />
		</imprint>
	</monogr>
	<note>Monocular Dense 3D Reconstruction of a Complex Dynamic Scene from Two Perspective Frames</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<title level="m">Pyramid Attention Network for Semantic Segmentation. arXiv 1805</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">10180</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bilinear CNN models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S ;</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
	<note>British Machine Vision Conference</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Second-order Democratic Aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="620" to="636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Dependency-aware Attention Control for Unconstrained Face Recognition with Image Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="548" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distinctive Image Features from Scale-Invariant Keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cundy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Parallelizing Linear Recurrent Neural Nets over Sequence Length. International Conference on Learning Representations Nakka KK</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Deep Attentional Structured Representation Learning for Visual Recognition. British Machine Vision Conference</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Survey of Structure from Motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ozyesil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Voroninski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Basri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="305" to="364" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paschalidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">RayNet: Learning Volumetric 3D Reconstruction with Ray Potentials. IEEE Conference on Computer Vision and Pattern Recognition pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3897" to="3906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Volumetric and Multi-View CNNs for Object Classification on 3D Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition pp</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Feed-Forward Networks with Attention Can Solve Some Long-Term Memory Problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dpw</forename><surname>Ellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<title level="m">Oct-NetFusion: Learning Depth Fusion from Data. International Conference on 3D Vision pp</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Attend and Rectify: a Gated Attention Mechanism for Fine-Grained Recovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rodríguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Gonfaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">X</forename><surname>Roca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gonzàlez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="349" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sarafianos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
		<title level="m">Deep Imbalanced Attribute Classification using Visual Attention Aggregation. European Conference on Computer Vision pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="680" to="697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multi-view Convolutional Neural Networks for 3D Shape Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision pp</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Computer</title>
		<imprint>
			<biblScope unit="page" from="2088" to="2096" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Bundle Adjustment -A Modern Synthesis. International Workshop on Vision Algorithms Vaswani A</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aw ;</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Attention Is All You Need. International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to Predict 3D Surfaces of Sculptures from Single and Multiple Views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Order Matters: Sequence to Sequence for Sets. International Conference on Learning Representations Wiles O, Zisserman A (2017) SilNet : Single-and Multi-View Reconstruction by Learning from Silhouettes. British Machine Vision Conference Wiles O</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="21" />
		</imprint>
	</monogr>
	<note>Zisserman A</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<title level="m">3D ShapeNets: A Deep Representation for Volumetric Shapes. IEEE Conference on Computer Vision and Pattern Recognition pp</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Active Object Reconstruction Using a Guided View Planner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4965" to="4971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Stacked Attention Networks for Image Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">MVSNet: Depth Inference for Unstructured Multi-view Stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="767" to="783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<title level="m">Statistically Motivated Second Order Pooling. European Conference on Computer Vision pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="600" to="616" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Multi-view Harmonized Bilinear Network for 3D Object Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="186" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<title level="m">Deep Sets. International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<title level="m">Self-Attention Generative Adversarial Networks. arXiv 1805</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">8318</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Attention-based Pyramid Aggregation Network for Visual Place Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
