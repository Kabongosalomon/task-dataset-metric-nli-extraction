<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
							<email>xuezhem@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
							<email>chuntinz@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian</forename><surname>Li</surname></persName>
							<email>xianl@fb.com</email>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
							<email>gneubig@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
							<email>ehovy@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Language Technologies Institute</orgName>
								<orgName type="institution" key="instit2">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most sequence-to-sequence (seq2seq) models are autoregressive; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through parallel processing on hardware such as GPUs. However, directly modeling the joint distribution of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind autoregressive models. In this paper, we propose a simple, efficient, and effective model for non-autoregressive sequence generation using latent variable models. Specifically, we turn to generative flow, an elegant technique to model complex distributions using neural networks, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art nonautoregressive NMT models and almost constant decoding time w.r.t the sequence length. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Neural sequence-to-sequence (seq2seq) models <ref type="bibr" target="#b0">(Bahdanau et al., 2015;</ref><ref type="bibr" target="#b27">Rush et al., 2015;</ref><ref type="bibr" target="#b31">Vaswani et al., 2017)</ref> generate an output sequence y = {y 1 , . . . , y T } given an input sequence x = {x 1 , . . . , x T } using conditional probabilities P θ (y|x) predicted by neural networks (parameterized by θ).</p><p>Most seq2seq models are autoregressive, meaning that they factorize the joint probability of the output sequence given the input sequence P θ (y|x) into the product of probabilities over the next to- * Equal contribution, in alphabetical order. ken in the sequence given the input sequence and previously generated tokens:</p><formula xml:id="formula_0">P θ (y|x) = T t=1 P θ (y t |y &lt;t , x).<label>(1)</label></formula><p>Each factor, P θ (y t |y &lt;t , x), can be implemented by function approximators such as RNNs <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> and Transformers <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>. This factorization takes the complicated problem of joint estimation over an exponentially large output space of outputs y, and turns it into a sequence of tractable multi-class classification problems predicting y t given the previous words, allowing for simple maximum loglikelihood training. However, this assumption of left-to-right factorization may be sub-optimal from a modeling perspective <ref type="bibr" target="#b8">(Gu et al., 2019;</ref><ref type="bibr" target="#b30">Stern et al., 2019)</ref>, and generation of outputs must be done through a linear left-to-right pass through the output tokens using beam search, which is not easily parallelizable on hardware such as GPUs.</p><p>Recently, there has been work on nonautoregressive sequence generation for neural machine translation (NMT; <ref type="bibr" target="#b7">Gu et al. (2018)</ref>; ; <ref type="bibr" target="#b6">Ghazvininejad et al. (2019)</ref>) and language modeling <ref type="bibr" target="#b35">(Ziegler and Rush, 2019)</ref>. Nonautoregressive models attempt to model the joint distribution P θ (y|x) directly, decoupling the dependencies of decoding history during generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>arXiv:1909.02480v3 [cs.CL] 9 Oct 2019</head><p>A naïve solution is to assume that each token of the target sequence is independent given the input: P θ (y|x) = T t=1 P θ (y t |x).</p><p>(2)</p><p>Unfortunately, the performance of this simple model falls far behind autoregressive models, as seq2seq tasks usually do have strong conditional dependencies between output variables <ref type="bibr" target="#b7">(Gu et al., 2018)</ref>. This problem can be mitigated by introducing a latent variable z to model these conditional dependencies:</p><formula xml:id="formula_1">P θ (y|x) = z P θ (y|z, x)p θ (z|x)dz,<label>(3)</label></formula><p>where p θ (z|x) is the prior distribution over latent z and P θ (y|z, x) is the "generative" distribution (a.k.a decoder). Non-autoregressive generation can be achieved by the following independence assumption in the decoding process: <ref type="bibr" target="#b7">Gu et al. (2018)</ref> proposed a z representing fertility scores specifying the number of output words each input word generates, significantly improving the performance over Eq.</p><formula xml:id="formula_2">P θ (y|z, x) = T t=1 P θ (y t |z, x).<label>(4)</label></formula><p>(2). But the performance still falls behind state-of-the-art autoregressive models due to the limited expressiveness of fertility to model the interdependence between words in y.</p><p>In this paper, we propose a simple, effective, and efficient model, FlowSeq, which models expressive prior distribution p θ (z|x) using a powerful mathematical framework called generative flow <ref type="bibr" target="#b26">(Rezende and Mohamed, 2015)</ref>. This framework can elegantly model complex distributions, and has obtained remarkable success in modeling continuous data such as images and speech through efficient density estimation and sampling <ref type="bibr" target="#b13">(Kingma and Dhariwal, 2018;</ref><ref type="bibr" target="#b23">Prenger et al., 2019;</ref>. Based on this, we posit that generative flow also has potential to introduce more meaningful latent variables z in the nonautoregressive generation in Eq. (3).</p><p>FlowSeq is a flow-based sequence-to-sequence model, which is (to our knowledge) the first non-autoregressive seq2seq model utilizing generative flows. It allows for efficient parallel decoding while modeling the joint distribution of the output sequence. Experimentally, on three benchmark datasets for machine translation -WMT2014, WMT2016 and IWSLT-2014, FlowSeq achieves comparable performance with state-of-the-art non-autoregressive models, and almost constant decoding time w.r.t. the sequence length compared to a typical left-to-right Transformer model, which is super-linear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>As noted above, incorporating expressive latent variables z is essential to decouple the dependencies between tokens in the target sequence in non-autoregressive models. However, in order to model all of the complexities of sequence generation to the point that we can read off all of the words in the output in an independent fashion (as in Eq. (4)), the prior distribution p θ (z|x) will necessarily be quite complex. In this section, we describe generative flows <ref type="bibr" target="#b26">(Rezende and Mohamed, 2015)</ref>, an effective method for arbitrary modeling of complicated distributions, before describing how we apply them to sequence-to-sequence generation in §3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Flow-based Generative Models</head><p>Put simply, flow-based generative models work by transforming a simple distribution (e.g. a simple Gaussian) into a complex one (e.g. the complex prior distribution over z that we want to model) through a chain of invertible transformations.</p><p>Formally, a set of latent variables υ ∈ Υ are introduced with a simple prior distribution p Υ (υ). We then define a bijection function f : Z → Υ (with g = f −1 ), whereby we can define a generative process over variables z:</p><formula xml:id="formula_3">υ ∼ p Υ (υ) z = g θ (υ).<label>(5)</label></formula><p>An important insight behind flow-based models is that given this bijection function, the change of variable formula defines the model distribution on z ∈ Z by:</p><formula xml:id="formula_4">p θ (z) = p Υ (f θ (z)) det( ∂f θ (z) ∂z ) .<label>(6)</label></formula><p>Here ∂f θ (z) ∂z is the Jacobian matrix of f θ at z. Eq. (6) provides a way to calculate the (complex) density of z by calculating the (simple) density of υ and the Jacobian of the transformation from z to υ. For efficiency purposes, flowbased models generally use certain types of transformations f θ where both the inverse functions g θ and the Jacobian determinants are tractable to compute. A stacked sequence of such invertible transformations is also called a (normalizing) flow (Rezende and Mohamed, 2015):</p><formula xml:id="formula_5">z f 1 ←→ g 1 H 1 f 2 ←→ g 2 H2 f 3 ←→ g 3 · · · f K ←→ g K υ,</formula><p>where f = f 1 • f 2 • · · · • f K is a flow of K transformations (omitting θs for brevity).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Variational Inference and Training</head><p>In the context of maximal likelihood estimation (MLE), we wish to minimize the negative loglikelihood of the parameters:</p><formula xml:id="formula_6">min θ∈Θ 1 N N i=1 − log P θ (y i |x i ),<label>(7)</label></formula><p>where</p><formula xml:id="formula_7">D = {(x i , y i )} N i=1</formula><p>is the set of training data. However, the likelihood P θ (y|x) after marginalizing out latent variables z (LHS in Eq. <ref type="formula" target="#formula_1">(3)</ref>) is intractable to compute or differentiate directly. Variational inference <ref type="bibr" target="#b33">(Wainwright et al., 2008)</ref> provides a solution by introducing a parametric inference model q φ (z|y, x) (a.k.a posterior) which is then used to approximate this integral by sampling individual examples of z. These models then optimize the evidence lower bound (ELBO), which considers both the "reconstruction error" log P θ (y|z, x) and KL-divergence between the posterior and the prior:</p><formula xml:id="formula_8">log P θ (y|x) ≥ E q φ (z|y,x) [log P θ (y|z, x)] − KL(q φ (z|y, x)||p θ (z|x)). (8)</formula><p>Both inference model φ and decoder θ parameters are optimized according to this objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">FlowSeq</head><p>We first overview FlowSeq's architecture (shown in <ref type="figure" target="#fig_1">Figure 2</ref>) and training process here before detailing each component in following sections. Similarly to classic seq2seq models, at both training and test time FlowSeq first reads the whole input sequence x and calculates a vector for each word in the sequence, the source encoding.</p><p>At training time, FlowSeq's parameters are learned using a variational training paradigm overviewed in §2.2. First, we draw samples of latent codes z from the current posterior q φ (z|y, x). Next, we feed z together with source encodings into the decoder network and the prior flow to compute the probabilities of P θ (y|z, x) and p θ (z|x) for optimizing the ELBO (Eq. <ref type="formula">(8)</ref>).</p><p>At test time, generation is performed by first sampling a latent code z from the prior flow by executing the generative process defined in Eq. (5). In this step, the source encodings produced from the encoder are used as conditional inputs. Then the decoder receives both the sampled latent code z and the source encoder outputs to generate the target sequence y from P θ (y|z, x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Source Encoder</head><p>The source encoder encodes the source sequences into hidden representations, which are used in computing attention when generating latent variables in the posterior network and prior network as well as the cross-attention with decoder. Any standard neural sequence model can be used as its encoder, including RNNs <ref type="bibr" target="#b0">(Bahdanau et al., 2015)</ref> or Transformers <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Posterior</head><p>Generation of Latent Variables. The latent variables z are represented as a sequence of continuous random vectors z = {z 1 , . . . , z T } with the same length as the target sequence y. Each z t is a d z -dimensional vector, where d z is the dimension of the latent space. The posterior distribution q φ (z|y, x) models each z t as a diagonal Gaussian with learned mean and variance:</p><formula xml:id="formula_9">q φ (z|y, x) = T t=1 N (z t |µ t (x, y), σ 2 t (x, y)) (9)</formula><p>where µ t (·) and σ t (·) are neural networks such as RNNs or Transformers.</p><p>Zero initialization. While we perform standard random initialization for most layers of the network, we initialize the last linear transforms that generate the µ and log σ 2 values with zeros. This ensures that the posterior distribution as a simple normal distribution, which we found helps train very deep generative flows more stably. One</p><p>Step of Flow "correct" token at each step t with z t as input. In this case, FlowSeq reduces to the baseline model in Eq.</p><formula xml:id="formula_10">∼  ( 0 ; ) Squeeze Split One Step of Flow × ( L − 1 ) × K × K 1 2 3 4 5 E m b 1 E m b 2 E m b 3 E m b 4 E m b 5 ein hund rannte weg .</formula><p>(2). To escape this undesired local optimum, we apply token-level dropout to randomly drop an entire token when calculating the posterior, to ensure the model also has to learn how to use contextual information. This technique is similar to the "masked language model" in previous studies <ref type="bibr" target="#b19">(Melamud et al., 2016;</ref><ref type="bibr" target="#b4">Devlin et al., 2018;</ref><ref type="bibr" target="#b17">Ma et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Decoder</head><p>As the decoder, we take the latent sequence z as input, run it through several layers of a neural sequence model such as a Transformer, then directly predict the output tokens in y individually and independently. Notably, unlike standard seq2seq decoders, we do not perform causal masking to prevent attending to future tokens, making the model fully non-autoregressive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Flow Architecture for Prior</head><p>The flow architecture is based on Glow (Kingma and Dhariwal, 2018). It consists of a series of steps of flow, combined in a multi-scale architecture (see <ref type="figure" target="#fig_1">Figure 2</ref>.) Each step of flow consists three types of elementary flows -actnorm, invertible multi-head linear, and coupling. Note that all three functions are invertible and conducive to calculation of log determinants (details in Appendix A).</p><p>Actnorm. The activation normalization layer (actnorm; Kingma and Dhariwal <ref type="formula" target="#formula_0">(2018)</ref>) is an alternative for batch normalization <ref type="bibr" target="#b10">(Ioffe and Szegedy, 2015)</ref>, that has mainly been used in the context of image data to alleviate problems in model training. Actnorm performs an affine transformation of the activations using a scale and bias parameter per feature for sequences:</p><formula xml:id="formula_11">z t = s zt + b.<label>(10)</label></formula><p>Both z and z are tensors of shape [T × d z ] with time dimension t and feature dimension d z . The parameters are initialized such that over each feature z t has zero mean and unit variance given an initial mini-batch of data.</p><p>Invertible Multi-head Linear Layers. To incorporate general permutations of variables along the feature dimension to ensure that each dimension can affect every other ones after a sufficient number of steps of flow, Kingma and Dhariwal (2018) proposed a trainable invertible 1×1 convolution layer for 2D images. It is straightforward to apply similar transformations to sequential data:</p><formula xml:id="formula_12">z t = ztW,<label>(11)</label></formula><p>where W is the weight matrix of shape</p><formula xml:id="formula_13">[d z × d z ].</formula><p>The log-determinant of this transformation is:  Unfortunately, d z in Seq2Seq generation is commonly large, e.g. 512, significantly slowing down the model for computing det(W). To apply this to sequence generation, we propose a multihead invertible linear layer, which first splits each d z -dimensional feature vector into h heads with dimension d h = d z /h. Then the linear transformation in <ref type="formula" target="#formula_0">(11)</ref> is applied to each head, with d h × d h weight matrix W, significantly reducing the dimension. For splitting of heads, one step of flow contains one linear layer with either rowmajor or column-major splitting format, and these steps with different linear layers are composed in an alternating pattern.</p><formula xml:id="formula_14">log det ∂linear(z; W) ∂z = T · log |det(W)| The cost of computing det(W) is O(d 3 z ).</formula><p>Affine Coupling Layers. To model interdependence across time steps, we use affine coupling layers <ref type="bibr">(Dinh et al., 2016)</ref>:</p><formula xml:id="formula_15">z a , z b = split(z) z a = z a z b = s(z a , x) z b + b(z a , x) z = concat(z a , z b ),</formula><p>where s(z a , x) and b(z a , x) are outputs of two neural networks with z a and x as input. These are shown in <ref type="figure" target="#fig_3">Figure 3</ref> (c). In experiments, we implement s(·) and b(·) with one Transformer decoder layer <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>: multi-head selfattention over z a , followed by multi-head interattention over x, followed by a position-wise feedforward network. The input z a is fed into this layer in one pass, without causal masking. As in <ref type="bibr">Dinh et al. (2016)</ref>, the split() function splits z the input tensor into two halves, while the concat operation performs the corresponding reverse concatenation operation. In our architecture, three types of split functions are used, based on the split dimension and pattern. <ref type="figure" target="#fig_3">Figure 3</ref> (b) illustrates the three splitting types. The first type of split groups z along the time dimension on alternate indices. In this case, FlowSeq mainly models the interactions between time-steps. The second and third types of splits perform on the feature dimension, with continuous and alternate patterns, respectively. For each type of split, we alternate z a and z b to increase the flexibility of the split function. Different types of affine coupling layers alternate in the flow, similar to the linear layers.</p><p>Multi-scale Architecture. We follow <ref type="bibr">Dinh et al. (2016)</ref> in implementing a multi-scale architecture using the squeezing operation on the feature dimension, which has been demonstrated helpful for training deep flows. Formally, each scale is a combination of several steps of the flow (see <ref type="figure" target="#fig_3">Figure 3</ref> (a)). After each scale, the model drops half of the dimensions with the third type of split in <ref type="figure" target="#fig_3">Figure 3</ref> (b) to reduce computational and memory cost, outputting the tensor with shape [T × d 2 ]. Then the squeezing operation transforms the T × d 2 tensor into an T 2 ×d one as the input of the next scale. We pad each sentence with EOS tokens to ensure T is divisible by 2. The right component of <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the multi-scale architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Predicting Target Sequence Length</head><p>In autoregressive seq2seq models, it is natural to determine the length of the sequence dynamically by simply predicting a special EOS token. However, for FlowSeq to predict the entire sequence in parallel, it needs to know its length in advance to generate the latent sequence z. Instead of predicting the absolute length of the target sequence, we predict the length difference between source and target sequences using a classifier with a range of <ref type="bibr">[−20, 20]</ref>. Numbers in this range are predicted by max-pooling the source encodings into a single vector, 2 running this through a linear layer, and taking a softmax. This classifier is learned jointly with the rest of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Decoding Process</head><p>At inference time, the model needs to identify the sequence with the highest conditional probability by marginalizing over all possible latent variables (see Eq. <ref type="formula" target="#formula_1">(3)</ref>), which is intractable in practice. We propose three approximating decoding algorithms to reduce the search space.</p><p>Argmax Decoding. Following <ref type="bibr" target="#b7">Gu et al. (2018)</ref>, one simple and effective method is to select the best sequence by choosing the highest-probability latent sequence z:</p><formula xml:id="formula_16">z * = argmax z∈Z p θ (z|x) y * = argmax y P θ (y|z * , x)</formula><p>where identifying y * only requires independently maximizing the local probability for each output position (see Eq. 4).</p><p>Noisy Parallel Decoding (NPD). A more accurate approximation of decoding, proposed in <ref type="bibr" target="#b7">Gu et al. (2018)</ref>, is to draw samples from the latent space and compute the best output for each latent sequence. Then, a pre-trained autoregressive model is adopted to rank these sequences. In FlowSeq, different candidates can be generated by sampling different target lengths or different samples from the prior, and both of the strategies can be batched via masks during decoding. In our experiments, we first select the top l length candidates from the length predictor in §3.5. Then, for each length candidate we use r random samples from the prior network to generate output sequences, yielding a total of l × r candidates.</p><p>Importance Weighted Decoding (IWD) The third approximating method is based on the lower bound of importance weighted estimation <ref type="bibr" target="#b2">(Burda et al., 2015)</ref>. Similarly to NPD, IWD first draws samples from the latent space and computes the best output for each latent sequence. Then, IWD <ref type="bibr">2</ref> We experimented with other methods such as meanpooling or taking the last hidden state and found no major difference in our experiments ranks these candidate sequences with K importance samples:</p><formula xml:id="formula_17">z i ∼ p θ (z|x), ∀i = 1, . . . , N y i = argmax y P θ (y|z i , x) z (k) i ∼ q φ (z|ŷ i , x), ∀k = 1, . . . , K P (ŷ i |x) ≈ 1 K K k=1 P θ (ŷ i |z (k) i ,x)p θ (z (k) i |x) q φ (z (k) i |ŷ i ,x)</formula><p>IWD does not rely on a separate pre-trained model, though it significantly slows down the decoding speed. The detailed comparison of these three decoding methods is provided in §4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Discussion</head><p>Different from the architecture proposed in <ref type="bibr" target="#b35">Ziegler and Rush (2019)</ref>, the architecture of FlowSeq is not using any autoregressive flow <ref type="bibr" target="#b12">(Kingma et al., 2016;</ref><ref type="bibr" target="#b22">Papamakarios et al., 2017)</ref>, yielding a truly non-autoregressive model with both efficient density estimation and generation. Note that FlowSeq remains non-autoregressive even if we use an RNN in the architecture because RNN is only used to encode a complete sequence of codes and all the input tokens can be fed into the RNN in parallel. This makes it possible to use highly-optimized implementations of RNNs such as those provided by cuDNN. 3 Thus while RNNs do experience some drop in speed, it is less extreme than that experienced when using autoregressive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setups</head><p>Translation Datasets We evaluate FlowSeq on three machine translation benchmark datasets: WMT2014 DE-EN (around 4.5M sentence pairs), WMT2016 RO-EN (around 610K sentence pairs) and a smaller dataset IWSLT2014 DE-EN (around 150K sentence pairs) <ref type="bibr" target="#b3">(Cettolo et al., 2012)</ref>. We use scripts from fairseq  to preprocess WMT2014 and IWSLT2014, where the preprocessing steps follow <ref type="bibr" target="#b31">Vaswani et al. (2017)</ref> for WMT2014. We use the data provided by  for WMT2016. For both WMT datasets, the source and target languages share the same set of subword embeddings while for IWSLT2014 we use separate embeddings. During training, we filter out sentences longer than 80 for WMT dataset and 60 for IWSLT, respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Modules and Hyperparameters</head><p>We implement the encoder, decoder and posterior networks with standard (unmasked) Transformer layers <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>. For WMT datasets, we use 8 attention heads, the encoder consists of 6 layers, and the decoder and posterior are composed of 4 layers. For IWSLT, we use 4 attention heads, the encoder has 5 layers, and decoder and posterior have 3 layers. Optimization Parameter optimization is performed with the Adam optimizer (Kingma and Ba, 2014) with β = (0.9, 0.999), = 1e −8 and AMS-Grad <ref type="bibr" target="#b25">(Reddi et al., 2018)</ref>. Each mini-batch consist of 2048 sentences. The learning rate is initialized to 5e − 4, and exponentially decays with rate 0.999995. The gradient clipping cutoff is 1.0. For all the FlowSeq models, we apply 0.1 label smoothing <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref> and averaged the 5 best checkpoints to create the final model. At the beginning of training, the posterior network is randomly initialized, producing noisy supervision to the prior. To mitigate this issue, we first set the weight of the KL term in the ELBO to zero for 30,000 updates to train the encoder, decoder and posterior networks. Then the KL weight  <ref type="table">Table 2</ref>: BLEU scores on two WMT datasets of models using advanced decoding methods. The first block are autoregressive Transformer-base <ref type="bibr" target="#b31">(Vaswani et al., 2017)</ref>. The second and third blocks are results of models trained w/w.o. knowledge distillation, respectively. n = l × r is the total number of rescoring candidates.</p><p>linearly increases to one for another 10,000 updates, which we found essential to accelerate training and achieve stable performance.</p><p>Knowledge Distillation Previous work on nonautoregressive generation <ref type="bibr" target="#b7">(Gu et al., 2018;</ref><ref type="bibr" target="#b6">Ghazvininejad et al., 2019)</ref> has used translations produced by a pre-trained autoregressive NMT model as the training data, noting that this can significantly improve the performance. We analyze the impact of distillation in § 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main Results</head><p>We first conduct experiments to compare the performance of FlowSeq with strong baseline models, including NAT w/ Fertility <ref type="bibr" target="#b7">(Gu et al., 2018)</ref>, NAT-IR , NAT-REG <ref type="bibr" target="#b34">(Wang et al., 2019)</ref>, LV NAR <ref type="bibr" target="#b29">(Shu et al., 2019)</ref>, CTC Loss <ref type="bibr" target="#b15">(Libovickỳ and Helcl, 2018)</ref>, and CMLM <ref type="bibr" target="#b6">(Ghazvininejad et al., 2019)</ref>. <ref type="table">Table 1</ref> provides the BLEU scores of FlowSeq with argmax decoding, together with baselines with purely non-autoregressive decoding methods that generate output sequence in one parallel pass. The first block lists results of models trained on raw data, while the second block shows results using knowledge distillation. Without using knowledge distillation, the FlowSeq base model achieves significant improvements (more than 9 BLEU points) over the baselines. This demonstrates the effectiveness of FlowSeq in modeling complex interdependences in the target languages.</p><p>Regarding the effect of knowledge distillation, we can mainly obtain two observations: i) Similar to the findings in previous work, knowledge distillation still benefits the translation quality of FlowSeq. ii) Compared to previous models, the benefit of knowledge distillation for FlowSeq is less significant, yielding less than 3 BLEU improvement on WMT2014 DE-EN corpus, and even no improvement on WMT2016 RO-EN corpus. We hypothesize that the reason for this is that FlowSeq's stronger model is more robust against multi-modality, making it less necessary to rely on knowledge distillation. <ref type="table">Table 2</ref> illustrates the BLEU scores of FlowSeq and baselines with advanced decoding methods such as iterative refinement, IWD and NPD rescoring. The first block in <ref type="table">Table 2</ref> includes the baseline results from autoregressive Transformer. For the sampling procedure in IWD and NPD, we sampled from a reduced-temperature model <ref type="bibr" target="#b13">(Kingma and Dhariwal, 2018)</ref> to obtain high-quality samples. We vary the temperature within {0.1, 0.2, 0.3, 0.4, 0.5, 1.0} and select the best temperature based on the performance on development sets. The analysis of the impact of sampling temperature and other hyperparameters on samples is shown in § 4.4. For FlowSeq, NPD obtains better results than IWD, showing that FlowSeq still falls behind the autore-gressive Transformer on modeling the distributions of target languages. Compared with CMLM <ref type="bibr" target="#b6">(Ghazvininejad et al., 2019)</ref> with 10 iterations of refinement, which is a contemporaneous work that achieves state-of-the-art translation performance, FlowSeq obtains competitive performance on both WMT2014 and WMT2016 corpora, with only slight degradation in translation quality. Notably we did not attempt to perform iterative refinement, but there is nothing that makes FlowSeq inherently incompatible with refinement -we leave connecting the two techniques to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis on Decoding Speed</head><p>In this section, we compare the decoding speed (measured in average time in seconds required to decode one sentence) of FlowSeq at test time with that of the autoregressive Transformer model. We use the test set of WMT14 EN-DE for evaluation and all experiments are conducted on a single NVIDIA TITAN X GPU.</p><p>How does batch size affect the decoding speed? First, we investigate how different decoding batch size can affect the decoding speed. We vary the decoding batch size within {1, 4, 8, 32, 64, 128}. <ref type="figure" target="#fig_4">Figure. 4a</ref> shows that for both FlowSeq and the autoregressive Transformer decoding is faster when using a larger batch size. However, FlowSeq has much larger gains in the decoding speed w.r.t. the increase in batch size, gaining a speed up of 594% of the base model and 403% of the large model when using a batch size of 128. We hypothesize that this is because the operations in FlowSeq are more friendly to batching while the incremental nature of left-to-right search in the autoregressive model is less efficient in benefiting from batching. How does sentence length affect the decoding speed? Next, we examine if sentence length is a major factor affecting the decoding speed. We bucket the test data by the target sentence length. From <ref type="figure" target="#fig_4">Fig. 4b</ref>, we can see that as the sentence length increases, FlowSeq achieves almost a constant decoding time while the autoregressive Transformer has a linearly increasing decoding time. The relative decoding speed of FlowSeq versus the Transformer linearly increases as the sequence length increases. The potential of decoding long sequences with constant time is an attractive property of FlowSeq.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis of Rescoring Candidates</head><p>In <ref type="figure" target="#fig_5">Fig. 5</ref>, we analyze how different sampling hyperparameters affect the performance of rescoring. First, we observe that the number of samples r for each length is the most important factor. The performance is always improved with a larger sample size. Second, a larger number of length candidates does not necessarily increase the rescoring performance. Third, we find that a larger sampling temperature (0.3 -0.5) can increase the diversity of translations and leads to better rescoring BLEU. However, the latent samples become noisy when a large temperature (1.0) is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Analysis of Translation Diversity</head><p>Following <ref type="bibr" target="#b9">He et al. (2018)</ref> and <ref type="bibr" target="#b28">Shen et al. (2019)</ref>, we analyze the output diversity of FlowSeq. They proposed pairwise-BLEU and BLEU computed in a leave-one-out manner to calibrate the diversity and quality of translation hypotheses. A lower pairwise-BLEU score implies a more diverse hy- pothesis set. And a higher BLEU score implies a better translation quality. We experiment on a subset of the test set of WMT14-ENDE with ten references for each sentence <ref type="bibr" target="#b20">(Ott et al., 2018)</ref>. In <ref type="figure" target="#fig_6">Fig. 6</ref>, we compare FlowSeq with other multihypothesis generation methods (ten hypotheses each sentence) to analyze how well the generation outputs of FlowSeq are in terms of diversity and quality. The right corner area of the figure indicates the ideal generations: high diversity and high quality. While FlowSeq still lags behind the autoregressive generation, by increasing the sampling temperature it provides a way of generating more diverse outputs while keeping the translation quality almost unchanged. More analysis of translation outputs and detailed results are provided in the Appendix D and E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose FlowSeq, an efficient and effective model for non-autoregressive sequence generation by using generative flows. One potential direction for future work is to leverage iterative refinement techniques such as masked language models to further improve translation quality. Another exciting direction is to, theoretically and empirically, investigate the latent space in FlowSeq, hence providing deeper insights into the model, and allowing for additional applications such as controllable text generation.   In <ref type="figure" target="#fig_7">Fig. 7</ref>, we plot the train and dev loss together with dev BLEU scores for the first 50 epochs. We can see that the reconstruction loss is increasing at the initial stage of training, then starts to decrease when training with full KL loss. In addition, we observed that FlowSeq does not suffer the KL collapse problem <ref type="bibr" target="#b1">(Bowman et al., 2015;</ref>. This is because the decoder of FlowSeq is non-autogressive, with latent variable z as the only input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Model Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Analysis of Translation Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Grundnahrungsmittel gibt es schlielich berall und jeder Supermarkt hat mittlerweile Sojamilch und andere Produkte.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>There are basic foodstuffs available everywhere , and every supermarket now has soya milk and other products. Sample 1</p><p>After all, there are basic foods everywhere and every supermarket now has soya amch and other products. Sample 2 After all, the food are available everywhere everywhere and every supermarket has soya milk and other products. Sample 3</p><p>After all, basic foods exist everywhere and every supermarket has now had soy milk and other products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Es kann nicht erklären, weshalb die National Security Agency Daten ber das Privatleben von Amerikanern sammelt und warum Whistleblower bestraft werden, die staatliches Fehlverhalten offenlegen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>And, most recently, it cannot excuse the failure to design a simple website more than three years since the Affordable Care Act was signed into law.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample 1</head><p>And recently, it cannot apologise for the inability to design a simple website in the more than three years since the adoption of Affordable Care Act.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample 2</head><p>And recently, it cannot excuse the inability to design a simple website in more than three years since the adoption of Affordable Care Act.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample 3</head><p>Recently, it cannot excuse the inability to design a simple website in more than three years since the Affordable Care Act has passed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Doch wenn ich mir die oben genannten Beispiele ansehe, dann scheinen sie weitgehend von der Regierung selbst gewählt zu sein.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Yet, of all of the examples that I have listed above, they largely seem to be of the administration's own choosing. Sample 1 However, when I look at the above mentioned examples, they seem to be largely elected by the government itself. Sample 2</p><p>But if I look at the above mentioned examples, they seem to have been largely elected by the government itself. Sample 3</p><p>But when I look at the above examples, they seem to be largely chosen by the government itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Source</head><p>Damit wollte sie auf die Gefahr von noch gröeren Ruinen auf der Schweizer Wiese hinweisen -sollte das Riesenprojekt eines Tages scheitern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>In so doing they wanted to point out the danger of even bigger ruins on the Schweizer Wiese -should the huge project one day fail. Sample 1</p><p>In so doing, it wanted to highlight the risk of even greater ruins on the Swiss meadow -the giant project should fail one day. Sample 2</p><p>In so doing, it wanted to highlight the risk of even greater ruins on the Swiss meadow -if the giant project fail one day. Sample 3</p><p>In doing so, it wanted point out the risk of even greater ruins on the Swiss meadow -the giant project would fail one day. In Tab. 4, we present randomly picked translation outputs from the test set of WMT14-DEEN. For each German input sentence, we pick three hypotheses from 30 samples. We have the following observations: First, in most cases, it can accurately express the meaning of the source sentence, sometimes in a different way from the reference sentence, which cannot be precisely reflected by the BLEU score. Second, by controlling the sampling hyper-parameters such as the length candidates l, the sampling temperature τ and the number of samples r under each length, FlowSeq is able to generate diverse translations expressing the same meaning. Third, repetition and broken translations also exist in some cases due to the lack of direct modeling of dependencies between target words.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Results of Translation Diversity</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>1 https://github.com/XuezheMax/flowseq (a) Autoregressive (b) non-autoregressive and (c) our proposed sequence generation models. x is the source, y is the target, and z are latent variables.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Neural architecture of FlowSeq, including the encoder, the decoder and the posterior networks, together with the multi-scale architecture of the prior flow. The architecture of each flow step is in Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>One step of flow. (b) Coupling layer splits. (c) NN function on the split of the coupling layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>(a) The architecture of one step of our flow. (b) The visualization of three split pattern for coupling layers, where the red color denotes z a and the blue color denotes zv b . (c) The attention-based architecture of the NN function in coupling layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>The decoding speed of the Transformer (batched, beam size 5) and FlowSeq on WMT14 EN-DE test set (a) w.r.t. different batch sizes (b) bucketed by different target sentence lengths (batch size 32).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Impact of sampling hyperparameters on the rescoring BLEU on the dev set of WMT14 DE-EN. Experiments are performed with FlowSeq-base trained with distillation data. l is the number of length candidates. r is the number of samples for each length.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Comparisons of FlowSeq with human translations, beam search and sampling results of Transformer-base, and mixture-of-experts model (Hard MoE<ref type="bibr" target="#b28">(Shen et al., 2019)</ref>) on the averaged leave-one-out BLEU score v.s pairwise-BLEU in descending order.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Training dynamics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Token Dropout. The motivation of introducing</figDesc><table><row><cell></cell><cell cols="3">Output Probabilities</cell><cell></cell></row><row><cell></cell><cell cols="3">Target Decoder</cell><cell></cell></row><row><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row><row><cell></cell><cell cols="3">Target Encoder</cell><cell></cell></row><row><cell>A</cell><cell>dog</cell><cell>runs</cell><cell>away</cell><cell>.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Comparison of model size in our experiments.</figDesc><table><row><cell cols="3">C Analysis of training dynamics</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Train Reconstruction Loss</cell><cell>60 70 80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Reconstruction Loss KL loss</cell><cell>100 200 300 Train KL loss</cell></row><row><cell></cell><cell>0</cell><cell>10</cell><cell>20</cell><cell>Epoch</cell><cell>30</cell><cell>40</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Examples of translation outputs from FlowSeq-base with sampling hyperparameters l = 3, r = 10, τ = 0.4 on WMT14-DEEN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc>shows the detailed results of translation diversity.</figDesc><table><row><cell>Models</cell><cell>τ</cell><cell cols="2">Pairwise BLEU LOO BLEU</cell></row><row><cell>Human</cell><cell>-</cell><cell>35.48</cell><cell>69.07</cell></row><row><cell>Sampling</cell><cell>-</cell><cell>24.10</cell><cell>37.80</cell></row><row><cell cols="2">Beam Search -</cell><cell>73.00</cell><cell>69.90</cell></row><row><cell>Hard-MoE</cell><cell>-</cell><cell>50.02</cell><cell>63.80</cell></row><row><cell></cell><cell>0.1</cell><cell>79.39</cell><cell>61.61</cell></row><row><cell></cell><cell>0.2</cell><cell>72.12</cell><cell>61.05</cell></row><row><cell>FlowSeq</cell><cell>0.3</cell><cell>67.85</cell><cell>60.79</cell></row><row><cell>l=1, r=10</cell><cell>0.4</cell><cell>64.75</cell><cell>60.07</cell></row><row><cell></cell><cell>0.5</cell><cell>61.12</cell><cell>59.54</cell></row><row><cell></cell><cell>1.0</cell><cell>43.53</cell><cell>52.86</cell></row><row><cell></cell><cell>0.1</cell><cell>70.32</cell><cell>60.54</cell></row><row><cell></cell><cell>0.2</cell><cell>66.45</cell><cell>60.21</cell></row><row><cell>FlowSeq</cell><cell>0.3</cell><cell>63.72</cell><cell>59.81</cell></row><row><cell>l=2, r=5</cell><cell>0.4</cell><cell>61.29</cell><cell>59.47</cell></row><row><cell></cell><cell>0.5</cell><cell>58.49</cell><cell>58.80</cell></row><row><cell></cell><cell>1.0</cell><cell>42.93</cell><cell>52.58</cell></row><row><cell></cell><cell>0.1</cell><cell>62.21</cell><cell>58.70</cell></row><row><cell></cell><cell>0.2</cell><cell>59.74</cell><cell>58.59</cell></row><row><cell>FlowSeq</cell><cell>0.3</cell><cell>57.57</cell><cell>57.96</cell></row><row><cell>l=3, r=4</cell><cell>0.4</cell><cell>55.66</cell><cell>57.45</cell></row><row><cell></cell><cell>0.5</cell><cell>53.49</cell><cell>56.93</cell></row><row><cell></cell><cell>1.0</cell><cell>39.75</cell><cell>50.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Translation diversity results of FlowSeq-large model on WMT14 EN-DE with knowledge distillation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://devblogs.nvidia.com/optimizing-recurrentneural-networks-cudnn-5/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Xuezhe MA was supported in part by DARPA grant FA8750-18-2-0018 funded under the AIDA program and Chunting Zhou was supported by DARPA grant HR0011-15-C-0114 funded under the LORELEI program. Any opinions, findings, and conclusions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA. The authors thank Amazon for their gift of AWS cloud credits and anonymous reviewers for their helpful suggestions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix: FlowSeq A Flow Layers</head><p>where h is the number of heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Affine Coupling</head><p>Log-determinant: sum(log |s|)</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349</idno>
		<title level="m">Generating sentences from a continuous space</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.00519</idno>
		<title level="m">Importance weighted autoencoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wit3: Web inventory of transcribed and translated talks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauro</forename><surname>Cettolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Girardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcello</forename><surname>Federico</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of European Association for Machine Translation</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="261" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. 2016. Density estimation using real nvp</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Constant-time machine translation with conditional masked language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09324</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Non-autoregressive neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">K</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>ICLR-2018</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Insertion-based decoding with automatically inferred generation order</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.01370</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequence to sequence mixture model for diverse machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanli</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gholamreza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Computational Natural Language Learning</title>
		<meeting>the 22nd Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="583" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving variational inference with inverse autoregressive flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 29th Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deterministic non-autoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elman</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">End-toend non-autoregressive neural machine translation with connectionist temporal classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Libovickỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jindřich</forename><surname>Helcl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3016" to="3021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.04208</idno>
		<title level="m">Macow: Masked convolutional generative flow</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Stackpointer networks for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zecong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1403" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mae: Mutual posterior-divergence regularization for variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th International Conference on Learning Representations (ICLR-2019)</title>
		<meeting>the 7th International Conference on Learning Representations (ICLR-2019)<address><addrLine>New Orleans, Louisiana, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">context2vec: Learning generic context embedding with bidirectional LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Melamud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K16-1006</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Analyzing uncertainty in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3953" to="3962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Masked autoregressive flow for density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Pavlakou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2338" to="2347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Waveglow: A flow-based generative network for speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Prenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3617" to="3621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the convergence of adam and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satyen</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A neural attention model for abstractive sentence summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Alexander M Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mixture models for diverse machine translation: Tricks of the trade</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianxiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5719" to="5728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Latent-variable nonautoregressive neural machine translation with deterministic inference using a delta posterior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideki</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07181</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Insertion transformer: Flexible sequence generation via insertion operations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.03249</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graphical models, exponential families, and variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="305" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiren</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.10245</idno>
		<title level="m">Non-autoregressive machine translation with auxiliary regularization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Latent normalizing flows for discrete sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7673" to="7682" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
