<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Counting Everyday Objects in Everyday Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prithvijit</forename><surname>Chattopadhyay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprasaath</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Virginia Tech</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
							<email>dbatra@gatech.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
							<email>parikh@gatech.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Counting Everyday Objects in Everyday Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We are interested in counting the number of instances of object classes in natural, everyday images. Previous counting approaches tackle the problem in restricted domains such as counting pedestrians in surveillance videos. Counts can also be estimated from outputs of other vision tasks like object detection. In this work, we build dedicated models for counting designed to tackle the large variance in counts, appearances, and scales of objects found in natural scenes. Our approach is inspired by the phenomenon of subitizing -the ability of humans to make quick assessments of counts given a perceptual signal, for small count values. Given a natural scene, we employ a divide and conquer strategy while incorporating context across the scene to adapt the subitizing idea to counting. Our approach offers consistent improvements over numerous baseline approaches for counting on the PASCAL VOC 2007 and COCO datasets. Subsequently, we study how counting can be used to improve object detection. We then show a proof of concept application of our counting methods to the task of Visual Question Answering, by studying the 'how many?' questions in the VQA and COCO-QA datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We study the scene understanding problem of counting common objects in natural scenes. That is, given for example the image in <ref type="figure">Fig. 1</ref>, we want to count the number of everyday object categories present in it: for example 4 chairs, 1 oven, 1 dining table, 1 potted plant and 3 spoons. Such an ability to count seems innate in humans (and even in some animals <ref type="bibr" target="#b9">[10]</ref>). Thus, as a stepping stone towards Artificial Intelligence (AI), it is desirable to have intelligent machines that can count.</p><p>Similar to scene understanding tasks such as object detection <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b28">29]</ref> and segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b35">36]</ref> which require a fine-grained understanding of the scene, object counting is a challenging problem that * Denotes equal contribution.  <ref type="table" target="#tab_2">Table : 1</ref> Oven : 1 <ref type="bibr">Figure 1:</ref> We study the problem of counting everyday objects in everyday scenes. Given an everyday scene, we want to predict the number of instances of common objects like bottle, chair etc.</p><p>requires us to reason about the number of instances of objects present while tackling scale and appearance variations. Another closely related vision task is visual question answering (VQA), where the task is to answer free form natural language questions about an image. Interestingly, questions related to the count of a particular object -How many red cars do you see? form a significant portion of the questions asked in common visual question answering datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35]</ref>. Moreover, we observe that end-to-end networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15]</ref> trained for this task do not perform well on such counting questions. This is not surprising, since the objective is often setup to minimize the crossentropy classification loss for the correct answer to a question, which ignores ordinal structure inherent to counting. In this work we systematically benchmark how well current VQA models do at counting, and study any benefits from dedicated models for counting on a subset of counting questions in VQA datasets in Sec. <ref type="bibr">5.4.</ref> Counts can also be used as complimentary signals to aid other vision tasks like detection. If we had an estimate of how many objects were present in the image, we could use that information on a per-image basis to detect that many objects. Indeed, we find that our object counting models improve object detection performance.</p><p>We first describe some baseline approaches to counting and subsequently build towards our proposed model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Glance</head><p>Associative Subitizing (aso-sub) Detect  <ref type="figure">Figure 2</ref>: A toy example explaining the motivation for three categories of counting approaches explored in this paper. The task is to count the number of stars and circles. In detect, the idea is to detect instances of a category, and then report the total number of instances detected as the count. In glance, we make a judgment of count based on a glimpse of the full image. In aso-sub, we divide the image into regions and judge count based on patterns in local regions. Counts from different regions are added through arithmetic.</p><p>Counting by Detection: It is easy to realize that perfect detection of objects would imply a perfect count. While detection is sufficient for counting, localizing objects is not necessary. Imagine a scene containing a number of mugs kept on a table where the objects occlude each other. In order to count the number of mugs, we need not determine with pixel-accurate segmentations or detections where they are (which is hard in the presence of occlusions) as long as say we can determine the number of handles. Relieving the burden of detecting objects is also effective for counting when objects occur at smaller scales where detection is hard <ref type="bibr" target="#b17">[18]</ref>. However, counting by detection or detect still forms a natural approach for counting. Counting by Glancing: Representations extracted from Deep Convolutional Neural Networks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b25">26]</ref> trained on image classification have been successfully applied to a number of scene understanding tasks such as finegrained recognition <ref type="bibr" target="#b11">[12]</ref>, scene classification <ref type="bibr" target="#b11">[12]</ref>, object detection <ref type="bibr" target="#b11">[12]</ref>, etc. We explore how well features from a deep CNN perform at counting through instantiations of our glancing (glance) models which estimate a global count for the entire scene in a single forward pass. This can be thought of as estimating the count at one shot or glance. This is in contrast with detect, which sequentially increments its count with each detected object <ref type="figure">(Fig. 2</ref>). Note that unlike detection, which optimizes for a localization objective, the glance models explicitly learn to count. Counting by Subitizing: Subitizing is a widely studied phenomenon in developmental psychology <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10]</ref> which indicates that children have an ability to directly map a perceptual signal to a numerical estimate, for a small number of objects (typically 1-4). Subitizing is crucial for development and assists arithmetic and reasoning skills. An example of subitizing is how we are able to figure out the number of pips on a face of a die without having to count them or how we are able to reason about tally marks. Inspired by subitizing, we devise a new counting approach which adopts a divide and conquer strategy, using the additive nature of counts. Note that glance can be thought of as an attempt to subitize from a glance of the image. However, as illustrated in <ref type="figure">Fig. 2</ref> (center), subitizing is difficult at high counts for humans.</p><p>Inspired by this, using the divide and conquer strategy, we divide the image into non-overlapping cells <ref type="figure">(Fig. 2  right)</ref>. We then subitize in each cell and use addition to get the total count. We call this method associative subitizing or aso-sub.</p><p>In practice, to implement this idea on real images, we incorporate context across the cells while sequentially subitizing in each one of them. We call this sequential subitizing or seq-sub. For each of these cells we curate realvalued ground truth, which helps us deal with scale variations. Interestingly, we found that by incorporating context seq-sub significantly outperforms the naive subitizing model aso-sub described above. (see Sec. 5.1 for more details). Counting by Ensembling: It is well known that when humans are given counting problems with large ground truth counts (e.g. counting number of pebbles in a jar), individual guesses have high variance, but an average across multiple responses tends to be surprisingly close to the ground truth. This phenomenon is popularly known as the wisdom of the crowd <ref type="bibr" target="#b15">[16]</ref>. Inspired by this, we create an ensemble of counting methods (ens).</p><p>In summary, we evaluate several natural approaches to counting, and propose a novel context and subitizing based counting model. Then we investigate how counting can improve detection. Finally, we study counting questions ('how many?') in the Visual Question Answering (VQA) <ref type="bibr" target="#b2">[3]</ref> and COCO-QA <ref type="bibr" target="#b34">[35]</ref> datasets and provide some comparisons with the state-of-the-art VQA models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Counting problems in niche settings have been studied extensively in computer vision <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b26">27]</ref>. <ref type="bibr" target="#b6">[7]</ref> explores a Bayesian Poisson regression method on low-level features for counting in crowds. <ref type="bibr" target="#b5">[6]</ref> segments a surveillance video into components of homogeneous motion and regresses to counts in each region using Gaussian Process regression. Since surveillance scenes tend to be constrained and highly occluded, counting by detection is infeasible. Thus density based approaches are popular. Lempitsky and Zisserman <ref type="bibr" target="#b26">[27]</ref> count people by estimating object density using low-level features. They show applications on surveillance and cell counting in biological images. Anchovi labs provided users interactive services to count specific objects such as swimming pools in satellite images, cells in biological images, etc. More recent work constructs CNN-based models for crowd counting <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b32">33]</ref> and penguin counting <ref type="bibr" target="#b3">[4]</ref> using lower level convolutional features from shallower CNN models.</p><p>Counting problems in constrained settings have a funda-mentally different set of challenges to the counting problem we study in this paper. In surveillance, for example, the challenge is to estimate the counts accurately in the presence of large number of ground truth counts, where there might be significant occlusions. In the counting problem on everyday scenes, a larger challenge is the intra-class variance in everyday objects, and high sparsity (most images will have 0 count for most object classes). Thus we need a qualitatively different set of tools to solve this problem. Other recent work <ref type="bibr" target="#b45">[46]</ref> studies the problem of salient object subitizing (SOS). This is the task of counting the number of salient objects in the image (independent of the category). In contrast, we are interested in counting the number of instances of objects per category. Unlike Zhang et al. <ref type="bibr" target="#b45">[46]</ref>, who use SOS to improve salient object detection, we propose to improve generic object detection using counts. Our VQA experiments to diagnose counting performance are also similar in spirit to recent work that studies how well models perform on specific question categories (counting, attribute comparison, etc.) <ref type="bibr" target="#b21">[22]</ref> or on compositional generalization <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our task is to accurately count the number of instances of different object classes in an image. For training, we use datasets where we have access to object annotations such as object bounding boxes and category wise counts. The count predictions from the models are evaluated using the metrics described in Sec. 4.2. The input to the glance, aso-sub and seq-sub models are fc7 features from a VGG-16 <ref type="bibr" target="#b41">[42]</ref> CNN model. We experiment using both offthe-shelf classification weights from ImageNet <ref type="bibr" target="#b37">[38]</ref> and the detection fine-tuned weights from our detect models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Detection (detect)</head><p>We use the Fast R-CNN <ref type="bibr" target="#b17">[18]</ref> object detector to count. Detectors typically perform two post processing steps on a set of preliminary boxes: non maximal suppression (NMS) and score thresholding. NMS discards highly overlapping and likely redundant detections (using a threshold to control the overlap), whereas the score threshold filters out all detections with low scores.</p><p>We steer the detector to count better by varying these two hyperparameters to find the setting where counting error is the least. We pick these parameters using grid search on a held-out val set. For each category, we first select a fixed NMS threshold of 0.3 for all the classes and vary the score threshold between 0 and 1. We then fix the score threshold to the best value and vary the NMS threshold from 0 to 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Glancing (glance)</head><p>Our glance approach repurposes a generic CNN architecture for counting by training a multi-layered perceptron <ref type="figure">Figure 3</ref>: Canonical counting scale: Consider images with grids 2 × 2 (left) and 6 × 6 (right). Notice the red cells in both images: it is evident that if the cell size is too large compared to the object (left), it is difficult to estimate the large integer count of 'sheep' in the cell. However, if the cell is too small (right), it might be hard to estimate the small fractional count of 'bus' in the cell. Hence, we hypothesize that there exists a sweet spot in discretization of the cells that would results in optimum counting performance.</p><p>(MLP) with a L2 loss to regress to image level counts from deep representations extracted from the CNN. The MLP has batch normalization <ref type="bibr" target="#b19">[20]</ref> and Rectified Linear Unit (ReLU) activations between hidden layers. The models were trained with a learning rate of 10 −3 and weight decay set to 0.95. We experiment with choices of a single hidden layer, and two hidden layers for the MLP, as well as the sizes of the hidden units. More details and ablation studies can be found in appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Subitizing (aso-sub, seq-sub)</head><p>In our subitizing inspired methods, we divide our counting problem into sub-problems on each cell in a nonoverlapping grid, and add the predicted counts across the grid. In practice, since objects in real images occur at different scales, such cells might contain fractions of an object. We adjust for this by allowing for real valued ground truth. If a cell overlapping an object is very small compared to the object, the small fractional count of the cell might be hard to estimate. On the other hand, if a cell is too large compared to objects present it might be hard to estimate the large integer count of the cell (see <ref type="figure">Fig. 3</ref>). This tradeoff suggests that at some canonical resolution, we would be able to count the smaller objects more easily by subitizing them, as well as predict the partial counts for larger objects. More concretely, we divide the image I, into a set of n nonoverlapping cells P = {p 1 , · · · , p n } such that I = n i=1 p i and p i ∩ p j (i =j) = φ. Given such a partition P of the image I and associated CNN features X = {x i , · · · , x n }, we now explain our models based on this approach: aso-sub : Our naive aso-sub model treats each cell independently to regress to the real-valued ground truth. We train on an augmented version of the dataset where the dataset size is n-fold (n cells per image). Unlike glance, where feature extracted on the full image is used to regress to integer valued counts, aso-sub models regress to real-  <ref type="figure">Figure 4</ref>: For both these images, the count of person is 1. Consider splitting this image into 2 × 1 cells (for illustration) for aso-sub. The bottom half of the left image and top half of the right image both contains similar visual signals -top-half of a person. However, the ground truth count on the cell on the left is 1, and the one on the right is 0.5. An approach that estimates counts from individual cells out of context is bound to fail at these cases. This motivates our proposed approach seq-sub.</p><p>valued counts on non-overlapping cells from features extracted per cell. Given class instance annotations as bounding boxes b = {b 1 , · · · , b N } for a category k in an image I, we compute the ground truth partial counts (c k gt ) for the grid-cells (p i ) to be used for training as follows:</p><formula xml:id="formula_0">p i : c k gt = N j=1 p i ∩ b j b j<label>(1)</label></formula><p>We compute the intersection of each box b i with the cell p i and add up the intersections normalized by b i . Further, given the cell-level count predictions c pi , the image level count prediction is computed as c = n i=1 max(0, c pi ). We use max to filter out negative predictions.</p><p>We experiment with dividing the image into equally sized 3 × 3, 5 × 5, and 7 × 7 grid-cells. The architecture of the models trained on the augmented dataset are the same as glance. For more details, refer to appendix. seq-sub : We motivate our proposed seq-sub (Sequential Subitizing) approach by identifying a potential flaw in the naive aso-sub approach. <ref type="figure">Fig. 4</ref> reveals the limitation of the aso-sub model. If the cells are treated independently, the naive aso-sub model will be unaware of the partial presence of the concerned object in other cells. This leads to situations where similar visual signals need to be mapped to partial and whole presence of the object in the cells (see <ref type="figure">Fig. 4</ref>). This is especially pathological since Huber or L-2 losses cannot capture this multi-modality in the output space, since the implicit density associated with such losses is either laplacian or gaussian.</p><p>Interestingly, a simple solution to mitigate this issue is to model context, which resolves this ambiguity in counts. That is, if we knew about the partial class presence in other cells, we could use that information to predict the correct cell count. Thus, although the independence assumption in aso-sub is convenient, it ignores the fact that the augmented dataset is not IID. While it is important to reason at  <ref type="figure">Figure 5</ref>: Architecture used for our seq-sub models. We extract a hidden layer representation of the fc7 feature volume corresponding to the 3 × 3 discretization of the image. Subsequently, we traverse this representation volume in two particular sequences in parallel as shown via two stacked bi-LSTMs per sequence and aggregate context over the image. We get output states corresponding to each of the cells and subsequently get cell-counts via another hidden layer. The hidden layers use ReLU as non-linearity. a cell level, it is also necessary to be aware of the global image context to produce meaningful predictions. In essence, we propose seq-sub, that takes the best of both worlds from glance and aso-sub. The architecture of seq-sub is shown in <ref type="figure">Fig. 5</ref>. It consists of a pair of 2 stacked bi-directional sequence-to-sequence LSTMs <ref type="bibr" target="#b39">[40]</ref>. We incorporate context across cells as</p><formula xml:id="formula_1">c pi = h(f 1 (x 1 , θ 1 ), · · · , f n (x n , θ n ), i, θ)<label>(2)</label></formula><p>where individual f i (x i , θ i ) are hidden layer representations of each cell feature with respective parameters and h(., θ) is the mechanism that captures context. This can be broken down as follows. Let H be the set containing f i (x i , θ i )s. Let H O1 and H O2 be 2 ordered sets which are permutations of H based on 2 particular sequence structures. The (traversal) sequences, as we move across the grid in the feature column, is decided on the basis of nearness of cells (see <ref type="figure">Fig. 5</ref>).</p><p>We experiment with the sequence structures best described for a 3 × 3 grid as N and Z which correspond to H O1 and H O2 . Each of these feature sequences are then fed to a pair of stacked Bi-LSTMs (L j (., i, θ l )) and the corresponding cell output states are concatenated to obtain a context vec-</p><formula xml:id="formula_2">tor (v i ) for each cell as v i = L 1 (H O1 , i, θ l )||L 2 (H O2 , i, θ l ).</formula><p>The cell counts are then obatined as c pi = g(v i , θ g ). The composition of L j (., i, θ l ) and g(., θ g ) implements h(., θ).</p><p>We use a Huber Loss objective to regress to the count values with a learning rate of 10 −4 and weight decay set to 0.95. For optimization, we use Adam <ref type="bibr" target="#b23">[24]</ref> with a minibatch size of 64. The ground truth construction procedure for training and the count aggregation procedure for evaluation are as defined in aso-sub.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We experiment with two datasets depicting everyday objects in everyday scenes: the PASCAL VOC 2007 <ref type="bibr" target="#b12">[13]</ref> and COCO <ref type="bibr" target="#b27">[28]</ref>.  <ref type="figure" target="#fig_5">Fig. 6</ref> shows a histogram of nonzero counts across all object categories. It can be clearly seen that although the two datasets have a fair amount of count variability, there is a clear bias towards lower count values. Note that this is unlike the crowd-counting datasets, in particular <ref type="bibr" target="#b18">[19]</ref> where mean count is 1279.48 ± 960.42 and also unlike PASCAL and COCO, the images have very little scale and appearance variations in terms of objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation</head><p>We adopt the root mean squared error (RMSE) as our metric. We also evaluate on a variant of RMSE that might be better suited to human perception. The intuition behind this metric is as follows. In a real world scenario, humans tend to perceive counts in the logarithmic scale <ref type="bibr" target="#b10">[11]</ref>. That is, a mistake of 1 for a ground truth count of 2 might seem egregious but the same mistake for a ground truth count of 25 might seem reasonable. Hence we scale each deviation by a function of the ground truth count.</p><p>We first post-process the count predictions from each method by thresholding counts at 0, and rounding predictions to closest integers to get predictionsĉ ik . Given these predictions and ground truth counts c ik for a category k and image i, we compute RMSE as follows:</p><formula xml:id="formula_3">RM SE k = 1 N N i=1 (ĉ ik − c ik ) 2<label>(3)</label></formula><p>and relative RMSE as:</p><formula xml:id="formula_4">relRM SE k = 1 N N i=1 (ĉ ik − c ik ) 2 c ik + 1<label>(4)</label></formula><p>where N is the number of images in the dataset. We then average the error across all categories to report numbers on the dataset (mRMSE and m-relRMSE).</p><p>We also evaluate the above metrics for ground truth instances with non-zero counts. This reflects more clearly how accurate the counts produced by a method (beyond predicting absence) are.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Methods and Baselines</head><p>We compare our approaches to the following baselines: always-0: predict most-frequent ground truth count (0). mean: predict the average ground truth count on the Countval set. always-1: predict the most frequent non-zero value (1) for all classes. category-mean: predict the average count per category on Count-val. gt-class: treat the ground truth counts as classes and predict the counts using a classification model trained with cross-entropy loss.</p><p>We evaluate the following variants of counting approaches (see Sec. 3 for more details): detect: We compare two methods for detect. The first method finds the best NMS and score thresholds as explained in Sec. 3.1. The second method uses vanilla Fast R-CNN as it comes out of the box, with the default NMS and score thresholds. glance: We explore the following choices of features: (1) vanilla classification fc7 features noft, (2) detection fine tuned fc7 features ft, (3) fc7 features from a CNN trained to perform Salient Object Subitizing sos <ref type="bibr" target="#b45">[46]</ref> and (4) flattened conv-3 features from a CNN trained for classification aso-sub, seq-sub: We examine three choices of grid sizes (Sec. 3.3): 3 × 3, 5 × 5, and 7 × 7 and noft and ft features as above. ens: We take the best performing subset of methods and average their predictions to perform counting by ensembling (ens).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>All the results presented in the paper are averaged on 10 random splits of the test set sampled with replacement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Counting Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL VOC 2007 :</head><p>We first present results <ref type="table">(Table.</ref> 1) for the best performing variants (picked based on the val set) of each method. We see that seq-sub outperforms all other methods. Both glance and detect which perform equally well as per both the metrics, while glance does slightly better on both metrics when evaluated on nonzero ground truth counts. To put these numbers in perspective, we find that the difference of 0.01 mRM SE-nonzero 0.51 ± 0.02 1.87 ± 0.08 0.29 ± 0.01 0.75 ± 0.02 aso-sub-ft-1L-3 × 3 0.43 ± 0.01 1.65 ± 0.07 0.22 ± 0.01 0.68 ± 0.02 seq-sub-ft-3 × 3 0.42 ± 0.01 1.65 ± 0.07 0.21 ± 0.01 0.68 ± 0.02 ens 0.42 ± 0.17 1.68 ± 0.08 0.20 ± 0.00 0.65 ± 0.01 between seq-sub and aso-sub leads to a difference of 0.19% mean F-measure performance in our counting to improve detection application (Sec. 5.3). We also experiment with conv3 features to regress to the counts, similar to Zhang.et.al. <ref type="bibr" target="#b44">[45]</ref>. We find that conv3 gets mRM SE of 0.63 which is much worse than fc7. We also tried PCA on the conv3 features but that did not improve performance. This indicates that our counting task is indeed more high level and needs to reason about objects rather than lowlevel textures. We also compare our approach with the SOS model <ref type="bibr" target="#b45">[46]</ref> by extracting fc7 features from a model trained to perform category-independent salient object subitizing. We observe that our best performing glance setup using Imagenet trained VGG-16 features outperforms the one using SOS features. This is also intuitive since SOS is a category independent task, while we want to count number of object instances of each category. Finally, we observe that the performance increment from aso-sub to seq-sub is not statistically significant. We hypothesize that this is because of the smaller size of the PASCAL dataset. Note that we get more consistent improvements on COCO <ref type="table">(Table.</ref> 2), which is not only a larger dataset, but also contains scenes that are contextually richer. 1 COCO : We present results for the best performing variants (picked based on the val set) of each method. The results are summarized in <ref type="table">Table.</ref> 2. We find that seq-sub does the best on both mRM SE and m-relRM SE as well as their non-zero variants by a significant margin. A comparison indicates that the always-0 baseline does better on COCO than on PASCAL. This is because COCO has many more categories than PASCAL. Thus, the chances of any particular object being present in an image decrease compared to PASCAL. The performance jump from aso-sub to seq-sub here is much more compared to PASCAL. Recent work by Ren and Zemel <ref type="bibr" target="#b35">[36]</ref> on Instance Segmentation also reports counting performance on two COCO categories person and zebra. 2 <ref type="bibr" target="#b0">1</ref> When the Count-val split is considered, PASCAL has an average of 1.98 annotated objects per scene, unlike COCO which has 7.22 annotated objects per scene. <ref type="bibr" target="#b1">2</ref> We compare our best performing seq-sub model with their ap-Approach mRMSE mRMSE-nz m-relRMSE m-relRMSE-nz always-0 0.54 ± 0.01 3.03 ± 0.03 0.21 ± 0.00 1.22 ± 0.01 mean 0.54 ± 0.00 2.96 ± 0.03 0.23 ± 0.00 1.17 ± 0.01 always-1</p><p>1.12 ± 0.00 2.39 ± 0.03 1.00 ± 0.00 0.80 ± 0.00 category-mean 0.52 ± 0.01 2.97 ± 0.03 0.22 ± 0.00 1.18 ± 0.01 gt-class 0.47 ± 0.00 2.70 ± 0.03 0.20 ± 0.00 1.08 ± 0.00 detect 0.49 ± 0.00 2.78 ± 0.03 0.20 ± 0.00 1.13 ± 0.01 glance-ft-1L 0.42 ± 0.00 2.25 ± 0.02 0.23 ± 0.00 0.91 ± 0.00 glance-sos-1L</p><p>0.44 ± 0.00 2.32 ± 0.03 0.24 ± 0.00 0.92 ± 0.01 aso-sub-ft-1L-3 × 3 0.38 ± 0.00 2.08 ± 0.02 0.24 ± 0.00 0.87 ± 0.01 seq-sub-ft-3 × 3 0.35 ± 0.00 1.96 ± 0.02 0.18 ± 0.00 0.82 ± 0.01 ens 0.36± 0.00 1.98± 0.02 0.18± 0.00 0.81± 0.01  For both PASCAL and COCO we observe that while ens outperforms other approaches in some cases, it does not always do so. We hypothesize that this is due to the poor performance of glance. For detailed ablation studies on ens see appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Analysis of the Predicted Counts</head><p>Count versus Count Error : We analyze the performance of each of the methods at different count values on the COCO Count-test set ( <ref type="figure" target="#fig_6">Fig. 7)</ref>. We pick each count value on the x-axis and compute the RM SE over all the instances at that count value. Interestingly, we find that the subitizing approaches work really well across a range of count values. This supports our intuition that aso-sub and seq-sub are able to capture partial counts (from larger objects) as well as integer counts (from smaller objects) better which is intuitive since larger counts are likely to occur at a smaller  <ref type="figure">Figure 8</ref>: We show the ground truth count (top), outputs of detect with a default score threshold of 0.8 (row 1), and outputs of detect with hyperparameters tuned for counting (row 2). Clearly, choosing a different threshold allows us to trade-off localization accuracy for counting accuracy (see bottle image). The method finds partial evidence for counts, even if it cannot localize the full object.</p><p>scale. Of the two approaches, seq-sub works better, likely because reasoning about global context helps us capture part-like features better compared to aso-sub. This is quite clear when we look at the performance of seq-sub compared to aso-sub in the count range 11 to 15. For lower count values, ens does the best <ref type="figure" target="#fig_6">(Fig. 7)</ref>. We can see that for counts &gt; 5, glance and detect performances start tailing off. Detection : We tune the hyperparameters of Fast R-CNN in order to find the setting where the mean squared error is the lowest, on the Count-val splits of the datasets. We show some qualitative examples of the detection ground truth, the performance without tuning for counting (using black-box Fast R-CNN), and the performance after tuning for counting on the PASCAL dataset in <ref type="figure">Fig. 8</ref>. We use untuned Fast R-CNN at a score threshold of 0.8 and NMS threshold of 0.3, as used by Girshick et al. <ref type="bibr" target="#b17">[18]</ref> in their demo. At this configuration, it achieves an mRM SE of 0.52 on Counttest split of COCO. We find that we achieve a gain of 0.02 mRM SE by tuning the hyperparameters for detect. Subitizing : We next analyze how different design choices in aso-sub affect performance on PASCAL. We pick the best performing aso-sub-ft-1L-3 × 3 model and vary the grid sizes (as explained in Sec. 4). We experiment with 3 × 3, 5 × 5, and 7 × 7 grid sizes. We observe that for aso-sub the performance of 3 × 3 grid is the best and performance deteriorates significantly as we reach 7 × 7 grids ( <ref type="figure">Fig. 9</ref>). <ref type="bibr" target="#b2">3</ref> This indicates that there is indeed a sweet spot in the discretization as we interpolate between the glance and detect settings. However, we notice that for seq-sub this sweet spot lies farther out to the right. <ref type="figure">Figure 9</ref>: We plot the mRM SE across all categories (y-axis) for aso-sub and seq-sub on PASCAL Count-val set against the size of subitizing grid cells (x-axis). As we vary the discretization we conceptually explore a continuum between glance and detect approaches. We find that for aso-sub there exists a sweet spot (3×3), where performance on counting is the best. Interestingly, for seq-sub the discretization sweetspot is farther out to the right than aso-sub's 3 × 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Counting to Improve Detection</head><p>We now explore whether counting can help improve detection performance (on the PASCAL dataset). Detectors are typically evaluated via the Average Precision (AP) metric, which involves a full sweep over the range of scorethresholds for the detector. While this is a useful investigative tool, in any real application (say autonomous driving), the detector must make hard decisions at some fixed threshold. This threshold could be chosen on a per-image or percategory basis. Interestingly, if we knew how many objects of a category are present, we could simply set the threshold so that those many objects are detected similar to Zhang et al. <ref type="bibr" target="#b45">[46]</ref>. Thus, we could use per-image-per-category counts as a prior to improve detection.</p><p>Note that since our goal is to intelligently pick a threshold for the detector, computing AP (which involves a sweep over the thresholds) is not possible. Hence, to quantify detection performance, we first assign to each detected box one ground truth box with which it has the highest overlap. Then for each ground truth box, we check if any detection box has greater than 0.5 overlap. If so, we assign a match between the ground truth and detection, and take them out of the pool of detections and ground truths. Through this procedure, we obtain a set of true positive and false positive detection outputs. With these outputs we compute the precision and recall values for the detector. Finally, we compute the F-measure as the harmonic mean of these precision and recall values, and average the F-measure values across images and categories. We call this the mF (mean F-measure) metric. As a baseline, we use the Fast-RCNN detector after NMS to do a sweep over the thresholds for each category on the validation set to find the threshold that maximizes Fmeasure for that category. We call this the base detector.</p><p>With a fixed per-category score threshold, the base detector gets a performance of 15.26% mF. With ground truth  <ref type="figure">Figure 10</ref>: Some examples from the Count-QA VQA subset. Given a question, we parse the nouns and resolve correspondence to COCO categories. The resolved ground truth category is denoted after the question. We show the VQA ground truth and COCO dataset resolved ground truth counts, followed by outputs from detect, glance, aso-sub, seq-sub and ens.</p><p>counts to select thresholds, we get a best-case oracle performance of 20.17%. Finally, we pick the outputs of ens and seq-sub-ft models and use the counts from each of these to set separate thresholds. Our counting methods undercount more often than they overcount 4 , a high count implies that the ground truth count is likely to be even higher. Thus, for counts of 0, we default to the base thresholds and for the other predicted counts, we use the counts to set the thresholds. With this procedure, we get a gains of 1.64% mF and 1.74% mF over the base performance using ens and seq-sub-ft predictions respectively. Thus, counting can be used as a complimentary signal to aid detector performance, by intelligently picking the detector threshold in an image specific manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">VQA Experiment</head><p>We explore how well our counting approaches do on simple counting questions. Recent work <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15]</ref> has explored the problem of answering free-form natural language questions for images. One of the large-scale datasets in the space is the Visual Question Answering <ref type="bibr" target="#b2">[3]</ref> dataset. We also evaluate using the COCO-QA dataset from <ref type="bibr" target="#b34">[35]</ref> which automatically generates questions from human captions. Around 10.28% and 7.07% of the questions in VQA and COCO-QA are "how many" questions related to counting objects. Note that both the datasets use images from the COCO <ref type="bibr" target="#b27">[28]</ref> dataset. We apply our counting models, along with some basic natural language pre-processing to answer some of these questions.</p><p>Given the question "how many bottles are there in the fridge?" we need to reason about the object of interest (bottles), understand referring expressions (in the fridge) etc. Note that since these questions are free form, the category of interest might not exactly correspond to an COCO category. We tackle this ambiguity by using word2vec embed- <ref type="bibr" target="#b3">4</ref> See appendix for more details.  <ref type="bibr" target="#b20">[21]</ref> 2.71 ± 0.23 N/A SOTA VQA <ref type="bibr" target="#b14">[15]</ref> 3.25 ± 0.94 N/A <ref type="table">Table 3</ref>: Performance of various methods on counting questions in the Count-QA splits of the VQA dataset and COCO-QA datasets respectively (L implies the number of hidden layers). Lower is better. ens is a combination of glance-ft-1L, aso-sub-ft-1L-3 × 3 and seq-sub-ft-3 × 3.</p><p>dings <ref type="bibr" target="#b31">[32]</ref>. Given a free form natural language question, we extract the noun from the question and compute the closest COCO category by checking similarity of the noun with the categories in the word2vec embedding space. In case of multiple nouns, we just retain the first noun in the sentence (since how many questions typically have the subject noun first). We then run the counting method for the COCO category (see <ref type="figure">Fig 10)</ref>. More details can be found in the supplementary. Note that parsing referring expressions is still an open research problem <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39]</ref>. Thus, we filter questions based on an "oracle" for resolving referring expressions. This oracle is constructed by checking if the ground truth count of the COCO category we resolve using word2vec matches with the answer for the question. Evaluating only on these questions allows us to isolate errors due to inaccurate counts. We evaluate our outputs using the RM SE metric. We use this procedure to compile a list of 1774 and 513 questions (Count-QA) from the VQA and COCO-QA datasets respectively, to evaluate on. We will publicly release our Count-QA subsets to help future work. We report performances in <ref type="table">Table.</ref> 3. The trend of increasing performance is visible from glance to ens. We find that seq-sub significantly outperforms the other approaches. We also evaluate a state-of-the-art VQA model <ref type="bibr" target="#b14">[15]</ref> on the Count-QA VQA subset and find that even glance does better by a substantial margin. <ref type="bibr" target="#b4">5</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We study the problem of counting everyday objects in everyday scenes. We evaluate some baseline approaches to this problem using object detection, regression using global image features, and associative subitizing which involves regression on non-overlapping image cells. We propose sequential subtizing, a variant of the associative subitizing model which incorporates context across cells using a pair of stacked bi-directional LSTMs. We find that our proposed models lead to improved performance on PASCAL VOC 2007 and COCO datasets. We thoroughly evaluate the relative strengths, weaknesses and biases of our approaches, providing a benchmark for future approaches on counting, and show that an ensemble of our proposed approaches peforms the best. Further, we show that counting can be used to improve object detection and present proof-of-concept experiments on answering 'how many?' questions in visual question answering tasks. Our code and datasets will be made publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Ablation Studies</head><p>We explain the architectures for glance, aso-sub and seq-sub in Sec. 3. Here we report results of some ablation studies conducted on these architectures.</p><p>For glance and aso-sub, we search over the following architecture space. Firstly, we vary the hidden layer sizes in the set [250, 500, 1000, 1500, 2000]. Secondly, we vary the number of hidden layers in the model between 1 and 2 (with the previously selected hidden layer size). Corresponding to these settings, we search for the best performing archiecture for ft (detection finetuned fc7) and noft (classification fc7) features extracted from PASCAL images. For aso-sub, in addition to this, we look for the best performing architecture across different grid sizes (3 × 3, 5 × 5, 7 × 7). We narrow down to some design choices with 3 × 3 and them compare different grid sizes.</p><p>For seq-sub, we vary the number of Bi-LSTM (context aggregator) units per sequence. Subsequently, we vary the grid size from 3 × 3 to 5 × 5. We report studies on both PASCAL and COCO.</p><p>All results are reported on the Count-val splits of the concerned datasets.</p><p>glance : We find that the performance for ft-1L remains more or less constant as we change the size of the hidden layers <ref type="figure">(Fig. 11)</ref>. In contrast, the noft-1L model does best at smaller hidden layer sizes. A two hidden layer noft model does better than both 1L models. Intuitively, this makes sense since the noft features are better suited to global image statistics than the detection finetuned ft features.</p><p>aso-sub-3 × 3 : We next contrast different design choices for aso-sub-3 × 3. Details of how the performance changes with different grid sizes in aso-sub has been discussed the main paper. In particular, just like the previous section, we study the impact of hidden layer sizes <ref type="figure">Figure 11</ref>: Variations in mRM SE (Lower is better) on the y-axis on PASCAL Count-val set with different sizes of hidden layers on the x-axis for glance. We show performance with 1 hidden layer for ft (detection finetuned features) as well as noft (classification features). We observe that the classification features do better. We then increase the number of hidden layers to 2 for the model noft features and find that it does the best. <ref type="figure">Figure 12</ref>: Variations in mRM SE (Lower is better) on the y-axis on PASCAL Count-val set with different sizes of hidden layers on the x-axis for aso-sub-3 × 3. We show performance with 1 hidden layer for ft (detection finetuned features) as well as noft (classification features). We observe that the detection finetuned features do better. We then increase the number of hidden layers to 2 for the model ft features and find that it does not give a substantial performance boost over ft. and number of hidden layers, as well as the choice of features (ft vs noft) for the aso-sub-3×3 model <ref type="figure">(Fig. 12</ref>). We find that the detection finetuned ft features do much better than the classification features noft for aso-sub. This is likely because the ft features are better adapted to statistics of local image regions than the noft image classification features. We also find that increasing the number of hidden layers does not improve performance over using a single hidden layer, unlike glance. aso-sub : We next compare how the performance of aso-sub varies as we change the size of the grids. We pick the best performing aso-sub-3 × 3 features (ft) and number of hidden layers -1. We then vary the size of the hidden layer and compare the performance of 3 × 3, 5 × 5, and 7×7 aso-sub approaches <ref type="figure">(Fig. 13</ref>). We find that 3×3 and 5 × 5 models do much better than the 7 × 7 model. The performance of 3 × 3 is slightly better than the 5 × 5 model. <ref type="figure">Figure 13</ref>: Variations in mRM SE (Lower is better) on the y-axis on PASCAL Count-val set with different sizes of hidden layers on the x-axis for aso-sub. We compare the grid sizes 3 × 3, 5 × 5, and 7 × 7, and find that the 3 × 3 setting performs the best. <ref type="figure">Figure 14</ref>: mRM SE (Lower is better) on the y-axis on the PAS-CAL Count-val set as a result of varying the grid size on the x-axis for seq-sub. We experiment with both ft and noft features. On going from 3 × 3 to 5 × 5, there is an improvement in performance. Note that models using noft features perform worse than models using ft features.</p><p>A similar comparison on the Count-test set can be found in the main paper. seq-sub : In <ref type="figure">Fig. 14 and Fig. 15</ref>, we compare the effect of changing the grid size from 3 × 3 to 5 × 5 for the seq-sub models. We use both ft and noft features extracted from PASCAL and COCO images. On PASCAL <ref type="figure">(Fig. 14)</ref>, we observe that increasing the grid size has a slight improvement in performance for both ft and noft features unlike COCO <ref type="figure">(Fig. 15</ref>) where going from 3 × 3 to 5 × 5 there is a drop in performance for both ft and noft features. We should also note that in general ft features perform better than noft features similar to aso-sub.</p><p>We also varied the number of Bi-LSTM (context aggregator) units from 2 to 1 per sequence in the seq-sub architectures for a grid size of 3 × 3. We observe that for ft features, change in the number of Bi-LSTM units does not make a difference on both PASCAL and COCO. However, for noft features, going from 2 to 1 leads to a drop of 0.01 mRM SE on COCO and PASCAL. <ref type="figure">Figure 15</ref>: mRM SE (Lower is better) on the y-axis on the COCO Countval set as a result of varying the grid size on the x-axis for seq-sub. We experiment with both ft and noft features. On going from 3×3 to 5×5, there is a decrease in performance. <ref type="figure" target="#fig_5">Figure 16</ref>: We visualize RM SE (Lower is better) on the y-axis for glance (gl), detect (det), aso-sub (aso), seq-sub (seq) and ens (ens) across categories of various sizes on PASCAL. On the x-axis we order categories in increasing order of object size from left to right. As the object size increases, all the methods start performing competitively. We find that seq-sub, aso-sub and ens perform consistently well for a wide range of category sizes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Count Analysis</head><p>Size versus Count Error : We compare seq-sub, glance, aso-sub and detect their performance for object categories of various sizes on PASCAL ( <ref type="figure" target="#fig_5">Fig. 16</ref>) and COCO <ref type="figure" target="#fig_6">(Fig. 17)</ref>. To get the object size, we sum the number of pixels occupied by an object across images where the object occurs in the Count-val set and divide this number by the average number of (non-zero) instances of the object. This gives us an estimate of the expected size occupied per instance of an object. We show a sorting of smaller to larger categories on the x-axis in <ref type="figure" target="#fig_5">Fig. 16</ref> and <ref type="figure" target="#fig_6">Fig. 17</ref>. We find that ens, seq-sub and aso-sub perform consistently well across the spectrum of object sizes on both PAS-CAL and COCO. On PASCAL, as the object size increases the error keeps on reducing. This trend is not consistent <ref type="figure" target="#fig_6">Figure 17</ref>: We visualize RM SE (Lower is better) on the y-axis for glance (gl), detect (det), aso-sub (aso), seq-sub (seq) and ens (ens) across categories of various sizes on COCO. On the x-axis we order categories in increasing order of object size from left to right. We find that seq-sub, aso-sub and ens perform consistently well for a wide range of category sizes. over the entire spectrum of sizes for COCO. Another interesting thing to observe is that as the object size increases, the methods start performing competitively. This also indicates that aso-sub and seq-sub are able to capture partial ground truth counts well, since the counts for larger categories will necessarily be partial.</p><p>Undercounting versus Overcounting :</p><p>We study whether the models proposed in the paper undercount or overcount. Specifically, we report the number of times the approaches overcount, undercount or predict the ground truth count on PASCAL ( <ref type="figure">Fig. 18</ref>) and COCO <ref type="figure">(Fig. 19</ref>). To do this, we first filter out all instances where the ground truth count is 0 (since we cannot undercount 0). We then check if the predicted count is greater than the ground truth (overcounting) or lesser (undercounting) or equal to the ground truth (equal). We perform this analysis on the Count-test split.</p><p>On PASCAL <ref type="figure">(Fig. 18)</ref>, we can observe that there is a clear increase in the number of times we get the count from detect to ens. The models, in general undercount more often than they overcount. As we go from detect to ens, the improvement in performance can be accounted to the increase of the frequency of equal versus undercount. Interestingly, for ens we get the count right more number of times as opposed to undercounting the ground truth. The number of times we overcount more or less stays the same.</p><p>On COCO <ref type="figure">(Fig. 19</ref>), we observe that although there is an increase in the number of times we get the count right as we go from detect to ens, the frequency of equal is much lower than the frequency of undercounting for all the models. This is understandable as COCO has more number of categories and objects of different categories have lesser chances of being in the same image.</p><p>Ensemble : We study different combinations of the predictions for constructing the ensemble on the Count-test set.</p><p>On PASCAL, when we compose an ensemble of seq-sub and aso-sub, we get a mRM SE of 0.427 as opposed to a mRM SE of 0.438 with seq-sub and glance. One can think of combinining global and local context by taking an ensemble of glance and aso-sub. However, we observe that such an ensemble underperforms when compared to seq-sub by 0.02 mRM SE. We also consider including the detect baseline in the ensemble. We see that an ensemble of detect, glance, aso-sub and seq-sub gives an error of 0.43 mRM SE as opposed to an ensemble of glance, aso-sub and seq-sub which gives mRM SE 0.42. Thus detect, when included in the ensemble hurts the counting performance.</p><p>On COCO, when we compose an ensemble of seq-sub and aso-sub, we get a mRM SE of 0.351 as opposed to a mRM SE of 0.363 with seq-sub and glance. We observe that an ensemble of glance and aso-sub underperforms when compared to seq-sub by 0.02 mRM SE. When detect is included we see that an ensemble of <ref type="figure">Figure 18</ref>: We plot the percentage number of times ens (ens), seq-sub (seq), aso-sub (aso), glance (gl) and detect (det) undercount, overcount and predict the ground truth count on PASCAL Count-test split. Going from detect to ens there is a steady increase in the number of times we get the count right. <ref type="figure">Figure 19</ref>: We plot the percentage number of times ens (ens), seq-sub (seq), aso-sub (aso), glance (gl) and detect (det) undercount, overcount and predict the ground truth count on COCO Count-test split. Although, going from detect to ens there is a steady increase in the number of times we get the count right but we undercount a lot more than getting the count right. detect, glance, aso-sub and seq-sub gives an error of 0.38 mRM SE as opposed to an ensemble of glance, aso-sub and seq-sub which gives mRM SE 0.36. Just like on PASCAL, detect when included in the ensemble hurts the counting performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Occlusion Studies</head><p>In <ref type="figure">Fig. 20</ref>, we perform occlusion studies to understand where glance, aso-sub and seq-sub look in the image while estimating the counts of different objects.</p><p>For this analysis, we pick images with a spread in counts from 10 (top row) to 1 in the middle row. For each count, we identified images where all three approaches agreed on the counts so that we could analyze where each method "looks" in order to derive the corresponding counts. We pick images from the COCO Count-test split where the predicted counts for glance, aso-sub and seq-sub are equal. The aso-sub and seq-sub models are trained on 3 × 3 discretization of the images. We move 4 × 4 sized masks across the image with a non-overlapping stride to get occlusion maps. It is interesting to observe that for the image with a large ground truth count, the occlusion maps from seq-sub are very similar to those from aso-sub. This confirms our intuition that for larger counts, one needs access to local texture like patterns to accumulate count densities across the image. For smaller counts (rows 2 and 3), we notice that the maps from glance and seq-sub are  <ref type="figure">Figure 20</ref>: Occlusion maps obtained on COCO Count-test images for glance, seq-sub and aso-sub by moving a mask of size 4 × 4 over the images before doing a forward pass through each of the models. Notice how glance and seq-sub approaches tend to look at similar regions for lower counts, while for higher counts seq-sub and aso-sub tend to be more similar.</p><p>more similar, indicating that global cues such as the number of parts appearing in the image (say the number of tails of elephants), potentially captured by the distributed CNN representation are sufficient for counting. Thus, this experiment confirms our intuition that seq-sub captures the best of both the glance and aso-sub approaches, providing us a way to "interpolate" between these approaches based on the counts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">VQA Experiment</head><p>We next elaborate on more details of the VQA experiment described in Sec. 5.4. More specifically we discuss how we pre-process ground truth to make it numeric and give details of how we solve correspondence between a noun in a question to counts of coco categories.</p><p>As reported in the paper, we use the VQA dataset <ref type="bibr" target="#b2">[3]</ref> and COCO-QA <ref type="bibr" target="#b34">[35]</ref> datasets for our counting experiments. We extract the how many? type questions, which have numerical answers. This includes both integers (VQA) and numbers written in the form of text (COCO-QA). We parse the latter into corresponding numbers on the COCO-QA dataset. That is five is mapped to 5. From the selected questions, we extract the Nouns (singular, plural, and proper), and convert them to their singular form using the Stanford Natural Language Parser (NLTK) <ref type="bibr" target="#b0">[1]</ref>.</p><p>We train word2vec word embeddings on Wikipedia 6 and use cosine similarities in the embedding space as word sim-6 https://www.wikipedia.org/ ilarity. Using these we find the COCO category or COCO super-category that matches the most with the extracted nouns. These super-category annotations are available as part of the COCO dataset. We run our models for the COCO category, and consider it the answer. For the extracted nouns, if the best match is with a COCO super category, we sum the counts obtained by our counting methods for each of the COCO sub-categories belonging to the particular super-category. For example, if the resolved noun is animal, we sum the counts for horse, giraffe, cat, dog, zebra, sheep, cow, elephant, bear, and bird and use the output as our predicted count.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Qualitative Results</head><p>We finally show some qualitative examples of our predictions on COCO images in <ref type="figure" target="#fig_8">Fig. 21</ref> where ens performs best. We can observe that whenever the objects present in the image are sufficiently salient, seq-sub and aso-sub do a sufficiently better job in estimating the count of objects as compared to glance. This is because as seq-sub, and aso-sub have to estimate partial counts at cell levels unlike glance which has to regress to the count of the entire image. For some cases when the objects present are highly occluded, we see that seq-sub and aso-sub do a much better job at estimating the count. In summary, we find that ens as a combination of glance, aso-sub and seq-sub gets the count right most number of times. the predictions given by our models where ens performs well. We can see that in cases where the objects are significantly occluded, detect has very poor performance compared to seq-sub or ens.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Histogram of the non-zero counts in the PASCAL (blue) and COCO (red) datasets across all objects categories and images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>We plot the mRM SE (across all categories) with error bars (too small to be visible) at a count against the count (x-axis) on the Count-test split of the COCO dataset. We find that the seq-sub-ft-3 × 3 and ens perform really well at higher count values whereas at lower count values the results of all the models are comparable except detect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 21 :</head><label>21</label><figDesc>Some qualitative examples of</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The PASCAL VOC dataset contains a train set of 2501 images, val set of 2510 images and a test set of 4952 images, and has 20 object categories. The COCO dataset contains a train set of 82783 images and a val set of 40, 504 images, with 80 object categories. On PASCAL, we use the val set as our Count-val set and the test set as our Count-test set. On COCO, we use the first half of val as the Count-val set and the second half of val as the Count-test set. The most frequent count per object category (as one would expect in everyday scenes) is 0.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>± 0.02 1.96 ± 0.03 0.28 ± 0.03 0.59 ± 0.00 mean 0.65 ± 0.02 1.81 ± 0.03 0.31 ± 0.01 0.52 ± 0.00 always-11.14 ± 0.01 0.96 ± 0.03 0.98 ± 0.00 0.17 ± 0.03 category-mean 0.64 ± 0.02 1.60 ± 0.03 0.30 ± 0.00 0.45 ± 0.00 gt-class 0.55 ± 0.02 2.12 ± 0.07 0.24 ± 0.00 0.88 ± 0.01 detect 0.50 ± 0.01 1.92 ± 0.08 0.26 ± 0.01 0.85 ± 0.02 glance-noft-2L 0.50 ± 0.02 1.83 ± 0.09 0.27 ± 0.00 0.73 ± 0.00 glance-sos-2L</figDesc><table><row><cell>Approach</cell><cell>mRMSE</cell><cell>mRMSE-nz m-relRMSE m-relRMSE-nz</cell></row><row><cell>always-0</cell><cell>0.66</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Counting performance on PASCAL VOC 2007 Count-test Set (L implies the number of hidden layers). Lower is better. ens is</figDesc><table /><note>a combination of glance-noft-2L, aso-sub-ft-1L-3 × 3 and seq-sub-ft-3 × 3.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Counting performance on COCO Count-test set (L implies the number of hidden layers). Lower is better. ens is a combination of glance-ft-1L, aso-sub-ft-1L-3 × 3 and seq-sub-ft-3 × 3.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Going from 1×1 to 3×3, one might argue that the gain in performance in aso-sub is due to more (augmented) training data. However, from the diminishing performance on increasing grid size to 5 × 5 (which has even more data to train from), we hypothesize that this is not the case.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">For the column corresponding to VQA, all methods are evaluated on the subset of the predictions where<ref type="bibr" target="#b20">[21]</ref> and<ref type="bibr" target="#b14">[15]</ref> both produced numerical answers. For<ref type="bibr" target="#b20">[21]</ref>, there were 11 non-numerical answers and for<ref type="bibr" target="#b14">[15]</ref> there were 3 (e.g., "many", "few", "lot")</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We are grateful to the developers of Torch <ref type="bibr" target="#b8">[9]</ref> for building an excellent framework. This work was funded in part by NSF CAREER awards to DB and DP, ONR YIP awards to DP and DB, ONR Grant N00014-14-1-0679 to DB, a Sloan Fellowship to DP, ARO YIP awards to DB and DP, an Allen Distinguished Investigator award to DP from the Paul G. Allen Family Foundation, Google Faculty Research Awards to DP and DB, Amazon Academic Research Awards to DP and DB, and NVIDIA GPU donations to DB. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>1. In Sec. 1, we report results of ablation studies conducted on the Count-val split for glance, aso-sub and seq-sub models 2. In Sec. 2, we report some analyses of the count predictions generated by our models, specifically comparing object sizes with count performance and overcountingundercounting statistics 3. In Sec. 4 we present some details of the VQA experiment performed in Sec. 5.4 in the main paper 4. In Sec. 3, we show results of occlusion studies performed to identify the regions of interest in the scene while estimating the counts 5. In Sec. 5, we present some qualitative examples of the predictions generated by our models</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<ptr target="http://www.nltk.org/.15" />
		<title level="m">NLTK</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Analyzing the behavior of visual question answering models. CoRR, abs/1606.07356</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">VQA: visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Counting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic segmentation with second-order pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), volume</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">7578</biblScope>
			<biblScope unit="page" from="430" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Privacy preserving crowd monitoring: Counting people without people models or tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian poisson regression for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th International Conference on Computer Vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="545" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Clements</surname></persName>
		</author>
		<title level="m">Subitizing: What is it? why teach it? Teaching children mathematics</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">400</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BigLearn, NIPS Workshop</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Subitizing and visual shortterm memory in human and non-human species: a common shared system?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cutini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bonato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Psychology</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Log or linear? distinct intuitions of the number scale in western and amazonian indigene cultures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dehaene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Izard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Spelke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">320</biblScope>
			<biblScope unit="issue">5880</biblScope>
			<biblScope unit="page" from="1217" to="1220" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A discriminatively trained, multiscale, deformable part model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">26th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">One Vote, One Value. 75:414</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Galton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1907-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object detection via a multiregion and semantic segmentation-aware cnn model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1134" to="1142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-source multi-scale counting in extremely dense crowd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Idrees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Saleemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Seibert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;13</title>
		<meeting>the 2013 IEEE Conference on Computer Vision and Pattern Recognition, CVPR &apos;13<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2547" to="2554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 32nd International Conference on Machine Learning</title>
		<meeting>The 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deeper lstm and normalized cnn visual question answering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<ptr target="https://github.com/VT-vision-lab/VQA_LSTM_CNN,2015.8" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Referit game: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Universals in the development of early arithmetic cognition. New Directions for Child and Adolescent Development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Starkey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="page" from="5" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning To Count Objects in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1324" to="1332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<idno>abs/1512.02325</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed Representations of Words and Phrases and their Compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards perspective-free object counting with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oñoro</forename><surname>Rubio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>López-Sastre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">End-to-end instance segmentation and counting with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno>abs/1605.09410</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">It&apos;s not polite to point: Describing people with uncertain attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3089" to="3096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Learning to count with deep object features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seguí</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pujol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vitrià</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Rapid object detection using a boosted cascade of simple features. Computer Vision and Pattern Recognition (CVPR), 1:I--511--I--518</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">End-to-end integration of a convolution network, deformable parts model and nonmaximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="851" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cross-Scene Crowd Counting via Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Salient object subitizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sameki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Betke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mȇch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
