<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Network Pruning via Transformable Architecture Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyi</forename><surname>Dong</surname></persName>
							<email>xuanyi.dong@student.uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">The ReLER Lab</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country>‡ Baidu Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yi.yang@uts.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">The ReLER Lab</orgName>
								<orgName type="institution">University of Technology Sydney</orgName>
								<address>
									<country>‡ Baidu Research</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Network Pruning via Transformable Architecture Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Network pruning reduces the computation costs of an over-parameterized network without performance damage. Prevailing pruning algorithms pre-define the width and depth of the pruned networks, and then transfer parameters from the unpruned network to pruned networks. To break the structure limitation of the pruned networks, we propose to apply neural architecture search to search directly for a network with flexible channel and layer sizes. The number of the channels/layers is learned by minimizing the loss of the pruned networks. The feature map of the pruned network is an aggregation of K feature map fragments (generated by K networks of different sizes), which are sampled based on the probability distribution. The loss can be back-propagated not only to the network weights, but also to the parameterized distribution to explicitly tune the size of the channels/layers. Specifically, we apply channel-wise interpolation to keep the feature map with different channel sizes aligned in the aggregation procedure. The maximum probability for the size in each distribution serves as the width and depth of the pruned network, whose parameters are learned by knowledge transfer, e.g., knowledge distillation, from the original networks. Experiments on CIFAR-10, CIFAR-100 and ImageNet demonstrate the effectiveness of our new perspective of network pruning compared to traditional network pruning algorithms. Various searching and knowledge transfer approaches are conducted to show the effectiveness of the two components. Code is at: https://github.com/D-X-Y/NAS-Projects. strategies <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18]</ref> for the pruned network to efficiently transfer the parameters of the unpruned networks and maximize the performance of the pruned networks.</p><p>Traditional network pruning approaches achieve effective impacts on network compacting while maintaining accuracy. Their network structure is intuitively designed, e.g., pruning 30% filters in each layer <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18]</ref>, predicting sparsity ratio <ref type="bibr" target="#b14">[15]</ref> or leveraging regularization <ref type="bibr" target="#b1">[2]</ref>. The accuracy of the pruned network is upper bounded by the hand-crafted structures or rules for structures. To break this limitation, we apply Neural Architecture Search (NAS) to turn the design of the architecture structure into a learning procedure and propose a new paradigm for network pruning as explained in <ref type="figure">Fig. 1(b)</ref>.</p><p>Prevailing NAS methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40]</ref> optimize the network topology, while the focus of this paper is automated network size. In order to satisfy the requirements and make a fair comparison between the previous pruning strategies, we propose a new NAS scheme termed Transformable Architecture Search (TAS). TAS aims to search for the best size of a network instead of topology, regularized by minimization of the computation cost, e.g., floating point operations (FLOPs). The parameters of the searched/pruned networks are then learned by knowledge transfer <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46]</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep convolutional neural networks (CNNs) have become wider and deeper to achieve high performance on different applications <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48]</ref>. Despite their great success, it is impractical to deploy them to resource constrained devices, such as mobile devices and drones. A straightforward solution to address this problem is using network pruning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18]</ref> to reduce the computation cost of overparameterized CNNs. A typical pipeline for network pruning, as indicated in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, is achieved by removing the redundant filters and then fine-tuning the slashed networks, based on the original networks. Different criteria for the importance of the filters are applied, such as L2-norm of the filter <ref type="bibr" target="#b29">[30]</ref>, reconstruction error <ref type="bibr" target="#b19">[20]</ref>, and learnable scaling factor <ref type="bibr" target="#b31">[32]</ref>. Lastly, researchers apply various fine-tuning TAS is a differentiable searching algorithm, which can search for the width and depth of the networks effectively and efficiently. Specifically, different candidates of channels/layers are attached with a learnable probability. The probability distribution is learned by back-propagating the loss generated by the pruned networks, whose feature map is an aggregation of K feature map fragments (outputs of networks in different sizes) sampled based on the probability distribution. These feature maps of different channel sizes are aggregated with the help of channel-wise interpolation. The maximum probability for the size in each distribution serves as the width and depth of the pruned network.</p><p>In experiments, we show that the searched architecture with parameters transferred by knowledge distillation (KD) outperforms previous state-of-the-art pruning methods on CIFAR-10, CIFAR-100 and ImageNet. We also test different knowledge transfer approaches on architectures generated by traditional hand-crafted pruning approaches <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18]</ref> and random architecture search approach <ref type="bibr" target="#b30">[31]</ref>. Consistent improvements on different architectures demonstrate the generality of knowledge transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Studies</head><p>Network pruning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b32">33]</ref> is an effective technique to compress and accelerate CNNs, and thus allows us to deploy efficient networks on hardware devices with limited storage and computation resources. A variety of techniques have been proposed, such as low-rank decomposition <ref type="bibr" target="#b46">[47]</ref>, weight pruning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b11">12]</ref>, channel pruning <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b32">33]</ref>, dynamic computation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7]</ref> and quantization <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b0">1]</ref>. They lie in two modalities: unstructured pruning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12]</ref> and structured pruning <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Unstructured pruning methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12]</ref> usually enforce the convolutional weights <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b13">14]</ref> or feature maps <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9]</ref> to be sparse. The pioneers of unstructured pruning, LeCun et al. <ref type="bibr" target="#b28">[29]</ref> and Hassibi et al. <ref type="bibr" target="#b13">[14]</ref>, investigated the use of the second-derivative information to prune weights of shallow CNNs. After deep network was born in 2012 <ref type="bibr" target="#b27">[28]</ref>, Han et al. <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref> proposed a series of works to obtain highly compressed deep CNNs based on L2 regularization. After this development, many researchers explored different regularization techniques to improve the sparsity while preserve the accuracy, such as L0 regularization <ref type="bibr" target="#b34">[35]</ref> and output sensitivity <ref type="bibr" target="#b40">[41]</ref>. Since these unstructured methods make a big network sparse instead of changing the whole structure of the network, they need dedicated design for dependencies <ref type="bibr" target="#b10">[11]</ref> and specific hardware to speedup the inference procedure.</p><p>Structured pruning methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b32">33]</ref> target the pruning of convolutional filters or whole layers, and thus the pruned networks can be easily developed and applied. Early works in this field <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b41">42]</ref> leveraged a group Lasso to enable structured sparsity of deep networks. After that, Li et al. <ref type="bibr" target="#b29">[30]</ref> proposed the typical three-stage pruning paradigm (training a large network, pruning, re-training). These pruning algorithms regard filters with a small norm as unimportant and tend to prune them, but this assumption does not hold in deep nonlinear networks <ref type="bibr" target="#b42">[43]</ref>. Therefore, many researchers focus on better criterion for the informative filters. For example, Liu et al. <ref type="bibr" target="#b31">[32]</ref> leveraged a L1 regularization; Ye et al. <ref type="bibr" target="#b42">[43]</ref> applied a ISTA penalty; and He et al. <ref type="bibr" target="#b18">[19]</ref> utilized a geometric median-based criterion. In contrast to previous pruning pipelines, our approach allows the number of channels/layers to be explicitly optimized so that the learned structure has high-performance and low-cost.</p><p>Besides the criteria for informative filters, the importance of network structure was suggested in <ref type="bibr" target="#b32">[33]</ref>. Some methods implicitly find a data-specific architecture <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b14">15]</ref>, by automatically determining   <ref type="figure" target="#fig_3">Figure 2</ref>: Searching for the width of a pruned CNN from an unpruned three-layer CNN. Each convolutional layer is equipped with a learnable distribution for the size of the channels in this layer, indicated by p i on the left side. The feature map for every layer is built sequentially by the layers, as shown on the right side. For a specific layer, K (2 in this example) feature maps of different sizes are sampled according to corresponding distribution and combined by channel-wise interpolation (CWI) and weighted sum. This aggregated feature map is fed as input to the next layer.</p><p>the pruning and compression ratio of each layer. In contrast, we explicitly discover the architecture using NAS. Most previous NAS algorithms <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40]</ref> automatically discover the topology structure of a neural network, while we focus on searching for the depth and width of a neural network. Reinforcement learning (RL)-based <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b2">3]</ref> methods or evolutionary algorithm-based <ref type="bibr" target="#b39">[40]</ref> methods are possible to search networks with flexible width and depth, however, they require huge computational resources and cannot be directly used on large-scale target datasets. Differentiable methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b3">4]</ref> dramatically decrease the computation costs but they usually assume that the number of channels in different searching candidates is the same. TAS is a differentiable NAS method, which is able to efficiently search for a transformable networks with flexible width and depth.</p><p>Network transformation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3]</ref> also studied the depth and width of networks. Chen et al. <ref type="bibr" target="#b4">[5]</ref> manually widen and deepen a network, and proposed Net2Net to initialize the lager network. Ariel et al. <ref type="bibr" target="#b9">[10]</ref> proposed a heuristic strategy to find a suitable width of networks by alternating between shrinking and expanding. Cai et al. <ref type="bibr" target="#b2">[3]</ref> utilized a RL agent to grow the depth and width of CNNs, while our TAS is a differentiable approach and can not only enlarge but also shrink CNNs.</p><p>Knowledge transfer has been proven to be effective in the literature of pruning. The parameters of the networks can be transferred from the pre-trained initialization <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b17">18]</ref>. Minnehan et al. <ref type="bibr" target="#b36">[37]</ref> transferred the knowledge of uncompressed network via a block-wise reconstruction loss. In this paper, we apply a simple KD approach <ref type="bibr" target="#b20">[21]</ref> to perform knowledge transfer, which achieves robust performance for the searched architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our pruning approach consists of three steps: (1) training the unpruned large network by a standard classification training procedure. (2) searching for the depth and width of a small network via the proposed TAS. (3) transferring the knowledge from the unpruned large network to the searched small network by a simple KD approach <ref type="bibr" target="#b20">[21]</ref>. We will introduce the background, show the details of TAS, and explain the knowledge transfer procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Transformable Architecture Search</head><p>Network channel pruning aims to reduce the number of channels in each layer of a network. Given an input image, a network takes it as input and produces the probability over each target class. Suppose X and O are the input and output feature tensors of the l-th convolutional layer (we take 3-by-3 convolution as an example), this layer calculates the following procedure:</p><formula xml:id="formula_0">O j = cin k=1 X k,:,: * W j,k,:,: where 1 ≤ j ≤ c out ,<label>(1)</label></formula><p>where W ∈ R cout×cin×3×3 indicates the convolutional kernel weight, c in is the input channel, and c out is the output channel. W j,k,:,: corresponds to the k-th input channel and j-th output channel. * denotes the convolutional operation. Channel pruning methods could reduce the number of c out , and consequently, the c in in the next layer is also reduced.</p><p>Search for width. We use parameters α ∈ R |C| to indicate the distribution of the possible number of channels in one layer, indicated by C and max(C) ≤ c out . The probability of choosing the j-th candidate for the number of channels can be formulated as:</p><formula xml:id="formula_1">p j = exp(α j ) |C| k=1 exp(α k ) where 1 ≤ j ≤ |C|,<label>(2)</label></formula><p>However, the sampling operation in the above procedure is non-differentiable, which prevents us from back-propagating gradients through p j to α j . Motivated by <ref type="bibr" target="#b7">[8]</ref>, we apply Gumbel-Softmax <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b35">36]</ref> to soften the sampling procedure to optimize α:</p><formula xml:id="formula_2">p j = exp((log (p j ) + o j )/τ ) |C| k=1 exp((log (p k ) + o k )/τ ) s.t. o j = − log(− log(u)) &amp; u ∼ U(0, 1),<label>(3)</label></formula><p>where U(0, 1) means the uniform distribution between 0 and 1. τ is the softmax temperature. When τ → 0,p = [p 1 , ...,p j , .</p><p>..] becomes one-shot, and the Gumbel-softmax distribution drawn fromp becomes identical to the categorical distribution. When τ → ∞, the Gumbel-softmax distribution becomes a uniform distribution over C. The feature map in our method is defined as the weighted sum of the original feature map fragments with different sizes, where weights arep. Feature maps with different sizes are aligned by channel wise interpolation (CWI) so as for the operation of weighted sum. To reduce the memory costs, we select a small subset with indexes I ⊆ [|C|] for aggregation instead of using all candidates. Additionally, the weights are re-normalized based on the probability of the selected sizes, which is formulated as:</p><formula xml:id="formula_3">O = j∈I exp((log(p j ) + o j )/τ ) k∈I exp((log(p k ) + o k )/τ ) × CWI(O 1:Cj ,:,: , max(C I )) s.t. I ∼ Tp,<label>(4)</label></formula><p>where Tp indicates the multinomial probability distribution parameterized byp. The proposed CWI is a general operation to align feature maps with different sizes. It can be implemented in many ways, such a 3D variant of spatial transformer network <ref type="bibr" target="#b24">[25]</ref> or adaptive pooling operation <ref type="bibr" target="#b15">[16]</ref>. In this paper, we choose the 3D adaptive average pooling operation <ref type="bibr" target="#b15">[16]</ref> as CWI 2 , because it brings no extra parameters and negligible extra costs. We use Batch Normalization <ref type="bibr" target="#b23">[24]</ref> before CWI to normalize different fragments. <ref type="figure" target="#fig_3">Fig. 2</ref> illustrates the above procedure by taking |I| = 2 as an example.</p><p>Discussion w.r.t. the sampling strategy in Eq. (4). This strategy aims to largely reduce the memory cost and training time to an acceptable amount by only back-propagating gradients of the sampled architectures instead of all architectures. Compared to sampling via a uniform distribution, the applied sampling method (sampling based on probability) could weaken the gradients difference caused by per-iteration sampling after multiple iterations.</p><p>Search for depth. We use parameters β ∈ R L to indicate the distribution of the possible number of layers in a network with L convolutional layers. We utilize a similar strategy to sample the number of layers following Eq. (3) and allow β to be differentiable as that of α, using the sampling distribution q l for the depth l. We then calculate the final output feature of the pruned networks as an aggregation from all possible depths, which can be formulated as:</p><formula xml:id="formula_4">O out = L l=1q l × CWI(Ô l , C out ),<label>(5)</label></formula><p>whereÔ l indicates the output feature map via Eq. (4) at the l-th layer. C out indicates the maximum sampled channel among allÔ l . The final output feature map O out is fed into the last classification layer to make predictions. In this way, we can back-propagate gradients to both width parameters α and depth parameters β. . We tried other forms of CWI, e.g., bilinear and trilinear interpolation. They obtain similar accuracy but are much slower than our choice.</p><p>Searching objectives. The final architecture A is derived by selecting the candidate with the maximum probability, learned by the architecture parameters A, consisting of α for each layers and β. The goal of our TAS is to find an architecture A with the minimum validation loss L val after trained by minimizing the training loss L train as:</p><formula xml:id="formula_5">min A L val (ω * A , A) s.t. ω * A = arg min ω L train (ω, A),<label>(6)</label></formula><p>where ω * A indicates the optimized weights of A. The training loss is the cross-entropy classification loss of the networks. Prevailing NAS methods <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40]</ref> optimize A over network candidates with different typologies, while our TAS searches over candidates with the same typology structure as well as smaller width and depth. As a result, the validation loss in our search procedure includes not only the classification validation loss but also the penalty for the computation cost:</p><formula xml:id="formula_6">L val = − log( exp(z y ) |z| j=1 exp(z j ) ) + λ cost L cost ,<label>(7)</label></formula><p>where z is a vector denoting the output logits from the pruned networks, y indicates the ground truth class of a corresponding input, and λ cost is the weight of L cost . The cost loss encourages the computation cost of the network (e.g., FLOP) to converge to a target R so that the cost can be dynamically adjusted by setting different R. We used a piece-wise computation cost loss as:</p><formula xml:id="formula_7">L cost = log(E cost (A)) F cost (A) &gt; (1 + t) × R 0 (1 − t) × R &lt; F cost (A) &lt; (1 + t) × R − log(E cost (A)) F cost (A) &lt; (1 − t) × R ,<label>(8)</label></formula><p>where E cost (A) computes the expectation of the computation cost, based on the architecture parameters A. Specifically, it is the weighted sum of computation costs for all candidate networks, where the weight is the sampling probability. Calculate L train on D t to update network weights <ref type="bibr">4:</ref> Sample batch data D v from D val</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Calculate L val on D v via Eq. <ref type="formula" target="#formula_6">(7)</ref> to update A 6: end while 7: Derive the searched network from A 8: Randomly initialize the searched network and optimize it by KD via Eq. (10) on the training set the speed of changing the searched architecture. Note that we use FLOP to evaluate the computation cost of a network, and it is readily to replace FLOP with other metric, such as latency <ref type="bibr" target="#b3">[4]</ref>.</p><p>We show the overall algorithm in Alg. 1. During searching, we forward the network using Eq. (5) to make both weights and architecture parameters differentiable. We alternatively minimize L train on the training set to optimize the pruned networks' weights and L val on the validation set to optimize the architecture parameters A. After searching, we pick up the number of channels with the maximum probability as width and the number of layers with the maximum probability as depth. The final searched network is constructed by the selected width and depth. This network will be optimized via KD, and we will introduced the details in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Knowledge Transfer</head><p>Knowledge transfer is important to learn a robust pruned network, and we employ a simple KD algorithm <ref type="bibr" target="#b20">[21]</ref> on a searched network architecture. This algorithm encourages the predictions z of the small network to match soft targets from the unpruned network via the following objective:</p><formula xml:id="formula_8">L match = − |z| i=1 exp(ẑ i /T ) |z| j=1 exp(ẑ j /T ) log( exp(z i /T ) |z| j=1 exp(z j /T ) ),<label>(9)</label></formula><p>where T is a temperature, andẑ indicates the logit output vector from the pre-trained unpruned network. Additionally, it uses a softmax with cross-entropy loss to encourage the small network to predict the true targets. The final objective of KD is as follows:  where y indicates the true target class of a corresponding input. λ is the weight of loss to balance the standard classification loss and soft matching loss. After we obtain the searched network (Sec. 3.1), we first pre-train the unpruned network and then optimize the searched network by transferring from the unpruned network via Eq. (10).</p><formula xml:id="formula_9">L KD = −λ log( exp(z y ) |z| j=1 exp(z j ) ) + (1 − λ)L match s.t. 0 ≤ λ ≤ 1,<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Analysis</head><p>We introduce the experimental setup in Sec. 4.1. We evaluate different aspects of TAS in Sec. 4.2, such as hyper-parameters, sampling strategies, different transferring methods, etc. Lastly, we compare TAS with other state-of-the-art pruning methods in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Settings</head><p>Datasets. We evaluate our approach on CIFAR-10, CIFAR-100 <ref type="bibr" target="#b26">[27]</ref> and ImageNet <ref type="bibr" target="#b5">[6]</ref>. CIFAR-10 contains 50K training images and 10K test images with 10 classes. CIFAR-100 is similar to CIFAR-10 but has 100 classes. ImageNet contains 1.28 million training images and 50K test images with 1000 classes. We use the typical data augmentation of these three datasets. On CIFAR-10 and CIFAR-100, we randomly crop 32×32 patch with 4 pixels padding on each border, and we also apply the random horizontal flipping. On ImageNet, we use the typical random resized crop, randomly changing the brightness / contrast / saturation, and randomly horizontal flipping for data augmentation. During evaluation, we resize the image into 256×256 and center crop a 224×224 patch.</p><p>The search setting. We search the number of channels over {0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0} of the original number in the unpruned network. We search the depth within each convolutional stage.</p><p>We sample |I| = 2 candidates in Eq. (4) to reduce the GPU memory cost during searching. We set R according to the FLOPs of the compared pruning algorithms and set λ cost of 2. We optimize the weights via SGD and the architecture parameters via Adam. For the weights, we start the learning rate from 0.1 and reduce it by the cosine scheduler <ref type="bibr" target="#b33">[34]</ref>. For the architecture parameters, we use the constant learning rate of 0.001 and a weight decay of 0.001. On both CIFAR-10 and CIFAR-100, we train the model for 600 epochs with the batch size of 256. On ImageNet, we train ResNets <ref type="bibr" target="#b16">[17]</ref> for 120 epochs with the batch size of 256. The toleration ratio t is always set as 5%. The τ in Eq. <ref type="formula" target="#formula_2">(3)</ref> is linearly decayed from 10 to 0.1.</p><p>Training. For CIFAR experiments, we use SGD with a momentum of 0.9 and a weight decay of 0.0005. We train each model by 300 epochs, start the learning rate at 0.1, and reduce it by the cosine scheduler <ref type="bibr" target="#b33">[34]</ref>. We use the batch size of 256 and 2 GPUs. When using KD on CIFAR, we use λ of 0.9 and the temperature T of 4 following <ref type="bibr" target="#b45">[46]</ref>. For ResNet models on ImageNet, we follow most hyper-parameters as CIFAR, but use a weight decay of 0.0001. We use 4 GPUs to train the model by 120 epochs with the batch size of 256. When using KD on ImageNet, we set λ as 0.5 and T as 4 on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Case Studies</head><p>In this section, we evaluate different aspects of our proposed TAS. We also compare it with different searching algorithm and knowledge transfer method to demonstrate the effectiveness of TAS.</p><p>The effect of different strategies to differentiate α. We apply our TAS on CIFAR-100 to prune ResNet-56. We try two different aggregation methods, i.e., using our proposed CWI to align feature maps or not. We also try two different kinds of aggregation weights, i.e., Gumbel-softmax sampling as Eq.</p><p>(3) (denoted as "sample" in <ref type="figure" target="#fig_5">Fig. 3</ref>) and vanilla-softmax as Eq.</p><p>(2) (denoted as "mixture" in <ref type="figure" target="#fig_5">Fig. 3)</ref>. Therefore, there are four different strategies, i.e., with/without CWI combining with Gumbelsoftmax/vanilla-softmax. Suppose we do not constrain the computational cost, then the architecture parameters should be optimized to find the maximum width and depth. This is because such network will have the maximum capacity and result in the best performance on CIFAR-100. We try all four strategies with and without using the constraint of computational cost. We show the results in  <ref type="figure" target="#fig_5">Fig. 3c</ref> and <ref type="figure" target="#fig_5">Fig. 3a</ref>. When we do not constrain the FLOPs, our TAS can successfully find the best architecture should have a maximum width and depth. However, other three strategies failed. When we use the FLOP constraint, we can successfully constrain the computational cost in the target range. We also investigate discrepancy between the highest probability and the second highest probability in <ref type="figure" target="#fig_5">Fig. 3d</ref> and <ref type="figure" target="#fig_5">Fig. 3b</ref>. Theoretically, a higher discrepancy indicates that the model is more confident to select a certain width, while a lower discrepancy means that the model is confused and does not know which candidate to select. As shown in <ref type="figure" target="#fig_5">Fig. 3d</ref>, with the training procedure going, our TAS becomes more confident to select the suitable width. In contrast, strategies without CWI can not optimize the architecture parameters; and "mixture with CWI" shows a worse discrepancy than ours.</p><p>Comparison w.r.t. structure generated by different methods in <ref type="table" target="#tab_1">Table 1</ref>. "Pre-defined" means pruning a fixed ratio at each layer <ref type="bibr" target="#b29">[30]</ref>. "Random Search" indicates an NAS baseline used in <ref type="bibr" target="#b30">[31]</ref>. "TAS †" is our proposed differentiable searching algorithm. We make two observations: (1) searching can find a better structure using different knowledge transfer methods; (2) our TAS is superior to the NAS random baseline. <ref type="table">Table 3</ref>: Comparison of different pruning algorithms for ResNet on CIFAR. "Acc" = accuracy, "FLOPs" = FLOPs (pruning ratio), "TAS (D)" = searching for depth, "TAS (W)" = searching for width, "TAS" = searching for both width and depth.  <ref type="table" target="#tab_1">Table 1</ref>. The first line in each block does not use any knowledge transfer method. "w/ Init" indicates using pre-trained unpruned network as initialization. "w/ KD" indicates using KD. From <ref type="table" target="#tab_1">Table 1</ref>, knowledge transfer methods can consistently improve the accuracy of pruned network, even if a simple method is applied (Init). Besides, KD is robust and improves the pruned network by more than 2% accuracy on CIFAR-100.</p><p>Searching width vs. searching depth. We try (1) only searching depth ("TAS (D)"), (2) only searching width ("TAS (W)"), and (3) searching both depth and width ("TAS") in <ref type="table">Table 3</ref>. Results of only searching depth are worse than results of only searching width. If we jointly search for both depth and width, we can achieve better accuracy with similar FLOP than both searching depth and searching width only.</p><p>The effect of selecting different numbers of architecture samples I in Eq. (4). We compare different numbers of selected channels in <ref type="table" target="#tab_2">Table 2</ref> and did experiments on a single NVIDIA Tesla V100. The searching time and the GPU memory usage will increase linearly to |I|. When |I|=1, since the re-normalized probability in Eq. (4) becomes a constant scalar of 1, the gradients of parameters α will become 0 and the searching failed. When |I|&gt;1, the performance for different |I| is similar.</p><p>The speedup gain. As shown in <ref type="table" target="#tab_2">Table 2</ref>, TAS can finish the searching procedure of ResNet-32 in about 3.8 hours on a single V100 GPU . If we use evolution strategy (ES) or random searching methods, we need to train network with many different candidate configurations one by one and then evaluate them to find a best. In this way, much more computational costs compared to our TAS are  <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b44">45]</ref>, which is beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Compared to the state-of-the-art</head><p>Results on CIFAR in <ref type="table">Table 3</ref>. We prune different ResNets on both CIFAR-10 and CIFAR-100.</p><p>Most previous algorithms perform poorly on CIFAR-100, while our TAS consistently outperforms then by more than 2% accuracy in most cases. On CIFAR-10, our TAS outperforms the state-ofthe-art algorithms on ResNet-20,32,56,110. For example, TAS obtains 72.25% accuracy by pruning ResNet-56 on CIFAR-100, which is higher than 69.66% of FPGM <ref type="bibr" target="#b18">[19]</ref>. For pruning ResNet-32 on CIFAR-100, we obtain greater accuracy and less FLOP than the unpruned network. We obtain a slightly worse performance than LCCL <ref type="bibr" target="#b6">[7]</ref> on ResNet-164. It because there are 8 163 × 18 3 candidate network structures to searching for pruning ResNet-164. It is challenging to search over such huge search space, and the very deep network has the over-fitting problem on CIFAR-10 <ref type="bibr" target="#b16">[17]</ref>.</p><p>Results on ImageNet in <ref type="table" target="#tab_4">Table 4</ref>. We prune ResNet-18 and ResNet-50 on ImageNet. For ResNet-18, it takes about 59 hours to search for the pruned network on 4 NVIDIA Tesla V100 GPUs. The training time of unpruned ResNet-18 costs about 24 hours, and thus the searching time is acceptable. With more machines and optimized implementation, we can finish TAS with less time cost. We show competitive results compared to other state-of-the-art pruning algorithms. For example, TAS prunes ResNet-50 by 43.5% FLOPs, and the pruned network achieves 76.20% accuracy, which is higher than FPGM by 0.7. Similar improvements can be found when pruning ResNet-18. Note that we directly apply the hyper-parameters on CIFAR-10 to prune models on ImageNet, and thus TAS can potentially achieve a better result by carefully tuning parameters on ImageNet.</p><p>Our proposed TAS is a preliminary work for the new network pruning pipeline. This pipeline can be improved by designing more effective searching algorithm and knowledge transfer method. We hope that future work to explore these two components will yield powerful compact networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a new paradigm for network pruning, which consists of two components. For the first component, we propose to apply NAS to search for the best depth and width of a network. Since most previous NAS approaches focus on the network topology instead the network size, we name this new NAS scheme as Transformable Architecture Search (TAS). Furthermore, we propose a differentiable TAS approach to efficiently and effectively find the most suitable depth and width of a network. For the second component, we propose to optimize the searched network by transferring knowledge from the unpruned network. In this paper, we apply a simple KD algorithm to perform knowledge transfer, and conduct other transferring approaches to demonstrate the effectiveness of this component. Our results show that new efforts focusing on searching and transferring may lead to new breakthroughs in network pruning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A comparison between the typical pruning paradigm and the proposed paradigm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>2</head><label></label><figDesc>The formulation of the selected CWI: suppose B = CWI(A, Cout), where B ∈ R C out HW and A ∈ R CHW ; then B i,h,w = mean(A s:e−1,h,w ), where s = i×C C out and e = (i+1)×C C out</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>F 2 :</head><label>2</label><figDesc>cost (A) indicates the actual cost of the searched architecture, whose width and depth are derived from A. t ∈ [0, 1] denotes a toleration ratio, which slows down Algorithm 1 The TAS Procedure Input: split the training set into two disjoint sets: D train and D val 1: while not converge do Sample batch data D t from D train 3:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>The FLOPs of the searched network over epochs when we do not constrain the FLOPs (λcost = 0). The mean discrepancy over epochs when we do not constrain the FLOPs (λcost = 0). The FLOPs of the searched network over epochs when we constrain the FLOPs (λcost = 2). sample w/o CWI mixture w/ CWI mixture w/o CWI (d) The mean discrepancy over epochs when we constrain the FLOPs (λcost = 2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>The impact of different choices to make architecture parameters differentiable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>1st layer pruned CNN-2nd layer pruned CNN-3rd layer pruned CNN-logits</head><label></label><figDesc></figDesc><table><row><cell></cell><cell cols="2">distribution of #channels</cell><cell>pruned CNN-image</cell><cell>image</cell><cell>image</cell><cell>image</cell></row><row><cell>channel=4 feature map</cell><cell>prob</cell><cell>p 1 1 p 1 2 p 1 3 p 1 4</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">C=1 2 3 4</cell><cell></cell><cell></cell></row><row><cell>feature map channel=4 feature map channel=4</cell><cell cols="2">C=1 2 3 4 p 3 1 p 3 2 p 3 3 p 3 prob 4 1st layer p 2 1 p 2 2 p 2 3 p 2 4 prob C=1 2 3 4 2nd layer</cell><cell>p 1 = [p 1 1 , p 1 2 , p 1 3 , p 1 4 ]</cell><cell>× p 2 1 × p 2 3 sample two channel choices: 1 and 3 via p 2</cell><cell>1 × p 3 2 × p 3 sample two channel choices: 1 and 2 via p 3</cell></row><row><cell></cell><cell></cell><cell>3rd layer</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The accuracy on CIFAR-100 when pruning about 40% FLOPs of ResNet-32.</figDesc><table><row><cell></cell><cell>FLOPs accuracy</cell></row><row><cell>Pre-defined</cell><cell>41.1 MB 68.18 %</cell></row><row><cell>Pre-defined w/ Init</cell><cell>41.1 MB 69.34 %</cell></row><row><cell>Pre-defined w/ KD</cell><cell>41.1 MB 71.40 %</cell></row><row><cell>Random Search</cell><cell>42.9 MB 68.57 %</cell></row><row><cell cols="2">Random Search w/ Init 42.9 MB 69.14 %</cell></row><row><cell cols="2">Random Search w/ KD 42.9 MB 71.71 %</cell></row><row><cell>TAS †</cell><cell>42.5 MB 68.95 %</cell></row><row><cell>TAS † w/ Init</cell><cell>42.5 MB 69.70 %</cell></row><row><cell>TAS † w/ KD (TAS)</cell><cell>42.5 MB 72.41 %</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results of different configurations when pruneResNet-32 on CIFAR-10 with one V100 GPU. "#SC" indicates the number of selected channels. "H" indicates hours.</figDesc><table><row><cell cols="3">#SC Search Time Memory Train Time FLOPs Accuracy</cell></row><row><cell>|I|=1 2.83 H</cell><cell>1.5GB</cell><cell>0.71 H 23.59 MB 89.85%</cell></row><row><cell>|I|=2 3.83 H</cell><cell>2.4GB</cell><cell>0.84 H 38.95 MB 92.98%</cell></row><row><cell>|I|=3 4.94 H</cell><cell>3.4GB</cell><cell>0.67 H 39.04 MB 92.63%</cell></row><row><cell>|I|=5 7.18 H</cell><cell>5.1GB</cell><cell>0.60 H 37.08 MB 93.18%</cell></row><row><cell>|I|=8 10.64 H</cell><cell>7.3GB</cell><cell>0.81 H 38.28 MB 92.65%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison of different pruning algorithms for different ResNets on ImageNet.</figDesc><table><row><cell>Model</cell><cell>Method</cell><cell cols="4">Top-1 Prune Acc Acc Drop Prune Acc Acc Drop Top-5</cell><cell>FLOPs</cell><cell>Prune Ratio</cell></row><row><cell></cell><cell>LCCL [7]</cell><cell>66.33%</cell><cell>3.65%</cell><cell>86.94%</cell><cell cols="2">2.29% 1.19E9 34.6%</cell></row><row><cell>ResNet-18</cell><cell>SFP [18] FPGM [19]</cell><cell>67.10% 68.41%</cell><cell>3.18% 1.87%</cell><cell>87.78% 88.48%</cell><cell cols="2">1.85% 1.06E9 41.8% 1.15% 1.06E9 41.8%</cell></row><row><cell></cell><cell>TAS</cell><cell>69.15%</cell><cell>1.50%</cell><cell>89.19%</cell><cell cols="2">0.68% 1.21E9 33.3%</cell></row><row><cell></cell><cell>SFP [18]</cell><cell>74.61%</cell><cell>1.54%</cell><cell>92.06%</cell><cell cols="2">0.81% 2.38E9 41.8%</cell></row><row><cell></cell><cell>CP [20]</cell><cell>-</cell><cell>-</cell><cell>90.80%</cell><cell cols="2">1.40% 2.04E9 50.0%</cell></row><row><cell>ResNet-50</cell><cell cols="2">Taylor [38] AutoSlim [45] 76.00% 74.50%</cell><cell>1.68% -</cell><cell>--</cell><cell>--</cell><cell>2.25E9 44.9% 3.00E9 26.6%</cell></row><row><cell></cell><cell>FPGM [19]</cell><cell>75.50%</cell><cell>0.65%</cell><cell>92.63%</cell><cell cols="2">0.21% 2.36E9 42.2%</cell></row><row><cell></cell><cell>TAS</cell><cell>76.20%</cell><cell>1.26%</cell><cell>93.07%</cell><cell cols="2">0.48% 2.31E9 43.5%</cell></row><row><cell cols="7">required. A possible solution to accelerate ES or random searching methods is to share parameters of</cell></row><row><cell cols="3">networks with different configurations</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An empirical study of binary neural networks&apos; optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fernández-Marqués</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning the number of neurons in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2270" to="2278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Efficient architecture search by network transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2787" to="2794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ProxylessNAS: Direct neural architecture search on target task and hardware</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Net2net: Accelerating learning via knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">More is less: A more complicated network with less inference complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5840" to="5848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Searching for a robust neural architecture in four gpu hours</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1761" to="1770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatially adaptive computation time for residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1039" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">MorphNet: Fast &amp; simple resource-constrained structure learning of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Eban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nachum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1586" to="1595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">EIE: efficient inference engine on compressed deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pedram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The ACM/IEEE International Symposium on Computer Architecture (ISCA)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="243" to="254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1135" to="1143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Second order derivatives for network pruning: Optimal brain surgeon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hassibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="1993" />
			<biblScope unit="page" from="164" to="171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">AMC: Automl for model compression and acceleration on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L Z L H W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><forename type="middle">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihui</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="183" to="202" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1904" to="1916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Soft filter pruning for accelerating deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2234" to="2240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pruning filter via geometric median for deep convolutional neural networks acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4340" to="4349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1389" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems Workshop (NeurIPS-W)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Quantized neural networks: Training neural networks with low precision weights and activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research (JMLR)</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="6869" to="6898" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Optimal brain damage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Solla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="1990" />
			<biblScope unit="page" from="598" to="605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning efficient convolutional networks through network slimming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2736" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rethinking the value of network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning sparse neural networks through l_0 regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cascaded projection: End-to-end network compression and acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Minnehan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10715" to="10724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Importance estimation for neural network pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11264" to="11272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient neural architecture search via parameter sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4092" to="4101" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning sparse neural networks via sensitivitydriven regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tartaglione</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lepsøy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fiandrotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Francini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3878" to="3888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning structured sparsity in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2074" to="2082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rethinking the smaller-norm-less-informative assumption in channel pruning of convolution layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Network slimming by slimmable networks: Towards one-shot architecture search for channel numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11728</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Accelerating very deep convolutional networks for classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1943" to="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
