<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ExFuse: Enhancing Feature Fusion for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenli</forename><surname>Zhang</surname></persName>
							<email>zhenlizhang14@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
							<email>pengchao@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Inc. (Face++)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ExFuse: Enhancing Feature Fusion for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Semantic Segmentation, Convolutional Neural Networks</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Modern semantic segmentation frameworks usually combine low-level and high-level features from pre-trained backbone convolutional models to boost performance. In this paper, we first point out that a simple fusion of low-level and high-level features could be less effective because of the gap in semantic levels and spatial resolution. We find that introducing semantic information into low-level features and highresolution details into high-level features is more effective for the later fusion. Based on this observation, we propose a new framework, named ExFuse, to bridge the gap between low-level and high-level features thus significantly improve the segmentation quality by 4.0% in total. Furthermore, we evaluate our approach on the challenging PASCAL VOC 2012 segmentation benchmark and achieve 87.9% mean IoU, which outperforms the previous state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Most state-of-the-art semantic segmentation frameworks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref> follow the design of Fully Convolutional Network (FCN) <ref type="bibr" target="#b12">[13]</ref>. FCN has a typical encoder-decoder structure -semantic information is firstly embedded into the feature maps via encoder then the decoder takes responsibility for generating segmentation results. Usually the encoder is the pre-trained convolutional model to extract image features and the decoder contains multiple upsampling components to recover resolution. Although the top-most feature maps of the encoder could be highly semantic, its ability to reconstruct precise details in segmentation maps is limited due to insufficient resolution, which is very common in modern backbone models such as <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. To address this, an "U-Net" architecture is proposed <ref type="bibr" target="#b6">[7]</ref> and adopted in many recent work <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>. The core idea of U-Net is to gradually fuse high-level low-resolution features from top layers with low-level but high-resolution features from bottom layers, which is expected to be helpful for the decoder to generate high-resolution semantic results.</p><p>Though the great success of U-Net, the working mechanism is still unknown and worth further investigating. Low-level and high-level features are complementary by nature, where low-level features are rich in spatial details but lack arXiv:1804.03821v1 [cs.CV] 11 Apr 2018 semantic information and vice versa. Consider the extreme case that "pure" low-level features only encode low-level concepts such as points, lines or edges. Intuitively, the fusion of high-level features with such "pure" low-level features helps little, because low-level features are too noisy to provide sufficient highresolution semantic guidance. In contrast, if low-level features include more semantic information, for example, encode relatively clearer semantic boundaries, then the fusion becomes easy -fine segmentation results could be obtained by aligning high-level feature maps to the boundary. Similarly, "pure" high-level features with little spatial information cannot take full advantage of low-level features; however, with additional high-resolution features embedded, high-level features may have chance to refine itself by aligning to the nearest low-level boundary. <ref type="figure" target="#fig_0">Fig 1 illustrates</ref> the above concepts. Empirically, the semantic and resolution overlap between low-level and high-level features plays an important role in the effectiveness of feature fusion. In other words, feature fusion could be enhanced by introducing more semantic concepts into low-level features or by embedding more spatial information into high-level features.</p><p>Motivated by the above observation, we propose to boost the feature fusion by bridging the semantic and resolution gap between low-level and high-level feature maps. We propose a framework named ExFuse, which addresses the gap from the following two aspects: 1) to introduce more semantic information into low-level features, we suggest three solutions -layer rearrangement, semantic supervision and semantic embedding branch; 2) to embed more spatial infor-mation into high-level features, we propose two novel methods: explicit channel resolution embedding and densely adjacent prediction. Significant improvements are obtained by either approach and a total increase of 4% is obtained by the combination. Furthermore, we evaluate our method on the challenging PASCAL VOC 2012 <ref type="bibr" target="#b19">[20]</ref> semantic segmentation task. In the test dataset, we achieve the score of 87.9% mean IoU, surpassing the previous state-of-the-art methods.</p><p>Our contributions can be summerized as follows:</p><p>-We suggest a new perspective to boost semantic segmentation performance, i.e. bridging the semantic and resolution gap between low-level and high-level features by more effective feature fusion. -We propose a novel framework named ExFuse, which introduces more semantic information into low-level features and more spatial high-resolution information into high-level features. Significant improvements are obtained from the enhanced feature fusion. -Our fully-equipped model achieves the new state-of-the-art result on the test set of PASCAL VOC 2012 segmentation benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Feature fusion in semantic segmentation. Feature fusion is frequently employed in semantic segmentation for different purposes and concepts. A lot of methods fuse low-level but high-resolution features and high-level low-resolution features together <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>. Besides, ASPP module is proposed in DeepLab <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref> to fuse multi-scale features to tackle objects of different size. Pyramid pooling module in PSPNet <ref type="bibr" target="#b9">[10]</ref> serves the same purpose through different implementation. BoxSup <ref type="bibr" target="#b20">[21]</ref> empirically fuses feature maps of bounding boxes and segmentation maps to further enhance segmentation.</p><p>Deeply supervised learning. To the best of our knowledge, deeply supervised training is initially proposed in <ref type="bibr" target="#b21">[22]</ref>, which aims to ease the training process of very deep neural networks since depth is the key limitation for training modern neural networks until batch normalization <ref type="bibr" target="#b22">[23]</ref> and residual networks <ref type="bibr" target="#b13">[14]</ref> are proposed. Extra losses are utilized in GoogleNet <ref type="bibr" target="#b14">[15]</ref> for the same purpose. Recently, PSPNet <ref type="bibr" target="#b9">[10]</ref> also employs this method to ease the optimization when training deeper networks.</p><p>Upsampling. There are mainly three approaches to upsample a feature map. The first one is bilinear interpolation, which is widely used in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>. The second method is deconvolution, which is initially proposed in FCN <ref type="bibr" target="#b12">[13]</ref> and utilized in later work such as <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11]</ref>. The third one is called "sub-pixel convolution", which derives from <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> in super resolution task and is widely broadcast to other tasks such as semantic segmentation. For instance, <ref type="bibr" target="#b8">[9]</ref> employs it to replace the traditional deconvolution operation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>In this work we mainly focus on the feature fusion problem in "U-Net" segmentation frameworks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11]</ref>. In general, U-Net have an encoder-decoder structure as shown in <ref type="figure" target="#fig_0">Fig 1.</ref> Usually the encoder part is based on a convolutional model pretrained on large-scale classification dataset (e.g. ImageNet <ref type="bibr" target="#b25">[26]</ref>), which generates low-level but high-resolution features from the bottom layers and highlevel low-resolution features from the top layers. Then the decoder part mixes up the features to predict segmentation results. A common way of feature fusion <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11]</ref> is to formulate as a residual form:</p><formula xml:id="formula_0">y l = U psample(y l+1 ) + F(x l )<label>(1)</label></formula><p>where y l is the fused feature at l-th level; x l stands for the l-th feature generated by the encoder. Features with larger l have higher semantic level but lower spatial resolution and vice versa (see <ref type="figure" target="#fig_1">Fig 2)</ref>. In Sec 1 we argue that feature fusion could become less effective if there is a large semantic or resolution gap between low-level and high-level features.</p><p>To study and verify the impact, we choose one of the start-of-the-art "U-Net" frameworks -Global Convolutional Network (GCN) [8] -as our backbone segmentation architecture (see <ref type="figure" target="#fig_1">Fig 2 for</ref> details). In GCN, 4 different semantic levels of feature maps are extracted from the encoder network, whose spatial resolutions, given the 512 × 512 input, are {128, 64, 32, 16} respectively. To examine the effectiveness of feature fusion, we select several subsets of feature levels and use them to retrain the whole system. Results are shown in <ref type="table" target="#tab_0">Table 1</ref>. It is clear that even though the segmentation quality increases with the fusion of more feature levels, the performance tends to saturate quickly. Especially, the lowest two feature levels (1 and 2) only contribute marginal improvements (0.24% for ResNet 50 and 0.05% for ResNeXt 101), which implies the fusion of low-level and high-level features is rather ineffective in this framework.</p><p>In the following subsections we will introduce our solutions to bridge the gap between low-level and high-level features -embedding more semantic information into low-level features and more spatial resolution clues into high-level features. First of all, we introduce our baseline settings:  <ref type="figure" target="#fig_1">2)</ref>. The feature extractor is based on pretrained ResNet50 <ref type="bibr" target="#b13">[14]</ref> and ResNeXt101 <ref type="bibr" target="#b17">[18]</ref> model. Performance is evaluated in mIoU.</p><formula xml:id="formula_1">Feature</formula><p>Baseline Settings. The overall semantic segmentation framework follows the fully-equipped GCN <ref type="bibr" target="#b7">[8]</ref> architecture, as shown in <ref type="figure" target="#fig_1">Fig 2.</ref> For the backbone encoder network, we use ResNeXt 101 <ref type="bibr" target="#b17">[18]</ref> model pretrained on ImageNet by default 1 unless otherwise mentioned. We use two public-available semantic segmentation benchmarks -PASCAL VOC 2012 <ref type="bibr" target="#b19">[20]</ref> and Semantic Boundaries Dataset <ref type="bibr" target="#b28">[29]</ref> -for training and evaluate performances on PASCAL VOC 2012 validation set, which is consistent with many previous work <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref>. The performance is measured by standard mean intersection-over-union (mean IoU). Other training and test details or hyper-parameters are exactly the same as <ref type="bibr" target="#b7">[8]</ref>. Our reproduced GCN baseline score is 76.0%, shown in <ref type="table">Table 3</ref> (#1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Introducing More Semantic Information into Low-level Features</head><p>Our solutions are inspired by the fact: for convolutional neural networks, feature maps close to semantic supervisions (e.g. classification loss) tend to encode more semantic information, which has been confirmed by some visualization work <ref type="bibr" target="#b29">[30]</ref>. We propose three methods as follows:</p><p>Layer Rearrangement In our framework, features are extracted from the tail of each stage in the encoder part (res-2 to res-5 in <ref type="figure" target="#fig_1">Fig 2)</ref>. To make lowlevel features (res-2 or res-3) 'closer' to the supervisions, one straight-forward approach is to arrange more layers in the early stages rather than the latter. For example, ResNeXt 101 <ref type="bibr" target="#b17">[18]</ref> model has {3, 4, 23, 3} building blocks for Stage 2-5 respectively; we rearrange the assignment into {8, 8, 9, 8} and adjust the number of channels to ensure the same overall computational complexity. Experiment shows that even though the ImageNet classification score of the newly designed model is almost unchanged, its segmentation performance increases by 0.8% ( <ref type="table">Table 3</ref>, compare #2 with #3), which implies the quality of low-level feature might be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Supervision</head><p>We come up with another way to improve low-level features, named Semantic Supervision (SS), by assigning auxiliary supervisions directly to the early stages of the encoder network (see <ref type="figure" target="#fig_1">Fig 2)</ref>. To generate semantic outputs in the auxiliary branches, low-level features are forced to encode more semantic concepts, which is expected to be helpful for later feature fusion. Such methodology is inspired by Deeply Supervised Learning used in some old classification networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b14">15]</ref> to ease the training of deep networks. However, more sophisticated classification models <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b17">18]</ref> suggest end-to-end training without auxiliary losses, which is proved to have no convergence issue even for models over 100 layers. Our experiment also shows that for ResNet or ResNeXt models deeply supervised training is useless or even harms the classification accuracy (see <ref type="table" target="#tab_1">Table 2</ref>). Therefore, our Semantic Supervision approach mainly focuses on improving the quality of low-level features, rather than boosting the backbone model itself.  <ref type="figure" target="#fig_2">Fig 3</ref> shows the detailed structure of our Semantic Supervision block. When pretraining the backbone encoder network, the components are attached to the tail of each stage as auxiliary supervisions (see <ref type="figure" target="#fig_1">Fig 2)</ref>. The overall classification loss equals to a weighted summation of all auxiliary branches. Then after pretraining, we remove these branches and use the remaining part for fine tuning.</p><p>Experiment shows the method boosts the segmentation result by 1.1%. Moreover, we find that if features are extracted from the second convolutional layer in the auxiliary module for fine tuning <ref type="figure" target="#fig_2">(Fig 3)</ref>, more improvement (1.5%) is obtained (see <ref type="table">Table 3</ref>, compare #1 with #2), which supports our intuition that feature maps closer to the supervision tend to encode more semantic information. It is worth noting that the recent semantic segmentation work PSPNet <ref type="bibr" target="#b9">[10]</ref> also employs deeply supervised learning and reports the improvements. Different from ours, the architecture of <ref type="bibr" target="#b9">[10]</ref> do not extract feature maps supervised by the auxiliary explicitly; and their main purpose is to ease the optimization during training. However, in our framework we find the improvements may result from different reasons. For instance, we choose a relatively shallower network ResNet 50 <ref type="bibr" target="#b13">[14]</ref> and pretrain with or without semantic supervision. From <ref type="table" target="#tab_1">Table 2</ref>, we find the auxiliary losses do not improve the classification score, which implies ResNet 50 is unlikely to suffer from optimization difficulty. However, it still boosts the segmentation result by 1.1%, which is comparable to the deeper case of ResNeXt 101 (1.0%). We believe the enhancement in our framework mainly results from more "semantic" low-level features.</p><p>Semantic Embedding Branch As mentioned above, many "U-Net" structures involve low-level feature as the residue to the upsampled high-level feature. In Equ 1 the residual term F(x l ) is a function of low-level but high-resolution feature, which is used to fill the spatial details. However, if the low-level feature contains little semantic information, it is insufficient to recover the semantic resolution. To address the drawback, we generalize the fusion as follows:</p><formula xml:id="formula_2">y l = U psample (y l+1 ) + F(x l , x l+1 , . . . , x L )<label>(2)</label></formula><p>where L is the number of feature levels. Our insight is to involve more semantic information from high-level features to guide the resolution fusion. The detailed design of function F (·) is illustrated in <ref type="figure" target="#fig_3">Fig 4,</ref> named Semantic Embedding Branch, (SEB). We use the component for features of Level 1-3 (see <ref type="figure" target="#fig_1">Fig 2)</ref>. In our experiment SEB improves the performance by 0.7% <ref type="table">(Table 3</ref>, compare #3 with #5). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Embedding More Spatial Resolution into High-level Features</head><p>For most backbone feature extractor networks, high-level features have very limited spatial resolution. For example, the spatial size of top-most feature map in ResNet or ResNeXt is 7 × 7 for 224 × 224 input size. To encode more spatial details, a widely used approach is dilated strategy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b11">12]</ref>, which is able to enlarge feature resolution without retraining the backbone network. However, since high-level feature maps involve a lot of channels, larger spatial size significantly increases the computational cost. So in this work we mainly consider another direction -we do not try to increase the "physical" resolution of the feature maps; instead, we expect more resolution information encoded within channels. We propose the following two methods:</p><p>Explicit Channel Resolution Embedding In our overall framework, segmentation loss is only connected to the output of decoder network (see <ref type="figure" target="#fig_1">Fig 2)</ref>, which is considered to have less impact on the spatial information of high-level features by intuition. One straight-forward solution is to borrow the idea of Semantic Supervision (Sec 3.1) -we could add an auxiliary supervision branch to the high-level feature map, upsample and force it to learn fine segmentation map. Following the insight, firstly we try adding an extra segmentation loss to the first deconvolution module (the light-blue component in <ref type="figure" target="#fig_1">Fig 2)</ref>, however, no improvements are obtained ( <ref type="table">Table 4</ref>, #2). Why does the auxiliary loss fail to work? Note that the purpose of the supervision is to embed high resolution information "explicitly" into feature map channels. However, since deconvolution layer includes weights, the embedding becomes implicit. To overcome this issue, we adopt a parameter-free upsampling method -Sub-pixel Upsample <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> -to replace the original deconvolution. Since sub-pixel upsample enlarge the feature map just by reshaping the spatial and channel dimensions, the auxiliary supervision is able to explicitly impact the features. Details of the component are shown in <ref type="figure" target="#fig_4">Fig 5.</ref> Experiment shows that it enhances the performance by 0.5% (see <ref type="table">Table 4</ref> and <ref type="table">Table 3)</ref>.</p><p>Moreover, to demonstrate that the improvement is brought by explicit resolution embedding rather than sub-pixel upsampling itself, we also try to replace the deconvolution layer only without auxiliary supervision. Densely Adjacent Prediction In the decoder upstream of the original architecture <ref type="figure" target="#fig_1">(Fig 2)</ref>, feature point at the spatial location (i, j) mainly takes responsibility for the semantic information at the same place. To encode as much spatial information into channels, we propose a novel mechanism named Densely Adjacent Prediction (DAP), which allows to predict results at the adjacent position, e.g. (i − 1, j + 1). Then to get the final segmentation map, result at the position (i, j) can be generated by averaging the associated scores. Formally, given the window size k × k, we divide the feature channels into k × k groups, then DAP works as follows:</p><formula xml:id="formula_3">r i,j = 1 k × k 0≤l,m&lt;k x (l×k+m) i+l− k/2 ,j+m− k/2 (3)</formula><p>where r i,j denotes the result at the position (i, j) and x (c) i,j stands for the features at the position (i, j) belonging to channel group c. In <ref type="figure" target="#fig_5">Fig 6 we</ref> illustrate the concept of DAP. We use DAP on the output of our decoder (see <ref type="figure" target="#fig_1">Fig 2)</ref>. In our experiment we set k = 3. Note that DAP requires the number of feature channels increased by k × k times, so we increase the output channels of each deconvolution block to 189 (21 × 3 × 3). For fair comparison, we also evaluate the baseline model with the same number of channels. Results are shown in <ref type="table">Table 5</ref>. It is clear that DAP improves the performance by 0.6% while the counterpart model without DAP only obtains marginal gain, which implies DAP may be helpful for feature maps to embed more spatial information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Index</head><p>Method mIoU (%) 1 Baseline 79.0 2 Baseline (more channels) 79.1 3 DAP ( <ref type="figure" target="#fig_5">Fig 6)</ref> 79.6 <ref type="table">Table 5</ref>. Ablation study on the effect of Densely Adjacent Prediction (DAP). The baseline model is in <ref type="table">Table 3</ref> (#5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discussions</head><p>Is Feature Fusion Enhanced? At the beginning of Sec 3 we demonstrate that feature fusion in our baseline architecture (GCN <ref type="bibr" target="#b7">[8]</ref>) is ineffective. Only marginal improvements are obtained by fusing low-level features (Level 1 and 2), as shown in <ref type="table" target="#tab_0">Table 1</ref>. We attribute the issue to the semantic and resolution gap between low-level and high-level features. In Sec 3.1 and Sec 3.2, we propose a series of solutions to introduce more semantic information into low-level features and more spatial details into high-level features. Despite the improved performance, a question raises: is feature fusion in the framework really improved? To justify this, similar to <ref type="table" target="#tab_0">Table 1</ref> we compare several subsets of different feature levels and use them to train original baseline (GCN) and our proposed model (ExFuse) respectively. For the ExFuse model, all the 5 approaches in Sec 3.1 and Sec 3.2 are used. <ref type="table" target="#tab_3">Table 6</ref> shows the results. We find that combined with low-level feature maps (Level 1 and 2) the proposed ExFuse still achieves considerable performance gain (∼1.3%), while the baseline model cannot benefit from them. The comparison implies our insights and methodology enhance the feature fusion indeed. <ref type="table" target="#tab_3">Table 6</ref> also shows that the proposed model is much better than the baseline in the case that only top-most feature maps (Level 4) are used, which implies the superior high-level feature quality to the original model. Our further study shows that methods in Sec 3.2 contribute most of the improvement. Empirically we conclude that boosting high-level features not only benefits feature fusion, but also contributes directly to the segmentation performance.</p><p>Do techniques work in a vanilla U-Net? Previously we would like to demonstrate that the proposed perspective and techniques are able to improve one of the state-of-the-art U-Net structure -GCN. To prove the good generalization of this paper, we apply techniques illustrated above to a vanilla U-Net without GCN module. Performance on PASCAL VOC 2012 is boosted from 72.7 to 79.6</p><p>Feature Levels Original GCN <ref type="bibr" target="#b7">[8]</ref>  in mIoU. We see that the gap is even bigger (6.9 instead of 4.0), which shows that techniques illustrated above generalize well.</p><p>Could the perspective and techniques generalize to other computer vision tasks? Since U-Net structure is widely applied to other vision tasks such as low-level vision <ref type="bibr" target="#b33">[34]</ref> and detection <ref type="bibr" target="#b34">[35]</ref>, a question raises naturally: could the proposed perspective and techniques generalize to other tasks? We carefully conducted ablation experiments and observe positive results. We leave detailed discussion for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PASCAL VOC 2012 Experiment</head><p>In the last section we introduce our methodology and evaluate their effectiveness via ablation experiments. In this section we investigate the fully-equipped system and report benchmark results on PASCAL VOC 2012 test set.</p><p>To further improve the feature quality, we use deeper ResNeXt 131 as our backbone feature extractor, in which Squeeze-and-excitation modules <ref type="bibr" target="#b32">[33]</ref> are also involved. The number of building blocks for Stage 2-5 is {8, 8, 19, 8} respectively, which follows the idea of Sec 3.1. With ResNeXt 131, we get 0.8% performance gain and achieve 80.8% mIoU when training with 10582 images from PASCAL VOC 2012 <ref type="bibr" target="#b19">[20]</ref> and Semantic Boundaries Dataset (SBD) <ref type="bibr" target="#b28">[29]</ref>, which is 2.3% better than DeepLabv3 <ref type="bibr" target="#b4">[5]</ref> at the same settings.</p><p>Following the same procedure as <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, we employ Microsoft COCO dataset <ref type="bibr" target="#b35">[36]</ref>   <ref type="bibr" target="#b7">[8]</ref>. COCO pretraining brings about another 4.6% increase in performance, as shown in <ref type="table" target="#tab_4">Table 7</ref> (#2 and #3). We further average the score map of an image with its horizontal flipped version and eventually get a 85.8% mIoU on PASCAL VOC 2012 validation set, which is 2.3% better than DeepLabv3+ <ref type="bibr" target="#b36">[37]</ref>  <ref type="table" target="#tab_4">(Table 7 #4</ref>).</p><p>Resembling <ref type="bibr" target="#b4">[5]</ref>, we then freeze the batch normalization parameters and fine tune our model on official PASCAL VOC 2012 trainval set. In particular, we duplicate the images that contain hard classes (namely bicycle, chair, dining table, potted plant and sofa). Finally, our ExFuse framework achieves 87.9% mIoU on PASCAL VOC 2012 test set without any DenseCRF <ref type="bibr" target="#b37">[38]</ref> post-processing, which surpasses previous state-of-the-art results, as shown in <ref type="table" target="#tab_5">Table 8</ref>. For fair comparison, we also evaluate our model using a standard ResNet101 and it achieves 86.2% mIoU, which is better than DeepLabv3 at the same setting.</p><p>Method mIOU Tusimple <ref type="bibr" target="#b8">[9]</ref> 83.1 Large Kernel Matters <ref type="bibr" target="#b7">[8]</ref> 83.6 Multipath RefineNet <ref type="bibr" target="#b10">[11]</ref> 84.2 ResNet 38 MS COCO <ref type="bibr" target="#b38">[39]</ref> 84.9 PSPNet <ref type="bibr" target="#b9">[10]</ref> 85.4 DeepLabv3 <ref type="bibr" target="#b4">[5]</ref> 85.7 SDN <ref type="bibr" target="#b39">[40]</ref> 86.6 DeepLabv3+ (Xception) <ref type="bibr" target="#b36">[37]</ref> 87.8 ExFuse ResNet101 (ours) 86.2 ExFuse ResNeXt131 (ours) 87.9   <ref type="bibr" target="#b7">[8]</ref> baseline and our proposed ExFuse framework. It is clear that the visualization quality of our method is much better than the baseline. For example, the boundary in ExFuse is more precise than GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we first point out the ineffective feature fusion problem in current U-Net structure. Then, we propose our ExFuse framework to tackle this problem via bridging the gap between high-level low-resolution and low-level  <ref type="table" target="#tab_4">Table 7</ref> #3.</p><p>high-resolution features. Eventually, better feature fusion is demonstrated by the performance boost when fusing with original low-level features and the overall segmentation performance is improved by a large margin. Our ExFuse framework also achieves new state-of-the-art performance on PASCAL VOC 2012 benchmark.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fusion of low-level and high-level features. a) "Pure" low-level high-resolution and "pure" high-level low-resolution features are difficult to be fused because of the significant semantic and resolution gaps. b) Introducing semantic information into lowlevel features or spatial information into high-level features benefits the feature fusion. "dn" and "up" blocks represent abstract up/down-sampling feature embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Overall architecture of our approach. Components with solid boxes belong to the backbone GCN framework<ref type="bibr" target="#b7">[8]</ref>, while others with dashed lines are proposed in this work. Similar to<ref type="bibr" target="#b7">[8]</ref>, Boundary Refinement blocks are actually used but omitted in the figure. Numbers (H ×W ×C) in blocks specify the output dimension of each component. SS -semantic supervision. ECRE -explicit channel resolution embedding. SEBsemantic embedding branch. DAP -densely adjacent prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Details of Semantic Supervision (SS) component in our pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Design of the Semantic Embedding Branch in Fig 2. The "×" sign means element-wise multiplication. If there are more than one groups of high-level features, the component outputs the production of each feature map after upsampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Illustration of the design of Explicit Channel Resolution Embedding (ECRE) module in Fig 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Illustration of Densely Adjacent Prediction (DAP) component in Fig 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig 7</head><label>7</label><figDesc>visualizes some representative results of the GCN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Examples of semantic segmentation results on PASCAL VOC 2012 validation set. (b) is our GCN [8] baseline which achieves 81.0% mIoU on val set. (c) is our method which achieves 85.4% on val set, as shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>GCN [8] segmentation results using given feature levels. Performances are evaluated by standard mean IoU(%) on PASCAL VOC 2012 validation set. Lower feature level involves less semantic but higher-resolution features and vice versa (see Fig</figDesc><table><row><cell cols="3">Levels ResNet 50 (%) ResNeXt 101 (%)</cell></row><row><cell>{4}</cell><cell>70.04</cell><cell>73.79</cell></row><row><cell>{3, 4}</cell><cell>72.17</cell><cell>75.97</cell></row><row><cell>{2, 3, 4}</cell><cell>72.28</cell><cell>75.98</cell></row><row><cell>{1, 2, 3, 4}</cell><cell>72.41</cell><cell>76.02</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Effects of Semantic Supervision (SS). Classification scores are evaluated on ImageNet 2012 validation set.</figDesc><table><row><cell>Model</cell><cell cols="2">Cls err (top-1, %) Seg mIoU (%)</cell></row><row><cell>Res50</cell><cell>24.15</cell><cell>72.4</cell></row><row><cell>SS Res50</cell><cell>24.77</cell><cell>73.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 (Table 3 Table 4 .</head><label>434</label><figDesc>#3) shows the result, which is even worse than the baseline. Ablation study on the design of Explicit Channel Resolution Embedding, (ECRE). The baseline model is inTable 3 (#3)</figDesc><table><row><cell cols="2">Index Baseline SS LR ECRE SEB DAP mIoU (%)</cell></row><row><cell>1</cell><cell>76.0</cell></row><row><cell>2</cell><cell>77.5</cell></row><row><cell>3</cell><cell>78.3</cell></row><row><cell>4</cell><cell>78.8</cell></row><row><cell>5</cell><cell>79.0</cell></row><row><cell>6</cell><cell>79.6</cell></row><row><cell>7</cell><cell>80.0</cell></row></table><note>. Ablation experiments of the methods in Sec 3. Performances are evaluated by standard mean IoU(%) on PASCAL VOC 2012 validation set. The baseline model is [8] (our impl.) SS -semantic supervision. LR -layer rearrangement. ECRE -explicit channel resolution embedding. SEB -semantic embedding branch. DAP -densely adjacent prediction.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 6 .</head><label>6</label><figDesc>Comparison of Original GCN<ref type="bibr" target="#b7">[8]</ref> and ExFuse on segmentation results using given feature levels. The backbone feature extractor networks are both ResNeXt 101.</figDesc><table><row><cell></cell><cell></cell><cell>(%) ExFuse (%)</cell></row><row><cell>{4}</cell><cell>73.79</cell><cell>77.29</cell></row><row><cell>{3, 4}</cell><cell>75.97</cell><cell>78.69</cell></row><row><cell>{2, 3, 4}</cell><cell>75.98</cell><cell>79.11</cell></row><row><cell>{1, 2, 3, 4}</cell><cell>76.02</cell><cell>80.04</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 7 .</head><label>7</label><figDesc>to pretrain our model. COCO has 80 classes and we only retain images including the same 20 classes in PASCAL VOC 2012 and all other Strategies and results on PASCAL VOC 2012 validation set classes are regarded as background. Training process has 3 stages. In stage-1, we mix up all images in COCO, SBD and standard PASCAL VOC 2012 images, resulting in 109892 images for training in total. In stage-2, we utilize SBD and PASCAL VOC 2012 training images. Finally for stage-3, we only employ standard PASCAL VOC 2012 training set. We keep image crop size unchanged during the whole training procedure and all other settings are exactly the same as</figDesc><table><row><cell cols="2">Index ResNeXt 131 COCO Flip mIoU (%)</cell></row><row><cell>1 (ResNeXt 101)</cell><cell>80.0</cell></row><row><cell>2</cell><cell>80.8</cell></row><row><cell>3</cell><cell>85.4</cell></row><row><cell>4</cell><cell>85.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Performance on PASCAL VOC 2012 test set</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Though ResNeXt 101 performs much better than ResNet 101<ref type="bibr" target="#b13">[14]</ref> on ImageNet classification task (21.2% vs. 23.6% in top-1 error), we find there are no significant differences on the semantic segmentation results (both are 76.0% mIoU).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Laplacian pyramid reconstruction and refinement for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="519" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Gated feedback refinement network for dense image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amirul</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rochan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Large kernel matters -improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="357" to="361" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="3431" to="3440" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="page" from="770" to="778" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deeply-supervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eprint Arxiv</title>
		<imprint>
			<biblScope unit="page" from="562" to="570" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Checkerboard artifact free sub-pixel convolution: A note on sub-pixel convolution, resize convolution and convolution resize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Full-resolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="991" to="998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="4278" to="4284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01507</idno>
		<title level="m">Squeeze-and-excitation networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Convolutional neural pyramid for image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="740" to="755" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Stacked deconvolutional network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
