<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Fine-grained Inpainting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Hui</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Senior Member, IEEE</roleName><forename type="first">Xinbo</forename><surname>Gao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiumei</forename><surname>Wang</surname></persName>
						</author>
						<title level="a" type="main">Image Fine-grained Inpainting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>A CORRESPONDENCE SUBMITTED TO JOURNAL 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-image fine-grained inpainting</term>
					<term>self-guided re- gression</term>
					<term>geometrical alignment</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Image inpainting techniques have shown promising improvement with the assistance of generative adversarial networks (GANs) recently. However, most of them often suffered from completed results with unreasonable structure or blurriness. To mitigate this problem, in this paper, we present a one-stage model that utilizes dense combinations of dilated convolutions to obtain larger and more effective receptive fields. Benefited from the property of this network, we can more easily recover large regions in an incomplete image. To better train this efficient generator, except for frequently-used VGG feature matching loss, we design a novel self-guided regression loss for concentrating on uncertain areas and enhancing the semantic details. Besides, we devise a geometrical alignment constraint item (feature center coordinates alignment) to compensate for the pixel-based distance between prediction features and ground-truth ones. We also employ a discriminator with local and global branches to ensure local-global contents consistency. To further improve the quality of generated images, discriminator feature matching on the local branch is introduced, which dynamically minimizes the similarity of intermediate features between synthetic and groundtruth patches. Extensive experiments on several public datasets demonstrate that our approach outperforms current state-of-theart methods. Code is available at https://github.com/Zheng222/ DMFN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Image inpainting (a.k.a. image completion) aims to synthesize proper contents in missing regions of an image, which can be used in many applications. For instance, it allows removing unwanted objects in image editing tasks, while filling the contents that are visually realistic and semantically correct. Early approaches to image inpainting are mostly based on patches of low-level features. PatchMatch <ref type="bibr" target="#b0">[1]</ref>, a typical method, iteratively searches optimal patches to fill in the holes. It can produce plausible results when painting image background or repetitive textures. However, it cannot generate pleasing results for cases where completing regions include complex scenes, faces, and objects, which is due to PatchMatch cannot synthesize new image contents, and missing patches cannot be found in remaining regions for challenging cases.</p><p>With the rapid development of deep convolutional neural networks (CNN) and generative adversarial networks (GAN) <ref type="bibr" target="#b1">[2]</ref>, image inpainting approaches have achieved remarkable success. Pathak et al. proposed context-encoder <ref type="bibr" target="#b2">[3]</ref>, which employs a deep generative model to predict missing parts of the scene from their surroundings using reconstruction and adversarial losses. Yang et al. <ref type="bibr" target="#b3">[4]</ref>  transfer into image inpainting to improve textural quality that propagates the high-frequency textures from the boundary to the hole. Li et al. <ref type="bibr" target="#b4">[5]</ref> presented semantic parsing in the generation to restrict synthesized semantically valid contents for the missing facial key parts from random noise. To be able to complete large regions, Iizuka et al. <ref type="bibr" target="#b5">[6]</ref> adopted stacked dilated convolutions in their image completion network to obtain lager spatial support and reached realistic results with the assistance of a globally and locally consistent adversarial training approach. Shortly afterward, Yu et al. <ref type="bibr" target="#b6">[7]</ref> extended this insight and developed a novel contextual attention layer, which uses the features of known patches as convolutional kernels to compute the correlation between the foreground and background patches. More specifically, they calculated attention score for each pixel and then performed transposed convolution on attention score to reconstruct missing patches with known patches. It might be failing when the relationship between unknown and known patches is not close (e.g.masking all of the critical components of a facial image). Wang et al. <ref type="bibr" target="#b7">[8]</ref> proposed a generative multi-column convolutional neural network (GMCNN) that uses varied receptive fields in branches by adopting different sizes of convolution kernels (i.e. 3 × 3, 5 × 5, and 7 × 7) in a parallel manner. This method produces advanced performance but suffers from substantial model parameters (12.562M) caused by large convolution kernels. In terms of image quality (more photo-realistic, fewer artifacts), it is still room for improvement.</p><p>The goals pursued by image inpainting are ensuring produced images with global semantic structure and finely detailed textures. Additionally, completed image should be approaching the ground truth as much as possible, especially for building and face images. Previous techniques more focus on solving how to yield holistically reasonable and photo-realistic images. This problem has been mitigated by GAN <ref type="bibr" target="#b1">[2]</ref> or its improved version WGAN-GP <ref type="bibr" target="#b8">[9]</ref> that is frequently utilized in image inpainting methods <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>. However, concerning fine-grained details, there is still much room to enhance. Besides, these existing methods haven't taken into account the consistency between outputs and targets, i.e., semantic structures should be as much similar as possible for facial images and building images.</p><p>To overcome the limitations of the methods as mentioned above, we present a unified generative network for image inpainting, which is denoted as dense multi-scale fusion network (DMFN). The dense multi-scale fusion block (DMFB), serving as the basic block of DMFN, is composed of four-way dilated convolutions as illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>  <ref type="bibr" target="#b13">[14]</ref> by using our method. The missing areas are shown in white. It is worth noting that they also recover well in terms of lighting and texture.</p><p>images with the realistic and semantic structure, we design a self-guided regression loss that constrains low-level features of the generated content according to the normalized discrepancy map (the difference between the output and target). Geometrical alignment constraint is developed for penalizing the coordinate center of estimated image high-level features away from the ground-truth. This loss can further help the processing of image fine-grained inpainting. We improve the discriminator using relativistic average GAN (RaGAN) <ref type="bibr" target="#b14">[15]</ref>. It is noteworthy that we use global and local branches in the discriminator as in <ref type="bibr" target="#b5">[6]</ref>, where one branch focuses on the global image while the other concentrates on the local patch of the missing region. To explicitly constraint the output and ground-truth images, we utilize the hidden layers of the local branch that belongs to the whole discriminator to evaluate their discrepancy through an adversarial training process. With all these improvements, the proposed method can produce highquality results on multiple datasets, including faces, building, and natural scene images.</p><p>Our contributions are summarized as follows:</p><p>• Self-guided regression loss corrects semantic structure errors to some extent through re-weighing VGG features guided by discrepancy map, which is novel for image/video completion task. • We present the geometrical alignment constraint to supplement the shortage of pixel-based VGG features matching loss, which restrains the results with a more reasonable semantic spatial location. • We propose the dense multiple fusion block (DMFB, enhancing the dilated convolution) to improve the network representation, which increases the receptive field while maintaining an acceptable parameter size. Our generative image inpainting framework achieves compelling visual results (as illustrated in <ref type="figure">Figure 1</ref>) on challenging datasets. As shown in <ref type="table" target="#tab_2">Table I</ref>, we summarized the difference between the typical method and the proposed approach. We are committed to improving the dilated convolution that frequently used in image completion, and developing more losses to measure the matching degree of features from different viewpoint. More details can be found in Section III.</p><p>The rest of this paper is organized as follows. Section II provides a brief review of related inpainting methods. Section III describes the proposed approach and loss functions in detail. In Section IV, we explain the experiments conducted for this work, experimental comparisons with other state-of-theart methods, and model analysis. In Section V, we conclude the study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>A variety of algorithms for image inpainting have been proposed. Traditional diffusion-based methods <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref> propagate information from neighboring regions to the holes. They can work well for small and narrow holes, where the texture and color variance are the same. However, these methods fail to recover meaning contents in the large missing regions. Patch-based approaches, such as <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, search for relevant patches from the known regions in an iterative fashion. Simakov et al. <ref type="bibr" target="#b20">[21]</ref> proposed bidirectional similarity scheme to capture better and summarize non-stationary visual data. However, these methods are computationally expensive due to  <ref type="bibr" target="#b2">[3]</ref> embed the 128 × 128 image with a 64 × 64 center hole as a low dimensional feature vector and then decode it to a 64×64 image. Iizuka et al. <ref type="bibr" target="#b5">[6]</ref> proposed a highperformance completion network with both global and local discriminators that is critical in obtaining semantically and locally consistent image inpainting results. Also, the authors employ the dilated convolution layers to increase receptive fields of the output neurons. Yang et al. <ref type="bibr" target="#b3">[4]</ref> use intermediate features extracted by pre-trained VGG network <ref type="bibr" target="#b22">[23]</ref> to find hole's most similar patch outside the hole. This approach performs multi-scale neural patch synthesis in a coarse-tofine manner, which noticeably takes a long time to fill a large image during the inference stage. For face completion, Li et al. <ref type="bibr" target="#b4">[5]</ref> trained a deep generative model with a combination of reconstruction loss, global and local adversarial losses, and a semantic parsing loss specialized for face images. Contextual Attention (CA) <ref type="bibr" target="#b6">[7]</ref> adopted two-stage network architecture where the first step produces a crude result, and the second refinement network using attention mechanism takes the coarse prediction as inputs and improves fine details. Liu et al. <ref type="bibr" target="#b23">[24]</ref> introduced partial convolution that employs computational operations only on valid pixels and presented an auto-update binary mask to determinate whether the current pixels are valid. Substituting convolutional layers with partial convolutions can help a UNet-like architecture <ref type="bibr" target="#b24">[25]</ref> achieve the state-of-the-art inpainting results. Yan et al. <ref type="bibr" target="#b10">[11]</ref> introduced a special shift-connection to the U-Net architecture for enhancing the sharp structures and fine-detailed textures in the filled holes. This method was mainly developed on building and natural landscape images. Similar to <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b6">[7]</ref>, Song et al. <ref type="bibr" target="#b9">[10]</ref> decoupled the completion process into two stages: coarse inference and fine textures translation. Nazeri et al. <ref type="bibr" target="#b25">[26]</ref> also proposed a two-stage network that comprises of an edge generator and an image completion network. Similar to this method, Li et al. <ref type="bibr" target="#b26">[27]</ref> progressively incorporated edge information into the feature to output more structured image. Xiong et al. <ref type="bibr" target="#b27">[28]</ref> inferred the contours of the objects in the image, then used the completed contours as a guidance to complete the image. Different from frequently-used two-stage processing <ref type="bibr" target="#b28">[29]</ref>, Sagong et al. <ref type="bibr" target="#b29">[30]</ref> proposed parallel path for semantic inpainting to reduce the computational costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dense Multi-scale Fusion Block</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv-5</head><p>Conv </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv-3-4</head><p>Conv-3-2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv-3-8</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv-3 Concat</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv-3</head><p>Conv-3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv-3</head><p>Conv-1 Our proposed inpainting system is trained in an end-to-end way. Given an input image with hole I in , its corresponding binary mask M (value 0 for known pixels and 1 denotes unknown ones), the output I out predicted by the network, and the ground-truth image I gt . We take the input image and mask as inputs, i.e., [I in , M]. We now elaborate on our network as follows.</p><formula xml:id="formula_0">2 K 3 K 4 K Conv-3-1 Conv-3-4 Conv-3-2 Conv-3-8 Conv-3 Combination Concat Conv-3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network structure</head><p>As depicted in <ref type="figure">Figure 3</ref>, our framework consists of a generator, and a discriminator with two branches. The generator produces plausible painted results, and the discriminator conducts adversarial training.</p><p>For image inpainting task, the size of the receptive fields should be sufficiently large. The dilated convolution is popularly adopted in the previous works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> to accomplish this purpose. This way increases the area that can use as input without increasing the number of learnable weights. However, the kernel of dilated convolution is sparse, which skips many pixels during applying to compute. Large convolution kernel (e.g.7 × 7) is applied in <ref type="bibr" target="#b7">[8]</ref> to implement this intention. However, this solution introduces heavy model parameters. To enlarge the receptive fields and ensure dense convolution kernels simultaneously, we propose our dense multi-scale fusion  <ref type="figure">Fig. 3</ref>. The framework of our method. The activation layer followed by each "convolution + norm" or convolution layer in the generator is omitted for conciseness. The activation function adopts ReLU except for the last convolution (Tanh) in the generator. Blue dotted box indicates our upsampler module (TConv-4 is 4 × 4 transposed convolution) and "s2" denotes the stride of 2.</p><p>block (DMFB, see in <ref type="figure" target="#fig_0">Figure 2</ref>) inspired by <ref type="bibr" target="#b30">[31]</ref>. Specifically, the first convolution on the left in DMFB reduces the channels of input features to 64 for decreasing the parameters, and then these processed features are sent to four branches to extract multi-scale features, denoted as x i (i = 1, 2, 3, 4), by using dilated convolutions with different dilation factors. Except for x 1 , each x i has a corresponding 3 × 3 convolution, denoted by K i (·). Through a cumulative addition fashion, we can get dense multi-scale features from the combination of various sparse multi-scale features. We denote by y i the output of K i (·). The combination part can be formulated as</p><formula xml:id="formula_1">y i =      x i , i = 1; K i (x i−1 + x i ) , i = 2; K i (y i−1 + x i ) , 2 &lt; i ≤ 4.<label>(1)</label></formula><p>The following step is the fusion of concatenated features simply using a 1 × 1 convolution. In a word, this basic block especially enhances the general dilated convolution and has fewer parameters than large kernels.</p><p>Different from previous generative inpainting networks <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> that apply WGAN-GP <ref type="bibr" target="#b8">[9]</ref> for adversarial training, we propose to use RaGAN <ref type="bibr" target="#b14">[15]</ref> to pursue more photo-realistic generated images <ref type="bibr" target="#b31">[32]</ref>. This discriminator also considers the consistency of global and local images. 1) Self-guided regression loss: Here, we address the semantic structure preservation issue. We scheme to take self-guided regression constraint to correct the image semantic level estimation. Briefly, we compute the discrepancy map between generated contents and corresponding ground truth to navigate the similarity measure of the feature map hierarchy from the pre-trained VGG19 <ref type="bibr" target="#b22">[23]</ref> network. At first, we investigate the characteristic of VGG feature maps. Given an input image I A , it is first fed forward through the VGG19 to yield a five-level feature map pyramid, where their spatial resolution reduces low progressively. Specifically, the l-th (l = 1, 2, 3, 4, 5) level is set to the feature tensor produced by relul 1 layer of VGG19. These feature tensors are denoted by F l A . We give an illustration of average feature maps F</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Loss functions</head><formula xml:id="formula_2">l A avg = 1 M M m=1 F l A m</formula><p>in <ref type="figure" target="#fig_1">Figure 4</ref>, which suggests that the deeper layers of a pretrained network represent higher-level semantic information, while lower-level features more focus on textural or structural details, such as edges, corners, and other simple conjunctions.</p><p>In this paper, we would intend to improve the detail fidelity of the completed image, especially for building and face images. To this end, through the error map between the output image produced by the generator and ground truth, we get the guidance map to distinguish between areas of challenging and manageable. Therefore, we propose to use the following equation to gain the average error map:</p><formula xml:id="formula_3">M error = 1 3 c∈C (I out,c − I gt,c ) 2 ,<label>(2)</label></formula><p>where C are the three color channels, I out,c denotes c-th channel of the output image. Then, the normalized guidance mask can be calculated by</p><formula xml:id="formula_4">M guidance,p = M error,p − min (M error ) max (M error ) − min (M error ) ,<label>(3)</label></formula><p>where M error,p is the error map value at position p. Note that our guidance mask with continuous values between 0 and 1, which is soft instead of binary. M l guidence corresponds l-th level feature maps and it can be expressed by</p><formula xml:id="formula_5">M l+1 guidance = AP M l guidance ,<label>(4)</label></formula><p>where AP denotes average pooling with kernel size of 2 and stride of 2. Here, M 1 guidance = M guidance <ref type="figure">(Equation 3</ref>). In this way, the value range of M l guidance is still between 0 and 1. In view of the fact that lower-level feature map contains more detailed information, we choose feature tensors from "relu1 1" and "relu2 1" layers to describe image semantic structures. Thus, our self-guided regression loss is defined as</p><formula xml:id="formula_6">L self −guided = 2 l=1 w l M l guidance Ψ l I gt − Ψ l I output 1 N Ψ l I gt ,<label>(5)</label></formula><p>where Ψ l I * is the activation map of the relul 1 layer given original input I * , N Ψ l I gt is the number of elements in Ψ l Igt , is the element-wise product operator, and w l = 1e3 C Ψ l I gt 2 followed by <ref type="bibr" target="#b32">[33]</ref>. Here, C is the channel size of feature map Ψ l</p><p>Igt . An obvious benefit for this regularization is to suppress regions with higher uncertainty (as shown in <ref type="figure" target="#fig_2">Figure 5</ref>). M guidance can be viewed as a spatial attention map, which preferably optimizes areas that are difficult to handle. Our selfguided regression loss is performed lower-level semantic space instead of pixel space. The merit of this way would appear in the perceptual image synthesis with pleasant structural information.</p><p>2) Geometrical alignment constraint: In the typical solutions, the metric evaluation in higher-level feature space is only achieved using pixel-based loss, e.g., L1 or L2. It doesn't take the alignment of each high-level feature map semantic hub into account. To better measure the distance between high-level features belong to prediction and ground-truth, we impose the geometrical alignment constraint on the response maps of "relu4 1" layer. This term can help the generator create a plausible image that aligned with the target image in position. Specifically, this term encourages the output feature center to be spatially close to the target feature center. The geometrical center for the k-th feature map along axis u is calculated as</p><formula xml:id="formula_7">c k u = u,v u · R (k, u, v) / u,v R (k, u, v) ,<label>(6)</label></formula><p>where response maps R = VGG (I; θ vgg ) ∈ R K×H×W .</p><formula xml:id="formula_8">R (k, u, v) u,v R (k, u, v)</formula><p>represents a spatial probability distribution function. c k u denotes coordinate expectation along axis u. Then, we pass both the completed image I output and ground-truth image I gt through the VGG network and obtain the corresponding response maps R and R. Given these response maps, we compute the centers c k u , c k v and c k u , c k v using Equation <ref type="bibr" target="#b5">6</ref>. Then, we formulate the geometrical alignment constraint as</p><formula xml:id="formula_9">L align = k c k u , c k v − c k u , c k v 2 2 . (7)</formula><p>3) Feature matching losses: The VGG feature matching loss L f m vgg compares the activation maps in the intermediate layers of well-trained VGG19 <ref type="bibr" target="#b22">[23]</ref> model, which can be written as</p><formula xml:id="formula_10">L f m vgg = 5 l=1 w l Ψ l Igt − Ψ l Ioutput 1 N Ψ l I gt ,<label>(8)</label></formula><p>where N Ψ l I gt is the number of elements in Ψ l Igt . Inspired by <ref type="bibr" target="#b33">[34]</ref>, we also introduce local branch in discriminator feature matching loss L f m dis , which is reasonable to assume that the output image are consistent with the ground-truth images under any measurements (i.e., any high-dimensional spaces). This feature matching loss is defined as</p><formula xml:id="formula_11">L f m dis = 5 l=1 w l D l local (I gt ) − D l local (I output ) 1 N D l local (Igt) ,<label>(9)</label></formula><p>where D l local (I * ) is the activation in the l-th selected layer of the discriminator given input I * (see in <ref type="figure">Figure 3</ref>). Note that the hidden layers of the discriminator are trainable, which is slightly different from the well-trained VGG19 network trained on the ImageNet dataset. It can adaptively update based on specific training data. This complementary feature matching can dynamically extract features that may be not mined in VGG model. 4) Adversarial loss: For improving the visual quality of inpainted results, we use relativistic average discriminator <ref type="bibr" target="#b14">[15]</ref> as in ESRGAN <ref type="bibr" target="#b31">[32]</ref>, which is the recent state-of-the-art perceptual image super-resolution algorithm. For the generator, the adversarial loss is defined as</p><formula xml:id="formula_12">L adv = −E xr [log (1 − D Ra (x r , x f ))] − E x f [log (D Ra (x f , x r ))] ,<label>(10)</label></formula><p>where</p><formula xml:id="formula_13">D Ra (x r , x f ) = sigmoid C (x r ) − E x f [C (x f )]</formula><p>and C (·) indicates the discriminator network without the last sigmoid function. Here, real/fake data pairs (x r , x f ) are sampled from ground-truth and output images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5) Final objective:</head><p>With self-guided regression loss, geometrical alignment constraint, VGG feature matching loss, discriminator feature matching loss, adversarial loss, and mean absolute error (MAE) loss, our overall loss function is defined as</p><formula xml:id="formula_14">L total = L mae + λ (L self −guided + L f m vgg ) + ηL f m dis + µL adv + γL align ,<label>(11)</label></formula><p>where λ, η, µ, and γ are used to balance the effects between the losses mentioned above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS We evaluate the proposed inpainting model on Paris Street</head><p>View <ref type="bibr" target="#b2">[3]</ref>, Places2 <ref type="bibr" target="#b34">[35]</ref>, CelebA-HQ <ref type="bibr" target="#b35">[36]</ref>, and a new challenging facial dataset FFHQ <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head><p>CA <ref type="bibr" target="#b6">[7]</ref> GMCNN <ref type="bibr" target="#b7">[8]</ref> DMFN (Ours) <ref type="figure">Fig. 6</ref>. Visual comparisons on CelebA-HQ. Best viewed with zoom-in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Experimental settings</head><p>For our experiments, we empirically set λ = 25, η = 5, µ = 0.003 and γ = 1 in Equation <ref type="bibr" target="#b10">11</ref>. The training procedure is optimized using Adam optimizer <ref type="bibr" target="#b36">[37]</ref> with β 1 = 0.5 and β 2 = 0.9. We set the learning rate to 2e − 4. The batch size is 16. We apply PyTorch framework to implement our model and train them using NVIDIA TITAN Xp GPU (12GB memory).</p><p>For training, given a raw image I gt , a binary image mask M (value 0 for known pixels and 1 denotes unknown ones) at a random position. In this way, the input image I in is obtained from the raw image as I in = I gt (1 − M). Our inpainting generator takes [I in , M] as input, and produces prediction I pred . The final output image is I out = I in + I pred M. All input and output are linearly scaled to [−1, 1]. We train our network on the training set and evaluate it on the validation set (Places2, CelebA-HQ, and FFHQ) or testing set (Paris street view and CelebA). For training, we use images of resolution 256×256 with the largest hole size 128×128 as in <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. For Paris street view (936 × 537), we randomly crop patches with resolution 537 × 537 and then scale down them to 256 × 256 for training. Similarly, for Places2 (512 × * ), 512 × 512 subimages are cropped at a random location. These images are scaled down to 256 × 256 for our model. For CelebA-HQ and FFHQ face datasets (1024 × 1024), images are directly scaled to 256. We use the irregular mask dataset provided by <ref type="bibr" target="#b23">[24]</ref>. For irregular masks, the random regular regions are cropped and sent to the local discriminator. All results generated by our model are not post-processed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Qualitative comparisons</head><p>As shown in <ref type="figure">Figures 7, 6</ref>, and 8, compared with other state-of-the-art methods, our model gives a noticeable visual improvement on textures and structures. For instance, our network generates plausible image structures in <ref type="figure">Figure 7</ref>, which mainly stems from the dense multi-scale fusion architecture and well-designed losses. The realistic textures are hallucinated via feature matching and adversarial training. For <ref type="figure">Figure 6</ref>, we show that our results with more realistic details and fewer artifacts than the compared approaches. Besides, we give partial results of our method and PICNet <ref type="bibr" target="#b11">[12]</ref> on Places2 dataset in <ref type="figure">Figure 8</ref>. The proposed DMFN creates more reasonable, natural, and photo-realistic images. Additionally, we also show some example results (masks at random position) of our model trained on FFHQ in <ref type="figure">Figure 9</ref>. In <ref type="figure">Figure 10</ref>, our method performs more stable and fine for large-area irregular masks than compared algorithms. More compelling results can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Quantitative comparisons</head><p>Following <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, we measure the quality of our results using peak signal-to-noise ratio (PSNR) and structural similarity (SSIM). Learned perceptual image patch similarity (LPIPS) <ref type="bibr" target="#b38">[39]</ref> is a new metric that can better evaluate the perceptual similarity between two images. Because the purpose of image inpainting is to pursue visual effects, we adopt LPIPS as the main qualitative assessment. The lower the values of LPIPS, the better. In Places2, 100 validation images from "canyon" scene category are chosen for evaluation. As shown in <ref type="table" target="#tab_2">Table II</ref>, our method produces acceptable results compared with CA <ref type="bibr" target="#b6">[7]</ref>, GMCNN <ref type="bibr" target="#b7">[8]</ref>, PICNet <ref type="bibr" target="#b11">[12]</ref>, and PENNet <ref type="bibr" target="#b12">[13]</ref> in terms of all evaluation measurements.</p><p>We also conducted user studies as illustrated in <ref type="figure" target="#fig_0">Figure 12</ref>. The scheme is based on blind randomized A/B/C tests deployed on Google Forms platform as in <ref type="bibr" target="#b7">[8]</ref>. Each survey includes 40 single-choice questions. Each question involves three options (completed images that are generated from the same corrupted input by three different methods). There are 20 participants invited to accomplish this survey. They are asked to select to the most realistic item in each question. The option order is shuffled each time. Finally, our method outperforms compared approaches by a large margin.</p><p>D. Ablation study 1) Effectiveness of DMFB: To validate the representation ability of our DMFB, we replace its middle part (4 dilated convolutions and combination operation) to a 3×3 dilated convolution (256 channels) with dilation rate of 2 or 8 ("rate=2" or "rate=8", see in <ref type="table" target="#tab_2">Table III)</ref>. Additionally, to verify the strength of K i (·) in combination operation, we perform the DMFB without K i (·) that denoted as "w/o K i (·)" in <ref type="table" target="#tab_2">Table III. Combined with Table III</ref> and <ref type="figure">Figure 13</ref>, we can clearly see that our model with DMFB (Parms: 471, 808) predicts reasonable and less artifact images than ordinary dilated convolutions (Parms: 803, 392). Specifically, in the second row of <ref type="figure">Figure 13</ref>, both "w/o combination" and "w/o K i (·)" have different degrees of the partial absence of lampposts. The visual effects of them can be obviously ranked as "DMFB" &gt; "w/o K i (·)" &gt; "w/o combination". Meanwhile, the results of "rate=2" and "rate=8" suggest the importance of spatial support as discussed in <ref type="bibr" target="#b5">[6]</ref>. It also demonstrates large and dense receptive field is beneficial to completing images with large holes.</p><p>2) Self-guided regression and geometrical alignment constraint: To investigate the effect of proposed self-guided regression loss and geometrical alignment constraint, we train a complete DMFN on CelebA-HQ dataset without the corresponding loss. Thanks to the effectiveness of the generator and the loss functions, the baseline model (w/o self-guided regression loss &amp; geometrical alignment constraint) can already achieve almost satisfactory results. Because of the subtle problems existing in the baseline model, we propose two loss Input Image CE <ref type="bibr" target="#b2">[3]</ref> Shift-Net <ref type="bibr" target="#b10">[11]</ref> GMCNN <ref type="bibr" target="#b7">[8]</ref> PICNet <ref type="bibr" target="#b11">[12]</ref> DMFN (Ours) <ref type="figure">Fig. 7</ref>. Visual comparisons on Paris street view.  effectiveness of various proposed losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussions</head><p>1) Intention of self-guided regression loss: The prior works (e.g., CA <ref type="bibr" target="#b6">[7]</ref> and GMCNN <ref type="bibr" target="#b7">[8]</ref>) assign less weight at masked region centers to formulate the variant of L1 loss. CA only use spatial discounted L1 loss in the coarse network (first stage). GMCNN first train their model with only confidence-driven L1 loss. Without the assistant of GAN, these first stages only aim to obtain a coarse results. Different from them, the self-guided regression loss apply to VGG features focuses on learning the hard areas measured by the current guidance map. And our framework is one-stage trained with all losses at the same time.</p><p>2) Analysis of self-guided regression loss: In this section, we conduct the ablation study of using different distance metrics in the average error map. Table V compares instantiations including Gaussian, dot product, and L2 when used in self-guided regression loss. The simple L2 performs the best LPIPS performance, and Gaussian achieves the best PSNR and SSIM performance. Considering that the purpose of image inpainting is the pursuit of plausible visual effect, We choose the simple and efficient L2 distance to measure the average error map. <ref type="figure" target="#fig_2">Figure 15</ref> shows the visual comparisons among these metrics, which indicates L2 can recover the better structural information.</p><p>3) Investigation of geometrical alignment constraint: As illustrated in <ref type="figure">Figure 16</ref> 'relu1 1', 'relu2 1', and 'relu3 1' layers almost have the completed face contour, which is unsuited to aligning the part components using geometrical alignment constraint. For the spatial resolution of each response map generated by 'relu5 1' layer is only 16×16, it will result in a small coordinate range. Thus, we choose the output response maps of 'relu4 1' layer to compute our geometrical alignment constraint, which guides the coordinate expectation registration.</p><p>V. CONCLUSION In this paper, we proposed a dense multi-scale fusion network with self-guided regression loss and geometrical alignment constraint for image fine-grained inpainting, which highly improves the quality of produced images. Specifically, dense multi-scale fusion block is developed to extracted better features. With the assistance of self-guided regression loss, the restoration of semantic structures becomes easier. Additionally, geometrical alignment constraint is inductive to the coordinate registration between generated image and ground-truth, which promotes the reasonableness of painted results.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>The architecture of the proposed dense multi-scale fusion block (DMFB). Here, "Conv-3-8" indicates 3 × 3 convolution layer with the dilation rate of 8 and ⊕ is element-wise summation. Instance normalization (IN) and ReLU activation layers followed by the first convolution, second column convolutions and concatenation layer are omitted for brevity. The last convolutional layer only connects an IN layer. The number of output channels for each convolution is set to 64 except for the last 1 × 1 convolution (256 channels) in DMFB.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Visualization of average VGG feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Visualization of guidance maps. (Left) Guidance map M 1 guidance for "relu1 1" layer. (Right) Guidance map M 2 guidance for "relu2 1" layer. These are corresponding to Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 14 .Fig. 15 .Fig. 16 .</head><label>141516</label><figDesc>, we visualize the first 64 feature maps of each selected VGG layer. The response maps of Input w/o self-guided w/o alignment with all Visual comparison of results using different losses. Best viewed with zoom-in. Visual comparisons on Paris street view. Best viewed with zoom-in. Visualization of VGG feature maps (the first 64 pieces).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>This work was supported in part by the National Key Research and Development Program of China under Grants 2018AAA0102702, 2018AAA0103202, in part by the National Natural Science Foundation of China under Grant 61772402, 61671339 and 61871308.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>introduced style The authors are with the Video &amp; Image Processing System (VIPS) Lab, School of Electronic Engineering, Xidian University, No.2, South Taibai Road, Xi'an 710071, China. (e-mail: zheng hui@aliyun.com, lee-jie@mail.xidian.edu.cn, xbgao@mail.xidian.edu.cn, wangxm@xidian.edu.cn)</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Oct 2020 Input Output Input Output</head><label></label><figDesc></figDesc><table /><note>. This basic block adopts the combination and fusion of hierarchical features extracted from various convolutions with different dilation rates to obtain better multi-scale features, compared with general dilated convolution (dense v.s. sparse). For generating arXiv:2002.02609v2 [cs.CV] 4Fig. 1. The inpainted results on FFHQ dataset</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I IMAGE</head><label>I</label><figDesc>INPAINTING METHODOLOGY EMPLOYED BY SOME REPRESENTATIVE MODELS.</figDesc><table><row><cell>Method</cell><cell>GMCNN (NeurIPS'2018) [8]</cell><cell>GC (ICCV'2019) [16]</cell><cell>DMFN (Ours)</cell></row><row><cell>Stage</cell><cell>one-stage</cell><cell>two-stage</cell><cell>one-stage</cell></row><row><cell>Generator</cell><cell>multi-column CNNs</cell><cell>gated CNN</cell><cell>dense multi-scale fusion network</cell></row><row><cell>Discriminator</cell><cell>WGAN-GP</cell><cell>SN-PatchGAN</cell><cell>RelativisticGAN</cell></row><row><cell>Losses</cell><cell cols="3">reconstruction + adversarial + ID-MRF l1 reconstruction + SN-PatchGAN l1 + self-guided regression + fm vgg + fm dis + adversarial + alignment</cell></row><row><cell cols="3">calculating the similarity scores of each output-target pair. To</cell><cell></cell></row><row><cell cols="3">relieve this problem, PatchMatch [1] is proposed, which speeds</cell><cell></cell></row><row><cell cols="3">it up by designing a faster similar patch searching algorithm.</cell><cell></cell></row><row><cell cols="3">Ding et al. [22] proposed a Gaussian-weighted nonlocal</cell><cell></cell></row><row><cell cols="3">texture similarity measure to obtain multiple candidate patches</cell><cell></cell></row><row><cell cols="2">for each target patch.</cell><cell></cell><cell></cell></row><row><cell cols="3">Recently, deep learning and GAN-based algorithms have</cell><cell></cell></row><row><cell cols="3">been a remarkable paradigm for image inpainting. Context</cell><cell></cell></row><row><cell cols="2">Encoders (CE)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>o Fa Sigmoid Global Branch Local Branch Instance Norm Self-guided Regression Mean Absolute Error Target Output Geometrical alignment Combination Conv-3-1</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">64 128</cell><cell>256</cell><cell cols="2">512 512</cell><cell>512</cell><cell></cell></row><row><cell></cell><cell cols="2">-3 64 128 128 256 Conv-3 Conv-3</cell><cell>DMFB</cell><cell>DMFB VGG Feature Matching</cell><cell cols="2">Conv-3 256 128 128 64 3 TConv-4 Conv-3 Conv-3 Upsampler</cell><cell>Conv-5 64 Conv-5</cell><cell>Batch Norm</cell><cell>Leaky ReLU Conv-5</cell><cell cols="2">128 256 Conv-5 Conv-5 Conv-5 Conv-5</cell><cell>Conv-5 512 Conv-5</cell><cell>512 Conv-5 Conv-5</cell><cell>Dense (512) Dense (512)</cell><cell>Concat</cell><cell>Leaky ReLU</cell><cell>Dense (1)</cell><cell>R</cell></row><row><cell>Input</cell><cell>s2</cell><cell>s2</cell><cell></cell><cell></cell><cell>s2</cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Discriminator</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Feature Matching</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Fake Sigmoid Global Branch Local Branch Instance Norm Self-guided Regression Mean Absolute Error Target Output Geometrical alignment</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">64 128</cell><cell>256</cell><cell cols="2">512 512</cell><cell>512</cell><cell></cell></row><row><cell></cell><cell cols="2">Conv-5 64 128 128 256 Conv-3 Conv-3 Conv-3</cell><cell>DMFB</cell><cell>DMFB VGG Feature Matching</cell><cell cols="2">Conv-3 256 128 128 64 TConv-4 Conv-3 Upsampler</cell><cell>Conv-3 3</cell><cell>Conv-5 64 Conv-5</cell><cell>Batch Norm</cell><cell>Leaky ReLU Conv-5</cell><cell cols="2">128 256 Conv-5 Conv-5 Conv-5 Conv-5</cell><cell>Conv-5 512 Conv-5</cell><cell>512 Conv-5 Conv-5</cell><cell>Dense (512) Dense (512)</cell><cell>Concat</cell><cell>Leaky ReLU</cell><cell>Dense (1)</cell><cell>Real or</cell></row><row><cell>Input</cell><cell>s2</cell><cell>s2</cell><cell></cell><cell></cell><cell>s2</cell><cell>s2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Discriminator</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Feature Matching</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE II QUANTITATIVE</head><label>II</label><figDesc>RESULTS (CENTER REGULAR MASK) ON FOUR TESTING DATASETS. Ours) 0.1018 / 25.00 / 0.8563 0.1188 / 22.36 / 0.8194 0.0460 / 26.50 / 0.8932 0.0457 / 26.49 / 0.8985</figDesc><table><row><cell>Method</cell><cell></cell><cell>Paris street view (100) LPIPS / PSNR / SSIM</cell><cell>Places2 (100) LPIPS / PSNR / SSIM</cell><cell>CelebA-HQ (2,000) LPIPS / PSNR / SSIM</cell><cell>FFHQ (10,000) LPIPS / PSNR / SSIM</cell></row><row><cell>CA [7]</cell><cell></cell><cell>N/A</cell><cell>0.1524 / 21.32 / 0.8010</cell><cell>0.0724 / 24.13 / 0.8661</cell><cell>N/A</cell></row><row><cell cols="2">GMCNN [8]</cell><cell>0.1243 / 24.38 / 0.8444</cell><cell>0.1829 / 19.51 / 0.7817</cell><cell>0.0509 / 25.88 / 0.8879</cell><cell>N/A</cell></row><row><cell cols="2">PICNet [12]</cell><cell>0.1263 / 23.79 / 0.8314</cell><cell>0.1622 / 20.70 / 0.7931</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="2">PENNet [13]</cell><cell>N/A</cell><cell>0.2384 / 21.93 / 0.7586</cell><cell>0.0676 / 25.50 / 0.8813</cell><cell>N/A</cell></row><row><cell>DMFN (Input</cell><cell>GT</cell><cell cols="2">PICNet [12] DMFN (Ours)</cell><cell></cell></row><row><cell cols="3">Fig. 8. Visual comparisons on Places2.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III QUANTITATIVE</head><label>III</label><figDesc>RESULTS OF DIFFERENT STRUCTURES ON PARIS STREET VIEW DATASET (CENTER REGULAR MASK).</figDesc><table><row><cell cols="2">Input</cell><cell cols="5">LBAM [38] PICNet [12] DMFN (Ours)</cell></row><row><cell cols="7">Fig. 10. Inpainted images with irregular masks on Paris StreetView.</cell></row><row><cell cols="2">Input</cell><cell cols="2">GC [16]</cell><cell cols="3">PICNet [12] DMFN (Ours)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input</cell><cell>DMFN (Ours)</cell><cell>GT</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Fig. 9. Visual results on FFHQ dataset.</cell></row><row><cell>Model</cell><cell>rate=2</cell><cell>rate=8</cell><cell cols="3">w/o combination w/o K i (·)</cell><cell>DMFB</cell></row><row><cell>Params</cell><cell>803,392</cell><cell>803,392</cell><cell></cell><cell>361,024</cell><cell>361,024</cell><cell>471,808</cell></row><row><cell>LPIPS↓</cell><cell>0.1059</cell><cell>0.1067</cell><cell></cell><cell>0.1083</cell><cell>0.1026</cell><cell>0.1018</cell></row><row><cell>PSNR↑</cell><cell>24.93</cell><cell>24.91</cell><cell></cell><cell>24.24</cell><cell>24.93</cell><cell>25.00</cell></row><row><cell>SSIM↑</cell><cell>0.8530</cell><cell>0.8549</cell><cell></cell><cell>0.8505</cell><cell>0.8561</cell><cell>0.8563</cell></row></table><note>functions to further refine the results, the so-called image fine- grained inpainting. For instance, in the first row of Figure 14, "w/o alignment" shows that the eyelid lines at the left eye (inred box) are dislocation, the eyebrows are slightly scattered. "w/o self-guided" yields correct double eyelids, but eyebrows are still unnatural. "with all" shows the best performance. Although the qualitative performance is not much improved, these new loss functions have a corrective effect on a few problematic results produced by the baseline model. And we give the quantitative results in Table IV, which validates theFig. 11. Inpainted images with irregular masks on CelebA-HQ.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV INVESTIGATION</head><label>IV</label><figDesc>OF SELF-GUIDED REGRESSION LOSS AND GEOMETRICAL ALIGNMENT CONSTRAINT ON CELEBA-HQ (RANDOM REGULAR MASK).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.8 0.9</cell><cell cols="2">78.13%</cell><cell>0.8 0.9</cell><cell>81.88%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.7</cell><cell></cell><cell>0.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.6</cell><cell></cell><cell>0.6</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.5</cell><cell></cell><cell>0.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.4</cell><cell></cell><cell>0.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.3</cell><cell></cell><cell>0.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2</cell><cell>13.88%</cell><cell>0.2</cell><cell>17.50%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1</cell><cell>8.00%</cell><cell>0.1</cell><cell>0.63%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell>0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Paris StreetView</cell><cell>CelebA-HQ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Shift-Net GMCNN DMFN(Ours)</cell><cell>CA</cell><cell>GMCNN</cell><cell>DMFN(Ours)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Fig. 12. Results of user study.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Input</cell><cell>rate=2</cell><cell>rate=8</cell><cell>w/o combination</cell><cell>w/o K i (·)</cell><cell>DMFB (Ours)</cell></row><row><cell cols="5">Metric w/o self-guided w/o align w/o dis fm with all</cell><cell></cell></row><row><cell>LPIPS↓</cell><cell>0.0537</cell><cell>0.0534</cell><cell>0.0542</cell><cell>0.0530</cell><cell></cell></row><row><cell>PSNR↑</cell><cell>25.73</cell><cell>25.63</cell><cell>25.65</cell><cell>25.83</cell><cell></cell></row><row><cell>SSIM↑</cell><cell>0.8892</cell><cell>0.8884</cell><cell>0.8870</cell><cell>0.8892</cell><cell></cell></row></table><note>Fig. 13. Visual comparison of different structures. Best viewed with zoom-in.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V THE</head><label>V</label><figDesc>COMPARISON OF SELF-GUIDED REGRESSION LOSS WITH VARIOUS DISTANCE METRICS ON PARIS STREETVIEW. HERE, X i ∈ R C×1 REPRESENTS THE VECTOR OF THE IMAGE X ∈ R C×H×W AT POSITION i. metric φ (X i , Y i ) PNSR↑ SSIM↑ LPIPS↓ Gaussian distance exp − X i − Y i</figDesc><table><row><cell cols="3">Distance 2 2 2σ 2</cell><cell>25.06</cell><cell>0.8596</cell><cell>0.1027</cell></row><row><cell>Dot product L2 distance</cell><cell>X i Y T i X i − Y i</cell><cell>2 2</cell><cell>24.77 25.00</cell><cell>0.8588 0.8563</cell><cell>0.1035 0.1018</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Patchmatch: a randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Highresolution image inpainting using multi-scale neural patch synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6721" to="6729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative face completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3911" to="3919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="107" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generative image inpainting with contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5505" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image inpainting via generative multi-column convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="331" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Contextual-based image inpainting: Infer, match, and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C. Jay</forename><surname>Kuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Shift-net: Image inpainting via deep feature rearrangement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pluralistic image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1438" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning pyramid-context encoder network for high-quality image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The relativistic discriminator: a key element missing from standard gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Free-form image inpainting with gated convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4471" to="4480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 27th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="417" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Filling-in by joint interpolation of vector fields and gray levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verdera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1200" to="1211" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 28th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Texture optimization for example-based synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Essa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwatra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="795" to="802" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Summarizing visual data using bidirectional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image inpainting using nonlocal texture matching and nonlinear filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Rodríguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1705" to="1719" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="85" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Edgeconnect: Structure guided image inpainting using edge prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nazeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Z</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ebrahimi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshop (ICCVW)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Progressive reconstruction of visual structure for image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5962" to="5971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Foregroundaware image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5840" to="5848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Structureflow: Image inpainting via structure-aware appearance flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="181" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pepsi: Fast image inpainting with parallel decoding network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sagong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="360" to="371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Progressive perception-oriented network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">546</biblScope>
			<biblScope unit="page" from="769" to="786" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Esrgan: Enhanced super-resolution generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision Workshop (ECCVW)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="63" to="79" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Non-stationary texture synthesis by adversarial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Perceptual adversarial networks for image-to-image transformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing (TIP)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4066" to="4079" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Image inpainting with learnable bidirectional attention maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8858" to="8867" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="586" to="595" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
