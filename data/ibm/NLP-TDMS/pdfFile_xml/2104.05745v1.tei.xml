<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fighting the COVID-19 Infodemic with a Holistic BERT Ensemble</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tziafas</surname></persName>
							<email>g.tziafas@student.rug.nl</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantinos</forename><surname>Kogkalidis</surname></persName>
							<email>k.kogkalidis@uu.nl</email>
							<affiliation key="aff1">
								<orgName type="institution">Utrecht University</orgName>
								<address>
									<settlement>Groningen The Netherlands, Utrecht</settlement>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
							<email>t.caselli@rug.nl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Groningen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fighting the COVID-19 Infodemic with a Holistic BERT Ensemble</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes the TOKOFOU system, an ensemble model for misinformation detection tasks based on six different transformer-based pre-trained encoders, implemented in the context of the COVID-19 Infodemic Shared Task for English. We fine tune each model on each of the task's questions and aggregate their prediction scores using a majority voting approach. TOKOFOU obtains an overall F1 score of 89.7%, ranking first.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Social media platforms, e.g., Twitter, Instagram, Facebook, TikTok among others, are playing a major role in facilitating communication among individuals and sharing of information. Social media, and in particular Twitter, are also actively used by governments and health organizations to quickly and effectively communicate key information to the public in case of disasters, political unrest, and outbreaks <ref type="bibr" target="#b5">(Househ, 2016;</ref><ref type="bibr" target="#b16">Stefanidis et al., 2017;</ref><ref type="bibr" target="#b6">LaLone et al., 2017;</ref><ref type="bibr" target="#b2">Daughton and Paul, 2019;</ref><ref type="bibr" target="#b12">Rogers et al., 2019)</ref>.</p><p>However, there are dark sides to the use of social media. The removal of forms of gate-keeping and the democratization process of the production of information have impacted the quality of the content that becomes available. Misinformation, i.e., the spread of false, inaccurate, misleading information such as rumors, hoaxes, false statements, is a particularly dangerous type of low quality content that affects social media platforms. The dangers of misinformation are best illustrated by considering the combination of three strictly interconnected factors: (i) the diminishing abilities to discriminate between trustworthy sources and information from hoaxes and malevolent agents <ref type="bibr" target="#b4">(Hargittai et al., 2010)</ref>; (ii) a faster, deeper, and broader spread than true information, especially for topics such as disasters and science <ref type="bibr" target="#b18">(Vosoughi et al., 2018)</ref>; (iii) the elicitation of fears and suspicions in the population, threatening the texture of societies.</p><p>The COVID-19 pandemic is the perfect target for misinformation: it is the first pandemic of the Information Age, where social media platforms have a primary role in the information-sphere; it is a natural disaster, where science plays a key role to understand and cure the disease; knowledge about the SARS-CoV-2 virus is limited and the scientific understanding is continually developing. To monitor and limit the threats of COVID-19 misinformation, different initiatives have been activated (e.g., #CoronaVirusFacts Alliance 1 , EUvs-Disinfo 2 ), while social media platforms have been enforcing more stringent policies. Nevertheless, the amount of produced misinformation is such that manual intervention and curation is not feasible, calling for the development of automatic solutions grounded on Natural Language Processing.</p><p>The proposed shared task on COVID-19 misinformation presents innovative aspects mirroring the complexity and variation of phenomena that accompanies the spread of misinformation about COVID-19, including fake news, rumors, conspiracy theories, racism, xenophobia and mistrust of science, among others. To embrace the variation of the phenomena, the task organizers have developed a rich annotation scheme based on seven questions <ref type="bibr" target="#b15">(Shaar et al., 2021)</ref>. Participants are asked to design a system capable of automatically labeling a set of messages from Twitter with a binary value (i.e., yes/no) for each of the seven questions. Train and test data are available in three languages, namely English, Arabic, and Bulgarian. Our team, TOKOFOU, submitted predictions only for the English data by developing an ensemble model based on a combination of different transformer-based pre-trained language encoders. Each pre-trained model has been selected to match the language va-riety of the data (i.e., tweet) and the phenomena entailed by each of the questions. With an overall F1 score of 89.7 our system ranked first 3 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Data</head><p>The English task provides both training and development data. The data have been annotated using a in-house crowdsourcing platform following the annotation scheme presented in <ref type="bibr" target="#b0">Alam et al. (2020)</ref>.</p><p>The scheme covers in a very extensive way the complexity of the phenomena that surrounds COVID-19 misinformation by means of seven key questions. The annotation follows a specific pattern after the first question (Q1), that aims at checking whether a message is a verifiable factual claim. In case of a positive answer, the annotator is presented with an additional set of four questions (Q2-Q5) addressing aspects such as presence of false information, interest for the public, presence of harmful content, and check-worthiness. After this block, the annotator has two further questions. Q6 can be seen as a refinement of the presence of harmful content (i.e, the content is intended to harm society or weaponized to mislead the society), while Q7 asks the annotator whether the message should receive the attention of a government authority. In case of a negative answer to Q1, the annotator jumps directly to Q6 and Q7. Quite interestingly, Q6 lists a number of categories to better identify the nature of the harm (e.g., satire, joke, rumor, conspiracy, xenophobic, racist, prejudices, hate speech, among others).</p><p>The labels of the original annotation scheme present fine-grained categories for each questions, including a not sure value. For the task, the set of labels has been simplified to three: yes, no, and nan, with this latter corresponding in some cases to the not sure value. Indeed, due to the dependence of Q2-Q5 to a positive answer to Q1, some nan values for this set of questions can also correspond to not applicable rather than to not sure making the task more challenging than one would expect.</p><p>For English, the organisers released 869 annotated messages for training, 53 for development, and 418 for testing. The distribution of the labels for each question in the training data is reported in <ref type="figure" target="#fig_0">Figure 1</ref>. As the figures show, the dataset is unbalanced for all questions. While the majority of messages present potential factual claims (Q1), only a tiny minority has been labelled as containing false information (Q2) with a very high portion re-3 Source code is available at https://git.io/JOtpH.  ceiving a nan label, suggesting that discriminating whether a claim is false or not is a difficult task for human annotators. Similar observations hold for Q3-Q5. Q6 is a refinement of Q4 about the nature of the harm. The low amount of nan values indicates a better reliability of the annotators in deciding the specific type of harms. Q7 also appears to elicit more clear-cut judgements. Finally, with the exception of questions Q4-Q7 which exhibit a weak pairwise covariance, no noteworthy correlation is discernible (refer to <ref type="figure" target="#fig_1">Figure 2</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Overview</head><p>Our system is a majority voting ensemble model based on a combination of six different transformerbased pre-trained encoders, each selected targeting a relevant aspect of the annotated data such as domain, topic, and specific sub-tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">BERT Models</head><p>Preliminary data analysis and manual inspection of the input texts strongly hint at the notable difficulty of the problem. The questions our model will be called to answer are high-level semantic tasks that sometimes go beyond sentential understanding, seemingly also relying on external world knowledge. The limited size of the dataset also rules out the possibility for a task-specific architecture, even more so if one considers the effective loss of data from nan labels and the small proportion of development samples, factors that increase the risk of overfitting. Knowledge grounding with a static external source becomes impractical in view of the rapid pace of events throughout the COVID-19 pandemic: a claim would need to be contrasted against a distinct version of the knowledge base depending on when it was expressed, inserting significant overhead and necessitating an additional timestamp input feature. <ref type="bibr">4</ref> In light of the above, we turn our attention to pretrained BERT-like models <ref type="bibr" target="#b3">(Devlin et al., 2019)</ref>. BERT-like models are the workhorses in NLP, boasting a high capacity for semantic understanding while acting as implicit rudimentary knowledge bases, owing to their utilization of massive amounts of unlabeled data <ref type="bibr" target="#b11">(Petroni et al., 2019;</ref><ref type="bibr" target="#b13">Rogers et al., 2020)</ref>. Among the many candidate models, the ones confined within the twitter domain make for the most natural choices. Language use in twitter messages differs from the norm, in terms of style, length, and content. A twitter-specific model should then already be accustomed to the particularities of the domain, relieving us from either having to account for domain adaptation, or relying on external data. We obtain our final set of models by filtering our selection in accordance with a refinement of the tasks, as expressed by the questions of the annotation schemes, and the domain. In particular, we focus our selection of models according to the following criteria: (i) models that have been pre-trained on the language domain (i.e, Twitter); (ii) models that have been pre-trained on data related to the COVID-19 pandemic; and (iii) models that have been pre-trained or fine tuned for high-level tasks (e.g., irony and hate speech detection) expressed by any of the target questions. In this way, we identified and used six variations of three pre-trained models, detailed in the following paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BERTWEET</head><p>( <ref type="bibr" target="#b10">Nguyen et al., 2020)</ref> is a RoBERTa base model <ref type="bibr" target="#b7">(Liu et al., 2019)</ref> trained from scratch on 850M tweets. It is a strong baseline that, fine tuned, achieves state-of-the-art benchmarks on the SemEval 2017 sentiment analysis and the SemEval 2018 irony detection shared tasks <ref type="bibr" target="#b14">(Rosenthal et al., 2017;</ref><ref type="bibr" target="#b17">Van Hee et al., 2018)</ref>. Here, we use a variant of the model, additionally trained on 23M tweets related to the COVID-19 pandemic, collected prior to September 2020. <ref type="figure" target="#fig_1">(Müller et al., 2020)</ref> is a pre-trained BERT large model, adapted for use in the twitter setting and specifically the COVID-19 theme by continued unsupervised training on 160M tweets related to the COVID-19 pandemic and collected between January and April 2020. Fine tuned and evaluated on a small range of tasks, it has been shown to slightly outperform the original.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CT-BERT</head><p>TWEETEVAL <ref type="bibr" target="#b1">(Barbieri et al., 2020</ref>) is a pretrained RoBERTa base model, further trained with 60M tweets, randomly collected, resulting in a Twitter-domain adapted version. We use a selection of four TWEETEVAL models, each fine tuned for a twitter-specific downstream task: hate speech-, emotion-and irony-detection, and offensive language identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Fine-tuning</head><p>The affinity between the above models and the task at hand allows us to use them for sentence vectorization as-is, requiring only an inexpensive fine tuning pass. We attach a linear projection on top of each model, which maps its [CLS] token representation to ||Q|| = 7 outputs, one per question. The sigmoid-activated outputs act as logits for binary classification and are trained independently with a cross-entropy loss. We train for 15 epochs on batches of 16 tweets, using the AdamW (Loshchilov and Hutter, 2017) optimizer with a learning rate of 3 · 10 −5 and weight decay of 0.01, without penalizing predictions corresponding to nan gold labels. We add dropout layers of rate 0.5 in each model's classification head. We perform model selection on the basis of mean F1score on the development set, and report results in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Aggregation</head><p>The proposed ensemble model aggregates predictions scores along the model axis by first rounding them (into positive or negative labels) and then selecting the final outcome by a majority rule. The ensemble performs better or equally to all individual models in 3 out of 7 questions in the development set, and its metrics lie above the average for 6 of them. Keeping in mind the small size of the development set, we refrain from altering the voting scheme, expecting the majority-based model to be the most robust.</p><p>During training, we do not apply any preprocessing of the data and rely the respective tokenizer of each model, but homogenize test data by removing URLs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results and Discussion</head><p>Results on the test data are illustrated in <ref type="table" target="#tab_2">Table 2</ref>. Two of the three organizers' baselines, namely the majority voting and the ngram baseline, provide already competitive scores. Our ensemble model largely outperforms all of them. The delta with the second best performing system is 0.6 points in F1 score, with a better Recall for TOKOFOU of 3 points.  and 100, respectively. This indicates that label imbalance affects the test data as well. At the same time, the performance of ngram baseline suggest that lexical variability is limited. This is not expected given the large variety of misinformation topics that seems affect the discussion around the COVID-19 pandemic. These results justify both our choice of models for the ensemble and majority voting as a robust aggregation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We participated in the COVID-19 misinformation shared task with an ensemble of pre-trained BERTbased encoders, fine-tuning each model for predictions in all questions and aggregating them into a final answer through majority voting. Our system is indeed a strong baseline for this task showing the effectiveness of available pre-trained language models for Twitter data, mixed with variants fine tuned for a specific topic (COVID-19) and multiple downstream tasks (emotion detection, hate-speech, etc.). Results indicate that this holistic approach to transfer learning allows for a data-efficient and compute-conscious methodology, omitting the often prohibitive computational requirement of retraining a model from scratch for a specific task, in favour of an ensemble architecture based on task/domain-similar solutions from a large ecosystem of publicly available models.</p><p>With appropriate scaling of the associated dataset, a system as proposed by this paper can be suitably integrated into a human-in-the-loop scenario, serving as an effective assistant in (semi-) automated annotation of Twitter data for misinformation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Distribution of the categories for each question in the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>φ coefficients between question pairs, excluding nan values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>As the figures show, no single model outperforms the rest. Indeed, performance largely varies both across models and questions, with best scores scattered over the table. Similar results occur when repeating the experiments with different random seeds.</figDesc><table><row><cell>Models</cell><cell>average</cell><cell>Q1</cell><cell>Q2</cell><cell>Q3</cell><cell>Q4</cell><cell>Q5</cell><cell>Q6</cell><cell>Q7</cell></row><row><cell>BERTWEET</cell><cell>83.6</cell><cell>86.5</cell><cell>78.4</cell><cell>86.9</cell><cell>88.8</cell><cell>73.4</cell><cell>87.9</cell><cell>83.0</cell></row><row><cell>CT-BERT</cell><cell>81.3</cell><cell>92.4</cell><cell>76.5</cell><cell>88.5</cell><cell>90.5</cell><cell>68.1</cell><cell>80.5</cell><cell>72.4</cell></row><row><cell>TWEETEVAL-hate</cell><cell>84.8</cell><cell>88.6</cell><cell>84</cell><cell>85.3</cell><cell>90.6</cell><cell>82.7</cell><cell>85.8</cell><cell>70.7</cell></row><row><cell>TWEETEVAL-emotion</cell><cell>84.5</cell><cell>78.2</cell><cell>85.9</cell><cell>91.8</cell><cell>89.0</cell><cell>81.4</cell><cell>85.0</cell><cell>80.0</cell></row><row><cell>TWEETEVAL-irony</cell><cell>85.7</cell><cell>86.5</cell><cell>96.1</cell><cell>85.2</cell><cell>81.6</cell><cell>81.5</cell><cell>88.7</cell><cell>76.7</cell></row><row><cell>TWEETEVAL-offensive</cell><cell>82.9</cell><cell>90.5</cell><cell>74.5</cell><cell>84.1</cell><cell>92.2</cell><cell>72.6</cell><cell>84.4</cell><cell>81.5</cell></row><row><cell>average</cell><cell>83.8</cell><cell>87.1</cell><cell>82.6</cell><cell>87.0</cell><cell>88.8</cell><cell>76.6</cell><cell>85.4</cell><cell>77.4</cell></row><row><cell>Ensemble</cell><cell>84.6</cell><cell>90.6</cell><cell>78.4</cell><cell>91.8</cell><cell>92.2</cell><cell>76.9</cell><cell>90.9</cell><cell>78.5</cell></row><row><cell cols="9">Table 1: Best mean F1-scores (%) reported in the development set individually for each question as well as their</cell></row><row><cell cols="6">average (with implicit exclusion of nan labels for Q2-Q5). Best scores are in bold.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Results on English test data -average on all questions -and comparison with organizers' baselines.</figDesc><table><row><cell>When looking at the results per question, 5 TOKO-</cell></row><row><cell>FOU achieves an F1 higher than 90 on Q2 (91.3),</cell></row><row><cell>Q3 (97.8), and Q6 (90.8). With the exclusion of</cell></row><row><cell>Q6, the majority baseline on Q2 and Q3 is 92.7</cell></row><row><cell>5 Leaderboard is available here: https://tinyurl.</cell></row><row><cell>com/2drvruc</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://bit.ly/3uGjwEr 2 https://bit.ly/3wPqsBg arXiv:2104.05745v1 [cs.CL] 12 Apr 2021</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This is especially relevant in the task's context, where the training/development and test data are temporally offset by about a year.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to acknowledge the RUG university computer cluster, Peregrine, for providing the computational infrastructure which allowed the implementation of the current work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fighting the covid-19 infodemic in social media: A holistic perspective and a call to arms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firoj</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahim</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaden</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadir</forename><surname>Durrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamdy</forename><surname>Mubarak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Abdelali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kareem</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darwish</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07996</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">TweetEval: Unified benchmark and comparative evaluation for tweet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Barbieri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Camacho-Collados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Espinosa Anke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.148</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1644" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identifying protective health behaviors on twitter: observational study of travel advisories and zika virus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ashlynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael J</forename><surname>Daughton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medical Internet research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">13090</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Trust online: Young adults&apos; evaluation of web content</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eszter</forename><surname>Hargittai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lindsay</forename><surname>Fullerton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ericka</forename><surname>Menchen-Trevino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristin</forename><forename type="middle">Yates</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of communication</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">27</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Communicating ebola through social media and electronic news media outlets: A cross-sectional study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mowafa</forename><surname>Househ</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Health informatics journal</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="470" to="478" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Embracing human noise as resilience indicator: twitter as power grid correlate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Lalone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Tapia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Caraega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Venkata Kishore Neppalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Halse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sustainable and Resilient Infrastructure</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Fixing weight decay regularization in adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno>abs/1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><surname>Salathé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Per</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kummervold</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.07503</idno>
		<title level="m">Covid-twitter-bert: A natural language processing model to analyse covid-19 content on twitter</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERTweet: A pre-trained language model for English Tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh</forename><surname>Dat Quoc Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1250</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2463" to="2473" />
		</imprint>
	</monogr>
	<note>Language models as knowledge bases?</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Calls to action on social media: Detection, social impact, and censorship potential</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-5005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda</title>
		<meeting>the Second Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="36" to="44" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A primer in BERTology: What we know about how BERT works</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Kovaleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rumshisky</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00349</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="842" to="866" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 4: Sentiment analysis in Twitter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Rosenthal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noura</forename><surname>Farra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2088</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="502" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaden</forename><surname>Shaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Firoj</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><surname>Da San</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Martino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wajdi</forename><surname>Nikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Zaghouani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nakov</surname></persName>
		</author>
		<title level="m">Proceedings of the Fourth Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda, NLP4IF@NAACL&apos; 21, Online</title>
		<meeting>the Fourth Workshop on Natural Language Processing for Internet Freedom: Censorship, Disinformation, and Propaganda, NLP4IF@NAACL&apos; 21, Online</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Findings of the NLP4IF-2021 shared task on fighting the COVID-19 infodemic</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Stefanidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Vraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Lamprianidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacek</forename><surname>Radzikowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><forename type="middle">H</forename><surname>Delamater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arie</forename><surname>Pfoser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Crooks</surname></persName>
		</author>
		<title level="m">Zika in twitter: temporal variations of locations, actors, and concepts. JMIR public health and surveillance</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">SemEval-2018 task 3: Irony detection in English tweets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cynthia</forename><surname>Van Hee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Els</forename><surname>Lefever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Véronique</forename><surname>Hoste</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S18-1005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th International Workshop on Semantic Evaluation</title>
		<meeting>The 12th International Workshop on Semantic Evaluation<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="39" to="50" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The spread of true and false news online</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soroush</forename><surname>Vosoughi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deb</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinan</forename><surname>Aral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">359</biblScope>
			<biblScope unit="issue">6380</biblScope>
			<biblScope unit="page" from="1146" to="1151" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
