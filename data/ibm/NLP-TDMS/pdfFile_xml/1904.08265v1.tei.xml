<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cycle-SUM: Cycle-consistent Adversarial LSTM Networks for Unsupervised Video Summarization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">Eh</forename><surname>Tay</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Hangzhou Dianzi University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Cycle-SUM: Cycle-consistent Adversarial LSTM Networks for Unsupervised Video Summarization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present a novel unsupervised video summarization model that requires no manual annotation. The proposed model termed Cycle-SUM adopts a new cycleconsistent adversarial LSTM architecture that can effectively maximize the information preserving and compactness of the summary video. It consists of a frame selector and a cycle-consistent learning based evaluator. The selector is a bi-direction LSTM network that learns video representations that embed the long-range relationships among video frames. The evaluator defines a learnable information preserving metric between original video and summary video and "supervises" the selector to identify the most informative frames to form the summary video. In particular, the evaluator is composed of two generative adversarial networks (GANs), in which the forward GAN is learned to reconstruct original video from summary video while the backward GAN learns to invert the processing. The consistency between the output of such cycle learning is adopted as the information preserving metric for video summarization. We demonstrate the close relation between mutual information maximization and such cycle learning procedure. Experiments on two video summarization benchmark datasets validate the state-of-theart performance and superiority of the Cycle-SUM model over previous baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>With explosion of video data, video summarization technologies <ref type="bibr" target="#b12">(Ma et al. 2002;</ref><ref type="bibr" target="#b13">Pritch et al. 2007;</ref><ref type="bibr" target="#b12">Lu and Grauman 2013)</ref> become increasingly attractive to help efficiently browse, manage and retrieve video contents. With such techniques, a long video can be shortened to different forms, e.g. key shots <ref type="bibr" target="#b6">(Gygli et al. 2014)</ref>, key frames <ref type="bibr" target="#b9">(Kim, Sigal, and Xing 2014)</ref> and key objects <ref type="bibr" target="#b12">(Meng et al. 2016)</ref>. Here, we aim at selecting key frames for summarizing a video.</p><p>Video summarization is usually formulated as a structure prediction problem <ref type="bibr" target="#b12">Mahasseni, Lam, and Todorovic 2017)</ref>. The model takes as input a sequence of video frames, and outputs a subset of original video frames containing critical information. Ideally, the summary video should keep all key information of the original video with minimal redundancy. Summary completeness and compactness are expected for good video summarization.</p><p>Copyright c 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. is selected by the selector from the input original video. To optimize the selector, a cycle-consistent adversarial LSTM evaluator is introduced to evaluate the summary quality through cycleconsistent learning to measure the mutual information between the original and summary video.</p><p>Existing approaches can be roughly grouped into supervised and unsupervised ones. Many supervised approaches <ref type="bibr" target="#b6">Gygli, Grabner, and Van Gool 2015)</ref> utilize human-annotated summary as ground truth to train a model. However, sufficient human-annotated video summarization examples are not always available or expensive to collect. Thus, unsupervised approaches that do not require human intervention become increasingly attractive due to their low cost. For these methods, it is very critical to design a proper summary quality metric. For instance, (Mahasseni, Lam, and Todorovic 2017) adopt the GAN <ref type="bibr" target="#b5">(Goodfellow et al. 2014)</ref> to measure the similarity between summary and original video and improve the summarization model by optimizing the induced objective, based on a basic idea that a good summary video should be able to faithfully reconstruct the original input video. However, this approach only considers one-direction reconstruction, thus some significant frames may dominate the quality measure, leading to severe information loss in the summary video.</p><p>In this paper, we propose a novel cycle-consistent unsupervised model, motivated by maximizing the mutual information between summary video and original video. Our model is developed with a new cycle-consistent adversarial learning objective to pursue optimal information preserving for the summary video, partially inspired by the cycle gen-erative adversarial network <ref type="bibr" target="#b18">(Zhu et al. 2017;</ref><ref type="bibr" target="#b17">Yi et al. 2017)</ref>. Moreover, to effectively capture the short-range and longrange dependencies among sequential frames , we propose a VAE-based LSTM network as the backbone model for learning video representation. We name such a cycle-consistent adversarial LSTM network for video summarization as the Cycle-SUM.</p><p>Cycle-SUM performs original and summary video reconstruction in a cycle manner, and leverages consistency between original/summary video and its cycle reconstruction result to "supervise" the video summarization. Such a cycle-consistent objective guarantees the summary completeness without additional supervision. Compared with the one-direction reconstruction (i.e., from summary video to original video) <ref type="bibr" target="#b18">(Zhu et al. 2017;</ref><ref type="bibr" target="#b17">Yi et al. 2017)</ref>, the bidirection model performs a reversed reconstruction and a cycle-consistent reconstruction to relieve information loss.</p><p>Structurally, the Cycle-SUM model consists of two components: a selector to predict an importance score for each frame and select the frames with high importance scores to form the summary video, and a cycle-consistent evaluator to evaluate the quality of selected frames through cycle reconstruction. To achieve effective information preserving, the supervisor employs two VAE-based generators and two discriminators to evaluate the cycle-consistent loss. The forward generator and discriminator are responsible for reconstructing the original video from the summary video, and the backward counterparts perform the backward reconstruction from original to the summary video. Both reconstructions are performed in the learned embedding feature space. The discriminator is trained to distinguish the summary video from original. If the summary video misses some informative frames, the discriminator would tell its difference with the original and thus serves as a good evaluator to encourage the selector to pick important frames.</p><p>An illustration of the proposed framework is given in <ref type="figure" target="#fig_0">Fig. 1</ref>. The summary video is a subset of all training video frames, selected by the selector based on the predicted frame-wise importance scores. The original video is reconstructed from the summary video, and then back again. Given a distance between original video and summary video in the deep feature space, the Cycle-SUM model tries to optimize the selector such that the distance is minimized over training examples. The closed loop of Cycle-SUM is aimed at 1) assisting the Bi-LSTM selector to select a subset of frames from the original video, and 2) keeping a suitable distance between summary video and original video in the deep features space to improve summary completeness and reduce redundancy.</p><p>Our contributions are three-fold. 1) We introduce a new unsupervised video summarization model that does not require any manual annotation on video frame importance yet achieves outstanding performance. 2) We propose a novel cycle-consistent adversarial learning model. Compared with one-direction reconstruction based models, our model is superior in information preserving and facilitating the learning procedure. 3) We theoretically derive the relation of mutual information maximization, between summary and original video, with the proposed cycle-consistent adversarial learn-ing model. To our best knowledge, this work is the first to transparently reveal how to effectively maximize mutual information by cycle adversarial learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Supervised video summarization approaches leverage videos with human annotation on frame importance to train models. For example, Gong et al. formulate video summarization as a supervised subset selection problem and propose a sequential determinantal point processing (seqDPP) based model to sample a representative and diverse subset from training data <ref type="bibr" target="#b4">(Gong et al. 2014)</ref>. To relieve human annotation burden and reduce cost, unsupervised approaches, which have received increasing attention, generally design different criteria to give importance ranking over frames for selection. For example, <ref type="bibr" target="#b16">(Wang et al. 2012;</ref><ref type="bibr" target="#b13">Potapov et al. 2014)</ref> propose to select frames according to their content relevance. <ref type="bibr" target="#b12">(Mei et al. 2015;</ref><ref type="bibr" target="#b1">Cong, Yuan, and Luo 2012)</ref> design unsupervised critera by trying to reconstruct the original video from selected key frames and key shots under the dictionary learning framework. Clusteringbased models <ref type="bibr" target="#b2">(De Avila et al. 2011;</ref><ref type="bibr" target="#b10">Kuanar, Panda, and Chowdhury 2013)</ref> and attention-based models <ref type="bibr" target="#b12">(Ma et al. 2002;</ref><ref type="bibr" target="#b3">Ejaz, Mehmood, and Baik 2013)</ref> are also developed to select key frames.</p><p>Recently, deep learning models are developed for both supervised and unsupervised video summarization, in which LSTM is usually taken as the video representation model. For example,  treat video summarization as a sequential prediction problem inspired by speech recognition. They present a bi-direction LSTM architecture to learn the representation of sequential frames in variable length and output a binary vector to indicate which frame to be selected. Our proposed Cycle-Sum model also adopts LSTM as backbone for learning long-range dependence between video frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>The proposed Cycle-SUM model formulates video summarization as a sequence-to-sequence learning problem, taking as input a sequence of video frames and outputting a sequence of frame-wise importance scores. The frames with high importance scores are selected to form a summary video. Throughout the paper, we use O and S to denote the original input video and summary video respectively, and o and s to denote the frame-level features of O and S respectively. To train Cycle-SUM in an unsupervised manner, we develop the cycle-consistent learning method for maximizing mutual information between o and s.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mutual Information Maximization via Cycle Learning</head><p>Video summarization is essentially aimed at extracting video frames that contain critical information of the original video. In this subsection, we explain how to derive our cycleconsistent learning objective through the desired objective of maximizing the mutual information between the summary video s and the original video o. </p><formula xml:id="formula_0">→ G f (s) → G b (G f (s)) ≈ s and the backward cycle o → G b (o) → G f (G b (o)) ≈ o are</formula><p>implemented to encourage the information to be consistent between o and s.</p><p>Formally, the mutual information I(o, s) is defined as</p><formula xml:id="formula_1">I(o, s) o p(o)D KL (p(s|o)||p(s)) ,</formula><p>where D KL is the KL-divergence between two distributions. Then the objective of video summarization is to extract the summary video s from o to maximize their mutual information. The video summarization model should try to produce s such that its conditional distribution p(s|o) gives the maximal mutual information with p(s). However, though it is easy to obtain empirical distribution estimation of original video o, it is difficult to obtain ground truth distribution p(s) of corresponding s in an unsupervised learning scenario. This makes one major challenge to unsupervised video summarization.</p><p>We propose a cycle-consistent learning objective to relieve such learning difficulty. We notice that <ref type="formula">(o)</ref>) .</p><formula xml:id="formula_2">I(o, s) = 1 2 o p(o)D KL (p(s|o)||p(s)) + s p(s)D KL (p(o|s)||p</formula><p>(1)</p><p>The above mutual information computation "anchors" at p(o) that can be faithfully estimated and thus eases the procedure of learning distribution of s even in an unsupervised learning setting.</p><p>To effectively model and optimize the above learning objective, we adopt the Fenchel conjugate to derive its bound that is easier to optimize. The Fenchel conjugate of a func-</p><formula xml:id="formula_3">tion f is defined as f * (t) sup u∈dom f {ut − f (u)}, or equivalently f (u) = sup t∈dom f * {ut − f * (t)}.</formula><p>Thus, defining f (u) = log u, we have the following upper bound for the KL-divergence between distributions p and q:</p><formula xml:id="formula_4">D KL (p||q) = − x q(x) log p(x) q(x) = − x q(x) sup t t p(x) q(x) − f * (t) = − x q(x) sup t t p(x) q(x) + 1 + log(−t) ≤ − sup T ∈T x p(x) log T (x) + x q(x) log(1 − T (x)) ,</formula><p>where t = T (x) − 1 and T is an arbitrary class of functions T : X → R. The above inequality is due to the Jensen's inequality and functions T is only a subset of all possible functions. Therefore, we have</p><formula xml:id="formula_5">D KL (p(s|o)||p(s)) ≤ − sup T ∈T p(s|o) log T (s) + p(s) log(1 − T (s)) ≈ − sup T ∈T 1 |S|   s∼p(s|o) log T (s) + s∼p(s) log(1 − T (s))   .</formula><p>Here S is the set of produced summary videos. We can use a generative model to estimate p(s|o). To this end, we follow the generative-adversarial approach <ref type="bibr" target="#b5">(Goodfellow et al. 2014</ref>) and use two neural networks, G f and D f , to implement sampling and data transformation. Here G f is the forward generative model, taking the condition o as input and outputting a sample of summary sample s. D f is the forward discriminative model. We learn the generative model G f by finding a saddle-point of the above objective function, where we minimize w.r.t. G f and maximize w.r.t. D f :</p><formula xml:id="formula_6">min G f max D f L(G f , D f ) = 1 |S|   s∼G f (o) log D f (s) + s∼p(s) log(1 − D f (s))   .</formula><p>(2) The above objective is similar to the one of GANs, but the generative model is a conditioned one.</p><p>Similarly, we can obtain the learning objective to optimize the KL-divergence D KL (p(o|s)||p(o)) by solving</p><formula xml:id="formula_7">min G b max D b L(G b , D b ) = 1 |O|   o∼G b (s) log D b (o) + o∼p(o) log(1 − D b (o))   .</formula><p>(3) Substituting Eqn.</p><p>(2) and Eqn. (3) into Eqn. (1) gives the following cycle learning objective to maximize the mutual information between the original and summary video:</p><formula xml:id="formula_8">min G f ,G b max D f ,D b L(G f , G b , D f , D b ) = s∼G f (o) log D f (s) + s∼p(s) log(1 − D f (s)) + o∼G b (s) log D b (o) + o∼p(o) log(1 − D b (o)),<label>(4)</label></formula><p>where we omit the constant number of original frames. To relieve the difficulties brought by the unknown distribution p(s), we use the following cycle-consistent constraint to further regularize the generative model and the cycle learning processing:</p><formula xml:id="formula_9">G b (G f (o)) ≈ o, and G f (G b (s)) ≈ s.</formula><p>We name cycle learning with the above consistent constraint as the cycle-consistent learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>Based on the above derivations, we design the cycleconsistent adversarial model for video summarization (Cycle-SUM). The architecture of our Cycle-SUM model is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. The selector is a Bi-LSTM network, which is trained to predict an importance score for every frame in the input video. The evaluator consists of two pairs of generators and discriminators. In particular, the forward generator G f and the discriminator D f form the forward GAN; the backward generator G b and the discriminator D b form the backward GAN. The two generators are implemented by variational auto-encoder LSTM, which encode the frame feature to the latent variable z and then decode it to corresponding features. The two discriminators are LSTM networks that learn to distinguish generated frame features and true features. We extensively use the LSTM architecture here for comprehensively modeling the temporal information across video frames. Moreover, by adopting the joint structure of VAE and GAN, the video similarity can be more reliably measured by generating better high-level representations <ref type="bibr" target="#b10">(Larsen et al. 2015)</ref>. The cycle structure (forward GAN and backward GAN) convert from original to summary video and back again, in which information loss is minimized.</p><p>Given a video O of k frames, the first step is to extract its deep features o = {o t |t = 1, . . . , k} via a deep CNN model. Given these features o, the selector predicts a sequence of importance scores x = {x t : x t ∈ [0, 1] | t = 1, . . . , k} indicating the importance level of corresponding frames. During training, the frame feature of summary video s = {s t = x t o t |t = 1, . . . , k}. But for testing, Cycle-SUM outputs discretized importance scores x = {x t : x t ∈ {0, 1}}; then frames with importance scores being 1 are selected.</p><p>With s and o, the supervisor performs cycle-consistent learning (see <ref type="figure" target="#fig_1">Fig. 2</ref>) to evaluate the quality of summary video s w.r.t. both completeness and compactness. Specifically, within the selector, the forward generator G f takes the current summary video s as input and outputs a sequence of reconstructed features for the original video, namely G f (s) = { o t |t = 1, ..., k}. The paired discriminator D f then estimates the distribution divergence between original video and summary video in the learned feature space. The backward generator G b and discriminator D b have a symmetrical network architecture and training procedure to the forward ones. In particular, the generator G b takes the original video feature o as input, and outputs G b (o) = {ŝ t : t = 1, ..., k} to reconstruct the summary video. The discriminator D b then tries to distinguish between s and s. The forward cycle-consistency processing</p><formula xml:id="formula_10">s → G f (s) → G b (G f (s)) ≈ s and the backward cycle- consistency o → G b (o) → G f (G b (o))</formula><p>≈ o are implemented to enhance the information consistency between o and s. This cycle-consistent processing guarantees the original video to be reconstructed from the summary video and vice versa, meaning the summary video can tell the same story as the original.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Loss</head><p>We design the following loss functions to train our Cycle-SUM model. The sparsity loss L sparsity is used to control the summary length for the selector; the prior loss L prior and the reconstruction loss L recon are used to train the two VAE-based generators; adversarial losses L GAN are derived from the forward GAN and backward GAN; and L cycle is the cycle-consistent loss.</p><p>Sparsity loss L sparsity This loss is designed to penalize the number of selected frames forming the summary video over the original video. A high sparsity ratio gives a shorter summary video. Formally, it is defined as</p><formula xml:id="formula_11">L sparsity = 1 k k t=1 x t − σ 2</formula><p>where k is the total number of video frames and σ is a pre-defined percentage of frames to select in video summarization. The ground truth of σ in standard benchmark is 15% <ref type="bibr" target="#b6">(Gygli et al. 2014;</ref><ref type="bibr" target="#b13">Song et al. 2015</ref>), but we empirically set σ as 30% for the selector in training (Mahasseni, Lam, and Todorovic 2017).</p><p>Generative loss L gen We adopt VAE as the generator for reconstruction, thus L gen contains the prior loss L prior and the reconstruction loss L recon . For the forward VAE (forward generator G f ), the encoder encodes input features s into the latent variable z. Assume p z (z) is the prior distribution of latent variables, and the typical reparameterization trick is to set p z (z) as Gaussian Normal distribution (Kingma and Welling 2013). Define q ψ (z|s) as the posterior distribution and p θ (s|z) as conditional generative distribution for s, where ψ is the parameter of the encoder and θ is that of the decoder. The objective of the forward generator is</p><formula xml:id="formula_12">L gen,f = D KL (q ψ (z|s) p z (z)) − E [log(p θ (s|z))] ,</formula><p>where the first term is KL divergence for the prior loss:</p><formula xml:id="formula_13">L prior = D KL (q ψ (z|s) p z (z)).</formula><p>The second term is an element-wise metric for measuring the similarity between samples, so we use it as the reconstruction loss L recon . The typical reconstruction loss for auto encoder networks is the Euclidean distance between input and reconstructed output: x −x 2 . According to <ref type="bibr" target="#b10">(Larsen et al. 2015)</ref>, element-wise metrics cannot model properties of human visual perception, thus they propose to jointly train the VAE (the generator) and the GAN discriminator, where hidden representation is used in the discriminator to measure sample similarity. Our proposed Cycle-SUM also adopts the same structure to measure the video distance and achieves a feature-wise reconstruction. Specifically, if x andx are the input and output of the VAE (the generator), the output of the last hidden layer of the discriminator are φ(x) and φ(x). Then, consider p(φ(x)|e) ∝ exp(− φ(x) − φ(x) ).</p><p>The expectation E can be computed by empirical average. The reconstruction loss can be re-written as</p><formula xml:id="formula_14">L recon = E[− log(p θ (s|e))] ∝ 1 k φ(o) − φ(ô) 2 .</formula><p>The backward generator G B has the same L prior and L recon as the forward generator. The only difference is that their input and output are reversed.</p><p>Adversarial loss L GAN The learning objective of the evaluator is to maximize the mutual information between the original and summary video. According to Eqn. (4), the adversarial losses for the forward GAN (G f and D f ) and the backward GAN (G b and D b ) are</p><formula xml:id="formula_15">L GAN,f = E[log D f (o)] + E[1 − log D f (G f (s))], L GAN,b = E[log D b (s)] + E[1 − log D b (G b (o))].</formula><p>To avoid mode collapse and improve stability of optimization, we use the loss suggested in Wasserstein GAN (Arjovsky, Chintala, and Bottou 2017):</p><formula xml:id="formula_16">L GAN,f = D f (o) − D f (G f (s)), L GAN,b = D b (s) − D b (G b (o)).</formula><p>Cycle-Consistent Loss L cycle Since we expect summary frame features to contain all the information of original frame features, the original video should be fully reconstructed from them. Thus, when we convert from original to summary video and then back again, we should obtain a video similar to the original one. In this way we can safely guarantee the completeness of the summary video. This processing is more advantageous than the one-direction reconstruction in existing image reconstruction works <ref type="bibr" target="#b18">(Zhu et al. 2017;</ref><ref type="bibr" target="#b17">Yi et al. 2017)</ref>. Based on such an intuition, we introduce the below cycle-consistent loss. The procedure for forward cycle is</p><formula xml:id="formula_17">s → G f (s) → G b (G f (s)) ≈ s. The L cycle, f is correspondingly defined as L cycle, f = 1 k G b (G f (s)) − s . For the backward cycle, the procedure is o → G b (o) → G f (G b (o)) ≈ o. So the L cycle, b is L cycle, b = 1 k G f (G b (o)) − o .</formula><p>We adopt L 1 distance for L cycle , since the L 2 often leads to blurriness <ref type="bibr" target="#b10">(Larsen et al. 2015;</ref>.</p><p>Overall Loss L The overall loss function is the overall objective for training the Cycle-SUM model:</p><formula xml:id="formula_18">L =L sparsity + λ 1 (L GAN,f + L GAN,b ) + λ 2 (L gen,f + L gen,b ) + λ 3 (L cycle,f + L cycle,b ),</formula><p>where λ 1 , λ 2 and λ 3 are hyper parameters to balance adversarial processing, generative processing and cycle-consistent processing.</p><p>Training Cycle-SUM Given the above training losses and final objective function, we adopt the Stochastic Gradient Variational Bayes estimation (Kingma and Welling 2014) to update the parameters in training. The selector and the generators in the Cycle-SUM are jointly trained to maximally confuse the discriminators. To stabilize the training process, we initialize all parameters with Xavier <ref type="bibr" target="#b4">(Glorot and Bengio 2010)</ref> and clip all parameters <ref type="bibr" target="#b0">(Arjovsky, Chintala, and Bottou 2017)</ref>. The clipping parameter c in this training falls in [−0.5, 0.5]. The typical value for the generator iteration per discriminator iteration n is 2 ∼ 5, which means the generator will iterate n times per discriminative iteration. Algorithm 1 summarizes all steps for training Cycle-SUM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment Experiment Setup</head><p>Datasets and Protocol We evaluate Cycle-SUM on two benchmark datasets: SumMe <ref type="bibr" target="#b6">(Gygli et al. 2014</ref>) and <ref type="bibr">TV-Sum (Song et al. 2015)</ref>. The SumMe contains 25 videos ranging from 1 to 7 minutes, with frame-level binary importance scores. The TVSum contains 50 videos downloaded from YouTube, with shot-level importance scores for each video taking constant from 1 to 5. Following the convention <ref type="bibr" target="#b6">(Gygli, Grabner, and Van Gool 2015;</ref><ref type="bibr" target="#b12">Mahasseni, Lam, and Todorovic 2017)</ref>, we adopt the F-measure as the performance metric. Given ground truth and produced summary video, we calculate the harmonic mean F-Scores according to precision and recall for evaluation.  </p><formula xml:id="formula_19">− L overall ⇒ Θ S , Θ G f , Θ G b 12: clip( Θ S , Θ G f , Θ G b , −c,</formula><formula xml:id="formula_20">+ L dis,f ⇒ Θ D f %maximization update 15: clip(Θ D f , −c, c) 16: + L dis,b ⇒ {Θ D b } %maximization update 17: clip(Θ D b , −c, c) 18: until convergence</formula><p>For the TVSum dataset, shot-level ground truths are provided while the outputs of Cycle-SUM in testing are framelevel scores. Thus we follow the method in  to convert frame-level evaluation to shot-level evaluation.</p><p>Implementation Details For fairness, the frame features used for training our model are the same with <ref type="bibr" target="#b18">(Zhang et al. 2016;</ref><ref type="bibr" target="#b12">Mahasseni, Lam, and Todorovic 2017)</ref>. We extract 1024-d frame features from the output of pool5 layer of the GoogLeNet network <ref type="bibr" target="#b15">(Szegedy et al. 2015)</ref> which is pre-trained on ImageNet <ref type="bibr" target="#b3">(Deng et al. 2009</ref>).</p><p>Each of the two generators in our Cycle-SUM is a VAE-based LSTM network consisting of an encoder and a decoder, which has two-layers with 300 hidden units per layer. The decoder LSTM which reconstructs the sequence reversely is easier to train <ref type="bibr" target="#b14">(Srivastava, Mansimov, and Salakhudinov 2015)</ref>, so the decoders in Cycle-SUM also reconstruct the frame features in a reverse order. The discriminators are LSTM networks followed by a fullyconnected network to produce probability (true or false) for the input. Following the architecture of WGAN <ref type="bibr" target="#b0">(Arjovsky, Chintala, and Bottou 2017)</ref>, we remove the Sigmoid function in the last layer of the discriminator to make the model easier to train. The selector is a Bi-LSTM network consisting of three layers, each with 300 hidden units.</p><p>The two VAE generators are initialized by pre-training on frame features of the original video. Similar to <ref type="bibr" target="#b12">(Mahasseni, Lam, and Todorovic 2017)</ref>, such an initialization strategy can also accelerate training and improve overall accuracy.</p><p>All experiments are conducted for five times on five random splits and we report the average performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantitative Results</head><p>We compare our Cycle-SUM model with several unsupervised state-of-the-arts in Tab. 1. One can see that the Cycle-SUM model outperforms all of them by a margin up to 2.8%. In particular, Cycle-SUM outperforms SUM-GAN (Mahasseni, Lam, and Todorovic 2017) across the two datasets, clearly demonstrating effectiveness of our proposed cycleconsistent loss. On TVSum, the performance improvement is over 5.9%. These results well prove the superior performance of Cycle-SUM for video summarization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Analysis of Cycle-SUM</head><p>We further conduct ablation analysis to study the effects of different components of the Cycle-SUM model, including the generative adversarial learning and cycle-consistent learning. In particular, we consider following ablation variants of Cycle-SUM. L cycle,f is not included in overall objective when training, and backward one is kept. The forward and backward adversarial learning are still kept.</p><p>Comparing F-scores of Cycle-SUM and Cycle-SUM-C in Tab. 2, we can see the adversarial learning improves the performance significantly, proving the positive effects of deploying GAN in unsupervised video summarization. Compared with one GAN variant Cycle-SUM-1G, the results of Cycle-SUM-2G have 2% gain on average. Both comparisons verify adversarial learning helps improve video summarization.</p><p>By comparing Cycle-SUM-2G and Cycle-SUM, we can see averagely F-scores rise by 2.2% on SumMe and 4.0% on TVSum. This proves that cycle-consistent reconstructions can ensure the summary video contain full information of the original video.</p><p>The results of Cycle-SUM-Gf are slightly better than Cycle-SUM-Gb. However, both variants bring performance gain over Cycle-SUM-2G, which also proves the forward and backward cycle-consistent processing can promote the ability to select a fully summary video from the original.</p><p>To sum up, the adversarial learning ensures the summary and original video to keep a suitable distance in the deep feature space, and the cycle-consistent learning ensures selected frames to retain full information of the original video. <ref type="figure">Fig. 3</ref> shows summarization examples from a sample video in TVSum. We compare the selected frames of Cycle-SUM with other two recent state-of-the-arts, vsLSTM <ref type="bibr" target="#b18">(Zhang et al. 2016</ref>) and SUM-GAN (Mahasseni, Lam, and Todorovic 2017) by using a successful example for all three models. As shown in <ref type="figure">Fig. 3</ref>, Cycle-SUM selects shorter but more key shots than the other two models. Compared with results of vsLSTM and SUM-GAN, some topic-specific and informative details, e.g. frames showing the doctor pushing medicinal liquid into dog's ear, are correctly selected by Cycle-SUM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we theoretically reveal how to effectively maximize mutual information by cycle-consistent adversarial learning. Based on the theoretical analysis, we propose a new Cycle-SUM model for frame-level video summarization. Experimental results show that the cycle-consistent mechanism can significantly improve video summarization, and our Cycle-SUM can produce more precise summary video than strong baselines, which well validates effectiveness of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Overview of the Cycle-SUM model. The summary video</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Demonstration of Cycle-SUM architecture. Red parts denote the components of our Cycle-SUM model while blue one denotes data-processing. Cycle-SUM has two parts: the selector for selecting frames and the cycle-consistent evaluator to "supervise" the selection. The feature of the frame in original video o is extracted from video O by a deep CNN. The selector takes o as input and outputs the importance scores x. During training, the generator G f takes s as input and reconstructs a sequence of features, G f (s). The discriminator D f is trained to distinguish o and o. The generator G b takes o as input and outputs G b (o); the discriminator D b also tries to distinguish between s and s. To achieve cycle consistency, the forward cycle s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 s</head><label>1</label><figDesc>Training Cycle-SUM model Require: Frame features of the training video: o Ensure: Learned parameters for the selector Θ S , the two generators and discriminators:Θ G f , Θ G b , Θ D f , Θ D b 1: Initialize all parameters by using Xavier approach 2→ selector(o) % selected frame-level features 6:ô → G f (s) % reconstruction by generator A 7:ŝ → G b (o) % reconstruction by generator B 8:s cycle → G b (ô) % forward cycle 9: o cycle → G f (ŝ)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>•</head><label></label><figDesc>Cycle-SUM-C. This variant is proposed to verify the effects of adversarial learning. It drops the adversarial loss L GAN,f and L GAN,b and keeps all other losses, especially cycle-consistent loss L cycle in the overall loss. • Cycle-SUM-2G. The cycle-consistent loss L cycle,f and L cycle,b are removed in overall loss. The forward GAN and backward GAN are still kept. We compare the results of Cycle-SUM-2G with Cycle-SUM to analyze the functions of cycle-consistent reconstruction. • Cycle-SUM-1G. The cycle-consistent loss L cycle,f and L cycle,b are not used when training this variant. Meanwhile, we remove the generator G b and discriminator D b , Figure 3: Comparison of selected frames w.r.t. importance score by Cycle-SUM and other state-of-arts (vsLSTM and SUM-GAN). Dark blue bars show ground-truth frame-level annotation; Red bars are selected subset shots of all frames. The example video (# 15) is from TVSum. so there is no backward reconstruction: o → G b (o) →ŝ. This model is similar to SUM-GAN (Mahasseni, Lam, and Todorovic 2017). It only has forward GAN, and Cycle-SUM-2G contains the two GANs during training. We are also interested in comparing Cycle-SUM-2G and Cycle-SUM-1G. • Cycle-SUM-Gf. The backward cycle-consistent loss L cycle,b is not included in overall objective when training, while forward cycle-consistent loss is still kept. The forward and backward adversarial learning are still kept. • Cycle-SUM-Gb. The forward cycle-consistent loss</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison on F-scores of Cycle-SUM with other unsupervised learning approaches on SumMe and TVSum.</figDesc><table><row><cell>SumMe TVSum</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison (F-scores) of the vanilla Cycle-SUM model and its ablation variants on SumMe and TVSum.</figDesc><table><row><cell></cell><cell cols="2">SumMe TVSum</cell></row><row><cell>Cycle-SUM-C</cell><cell>34.8</cell><cell>49.5</cell></row><row><cell>Cycle-SUM-1G</cell><cell>38.2</cell><cell>51.4</cell></row><row><cell>Cycle-SUM-2G</cell><cell>39.7</cell><cell>53.6</cell></row><row><cell>Cycle-SUM-Gf</cell><cell>40.3</cell><cell>55.2</cell></row><row><cell>Cycle-SUM-Gb</cell><cell>39.9</cell><cell>55.0</cell></row><row><cell>Cycle-SUM</cell><cell>41.9</cell><cell>57.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was spported in part to Jiashi Feng by NUS IDS R-263-000-C67-646, ECRA R-263-000-C87-133 and MOE Tier-II R-263-000-D17-112, in part to Ping Li by NSFC under Grant 61872122, 61502131, and in part by the Zhejiang Provincial Natural Science Foundation of China under Grant LY18F020015.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chintala</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bottou ; Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards scalable summarization of consumer videos via sparse dictionary selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="66" to="75" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vsumm: A mechanism designed to produce static video summaries and a novel evaluation method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Avila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="56" to="68" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Efficient visual attention based framework for extracting key frames from videos. Signal Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="34" to="44" />
		</imprint>
	</monogr>
	<note>Imagenet: A large-scale hierarchical image database</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2069" to="2077" />
		</imprint>
	</monogr>
	<note>AISTATS</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Video summarization by learning submodular mixtures of objectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Gygli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3090" to="3098" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale video summarization using web-image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Khosla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2698" to="2705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Joint summarization of large-scale collections of web images and videos for storyline reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sigal</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P X</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">Auto-encoding variational bayes. arXiv preprint</note>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Video key frame extraction through dynamic delaunay clustering with a structural constraint</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Panda</forename><surname>Kuanar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Kuanar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.09300</idno>
	</analytic>
	<monogr>
		<title level="m">Autoencoding beyond pixels using a learned similarity metric</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1212" to="1227" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-video summarization based on video-mmr</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Merialdo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Image Analysis for Multimedia Interactive Services</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unsupervised video summarization with adversarial lstm networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-F</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lam</forename><surname>Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Todorovic ; Mahasseni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1039" to="1048" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tvsum: Summarizing web videos using titles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Potapov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5179" to="5187" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mansimov</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Event driven web video summarization by tag localization and key-shot identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="975" to="985" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Dualgan: Unsupervised dual learning for image-toimage translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Yi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="766" to="782" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Video summarization with long shortterm memory</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
