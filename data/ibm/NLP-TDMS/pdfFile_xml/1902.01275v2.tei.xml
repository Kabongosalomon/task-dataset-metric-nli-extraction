<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Augmented Autoencoders: Implicit 3D Orientation Learning for 6D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltan-Csaba</forename><surname>Marton</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><forename type="middle">Maximilian</forename><surname>Durner</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">·</forename><forename type="middle">Rudolph</forename><surname>Triebel</surname></persName>
						</author>
						<title level="a" type="main">Augmented Autoencoders: Implicit 3D Orientation Learning for 6D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Received: date / Accepted: date</note>
					<note>Noname manuscript No. (will be inserted by the editor)</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>6D Object Detection · Pose Estimation · Domain Randomization · Autoencoder · Synthetic Data · Symmetries</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a real-time RGB-based pipeline for object detection and 6D pose estimation. Our novel 3D orientation estimation is based on a variant of the Denoising Autoencoder that is trained on simulated views of a 3D model using Domain Randomization.</p><p>This so-called Augmented Autoencoder has several advantages over existing methods: It does not require real, pose-annotated training data, generalizes to various test sensors and inherently handles object and view symmetries. Instead of learning an explicit mapping from input images to object poses, it provides an implicit representation of object orientations defined by samples in a latent space. Our pipeline achieves stateof-the-art performance on the T-LESS dataset both in the RGB and RGB-D domain. We also evaluate on the LineMOD dataset where we can compete with other synthetically trained approaches.</p><p>We further increase performance by correcting 3D orientation estimates to account for perspective errors when the object deviates from the image center and show extended results. Our code is available here 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>robotic manipulation and augmented reality is a reliable and fast 6D object detection module. Although, there are very encouraging recent results from <ref type="bibr" target="#b49">Xiang et al. (2017)</ref>; <ref type="bibr" target="#b22">Kehl et al. (2017)</ref>; <ref type="bibr" target="#b17">Hodaň et al. (2017)</ref>; <ref type="bibr" target="#b47">Wohlhart and Lepetit (2015)</ref>; <ref type="bibr" target="#b45">Vidal et al. (2018)</ref>; <ref type="bibr" target="#b13">Hinterstoisser et al. (2016)</ref>; <ref type="bibr" target="#b43">Tremblay et al. (2018)</ref>, a general, easily applicable, robust and fast solution is not available, yet. The reasons for this are manifold. First and foremost, current solutions are often not robust enough against typical challenges such as object occlusions, different kinds of background clutter, and dynamic changes of the environment. Second, existing methods often require certain object properties such as enough textural surface structure or an asymmetric shape to avoid confusions. And finally, current systems are not efficient in terms of run-time and in the amount and type of annotated training data they require.</p><p>Therefore, we propose a novel approach that directly addresses these issues. Concretely, our method operates on single RGB images, which significantly increases the usability as no depth information is required. We note though that depth maps may be incorporated optionally to refine the estimation. As a first step, we build upon state-of-the-art 2D Object Detectors of <ref type="bibr" target="#b26">(Liu et al. (2016)</ref>; <ref type="bibr" target="#b25">Lin et al. (2018)</ref>) which provide object bounding boxes and identifiers. On the resulting scene crops, we employ our novel 3D orientation estimation algorithm, which is based on a previously trained deep network architecture. While deep networks are also used in existing approaches, our approach differs in that we do not explicitly learn from 3D pose annotations during training. Instead, we implicitly learn representations from rendered 3D model views. This is accomplished by training a generalized version of the Denoising Autoencoder from <ref type="bibr" target="#b46">Vincent et al. (2010)</ref>, that we call 'Augmented Autoencoder (AAE)', using a novel Do-  <ref type="figure">Fig. 1</ref>: Our full 6D Object Detection pipeline: after detecting an object (2D Object Detector), the object is quadratically cropped and forwarded into the proposed Augmented Autoencoder. In the next step, the bounding box scale ratio at the estimated 3D orientationR obj2cam is used to compute the 3D translationt obj2cam . The resulting euclidean transformationĤ obj2cam ∈ R 4x4 already shows promising results as presented in <ref type="bibr" target="#b40">Sundermeyer et al. (2018)</ref>, however it still lacks of accuracy given a translation in the image plane towards the borders. Therefore, the pipeline is extended by the Perspective Correction block which addresses this problem and results in more accurate 6D pose estimatesĤ obj2cam for objects which are not located in the image center. Additionally, given depth data, the result can be further refined (Ĥ main Randomization strategy. Our approach has several advantages: First, since the training is independent from concrete representations of object orientations within SO(3) (e.g. quaternions), we can handle ambiguous poses caused by symmetric views because we avoid one-to-many mappings from images to orientations. Second, we learn representations that specifically encode 3D orientations while achieving robustness against occlusion, cluttered backgrounds and generalizing to different environments and test sensors. Finally, the AAE does not require any real pose-annotated training data. Instead, it is trained to encode 3D model views in a self-supervised way, overcoming the need of a large pose-annotated dataset. A schematic overview of the approach based on <ref type="bibr" target="#b40">Sundermeyer et al. (2018)</ref> is shown in <ref type="figure">Fig 1.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Depth-based methods (e.g. using Point Pair Features (PPF) from <ref type="bibr" target="#b45">Vidal et al. (2018);</ref><ref type="bibr" target="#b13">Hinterstoisser et al. (2016)</ref>) have shown robust pose estimation performance on multiple datasets, winning the SIXD challenge <ref type="bibr" target="#b15">(Hodan, 2017;</ref><ref type="bibr" target="#b18">Hodan et al., 2018)</ref>. However, they usually rely on the computationally expensive evaluation of many pose hypotheses and do not take into account any high level features. Furthermore, existing depth sensors are often more sensitive to sunlight or specular object surfaces than RGB cameras.</p><p>Convolutional Neural Networks (CNNs) have revolutionized 2D object detection from RGB images <ref type="bibr" target="#b35">(Ren et al., 2015;</ref><ref type="bibr" target="#b26">Liu et al., 2016;</ref><ref type="bibr" target="#b25">Lin et al., 2018)</ref>. But, in comparison to 2D bounding box annotation, the effort of labeling real images with full 6D object poses is magnitudes higher, requires expert knowledge and a complex setup <ref type="bibr" target="#b17">(Hodaň et al., 2017)</ref>.</p><p>Nevertheless, the majority of learning-based pose estimation methods, namely <ref type="bibr" target="#b41">Tekin et al. (2017)</ref>; <ref type="bibr" target="#b47">Wohlhart and Lepetit (2015)</ref>; <ref type="bibr" target="#b3">Brachmann et al. (2016)</ref>; <ref type="bibr" target="#b34">Rad and Lepetit (2017)</ref>; <ref type="bibr" target="#b49">Xiang et al. (2017)</ref>, use real labeled images that you only obtain within pose-annotated datasets.</p><p>In consequence, <ref type="bibr" target="#b22">Kehl et al. (2017)</ref>; <ref type="bibr" target="#b47">Wohlhart and Lepetit (2015)</ref>; <ref type="bibr" target="#b43">Tremblay et al. (2018)</ref>; <ref type="bibr" target="#b50">Zakharov et al. (2019)</ref> have proposed to train on synthetic images rendered from a 3D model, yielding a great data source with pose labels free of charge. However, naive training on synthetic data does not typically generalize to real test images. Therefore, a main challenge is to bridge the domain gap that separates simulated views from real camera recordings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Simulation to Reality Transfer</head><p>There exist three major strategies to generalize from synthetic to real data:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1">Photo-Realistic Rendering</head><p>The works of <ref type="bibr" target="#b32">Movshovitz-Attias et al. (2016)</ref>; <ref type="bibr" target="#b39">Su et al. (2015)</ref>; <ref type="bibr" target="#b30">Mitash et al. (2017)</ref>; <ref type="bibr" target="#b36">Richter et al. (2016)</ref> have shown that photo-realistic renderings of object views and backgrounds can in some cases benefit the generalization performance for tasks like object detection and viewpoint estimation. It is especially suitable in simple environments and performs well if jointly trained with a relatively small amount of real annotated images. However, photo-realistic modeling is often imperfect and requires much effort. Recently, <ref type="bibr" target="#b19">Hodan et al. (2019)</ref> have shown promising results for 2D Object Detection trained on physically-based renderings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Domain Adaptation</head><p>Domain Adaptation (DA) <ref type="bibr" target="#b5">(Csurka, 2017)</ref> refers to leveraging training data from a source domain to a target domain of which a small portion of labeled data (supervised DA) or unlabeled data (unsupervised DA) is available. Generative Adversarial Networks (GANs) have been deployed for unsupervised DA by generating realistic from synthetic images to train classifiers <ref type="bibr" target="#b38">(Shrivastava et al., 2017)</ref>, 3D pose estimators <ref type="bibr" target="#b2">(Bousmalis et al., 2017b)</ref> and grasping algorithms <ref type="bibr" target="#b1">(Bousmalis et al., 2017a)</ref>. While constituting a promising approach, GANs often yield fragile training results. Supervised DA can lower the need for real annotated data, but does not abstain from it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Domain Randomization</head><p>Domain Randomization (DR) builds upon the hypothesis that by training a model on rendered views in a variety of semi-realistic settings (augmented with random lighting conditions, backgrounds, saturation, etc.), it will also generalize to real images. <ref type="bibr" target="#b42">Tobin et al. (2017)</ref> demonstrated the potential of the DR paradigm for 3D shape detection using CNNs. <ref type="bibr" target="#b14">Hinterstoisser et al. (2017)</ref> showed that by training only the head network of FasterRCNN of <ref type="bibr" target="#b35">Ren et al. (2015)</ref> with randomized synthetic views of a textured 3D model, it also generalizes well to real images. It must be noted, that their rendering is almost photo-realistic as the textured 3D models have very high quality. <ref type="bibr" target="#b22">Kehl et al. (2017)</ref> pioneered an end-to-end CNN, called 'SSD6D', for 6D object detection that uses a moderate DR strategy to utilize synthetic training data. The authors render views of textured 3D object reconstructions at random poses on top of MS COCO background images <ref type="bibr" target="#b24">(Lin et al., 2014)</ref> while varying brightness and contrast. This lets the network generalize to real images and enables 6D detection at 10Hz. Like us, for accurate distance estimation they rely on Iterative Closest Point (ICP) post-processing using depth data. In contrast, we do not treat 3D orientation estimation as a classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training Pose Estimation with SO(3) targets</head><p>We describe the difficulties of training with fixed SO(3) parameterizations which will motivate the learning of view-based representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Regression</head><p>Since rotations live in a continuous space, it seems natural to directly regress a fixed SO(3) parameterizations like quaternions. However, representational constraints and pose ambiguities can introduce convergence issues as investigated by <ref type="bibr">Saxena et al. (2009)</ref>. In practice, direct regression approaches for full 3D object orientation estimation have not been very successful <ref type="bibr" target="#b27">(Mahendran et al., 2017)</ref>. Instead <ref type="bibr" target="#b43">Tremblay et al. (2018)</ref>; <ref type="bibr" target="#b41">Tekin et al. (2017)</ref>; <ref type="bibr" target="#b34">Rad and Lepetit (2017)</ref> regress local 2D-3D correspondences and then apply a Perspective-n-Point (PnP) algorithm to obtain the 6D pose. However, these approaches can also not deal with pose ambiguities without additional measures (see Sec. 2.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Classification</head><p>Classification of 3D object orientations requires a discretization of SO(3). Even rather coarse intervals of ∼ 5 o lead to over 50.000 possible classes. Since each class appears only sparsely in the training data, this hinders convergence. In SSD6D <ref type="bibr" target="#b22">(Kehl et al., 2017)</ref> the 3D orientation is learned by separately classifying a discretized viewpoint and in-plane rotation, thus reducing the complexity to O(n 2 ). However, for non-canonical views, e.g. if an object is seen from above, a change of viewpoint can be nearly equivalent to a change of inplane rotation which yields ambiguous class combinations. In general, the relation between different orientations is ignored when performing one-hot classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3">Symmetries</head><p>Symmetries are a severe issue when relying on fixed representations of 3D orientations since they cause pose ambiguities ( <ref type="figure" target="#fig_1">Fig. 2</ref>). If not manually addressed, identical training images can have different orientation labels assigned which can significantly disturb the learning process. In order to cope with ambiguous objects, most approaches in literature are manually adapted <ref type="bibr" target="#b47">(Wohlhart and Lepetit, 2015;</ref><ref type="bibr" target="#b11">Hinterstoisser et al., 2012a;</ref><ref type="bibr" target="#b22">Kehl et al., 2017;</ref><ref type="bibr" target="#b34">Rad and Lepetit, 2017)</ref>. The strategies reach from ignoring one axis of rotation <ref type="bibr" target="#b47">(Wohlhart and Lepetit, 2015;</ref><ref type="bibr" target="#b11">Hinterstoisser et al., 2012a)</ref> over adapting the discretization according to the object <ref type="bibr" target="#b22">(Kehl et al., 2017)</ref> to the training of an extra CNN to predict symmetries <ref type="bibr" target="#b34">(Rad and Lepetit, 2017)</ref>. These depict tedious, manual ways to filter out object symmetries ( <ref type="figure" target="#fig_1">Fig. 2a</ref>) in advance, but treating ambiguities due to self-occlusions ( <ref type="figure" target="#fig_1">Fig. 2b</ref>) and occlusions ( <ref type="figure" target="#fig_1">Fig. 2c</ref>) are harder to address.</p><p>Symmetries do not only affect regression and classification methods, but any learning-based algorithm that discriminates object views solely by fixed SO(3) representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Learning Representations of 3D orientations</head><p>We can also learn indirect pose representations that relate object views in a low-dimensional space. The descriptor learning can either be self-supervised by the object views themselves or still rely on fixed SO(3) representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Descriptor Learning</head><p>Wohlhart and Lepetit (2015) introduced a CNN-based descriptor learning approach using a triplet loss that minimizes/maximizes the Euclidean distance between similar/dissimilar object orientations. In addition, the distance between different objects is maximized. Although mixing in synthetic data, the training also relies on pose-annotated sensor data. The approach is not immune against symmetries since the descriptor is built using explicit 3D orientations. Thus, the loss can be dominated by symmetric object views that appear the same but have opposite orientations which can produce incorrect average pose predictions. <ref type="bibr" target="#b0">Balntas et al. (2017)</ref> extended this work by enforcing proportionality between descriptor and pose distances. They acknowledge the problem of object symmetries by weighting the pose distance loss with the depth difference of the object at the considered poses. This heuristic increases the accuracy on symmetric objects with respect to <ref type="bibr" target="#b47">Wohlhart and Lepetit (2015)</ref>.</p><p>Our work is also based on learning descriptors, but in contrast we train our Augmented Autoencoders (AAEs) such that the learning process itself is independent of any fixed SO(3) representation. The loss is solely based on the appearance of the reconstructed object views and thus symmetrical ambiguities are inherently regarded. Thus, unlike <ref type="bibr" target="#b0">Balntas et al. (2017)</ref>; <ref type="bibr" target="#b47">Wohlhart and Lepetit (2015)</ref> we abstain from the use of real labeled data during training and instead train completely self-supervised. This means that assigning 3D orientations to the descriptors only happens after the training. <ref type="bibr" target="#b21">Kehl et al. (2016)</ref> train an Autoencoder architecture on random RGB-D scene patches from the LineMOD dataset <ref type="bibr" target="#b10">Hinterstoisser et al. (2011)</ref>. At test time, descriptors from scene and object patches are compared to find the 6D pose. Since the approach requires the evaluation of a lot of patches, it takes about 670ms per prediction. Furthermore, using local patches means to ignore holistic relations between object features which is crucial if few texture exists. Instead we train on holistic object views and explicitly learn domain invariance.</p><formula xml:id="formula_0">(a) X s=1.0,txy =0.0,r∈[0,2π] (b) X s=0.6,txy =0.0,r∈[0,2π] (c) X s=1.0,txy ∼U (−1,1),r∈[0,2π] (d) X s∼U (0.5,1),txy ∼U (−1,1),r∈[0,2π]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In the following, we mainly focus on the novel 3D orientation estimation technique based on the AAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Autoencoders</head><p>The original AE, introduced by <ref type="bibr" target="#b37">Rumelhart et al. (1985)</ref>, is a dimensionality reduction technique for high dimensional data such as images, audio or depth. It consists of an Encoder Φ and a Decoder Ψ , both arbitrary learnable function approximators which are usually neural networks. The training objective is to reconstruct the input x ∈ R D after passing through a low-dimensional bottleneck, referred to as the latent representation z ∈ R n with n &lt;&lt; D :</p><formula xml:id="formula_1">x = (Ψ • Φ)(x) = Ψ (z)<label>(1)</label></formula><p>The per-sample loss is simply a sum over the pixel-wise L2 distance</p><formula xml:id="formula_2">2 = i∈D x i −x i 2<label>(2)</label></formula><p>The resulting latent space can, for example, be used for unsupervised clustering. Denoising Autoencoders introduced by Vincent et al. (2010) have a modified training procedure. Here, artificial random noise is applied to the input images x ∈ R D while the reconstruction target stays clean. The trained model can be used to reconstruct denoised test images. But how is the latent representation affected?</p><p>Hypothesis 1: The Denoising AE produces latent representations which are invariant to noise because it facilitates the reconstruction of de-noised images.</p><p>We will demonstrate that this training strategy actually enforces invariance not only against noise but </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Augmented Autoencoder</head><p>The motivation behind the AAE is to control what the latent representation encodes and which properties are ignored. We apply random augmentations f augm (.) to the input images x ∈ R D against which the encoding should become invariant. The reconstruction target remains eq. (2) but eq. (1) becomeŝ</p><formula xml:id="formula_3">x = (Ψ • Φ • f augm )(x) = (Ψ • Φ)(x ) = Ψ (z )<label>(3)</label></formula><p>To make evident that Hypothesis 1 holds for geometric transformations, we learn latent representations of binary images depicting a 2D square at different scales, in-plane translations and rotations. Our goal is to encode only the in-plane rotations r ∈ [0, 2π] in a two dimensional latent space z ∈ R 2 independent of scale or translation. <ref type="figure" target="#fig_2">Fig. 3</ref> depicts the results after training a CNN-based AE architecture similar to the model in <ref type="figure" target="#fig_4">Fig. 5</ref>. It can be observed that the AEs trained on reconstructing squares at fixed scale and translation (1) or random scale and translation (2) do not clearly encode rotation alone, but are also sensitive to other latent factors. Instead, the encoding of the AAE (3) becomes invariant to translation and scale such that all squares with coinciding orientation are mapped to the same code. Furthermore, the latent representation is much smoother and the latent dimensions imitate a shifted sine and cosine function with frequency f = 4 2π respectively. The reason is that the square has two perpendicular axes of symmetry, i.e. after rotating π 2 the square appears the same. This property of representing the orientation based on the appearance of an object rather than on a fixed parametrization is valuable to avoid ambiguities due to symmetries when teaching 3D object orientations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning 3D Orientation from Synthetic Object Views</head><p>Our toy problem showed that we can explicitly learn representations of object in-plane rotations using a geometric augmentation technique. Applying the same geometric input augmentations we can encode the whole SO(3) space of views from a 3D object model (CAD or 3D reconstruction) while being robust against inaccurate object detections. However, the encoder would still be unable to relate image crops from real RGB sensors because (1) the 3D model and the real object differ, (2) simulated and real lighting conditions differ, (3) the network can't distinguish the object from background clutter and foreground occlusions. Instead of trying to imitate every detail of specific real sensor recordings in simulation we propose a Domain Randomization (DR) technique within the AAE framework to make the encodings invariant to insignificant environment and sensor variations. The goal is that the trained encoder treats the differences to real camera images as just another irrelevant variation. Therefore, while keeping reconstruction targets clean, we randomly apply additional augmentations to the input training views: (1)    <ref type="bibr" target="#b33">(Phong, 1975)</ref> in OpenGL), (2) inserting random background images from the Pascal VOC dataset <ref type="bibr" target="#b7">(Everingham et al., 2012)</ref>, (3) varying image contrast, brightness, Gaussian blur and color distortions, (4) applying occlusions using random object masks or black squares. <ref type="figure" target="#fig_3">Fig. 4</ref> depicts an exemplary training process for synthetic views of object 5 from T-LESS <ref type="bibr" target="#b17">(Hodaň et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Network Architecture and Training Details</head><p>The convolutional Autoencoder architecture that is used in our experiments is depicted in <ref type="figure" target="#fig_4">Fig. 5</ref>. We use a bootstrapped pixel-wise L2 loss, first introduced by <ref type="bibr" target="#b48">Wu et al. (2016)</ref>. Only the pixels with the largest reconstruction errors contribute to the loss. Thereby, finer details are reconstructed and the training does not converge to local minima like reconstructing black images for all views. In our experiments, we choose a bootstrap factor of k = 4 per image, meaning that 1 4 of all pixels contribute to the loss. Using OpenGL, we render 20000 views of each object uniformly at random 3D orientations and constant distance along the camera axis (700mm). The resulting images are quadratically cropped using the longer side of the bounding box and resized (nearest neighbor) to 128 × 128 × 3 as shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. All geometric and color input augmentations besides the rendering with random lighting are applied online during training at uniform random strength, parameters are found in Tab. 1. We use the Adam <ref type="bibr" target="#b23">(Kingma and Ba, 2014)</ref> optimizer with a learning rate of 2 × 10 −4 , Xavier initialization <ref type="bibr" target="#b8">(Glorot and Bengio, 2010)</ref>, a batch size = 64 and 40000 iterations which takes ∼ 4 hours on a single Nvidia Geforce GTX 1080.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Codebook Creation and Test Procedure</head><p>After training, the AAE is able to extract a 3D object from real scene crops of many different camera sensors <ref type="figure" target="#fig_5">(Fig. 6)</ref>. The clarity and orientation of the decoder reconstruction is an indicator of the encoding quality. To determine 3D object orientations from test scene crops we create a codebook <ref type="figure" target="#fig_6">(Fig. 7 (top)</ref>): At test time, the considered object(s) are first detected in an RGB scene. The image is quadratically cropped using the longer side of the bounding box multiplied with a padding factor of 1.2 and resized to match the encoder input size. The padding accounts for imprecise bounding boxes. After encoding we compute the cosine similarity between the test code z test ∈ R 128 and all codes z i ∈ R 128 from the codebook:</p><formula xml:id="formula_4">cos i = z z z i z z z test z z z i z z z test<label>(4)</label></formula><p>The highest similarities are determined in a k-Nearest-Neighbor (kNN) search and the corresponding rotation matrices {R kN N } from the codebook are returned as estimates of the 3D object orientation. For the quantitative evaluation we use k = 1, however the next neighbors can yield valuable information on ambiguous views and could for example be used in particle filter based tracking. We use cosine similarity because (1) it can be very efficiently computed on a single GPU even for large codebooks. In our experiments we have 2562 equidistant viewpoints × 36 in-plane rotation = 92232 total entries. (2) We observed that, presumably due to the circular nature of rotations, scaling a latent test code  does not change the object orientation of the decoder reconstruction <ref type="figure" target="#fig_7">(Fig. 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Extending to 6D Object Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Training the 2D Object Detector.</head><p>We finetune the 2D Object Detectors using the object views on black background which are provided in the training datasets of LineMOD and T-LESS. In LineMOD we additionally render domain randomized views of the provided 3D models and freeze the backbone like in <ref type="bibr" target="#b14">Hinterstoisser et al. (2017)</ref>. Multiple object views are sequentially copied into an empty scene at random translation, scale and in-plane rotation. Bounding box annotations are adapted accordingly. If an object view is more than 40% occluded, we re-sample it. Then, as for the AAE, the black background is replaced with Pascal VOC images. The randomization schemes and parameters can be found in <ref type="table" target="#tab_2">Table 2</ref>. In T-LESS we train SSD <ref type="bibr" target="#b26">(Liu et al., 2016)</ref> with VGG16 backbone and RetinaNet  with ResNet50 backbone which is slower but more accurate, on LineMOD we only train RetinaNet. For T-LESS we generate 60000 training samples from the provided training dataset and for LineMOD we generate 60000 samples from the training dataset plus 60000 samples from 3D model renderings with randomized lighting conditions (see <ref type="table" target="#tab_2">Table 2</ref>). The RetinaNet achieves 0.73mAP@0.5IoU on T-LESS and 0.62mAP@0.5IoU on LineMOD. On Occluded LineMOD, the detectors trained on the simplistic renderings failed to achieve good detection performance. However, recent work of <ref type="bibr" target="#b19">Hodan et al. (2019)</ref> quantitatively investigated the training of 2D detectors on synthetic data and they reached decent detection performance on Occluded LineMOD by fine-tuning FasterRCNN on photo-realistic synthetic images showing the feasibility of a purely synthetic pipeline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Projective Distance Estimation</head><p>We estimate the full 3D translation t real from camera to object center, similar to <ref type="bibr" target="#b22">Kehl et al. (2017)</ref>. Therefore, we save the 2D bounding box for each synthetic object view in the codebook and compute its diagonal length bb syn,i . At test time, we compute the ratio between the detected bounding box diagonal bb real and the corresponding codebook diagonal bb syn,argmax(cosi) , i.e. at similar orientation. The pinhole camera model yields the distance estimatet real,ẑ t real,z = t syn,z × bb syn,argmax(cosi) bb real × f real f syn <ref type="formula">(5)</ref> with synthetic rendering distance t syn,z and focal lengths f real , f syn of the real sensor and synthetic views. It follows that ∆t =t real,z K −1 real bb real,c − t syn,z K −1 syn bb syn,c (6) t real = t syn + ∆t <ref type="formula">(7)</ref> where ∆t is the estimated vector from the synthetic to the real object center, K real , K syn are the camera matrices, bb real,c , bb syn,c are the bounding box centers in homogeneous coordinates andt real , t syn = (0, 0, t syn,z ) are the translation vectors from camera to object centers. In contrast to <ref type="bibr" target="#b22">Kehl et al. (2017)</ref>, we can predict the 3D translation for different test intrinsics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3">Perspective Correction</head><p>While the codebook is created by encoding centered object views, the test image crops typically do not originate from the image center. Naturally, the appearance of the object view changes when translating the object in the image plane at constant object orientation. This causes a noticeable error in the rotation estimate from the codebook towards the image borders. However, this error can be corrected by determining the object rotation that approximately preserves the appearance of the object when translating it to our estimatet real .</p><formula xml:id="formula_5">α x α y = − arctan(t real,y /t real,z ) arctan(t real,x / t 2 real,z +t 2 real,y ) (8) R obj2cam = R y (α y )R x (α x )R obj2cam<label>(9)</label></formula><p>where α x , α y describe the angles around the camera axes and R y (α y )R x (α x ) the corresponding rotation matrices to correct the initial rotation estimateR obj2cam from object to camera. The perspective corrections give a notable boost in accuracy as reported in <ref type="table" target="#tab_7">Table 7</ref>. If strong perspective distortions are expected at test time, the training images x could also be recorded at random distances as opposed to constant distance. However, in the benchmarks, perspective distortions are minimal and consequently random online image-plane scaling of x is sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.4">ICP Refinement</head><p>Optionally, the estimate is refined on depth data using a point-to-plane ICP approach with adaptive thresholding of correspondences based on <ref type="bibr" target="#b4">Chen and Medioni (1992)</ref>; Zhang (1994) taking an average of ∼ 320ms.</p><p>The refinement is first applied in direction of the vector pointing from camera to the object where most of the RGB-based pose estimation errors stem from and then on the full 6D pose.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.5">Inference Time</head><p>The Single Shot Multibox Detector (SSD) with VGG16 base and 31 classes plus the AAE <ref type="figure" target="#fig_4">(Fig. 5)</ref> with a codebook size of 92232 × 128 yield the average inference times depicted in <ref type="table" target="#tab_3">Table 3</ref>. We conclude that the RGBbased pipeline is real-time capable at ∼42Hz on a Nvidia GTX 1080. This enables augmented reality and robotic applications and leaves room for tracking algorithms. Multiple encoders (15MB) and corresponding codebooks (45MB each) fit into the GPU memory, making multiobject pose estimation feasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluate the AAE and the whole 6D detection pipeline on the T-LESS <ref type="bibr" target="#b17">(Hodaň et al., 2017)</ref> and LineMOD <ref type="bibr" target="#b10">(Hinterstoisser et al., 2011)</ref> datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Test Conditions</head><p>Few RGB-based pose estimation approaches (e.g. <ref type="bibr" target="#b22">Kehl et al. (2017)</ref>; <ref type="bibr" target="#b44">Ulrich et al. (2009))</ref> only rely on 3D model information. Most methods like <ref type="bibr" target="#b47">Wohlhart and Lepetit (2015)</ref>; <ref type="bibr" target="#b0">Balntas et al. (2017);</ref><ref type="bibr" target="#b3">Brachmann et al. (2016)</ref> make use of real pose annotated data and often even train and test on the same scenes (e.g. at slightly different viewpoints, as in the official LineMOD benchmark). It is common practice to ignore in-plane rotations or to only consider object poses that appear in the dataset <ref type="bibr" target="#b34">(Rad and Lepetit, 2017;</ref><ref type="bibr" target="#b47">Wohlhart and Lepetit, 2015)</ref> which also limits applicability. Symmetric object views are often individually treated <ref type="bibr" target="#b34">(Rad and Lepetit, 2017;</ref><ref type="bibr" target="#b0">Balntas et al., 2017)</ref> or ignored <ref type="bibr" target="#b47">(Wohlhart and Lepetit, 2015)</ref>. The SIXD challenge <ref type="bibr" target="#b15">(Hodan, 2017)</ref> is an attempt to make fair comparisons between 6D localization algorithms by prohibiting the use of test scene pixels. We follow these strict evaluation guidelines, but treat the harder problem of 6D detection where it is unknown which of the considered objects are present in the scene. This is especially difficult in the T-LESS dataset since objects are very similar. We train the AAEs on the reconstructed 3D models, except for object 19-23 where we train on the CAD models because the pins are missing in the reconstructed plugs. We noticed, that the geometry of some 3D reconstruction in T-LESS is slightly inaccurate which badly influences the RGB-based distance estimation (Sec. 3.6.2) since the synthetic bounding box diagonals are wrong. Therefore, in a second training run we only train on the 30 CAD models. <ref type="bibr" target="#b16">Hodaň et al. (2016)</ref> introduced the Visible Surface Discrepancy (err vsd ), an ambiguity-invariant pose error function that is determined by the distance between the estimated and ground truth visible object depth surfaces. As in the SIXD challenge, we report the recall of correct 6D object poses at err vsd &lt; 0.3 with tolerance τ = 20mm and &gt; 10% object visibility. Although the Average Distance of Model Points (ADD) metric introduced by <ref type="bibr" target="#b12">Hinterstoisser et al. (2012b)</ref> cannot handle pose ambiguities, we also present it for the LineMOD dataset following the official protocol in <ref type="bibr" target="#b12">Hinterstoisser et al. (2012b)</ref>. For objects with symmetric views (eggbox, glue), <ref type="bibr" target="#b12">Hinterstoisser et al. (2012b)</ref> adapts the metric by calculating the average distance to the closest model point. <ref type="bibr" target="#b28">Manhardt et al. (2018)</ref> has noticed inaccurate intrinsics and sensor registration errors between RGB and D in the LineMOD dataset. Thus, purely synthetic RGB-based approaches, although visually correct, suffer from false pose rejections. The focus of our experiments lies on the T-LESS dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Metrics</head><p>In our ablation studies we also report the AU C vsd , which represents the area under the 'err vsd vs. recall' curve:  <ref type="figure">Fig. 9</ref>: Testing object 5 on all 504 Kinect RGB views of scene 2 in T-LESS</p><formula xml:id="formula_6">AU C vsd = 1 0 recall(err vsd ) derr vsd<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>To assess the AAE alone, in this subsection we only predict the 3D orientation of Object 5 from the T-LESS dataset on Primesense and Kinect RGB scene crops. <ref type="table" target="#tab_5">Table 5</ref> shows the influence of different input augmentations. It can be seen that the effect of different color augmentations is cumulative. For textureless objects, even the inversion of color channels seems to be beneficial since it prevents overfitting to synthetic color information. Furthermore, training with real object recordings provided in T-LESS with random Pascal VOC background and augmentations yields only slightly better performance than the training with synthetic data. <ref type="figure">Fig. 9a</ref> depicts the effect of different latent space sizes on the 3D pose estimation accuracy. Performance starts to saturate at dim = 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussion of 6D Object Detection results</head><p>Our RGB-only 6D Object Detection pipeline consists of 2D detection, 3D orientation estimation, projective distance estimation and perspective error correction.</p><p>Although the results are visually appealing, to reach the performance of state-of-the-art depth-based methods we also need to refine our estimates using a depthbased ICP. <ref type="table" target="#tab_6">Table 6</ref> presents our 6D detection evaluation on all scenes of the T-LESS dataset, which contains a high amount of pose ambiguities. Our pipeline outperforms all 15 reported T-LESS results on the 2018 BOP benchmark from <ref type="bibr" target="#b18">Hodan et al. (2018)</ref> in a fraction of the runtime. <ref type="table" target="#tab_6">Table 6</ref> shows an extract of competing methods. Our RGB-only results can compete with the RGB-D learning-based approaches of <ref type="bibr" target="#b3">Brachmann et al. (2016)</ref> and <ref type="bibr" target="#b21">Kehl et al. (2016)</ref>. Previous state-of-the-art approaches from <ref type="bibr" target="#b45">Vidal et al. (2018)</ref>; <ref type="bibr" target="#b6">Drost et al. (2010)</ref> perform a time consuming refinement search through multiple pose hypotheses while we only perform the ICP on a single pose hypothesis. That being said, the codebook is well suited to generate multiple hypotheses using k &gt; 1 nearest neighbors. The right part of <ref type="table" target="#tab_6">Table 6</ref> shows results with ground truth bounding boxes yielding an upper bound on the pose estimation performance.  The results in <ref type="table" target="#tab_6">Table 6</ref> show that our domain randomization strategy allows to generalize from 3D reconstructions as well as untextured CAD models as long as the considered objects are not significantly textured. Instead of a performance drop we report an increased err vsd &lt; 0.3 recall due to the more accurate geometry of the model which results in correct bounding box diagonals and thus a better projective distance estimation in the RGB-domain.</p><p>In <ref type="table" target="#tab_8">Table 8</ref> we also compare our pipeline against state-of-the-art methods on the LineMOD dataset. Here, our synthetically trained pipeline does not reach the performance of approaches that use real pose annotated training data.</p><p>There are multiple issues: (1) As described in Sec 4.1 the real training and test set are strongly correlated and approaches using the real training set can over-fit to it; (2) the models provided in LineMOD are quite bad which affects both, the detection and pose estimation performance of synthetically trained approaches;</p><p>(3) the advantage of not suffering from pose-ambiguities does not matter much in LineMOD where most object views are pose-ambiguity free; (4) We train and test poses from the whole SO(3) as opposed to only a limited range in which the test poses lie. SSD6D also trains only on synthetic views of the 3D models and we outperform their approach by a big margin in the RGB-only domain before ICP refinement. <ref type="figure" target="#fig_8">Figure 11</ref> shows qualitative failure cases, mostly stemming from missed detections and strong occlusions. A weak point is the dependence on the bounding box size at test time to predict the object distance. Specifically, under sever occlusions the predicted bounding box tends to shrink such that it does not encompass the occluded parts of the detected object even if it is trained to do so. If the usage of depth data is clear in advance other methods for directly using depth-based (b) Object 28, two view-dependent symmetries <ref type="figure">Fig. 10</ref>: Rotation and translation error histograms on all T-LESS test scenes with our RGB-based (left columns) and ICP-refined (right columns) 6D Object Detection methods for distance estimation might be better suited. Furthermore, on strongly textured objects, the AAEs should not be trained without rendering the texture since otherwise the texture might not be distinguishable from shape at test time. The sim2real transfer on strongly reflective objects like satellites can be challenging and might require physically-based renderings. Some objects, like long, thin pens can fail because their tight object crops at training and test time appear very near from some views and very far from other views, thus hindering the learning of proper pose representa-tions. As the object size is unknown during test time, we cannot simply crop a constantly sized area.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Failure Cases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Rotation and Translation Histograms</head><p>To investigate the effect of ICP and to obtain an intuition about the pose errors, we plot the rotation and translation error histograms of two T-LESS objects <ref type="figure">(Fig.  10)</ref>. We can see the view-dependent symmetry axes of both objects in the rotation errors histograms. We also observe that the translation error is strongly improved through the depth-based ICP while the rotation es- timates from the AAE are hardly refined. Especially when objects are partly occluded, the bounding boxes can become inaccurate and the projective distance estimation (Sec. 3.6.2) fails to produce very accurate distance predictions. Still, our global and fast 6D Object Detection provides sufficient accuracy for an iterative local refinement method to reliably converge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Demonstration on Embedded Hardware</head><p>The presented AAEs were also ported onto a Nvidia Jetson TX2 board, together with a small footprint Mo-bileNet from <ref type="bibr" target="#b20">Howard et al. (2017)</ref> for the bounding box detection. A webcam was connected, and this setup was demonstrated live at ECCV 2018, both in the demo session and during the oral presentation. For this demo we acquired several of the T-LESS objects. As can be seen in <ref type="figure" target="#fig_1">Figure 12</ref>, lighting conditions were dramatically different than in the test sequences from the T-LESS dataset which validates the robustness and applicability of our approach outside lab conditions. No ICP was used, so the errors in depth resulting from the scaling errors of the MobileNet, were not corrected. However, since small errors along the depth direction are less perceptible for humans, our approach could be interesting for augmented reality applications. The detection, pose estimation and visualization of the three test objects ran at over 13Hz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed a new self-supervised training strategy for Autoencoder architectures that enables robust 3D object orientation estimation on various RGB sensors while training only on synthetic views of a 3D model. By demanding the Autoencoder to revert geometric and color input augmentations, we learn representations that (1) specifically encode 3D object orientations, (2) are invariant to a significant domain gap between synthetic and real RGB images, (3) inherently regard pose ambiguities from symmetric object views. Around this approach, we created a real-time (42 fps), RGB-based pipeline for 6D object detection which is especially suitable when pose-annotated RGB sensor data is not available.</p><p>Zhang Z (1994) Iterative point matching for registration of free-form curves and surfaces. International journal of computer vision 13(2):119-152</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>(</head><label></label><figDesc>ref ined) obj2cam ) by applying an Iterative Closest Point post-processing (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Causes of pose ambiguities</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Experiment on the dsprites dataset of<ref type="bibr" target="#b29">Matthey et al. (2017)</ref>. Left: 64x64 squares from four distributions (a,b,c and d) distinguished by scale (s) and translation (t xy ) that are used for training and testing. Right: Normalized latent dimensions z 1 and z 2 for all rotations (r) of the distribution (a), (b) or (c) after training ordinary Autoencoders (AEs) (1),(2) and an AAE (3) to reconstruct squares of the same orientation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Training process for the AAE; a) reconstruction target batch x x x of uniformly sampled SO(3) object views; b) geometric and color augmented input; c) reconstructionx x x after 40000 iterations against a variety of different input augmentations. Finally, it allows us to bridge the domain gap between simulated and real data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Autoencoder CNN architecture with occluded test input, "resize2x" depicts nearest-neighbor upsampling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>AAE decoder reconstruction of LineMOD (left) and T-LESS (right) scene crops rendering with random light positions and randomized diffuse and specular reflection (simple Phong model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Top: creating a codebook from the encodings of discrete synthetic object views; bottom: object detection and 3D orientation estimation using the nearest neighbor(s) with highest cosine similarity from the codebook 1) Render clean, synthetic object views at nearly equidistant viewpoints from a full view-sphere (based on a refined icosahedron<ref type="bibr" target="#b9">(Hinterstoisser et al., 2008)</ref>) 2) Rotate each view in-plane at fixed intervals to cover the whole SO(3) 3) Create a codebook by generating latent codes z ∈ R 128 for all resulting images and assigning their corresponding rotation R cam2obj ∈ R 3x3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>AAE decoder reconstruction of a test code z test ∈ R 128 scaled by a factor s ∈ [0, 2.5]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 :</head><label>11</label><figDesc>Failure cases; Blue: True poses; Green: Predictions; (a) Failed detections due to occlusions and object ambiguity, (b) failed AAE predictions of Glue (middle) and Eggbox (right) due to strong occlusion, (c) inaccurate predictions due to occlusion Fig. 12: MobileNetSSD and AAEs on T-LESS objects, demonstrated live at ECCV 2018 on a Jetson TX2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Augmentation Parameters of AAE; Scale and translation is in relation to image shape and occlusion is in proportion of the object mask</figDesc><table><row><cell></cell><cell>50% chance</cell><cell cols="2">light (random position)</cell></row><row><cell></cell><cell>(30% per channel)</cell><cell cols="2">&amp; geometric</cell></row><row><cell>add</cell><cell>U (−0.1, 0.1)</cell><cell>ambient</cell><cell>0.4</cell></row><row><cell>contrast</cell><cell>U (0.4, 2.3)</cell><cell>diffuse</cell><cell>U (0.7, 0.9)</cell></row><row><cell>multiply</cell><cell>U (0.6, 1.4)</cell><cell>specular</cell><cell>U (0.2, 0.4)</cell></row><row><cell>invert</cell><cell></cell><cell>scale</cell><cell>U (0.8, 1.2)</cell></row><row><cell>gaussian blur</cell><cell>σ ∼ U (0.0, 1.2)</cell><cell>translation</cell><cell>U (−0.15, 0.15)</cell></row><row><cell></cell><cell></cell><cell>occlusion</cell><cell>∈ [0, 0.25]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Augmentation Parameters for Object Detectors, top five are applied in random order; bottom part describes phong lighting from random light positions</figDesc><table><row><cell></cell><cell>chance</cell><cell>SIXD train</cell><cell>Rendered</cell></row><row><cell></cell><cell>(per ch.)</cell><cell></cell><cell>3D models</cell></row><row><cell>add</cell><cell>0.5 (0.15)</cell><cell>U (−0.08, 0.08)</cell><cell>U (−0.1, 0.1)</cell></row><row><cell>contrast norm.</cell><cell>0.5 (0.15)</cell><cell>U (0.5, 2.2)</cell><cell>U (0.5, 2.2)</cell></row><row><cell>multiply</cell><cell>0.5 (0.25)</cell><cell>U (0.6, 1.4)</cell><cell>U (0.5, 1.5)</cell></row><row><cell>gaussian blur</cell><cell>0.2</cell><cell>σ ∼ U (0.5, 1.0)</cell><cell>σ = 0.4</cell></row><row><cell>gaussian noise</cell><cell>0.1 (0.1)</cell><cell>σ = 0.04</cell><cell>-</cell></row><row><cell>ambient</cell><cell>1.0</cell><cell>-</cell><cell>0.4</cell></row><row><cell>diffuse</cell><cell>1.0</cell><cell>-</cell><cell>U (0.7, 0.9)</cell></row><row><cell>specular</cell><cell>1.0</cell><cell>-</cell><cell>U (0.2, 0.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Inference time of the RGB pipeline using SSD on CPUs or GPU</figDesc><table><row><cell></cell><cell>4 CPUs</cell><cell>GPU</cell></row><row><cell>SSD</cell><cell>-</cell><cell>∼17ms</cell></row><row><cell>Encoder</cell><cell>∼100ms</cell><cell>∼5ms</cell></row><row><cell>Cosine Similarity</cell><cell>2.5ms</cell><cell>1.3ms</cell></row><row><cell>Nearest Neighbor</cell><cell>0.3ms</cell><cell>3.2ms</cell></row><row><cell>Projective Distance</cell><cell>0.4ms</cell><cell>-</cell></row><row><cell></cell><cell cols="2">∼24ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">: Single object pose estimation run-</cell></row><row><cell>time w/o refinement</cell><cell></cell></row><row><cell>Method</cell><cell>fps</cell></row><row><cell>Vidal et al. (2018)</cell><cell>0.2</cell></row><row><cell>Brachmann et al. (2016)</cell><cell>2</cell></row><row><cell>Kehl et al. (2016)</cell><cell>2</cell></row><row><cell>Rad and Lepetit (2017)</cell><cell>4</cell></row><row><cell>Kehl et al. (2017)</cell><cell>12</cell></row><row><cell>OURS</cell><cell>13 (RetinaNet)</cell></row><row><cell></cell><cell>42 (SSD)</cell></row><row><cell>Tekin et al. (2017)</cell><cell>50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on color augmentations for different test sensors. Object 5 tested on all scenes, T-LESS<ref type="bibr" target="#b17">Hodaň et al. (2017)</ref>. Standard deviation of three runs in brackets.</figDesc><table><row><cell>Train RGB</cell><cell>Test RGB</cell><cell cols="2">dyn. light</cell><cell>add</cell><cell>contrast</cell><cell>multiply</cell><cell>invert</cell><cell>AUC vsd</cell></row><row><cell>3D Reconstruction</cell><cell>Primesense</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.472 (± 0.013)</cell></row><row><cell>(synthetic)</cell><cell>(real)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.611 (± 0.030)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.825 (± 0.015)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.876 (± 0.019)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.877 (± 0.005)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.861 (± 0.014)</cell></row><row><cell>Primesense (real)</cell><cell>Primesense (real)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.890 (± 0.003)</cell></row><row><cell>3D Reconstruction</cell><cell>Kinect</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.461 (± 0.022)</cell></row><row><cell>(synthetic)</cell><cell>(real)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.580 (± 0.014)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.701 (± 0.046)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.855 (± 0.016)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.897 (± 0.008)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.903 (± 0.016)</cell></row><row><cell>Kinect (real)</cell><cell>Kinect (real)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.917 (± 0.007)</cell></row><row><cell cols="3">(a) Effect of latent space size, stan-</cell><cell cols="6">(b) Training on CAD model (bottom) vs. textured 3D</cell></row><row><cell cols="2">dard deviation in red</cell><cell></cell><cell cols="3">reconstruction (top)</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>T-LESS: Object recall for err vsd &lt; 0.3 on all Primesense test scenes (SIXD/BOP benchmark from<ref type="bibr" target="#b18">Hodan et al. (2018)</ref>). RGB † depicts training with 3D reconstructions, except objects 19-23 −→ CAD models;RGB ‡ depicts training on untextured CAD models only</figDesc><table><row><cell>AAE</cell><cell>AAE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Effect of Perspective Corrections on T-LESS</figDesc><table><row><cell>Method</cell><cell>RGB  †</cell></row><row><cell>w/o correction</cell><cell>18.35</cell></row><row><cell cols="2">w/ correction 19.26 (+0.91)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>LineMOD: Object recall (ADD<ref type="bibr" target="#b12">Hinterstoisser et al. (2012b)</ref> metric) of methods that use different amounts of training and test data, results taken from<ref type="bibr" target="#b41">Tekin et al. (2017)</ref> </figDesc><table><row><cell></cell><cell>Test data</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RGB</cell><cell></cell><cell></cell><cell></cell><cell>+Depth (ICP)</cell></row><row><cell cols="6">Train data RGB w/o real pose labels</cell><cell></cell><cell></cell><cell cols="4">RGB with real pose labels</cell><cell>-</cell></row><row><cell></cell><cell cols="3">Object Kehl et al.</cell><cell>OURS</cell><cell></cell><cell cols="7">Brachmann et al. Rad and Lepetit Tekin et al.</cell><cell>Xiang et al.</cell><cell>OURS Kehl et al.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>+refine</cell><cell></cell><cell cols="2">+refine</cell><cell></cell><cell>+DeepIm</cell></row><row><cell></cell><cell>Ape</cell><cell cols="2">0.00</cell><cell>4.18</cell><cell></cell><cell>-</cell><cell>33.2</cell><cell>27.9</cell><cell></cell><cell>40.4</cell><cell>21.62</cell><cell>-</cell><cell>77.0</cell><cell>24.35</cell><cell>65</cell></row><row><cell></cell><cell>Benchvise</cell><cell cols="2">0.18</cell><cell>22.85</cell><cell></cell><cell>-</cell><cell>64.8</cell><cell>62.0</cell><cell></cell><cell>91.8</cell><cell>81.80</cell><cell>-</cell><cell>97.5</cell><cell>89.13</cell><cell>80</cell></row><row><cell></cell><cell>Cam</cell><cell cols="2">0.41</cell><cell>32.91</cell><cell></cell><cell>-</cell><cell>38.4</cell><cell>40.1</cell><cell></cell><cell>55.7</cell><cell>36.57</cell><cell>-</cell><cell>93.5</cell><cell>82.10</cell><cell>78</cell></row><row><cell></cell><cell>Can</cell><cell cols="2">1.35</cell><cell>37.03</cell><cell></cell><cell>-</cell><cell>62.9</cell><cell>48.1</cell><cell></cell><cell>64.1</cell><cell>68.80</cell><cell>-</cell><cell>96.5</cell><cell>70.82</cell><cell>86</cell></row><row><cell></cell><cell>Cat</cell><cell cols="2">0.51</cell><cell>18.68</cell><cell></cell><cell>-</cell><cell>42.7</cell><cell>45.2</cell><cell></cell><cell>62.6</cell><cell>41.82</cell><cell>-</cell><cell>82.1</cell><cell>72.18</cell><cell>70</cell></row><row><cell></cell><cell>Driller</cell><cell cols="2">2.58</cell><cell>24.81</cell><cell></cell><cell>-</cell><cell>61.9</cell><cell>58.6</cell><cell></cell><cell>74.4</cell><cell>63.51</cell><cell>-</cell><cell>95.0</cell><cell>44.87</cell><cell>73</cell></row><row><cell></cell><cell>Duck</cell><cell cols="2">0.00</cell><cell>5.86</cell><cell></cell><cell>-</cell><cell>30.2</cell><cell>32.8</cell><cell></cell><cell>44.3</cell><cell>27.23</cell><cell>-</cell><cell>77.7</cell><cell>54.63</cell><cell>66</cell></row><row><cell></cell><cell>Eggbox</cell><cell cols="2">8.90</cell><cell>81.00</cell><cell></cell><cell>-</cell><cell>49.9</cell><cell>40.0</cell><cell></cell><cell>57.8</cell><cell>69.58</cell><cell>-</cell><cell>97.1</cell><cell>96.62</cell><cell>100</cell></row><row><cell></cell><cell>Glue</cell><cell cols="2">0.00</cell><cell>46.17</cell><cell></cell><cell>-</cell><cell>31.2</cell><cell>27.0</cell><cell></cell><cell>41.2</cell><cell>80.02</cell><cell>-</cell><cell>99.4</cell><cell>94.18</cell><cell>100</cell></row><row><cell cols="2">Holepuncher</cell><cell cols="2">0.30</cell><cell>18.20</cell><cell></cell><cell>-</cell><cell>52.8</cell><cell>42.4</cell><cell cols="2">67.2</cell><cell>42.63</cell><cell>-</cell><cell>52.8</cell><cell>51.25</cell><cell>49</cell></row><row><cell></cell><cell>Iron</cell><cell cols="2">8.86</cell><cell>35.05</cell><cell></cell><cell>-</cell><cell>80.0</cell><cell>67.0</cell><cell></cell><cell>84.7</cell><cell>74.97</cell><cell>-</cell><cell>98.3</cell><cell>77.86</cell><cell>78</cell></row><row><cell></cell><cell>Lamp</cell><cell></cell><cell>8.2</cell><cell>61.15</cell><cell></cell><cell>-</cell><cell>67.0</cell><cell>39.9</cell><cell></cell><cell>76.5</cell><cell>71.11</cell><cell>-</cell><cell>97.5</cell><cell>86.31</cell><cell>73</cell></row><row><cell></cell><cell>Phone</cell><cell cols="2">0.18</cell><cell>36.27</cell><cell></cell><cell>-</cell><cell>38.1</cell><cell>35.2</cell><cell></cell><cell>54.0</cell><cell>47.74</cell><cell>-</cell><cell>87.7</cell><cell>86.24</cell><cell>79</cell></row><row><cell></cell><cell>Mean</cell><cell cols="2">2.42</cell><cell>32.63</cell><cell></cell><cell>32.3</cell><cell>50.2</cell><cell>43.6</cell><cell></cell><cell>62.7</cell><cell>55.95</cell><cell>62.7</cell><cell>88.6</cell><cell>71.58</cell><cell>79</cell></row><row><cell></cell><cell>800</cell><cell></cell><cell></cell><cell>800</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>600</cell><cell></cell><cell>600</cell></row><row><cell>views</cell><cell>400 600</cell><cell></cell><cell></cell><cell>400 600</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>views</cell><cell>400</cell><cell></cell><cell>400</cell></row><row><cell></cell><cell>200</cell><cell></cell><cell></cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>200</cell><cell></cell><cell>200</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell>0</cell></row><row><cell></cell><cell>0</cell><cell>50</cell><cell>100 150</cell><cell></cell><cell>0</cell><cell>50</cell><cell>100 150</cell><cell></cell><cell></cell><cell>0</cell><cell>50</cell><cell>100 150</cell><cell>0</cell><cell>50</cell><cell>100 150</cell></row><row><cell></cell><cell></cell><cell cols="2">Rotation err [deg]</cell><cell></cell><cell></cell><cell cols="2">Rotation err [deg]</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Rotation err [deg]</cell><cell>Rotation err [deg]</cell></row><row><cell></cell><cell>200</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>150</cell><cell></cell><cell>800</cell></row><row><cell>views</cell><cell>100</cell><cell></cell><cell></cell><cell>600 900</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>views</cell><cell>100</cell><cell></cell><cell>400 600</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>300</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>50</cell><cell></cell><cell>200</cell></row><row><cell></cell><cell>0</cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0</cell><cell></cell><cell>0</cell></row><row><cell></cell><cell>0</cell><cell cols="3">25 50 75 100</cell><cell>0</cell><cell cols="3">25 50 75 100</cell><cell></cell><cell>0</cell><cell cols="2">25 50 75 100</cell><cell>0</cell><cell>25 50 75 100</cell></row><row><cell></cell><cell cols="4">Translation err [mm]</cell><cell cols="3">Translation err [mm]</cell><cell></cell><cell></cell><cell cols="3">Translation err [mm]</cell><cell>Translation err [mm]</cell></row><row><cell></cell><cell cols="7">(a) Object 5, one view-dependent symmetry</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We would like to thank Dr. Ingo Kossyk, Dimitri Henkel and Max Denninger for helpful discussions. We also thank the reviewers for their useful comments. This work has been partly funded by Robert Bosch GmbH, Corporate Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Pose Guided RGB-D Feature Learning for 3D Object Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doumanoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kouskouridas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Using simulation and domain adaptation to improve efficiency of deep robotic grasping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Irpan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kelcey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Downs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ibarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pastor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<idno>arXiv:170907857</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncertainty-driven 6D pose estimation of objects and scenes from a single RGB image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gumhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3364" to="3372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object modelling by registration of multiple range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and vision computing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="155" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Domain adaptation for visual applications: A comprehensive survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<idno>arXiv:170205374</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Model globally, match locally: Efficient and robust 3d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE computer society conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="998" to="1005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cki</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Simultaneous Recognition and Homography Extraction of Local Patches with a Simple Linear Classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benhimane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Conference, pages</title>
		<meeting>the British Machine Conference, pages</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="10" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multimodal templates for real-time detection of texture-less objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="858" to="865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gradient response maps for real-time detection of textureless objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cagniart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="876" to="888" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Model based training, detection and pose estimation of texture-less 3D objects in heavily cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Holzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="548" to="562" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Going further with Point Pair Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hinterstoisser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<idno>arXiv:171010710</idno>
		<title level="m">On Pre-Trained Image Features and Synthetic Images for Deep Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hodan</surname></persName>
		</author>
		<ptr target="http://cmp.felk.cvut.cz/sixd/challenge" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On evaluation of 6D object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hodaň</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Obdržálekš</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="606" to="619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">T-LESS: An RGB-D Dataset for 6D Pose Estimation of Texture-less Objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hodaň</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haluza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Obdržálekš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lourakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zabulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<publisher>WACV</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bop: benchmark for 6d object pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Glentbuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kraft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Drost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ihrke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zabulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Photorealistic image synthesis for object instance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hodan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shalev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hanzelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Connell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Urbina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Guenter</surname></persName>
		</author>
		<idno>ArXiv abs/1902.03334</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>arXiv:170404861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning of local RGB-D patches for 3D object detection and 6D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="205" to="220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SSD-6D: Making RGB-Based 3D Detection and 6D Pose Estimation Great Again</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1521" to="1529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>arXiv:14126980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<idno>arXiv:170805628</idno>
		<title level="m">3D Pose Regression using Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep model-based 6d pose refinement in rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Manhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kehl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hassabis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<ptr target="https://github.com/deepmind/dsprites-dataset/" />
		<title level="m">dsprites: Disentanglement testing Sprites dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A selfsupervised learning system for object detection using physics simulation and multi-view pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mitash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Bekris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boularias</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IROS</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on, IEEE</title>
		<imprint>
			<biblScope unit="page" from="545" to="551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">How useful is photo-realistic rendering for visual learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="202" to="217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Illumination for computer generated pictures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Phong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="311" to="317" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">BB8: A Scalable, Accurate, Robust to Partial Occlusion Method for Predicting the 3D Poses of Challenging Objects without Using Depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<idno>arXiv:170310896</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<editor>Robotics and Automation</editor>
		<imprint>
			<date type="published" when="1985" />
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="794" to="800" />
			<pubPlace>Driemeyer J, Ng AY</pubPlace>
		</imprint>
		<respStmt>
			<orgName>California Univ San Diego La Jolla Inst for Cognitive Science Saxena A</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Tech. rep.</note>
	<note>ICRA&apos;09. IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning from Simulated and Unsupervised Images through Adversarial Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2242" to="2251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Render for CNN: Viewpoint estimation in images using CNNs trained with rendered 3D model views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2686" to="2694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Implicit 3d orientation learning for 6d object detection from rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Durner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Triebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="699" to="715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno>arXiv:171108848</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Real-Time Seamless Single Shot 6D Object Pose Prediction. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Domain randomization for transferring deep neural networks from simulation to the real world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tobin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2017 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="23" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep object pose estimation for semantic robotic grasping of household objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tremblay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>To</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sundaralingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Birchfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="306" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">CAD-based recognition of 3D objects in monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ulrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wiedemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Steger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: ICRA</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1191" to="1198" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">6D Pose Estimation using an Improved Method based on Point Pair Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martí</surname></persName>
		</author>
		<idno>arXiv:180208516</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning descriptors for object recognition and 3D pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3109" to="3118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Bridging categorylevel and instance-level semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengel</forename><surname>Avd</surname></persName>
		</author>
		<idno>arXiv:160506885</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Posecnn: A convolutional neural network for 6d object pose estimation in cluttered scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<idno>arXiv:171100199</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Dpod: Dense 6d pose object detector in rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shugurov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
		<idno>arXiv:190211020</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
