<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deformable PV-RCNN: Improving 3D Object Detection with Learned Deformations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prarthana</forename><surname>Bhattacharyya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Czarnecki</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deformable PV-RCNN: Improving 3D Object Detection with Learned Deformations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present Deformable PV-RCNN, a high-performing pointcloud based 3D object detector. Currently, the proposal refinement methods used by the state-of-the-art two-stage detectors cannot adequately accommodate differing object scales, varying point-cloud density, partdeformation and clutter. We present a proposal refinement module inspired by 2D deformable convolution networks that can adaptively gather instance-specific features from locations where informative content exists. We also propose a simple context gating mechanism which allows the keypoints to select relevant context information for the refinement stage. We show state-of-the-art results on the KITTI dataset.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>3D object detection from point clouds is critical for autonomous driving and robotics. We build on the success of PV-RCNN <ref type="bibr" target="#b6">[7]</ref>, a state-of-the-art 3D object detector.</p><p>Motivation Part of PV-RCNNs success is due to the randomly sampled keypoints which capture multi-scale features for proposal refinement while retaining fine-grained localization information. However, random sampling is not effective over potentially ambiguous scenes. For example, pedestrians and traffic poles can be very hard to distinguish in point clouds. In this case, we wish to align the keypoints towards the most discriminative areas, so that principal features for a pedestrian can be highlighted. Similarly, the scales for cars, pedestrians and cyclists are very different. While multi-scale feature aggregation is advantageous for image features, the non-uniform density of point clouds makes it hard to detect them with a single model. We wish to adaptively aggregate and focus on their most salient features at various scales. Lastly to handle clutter and avoid false positives, for example, to avoid detecting all seated individuals as cyclists, we need to pick up on unevenly distributed contextual information.</p><p>Contribution We construct Deformable PV-RCNN, a 3D detector that handles sparsity of LIDAR points, is adaptive to non-uniform point cloud density especially at long distances and can address clutter in real-world traffic scenes. We show that we can outperform PV-RCNN across different categories and especially at long-distances on the KITTI 3D object detection dataset.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>Our 3D detection pipeline is presented in <ref type="figure" target="#fig_0">Fig. 1</ref>. It consists of an Adaptive Deformation module ( <ref type="figure" target="#fig_1">Fig. 2</ref>) and a Context Fusion module ( <ref type="figure" target="#fig_2">Fig. 3)</ref>.</p><p>Adaptive Deformation The n sampled keypoints (shown in yellow in <ref type="figure" target="#fig_0">Fig. 1</ref>) have a 3D position v i and a feature vector f i corresponding to either of Conv3 or Conv4 layers. Our module computes updated features f i as follows:</p><formula xml:id="formula_0">f i = 1 n ReLU( j∈N (i) W offset (f i − f j ) · (v i − v j ))</formula><p>, where N i gives the i-th keypoint's neighbors in the point-cloud and W offset is a learned weight matrix. We then obtain the new deformed keypoint positions as</p><formula xml:id="formula_1">v i = v i +tanh(W align [f i ]),</formula><p>where W align is a learned weight matrix. This is similar to <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>. We then proceed to compute the features for the deformed keypoints using PointNet++ similar to the PV-RCNN pipeline.</p><p>Context Fusion This module uses context gating to dynamically select representative and discriminative features from local evidence, highlighting object features and suppressing clutter. Given a keypoint feature f i , the modulating feature is obtained as g = σ(W gate f i + b gate ) and the context-gated feature is computed as  </p><formula xml:id="formula_2">f g i = g W fc f i , where W gate , b gate , W fc are learned from data [2].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Results</head><p>The 3D object detection benchmark of KITTI <ref type="bibr" target="#b2">[3]</ref> contains 7481 training samples. Following <ref type="bibr" target="#b0">[1]</ref>, we divide them into 3712 training samples and 3769 validation samples. We report our results on the validation (val ) split of KITTI.</p><p>Comparison with state-of-the-art methods <ref type="table" target="#tab_0">Table 1</ref> shows the performance of Deformable PV-RCNN on the KITTI val split. We run the officially released checkpoints for SECOND, Part-A2 and PV-RCNN. <ref type="bibr" target="#b0">1</ref> Our method outperforms the state-of-the-art on the cyclist class. Compared to PV-RCNN, it performs better on pedestrian and cyclist class by 3.5 % and 4%, respectively, and on par for the car class. <ref type="table">Table 2</ref> validates the effectiveness of learning deformable offsets and context gating by comparing with a retrained PV-RCNN baseline. We find that deformation prediction contributes to the performance increase over all classes, but especially for the pedestrian class, which is probably due to being able to focus on representative features like arms and legs. Context fusion also increases performance over all classes, indicating that contextual information is important for refinement. <ref type="table" target="#tab_1">Table 3</ref> shows that our model outperforms the baseline at different distances for the pedestrian and cyclist class. We also find a 2% increase in AP for the car class at long distances where the scans are sparse and context information becomes important. <ref type="figure">Fig. 4</ref> shows that sim- ilar AP performance can be obtained with fewer keypoints as compared to the baseline across all classes, which is probably due to their deformability to cover representative locations. <ref type="figure" target="#fig_3">Fig. 5</ref> shows qualitative 3D detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with PV-RCNN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We present Deformable PV-RCNN, a 3D object detector for detecting 3D objects from raw point cloud. Our proposed deformation prediction method adaptively aligns the keypoints encoding the scene towards the most discriminative and representative features. The proposed context gating network adaptively highlights relevant context features thereby favouring more accurate proposal refinement. Our experiments show that Deformable PV-RCNN outperforms PV-RCNN in various challenging cases on the 3D detection benchmark of KITTI dataset, especially benefiting smaller objects and complex scenes more. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Our architecture for Deformable PV-RCNN. We propose an Adaptive Deformation and Context Fusion module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Adaptive deformation module adaptively aligns keypoints towards the mostcontent rich and discriminative features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Context fusion module dynamically selects most relevant context features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>From left to right: RGB-image; PV-RCNN output; Deformable PV-RCNN output. GT boxes visualized in red and predictions in green. Yellow and pink circled objects are picked up by our detector but missed or oriented wrongly by PV-RCNN; Objects circled blue are clutter which our detector suppressed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison on the moderate level of KITTI val split with AP calculated calculated by 11 recall positions</figDesc><table><row><cell>Model</cell><cell cols="3">Car Cyclist Pedestrian</cell></row><row><cell>SECOND [9]</cell><cell cols="2">78.62 67.75</cell><cell>52.98</cell></row><row><cell cols="3">F-PointNet [5] 70.92 56.49</cell><cell>61.32</cell></row><row><cell>Part-A2 [8]</cell><cell cols="2">79.40 69.90</cell><cell>60.05</cell></row><row><cell cols="3">PV-RCNN [7] 83.69 69.47</cell><cell>54.84</cell></row><row><cell>Ours</cell><cell cols="2">83.30 73.46</cell><cell>58.33</cell></row><row><cell cols="4">Table 2. Ablation study on the moderate level of KITTI val split with AP calculated</cell></row><row><cell>calculated by 40 recall positions</cell><cell></cell><cell></cell></row><row><cell cols="2">Deformations Context-Fusion</cell><cell cols="2">Car Cyclist Pedestrian</cell></row><row><cell></cell><cell></cell><cell cols="2">84.20 69.65</cell><cell>54.49</cell></row><row><cell></cell><cell></cell><cell cols="2">84.24 70.21</cell><cell>57.31</cell></row><row><cell></cell><cell></cell><cell cols="2">84.71 73.03</cell><cell>57.65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Comparison of nearby and distant-object detection on the moderate level of KITTI val split with AP calculated by 40 recall positions</figDesc><table><row><cell>Distance</cell><cell>Model</cell><cell cols="2">Car Cyclist Pedestrian</cell></row><row><cell>0-30 m</cell><cell cols="2">PV-RCNN 91.71 73.76 Ours 91.65 74.89</cell><cell>56.82 59.61</cell></row><row><cell>30-50 m</cell><cell cols="2">PV-RCNN 50.00 35.15 Ours 52.02 47.00</cell><cell>--</cell></row><row><cell cols="4">Fig. 4. AP of PV-RCNN (black) vs. Deformable PV-RCNN (blue) for a number of</cell></row><row><cell cols="2">keypoints, varying from 512 to 2048</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/open-mmlab/OpenPCDet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<idno>abs/1611.07759</idno>
		<ptr target="http://arxiv.org/abs/1611.07759" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno>abs/1612.08083</idno>
		<ptr target="http://arxiv.org/abs/1612.08083" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vision meets robotics: the KITTI dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="DOI">10.1177/0278364913491297</idno>
		<ptr target="https://doi.org/10.1177/0278364913491297" />
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<idno>abs/1906.02739</idno>
		<ptr target="http://arxiv.org/abs/1906.02739" />
		<editor>Mesh R-CNN. CoRR</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from RGB-D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1711.08488</idno>
		<ptr target="http://arxiv.org/abs/1711.08488" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Pointdan: A multi-scale 3d domain adaption network for point cloud representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">PV-RCNN: Pointvoxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno>ArXiv abs/1912.13192</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Part-A 2 net: 3d part-aware and aggregation neural network for object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1907.03670</idno>
		<ptr target="http://arxiv.org/abs/1907.03670" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">SECOND: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.3390/s18103337</idno>
		<ptr target="https://doi.org/10.3390/s18103337" />
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
