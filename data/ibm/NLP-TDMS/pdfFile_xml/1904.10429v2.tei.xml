<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DenseNet Models for Tiny ImageNet Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<title level="a" type="main">DenseNet Models for Tiny ImageNet Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T16:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present two image classification models on the Tiny ImageNet dataset. We built two very different networks from scratch based on the idea of Densely Connected Convolution Networks. The architecture of the networks is designed based on the image resolution of this specific dataset and by calculating the Receptive Field of the convolution layers. We also used some nonconventional techniques related to image augmentation and Cyclical Learning Rate to improve the accuracy of our models. The networks are trained under high constraints and low computation resources. We aimed to achieve top-1 validation accuracy of 60%; the results and error analysis are also presented.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>There are several Image Classification competitions such as the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), where several convolution neural network architectures and models are presented. In 2015, the ImageNet [1] challenge was won by the ResNet model with 152 layers which achieved a top-5 classification error of 3.57% <ref type="bibr" target="#b0">[2]</ref>. In this paper, we present our approach for building classification models for a subset of ImageNet dataset called the Tiny ImageNet [4]. We did not use any pre-trained network available for the original ImageNet challenge. Instead, we built two different models from scratch taking inspiration from the DenseNet architecture <ref type="bibr" target="#b1">[3]</ref>. We aimed to reach 60% validation accuracy with these custom models under several constraints and limited resources. Our biggest challenge was low computation power provided by Google Colab [5] free services. Google Colab provides only 12 hours of continuous computation time, after which the session needs to be reconnected. Also, we restricted our models from using any dense or fully connected layers, any dropout layers, 1x1 filters to increase the number of channels and more than 500 training epochs. We have used several innovative techniques tailored to this challenge in order to improve accuracy and reduce computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Dataset</head><p>The Tiny ImageNet dataset [4] is a modified subset of the original ImageNet dataset <ref type="bibr">[1]</ref>. Here, there are 200 different classes instead of 1000 classes of ImageNet dataset, with 100,000 training examples and 10,000 validation examples. The resolution of the images is just 64x64 pixels, which makes it more challenging to extract information from it. A glance at the images shows that it is difficult for the human eye to detect objects in some images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DenseNet</head><p>We used the concept of DenseNet <ref type="bibr" target="#b1">[3]</ref>   through summation; instead, they are combined by concatenation. Thus the information is passed from one layer to all the subsequent layers ensuring better flow of information and gradients throughout the network. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Receptive Field</head><p>For both of our networks, we implemented the number of layers in our model by first calculating the receptive field <ref type="bibr" target="#b2">[6]</ref>. Throughout our model, we used (3x3) kernels with strides (1,1). So, for the first layer, the receptive field is (3x3) as each kernel convolutes over (3x3) pixels or 9 pixels at a time. Every such convolution operation decreases the spatial dimensions of the matrix by 2, thus increasing the receptive field of the network by 2. While at the MaxPooling layer, the spatial dimensions of the matrix are reduced by half, hence doubling the entire receptive field. The receptive field of the networks is calculated as shown in <ref type="figure" target="#fig_2">Figure 3</ref>: As we reach the receptive field of original image size 64x64, we expect our model to detect the object in each image distinctly and be able to classify them. However, we have gone further, to a receptive field of more than double the original image size, so that network learns the background details too. Many images in our dataset are confusing due to background domination, so for those images, we want our models to understand the context in which our objects are found. For example, in the class bullfrog as shown in <ref type="figure" target="#fig_3">Figure 4</ref>, we can observe that most of the images have a green background in addition to our object, i.e. bullfrog. We want our network to learn that in addition to our object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Design</head><p>In the beginning, without much effort, we implemented the vanilla ResNet-34 and ResNet-50 models as our Network 1 and 2 respectively. For ResNet-34, we achieved an accuracy of 99.8% on the training set and 33.5% on the validation set with a batch size of 500 images running for 100 epochs. For ResNet-50 we achieved an accuracy of 49% on 2 the training set and 26.2% on the validation set with a batch size of 512 images running for 50 epochs. Although this shows the learning power of ResNet models, we notice an early saturation in validation accuracy, due to the use of deep networks on shallow datasets <ref type="bibr" target="#b3">[7]</ref>. Motivated by approaches explained in few Stanford 231n course project reports [8], we considered a modified approach for building a custom DenseNet model, having design elements of ResNet-18. The suitable reason behind the approach can be established from the fact that for deep networks on 64x64 images, the problem arises after first few layers where it starts working as an identity map of size 1x1 learning negligible with every additional layer. So we opt for a shallow network of around 15 convolution layers and wide enough to contain around 1000 channels before the output. For Network 1 :</p><p>i. We built a custom architecture of 3 'bottleneck' blocks having 5 convolution layers of increasing channels and 1 MaxPooling layer at the end of each block. ii. Just before applying concatenation, we feed the output of each block to space_to_depth function. This function ensures that the spatial dimensions of both the layers are equal before concatenation. iii. We concatenate the skip connections from the output of each block with the output of the next block, preserving the information from both the blocks before being fed to the subsequent block. iv. The final layers in the model have 1x1 convolution layer followed by a GlobalAveragePooling layer which averages the spatial dimensions of a matrix of any size. This layer gives us the ability to design a model which can take input image of any size. We shall use this fact to train our model better (to b e e x p l a i n e d f u r t h e r i n I m a g e Augmentation).</p><p>For Network 2 we modified the ResNet-18 architecture as follows: i. We replaced the first convolution layer that initially consisted of 64 (7x7) filters with stride (2,2), by 32 (3x3) filters with stride (1,1) and removed the max pooling layer. ii. We removed the 1st block consisting 4 convolution layers of 64 (3x3) filters. iii. We removed the skip connections after every 2 convolution layers instead m a i n t a i n e d i t b e t w e e n e v e r y 4 convolution layers. We replaced the original add function in shortcuts with concatenation so that it preserves the channels from the previous block and not merge them. We added a Batch Normalization and ReLU activation layer after each shortcut. iv. As per requirement of the project, we replaced final FC layers with 1x1 convolution layer for decreasing the number of channels to the required number of classes followed by a GlobalAveragePooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Image Augmentation</head><p>In order to artificially increase the amount of training data and avoid overfitting, we rely on image augmentation. For Network 1, we used both direct and indirect approach towards implementing image augmentation. Firstly, as an indirect approach, we fed images with 32x32 resolutions for the beginning few epochs followed by 64x64 resolution images. Then we fed images with 16x16 resolutions for the next few numbers of epochs and finally fed the model with 64x64 resolutions images. The reason behind this was to feed low information scaled out data into the model till validation accuracy gets saturated so that it can learn a few features from these images. This technique gave us an increase of 3-4% in validation accuracy. At 50% validation accuracy, we directly applied image augmentations from the imgaug library <ref type="bibr">[11]</ref>. We manually provided the range of the parameters and randomly selected the 3 number of augmentations (between 0 to all) from the list for each image. We used the following augmentations:</p><p>• Scale • CoarseDropout • Rotate • Additive Gaussian Noise • Crop and Pad Applying this we got an increase of 9.5% on validation accuracy after an additional run for 150 epochs. For Network 2, we kept default 64x64 image resolution as input size throughout our complete run. Instead of applying image augmentations after a certain number of iterations as in Network 1, we applied a random sequence of 11 transformations [8] to half of the dataset from the beginning of the model run using the imgaug library <ref type="bibr">[11]</ref>. The intensity of each transformation was randomly determined within a specified range, but these range parameters were manually provided so that these transformed training images could closely represent the images in the validation dataset. The augmentations applied were :</p><p>• </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Regularizers, Optimizers, Hyperparameters and Callbacks</head><p>Batch Normalization helps to normalise the inputs of the previous layer at each batch keeping the values in a comparable range with the mean equal to 0 and standard deviation equal to 1. This ensures the activations of our models do not get skewed at any one particular point and also increases the speed of computation. For both the networks we applied Batch Normalization after every convolution   <ref type="bibr" target="#b6">[12]</ref> where we vary the learning rate within a range of values for a specific number of iterations. This technique helps us in getting better validation accuracy in a fewer number of epochs. We used triangular2 rate policy, which halves the max learning rate with every cycle. As a modification to what is proposed in the original paper <ref type="bibr" target="#b6">[12]</ref>, we did not gradually reduce the range value of cycles and step sizes, after a certain number of cycles till the end. Instead, for the cycles where the validation accuracy does not show any increase than the previous range, we increase the learning rate range for those cycles and then lower the learning rate range for the subsequent cycle. Here, for the cycles of learning rate range 1e-7 to 6e-7 after the cycles of range 1e-6 to 6e-6, we did not see an improvement in validation accuracy, so we increased the learning rate range 1e-5 to 6e-5 for those cycles and then reduced the range 1e-7 to 6e-7 for the next cycle. This gave us an increase in the validation accuracy of 0.3%. This is summarised in <ref type="table" target="#tab_3">Table 1</ref> below.</p><p>As Google Colab [5] notebook is restricted to 12 hours of continuous use, for both the model runs we used a Model Checkpointer to save the model with best validation accuracy as we had to run over 100 epochs for each network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Results</head><p>For Network 1 we trained our model with 17.9 Million parameters for 235 epochs with a batch size of 256 for 32x32 and 16x16 resolution images and a batch size of 64 for 64x64 resolution images. At first, we ran 32x32 resolution training images for 15 epochs and reached a validation accuracy of 25%. At this point, the model had already started overfitting, so we switched to 64x64 resolution images and ran it for 30 epochs which increased the validation accuracy to 48%. In order to train the model on low-resolution images and simulate scaled out augmentation, we trained it on 16x16 resolution images for just 10 epochs. We kept epochs low here because although we wanted the model to learn from low information images, we also wanted to avoid overfitting. We changed back to 64x64 resolution images and stuck to it for the rest of the training. After running it for 30 more epochs, our validation accuracy saturated around 48-49%, and now we implemented image augmentation. We performed some of the augmentations on every image and ran our model for another 150 epochs. Due to several augmentations, now the model trained much slower but had yet to begin overfitting. The gap between training and validation accuracy remained small, and we trained it until the validation accuracy saturated to around 59%. Here, the training accuracy reached 67%, so we did not train it further to avoid overfitting. We selected the best model from our training using model checkpoint which gave an accuracy of 59.5% on the validation dataset. For Network 2 we trained our model with 11.8 Million parameters for 108 epochs with a batch size of 128 images. As we can see from <ref type="figure" target="#fig_6">Figure  7</ref>, Cyclical Learning Rate helped us in early attainment of higher validation accuracy steeply bringing down the validation loss. It is also distinctly visible that after 70 epochs our validation loss reached a plateau for almost 10 epochs.That is when we applied higher Cyclical Learning Rate range due to which a  sudden increase in loss is visible, after which the validation accuracy attains a new plateau although training accuracy continues decreasing. At the end of 108 epochs, we achieve a validation accuracy of 62.73% whereas our training accuracy remained at 68.11%, indicating a narrow gap again. Both the networks achieved validation accuracy higher than Shallow ConvNets <ref type="bibr" target="#b3">[7]</ref> and ResNet-18 (with no Dropout) models <ref type="bibr" target="#b4">[9]</ref> and are very close to state-of-the-art ResNet-18 (with various Dropout techniques) models <ref type="bibr" target="#b5">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Error Analysis</head><p>We  Overall, these errors are generally due to three main reasons [8]: the low resolution of the images, the misunderstanding of the primary entity in the image, and the confusion by similar items. Through manual inspection in both correctly and incorrectly classified images, we were able to draw the following detailed conclusions:</p><p>In the correctly classified images, we observed that there is a clear distinction between the object and the background. Images of every best classifying class tend to share same features within themselves such as colour, texture etc. Also, the model performs very well on detecting objects that cover the entire 64x64 resolution and has a minimal background. Though for some classes like 'flagpole', 'flagstaff', the model was able to detect zoomed in and zoomed out object. This could be because we fed low-resolution images and used the scaling augmentation.</p><p>In the incorrectly classified images, many images were misclassified due to the low resolution of the images. Some of them are even too hard for humans to differentiate. For example, an 'Egyptian cat' is misclassified as a 'Tabby cat' or a 'Labrador retriever' is misclassified as a 'Golden retriever'. Here, since there are not many details in the images, given the low resolution of 64x64, the model can detect the type of the object (such as a dog), but unable to further sub-classify it into the correct class (such as Labrador retriever and Golden retriever). Another type of incorrectly classified images is due to the misunderstanding of the intent of the image [8]. The plate of fruits is classified as 'banana' simply because there is indeed a banana in it; however, the true label is 'orange'. A plate of food which is supposed to be categorised as 'meatloaf', is instead classified as a 'plate', which is perfectly reasonable. These kind of errors are impossible to fix. There is often more than one entity in a single image; our network interprets these images correctly but does not interpret what is indeed correct. The last kind of misclassified images is due to the reason that the categories are too close to each other, sometimes there is even some overlap between each other. 'Convertible Cars' are misclassified as 'Sports Cars', but in reality, a large part of 'Convertible Cars' are also 'Sports Cars'. In summary, although almost 30% of the images are not classified correctly, a large number of the errors are reasonable, some of them are even forgivable. <ref type="table" target="#tab_4">Table 2</ref> shows the top worst classified classes by our networks:</p><p>After analysing our predictions, we separated correctly and incorrectly labelled images and oversampled the incorrectly labelled images three times the correctly labelled images. We also tried another approach by soft weighing the low precision classes. We expected our models to learn the incorrectly classified images better using these techniques; however, our model started overfitting, and we observed no improvement in validation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We tried to overcome the challenge of Tiny ImageNet dataset under very high constraints and scarce computation resources. By designing two custom DenseNet models that are well suited for the challenge, we were able to achieve great results with Top-1 accuracy of 59.5% and 62.7% for our networks. We made our model as wide as possible and not much shallow to extract maximum features and prevent overfitting for our 64 resolution images. We used several data augmentation techniques for both models to increase our validation accuracy. To extract more information from the images, we used some promising techniques such as feeding varying resolutions of the input images and providing various ranges of Cyclic Learning Rate ranges. Implementing both of these techniques, we observed that for a few cycles it might have a short-term negative impact on the training but clearly shows long term beneficial effects evident from the jump of validation accuracy. We shall further endeavour on these techniques by applying them on other open datasets. At last, we manually inspected the misclassified images produced by our final model and gained valuable insights. For further studies, we shall fine-tune our model by custom tailoring the image augmentation parameters based on insights gained. We shall also focus on the architecture of the network and aim for achieving higher accuracy. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Standard vs ResNet vs DenseNet layers. (Image Source: CVPR 2017 Presentation Slides)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>DenseNet Architectures of Networks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Receptive Field of Networks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Images of Bullfrog</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Examples of Image Augmentations</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Loss curve of Network 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Loss curve of Network 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>visualise some interesting examples of top-1 errors made by our final model, as shown in the Figure 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 .</head><label>8</label><figDesc>Examples of Mislabelled Classes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Base_lr Max_lr</cell><cell>Step-</cell><cell>Start</cell><cell>End</cell><cell>Max.</cell></row><row><cell></cell><cell>size in</cell><cell>Epoch</cell><cell>Epoch</cell><cell>Val.</cell></row><row><cell></cell><cell>no. of</cell><cell></cell><cell></cell><cell>Accura</cell></row><row><cell></cell><cell>epochs</cell><cell></cell><cell></cell><cell>cy</cell></row><row><cell>1E-04 6E-04</cell><cell>6</cell><cell>0</cell><cell cols="2">24 53.26</cell></row><row><cell>1E-05 6E-05</cell><cell>6</cell><cell>24</cell><cell cols="2">48 60.86</cell></row><row><cell>1E-05 6E-05</cell><cell>4</cell><cell>48</cell><cell cols="2">72 61.74</cell></row><row><cell>1E-06 6E-06</cell><cell>2</cell><cell>72</cell><cell cols="2">84 62.43</cell></row><row><cell>1E-05 6E-05</cell><cell>2</cell><cell>84</cell><cell cols="2">96 61.83</cell></row><row><cell>1E-07 6E-07</cell><cell>2</cell><cell>96</cell><cell cols="2">108 62.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Class Name</cell><cell>Val. Accuracy</cell></row><row><cell>Plunger</cell><cell>25%</cell></row><row><cell>Umbrella</cell><cell>27.27%</cell></row><row><cell>Water Jug</cell><cell>30%</cell></row><row><cell>Bucket</cell><cell>30.43</cell></row><row><cell>Wooden Spoon</cell><cell>31.03%</cell></row><row><cell>Bannister</cell><cell>31.71%</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We want to acknowledge Rohan Shravan [13]   for his constant support and guidance. Colab Notebook of the work is available at https:// github.com/ZohebAbai/Tiny-ImageNet-Challenge</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385v1</idno>
		<title level="m">Deep Residual Learning for Image Recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993v5</idno>
		<title level="m">Densely Connected Convolutional Networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07049v2</idno>
		<title level="m">What are the Receptive, Effective Receptive, and Projective Fields of Neurons in Convolutional Neural Networks?</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Impact of Fully Connected Layers on Performance of Convolutional Neural N e t w o r k s f o r I m a g e C l a s s i</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H S</forename><surname>Basha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>S R Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pulabaigari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mukherjee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02771v2</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Abadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.00325v1</idno>
		<title level="m">Study of Residual Networks for Image Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Keshari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vatsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guided</forename><surname>Dropout</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03965v1</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01186v6</idno>
		<title level="m">Cyclical Learning Rates for T r a i n i n g N e u r a l N e t w o r k s</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
