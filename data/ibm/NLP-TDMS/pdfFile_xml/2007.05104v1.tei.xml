<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">n-Reference Transfer Learning for Saliency Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Minnesota</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computing</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">n-Reference Transfer Learning for Saliency Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Deep learning</term>
					<term>Saliency prediction</term>
					<term>n-shot transfer learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Benefiting from deep learning research and large-scale datasets, saliency prediction has achieved significant success in the past decade. However, it still remains challenging to predict saliency maps on images in new domains that lack sufficient data for data-hungry models. To solve this problem, we propose a few-shot transfer learning paradigm for saliency prediction, which enables efficient transfer of knowledge learned from the existing large-scale saliency datasets to a target domain with limited labeled samples. Specifically, few target domain samples are used as the reference to train a model with a source domain dataset such that the training process can converge to a local minimum in favor of the target domain. Then, the learned model is further fine-tuned with the reference. The proposed framework is gradient-based and model-agnostic. We conduct comprehensive experiments and ablation study on various source domain and target domain pairs. The results show that the proposed framework achieves a significant performance improvement. The code is publicly available at https://github.com/luoyan407/n-reference.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Saliency prediction is the task that aims to model human attention to predict where people look in the given image. Thanks to the power of deep neural networks <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b47">48]</ref> (DNNs), state-of-the-art saliency models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b49">50]</ref> perform very well in predicting human attention on naturalistic images. Behind the success of this task, a considerable amount of real-world images and corresponding human fixations fuels the process of training the data-hungry DNNs.</p><p>However, it is still difficult to predict saliency maps on images in novel domains, which has insufficient or few data to train saliency models with desired performance. As the time/money cost of collecting human fixations is prohibitive <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b19">20]</ref>, a feasible solution is to reuse the existing large-scale saliency datasets along with a few target domain samples to solve this problem. Along this line, we study how to transfer the knowledge learned from the existing large-scale saliency datasets to the target domain in a few-shot transfer learning setting.  <ref type="figure">Figure 1</ref>: The proposed n-reference transfer learning framework for saliency prediction. This framework aims to generate a better initialization with n reference samples from the target domain when training on the source domain, followed by fine-tuning to maximize knowledge transfer. It is based on the widely-used two-stage transfer learning framework (i.e., first training and then fine-tuning) and can easily adapt to other fine-tuning strategies</p><p>The necessity of few-shot transfer learning for saliency prediction lies in the nature of the task. Based on findings drawn from the behavioral experiments, the way that humans attend to regions is significantly affected by the scene context <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref>. The scene context is correlated to the image domain <ref type="bibr" target="#b42">[43]</ref>. In other words, each image from a specific domain could be representative of the others from the same domain to some degree, e.g., webpage images generally have a similar layout and design <ref type="bibr" target="#b40">[41]</ref>. In visual saliency study, existing datasets <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b41">42]</ref> in non-natural images domain are much smaller than the natural image ones <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b21">22]</ref>. Moreover, there are numerous images used in the subfields of medicine, biology, etc., which may not have any human fixation data yet. In this work, we assume that it is feasible and viable to collect human fixations on a small number of images to enable few-shot learning.</p><p>Compared to n-reference transfer learning for classification task <ref type="bibr" target="#b0">[1]</ref>, we focus on how to use very few target domain samples as references to learn a better initial model for fine-tuning. Moreover, there exists no such works for saliency prediction task. Models designed for classification may not work for saliency prediction. First, visual samples in existing classification tasks often contain limited visual concepts (i.e., pre-defined object classes), while objects of any class may appear in the images used for saliency prediction. In this sense, saliency prediction often handles images with higher diversity than the ones used for classification. Second, the output of classification models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44</ref>] is a discrete label, while saliency models <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b49">50]</ref> output a matrix of real numbers.</p><p>In this work, we follow the widely-used two-stage transfer learning framework <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b40">41]</ref>, i.e., first training and then fine-tuning, and propose a n-reference transfer learning framework. Specifically, in the training stage, it aims to use a small number of samples in the target domain as references to guide the knowledge learned from the source domain dataset. In this way, the learned model is adapted to the target domain and can be seen as a better initialization than the one trained without the references. The small number of target domain samples are used as references in both the training stage and as the training data in the fine-tuning stage. The proposed framework is shown in <ref type="figure">Fig. 1</ref>.</p><p>Mathematically, we use cosine similarity between two gradients to facilitate the reference aware model training, where the two gradients are respectively computed by samples in the source and target domain. If the angle between the two gradients is greater than 90 degrees, which implies that the directions of the model update are significantly different from each other, we optimize the gradient for the update to have smaller differences with the target-domain referenced gradient in cosine similarity. The intuition behind is to mimic the process of human learning with the reference sample, i.e., we adaptively learn from new information so that the newly absorbed knowledge will not contradict the observation of the reference samples <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b30">31]</ref>. The proposed framework is gradient-based and it is model-agnostic.</p><p>To comprehensively evaluate the proposed framework, we employ SALICON <ref type="bibr" target="#b19">[20]</ref> and MIT1003 <ref type="bibr" target="#b21">[22]</ref> as the source domain datasets (i.e., the knowledge sets), and WebSal <ref type="bibr" target="#b41">[42]</ref> and the art subset in CAT2000 <ref type="bibr" target="#b2">[3]</ref> as the target domain data. We randomly select 1, 5, or 10 samples from the target domain data as references. The contributions of this work can be summarized as follows:</p><p>-To study how humans perceive scenes from a partially explored domain, we propose a model-agnostic few-shot transfer learning paradigm to transfer knowledge from the source domain to the target domain. This is the first work that studies few-shot transfer learning for saliency prediction. -We propose a n-reference transfer learning framework to adaptively guide the training process. It guarantees that the knowledge learned with the source domain data would not contradict the references in the target domain, and produce a good initialization for further fine-tuning. The proposed framework is model-agnostic and can generally work with existing saliency models. -Comprehensive experiments show the proposed framework works on various combinations of source domain and target domain pairs. The experiment with various baseline models show that the proposed approach can efficiently transfer the knowledge from the source domain to the target domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Saliency Prediction</head><p>Saliency prediction aims to mimic human vision system to perceive interesting regions in a cluttered visual world. Itti et al. <ref type="bibr" target="#b18">[19]</ref> develop the first bottomup stimulus-driven saliency model. Since then, many works emerge to interpret visual saliency from various perspectives <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b51">52]</ref>. With the advent of DNNs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b47">48]</ref>, saliency prediction benefitted from data-driven discriminative features instead of relying on hand-crafted features <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref>. Recently, Cornia et al. <ref type="bibr" target="#b6">[7]</ref> introduce a network that integrates ResNet-50 <ref type="bibr" target="#b14">[15]</ref> and convolutional LSTMs to better attend to salient regions by iteratively refining the predictions. Yang et al. <ref type="bibr" target="#b49">[50]</ref> propose a dilated inception network (DINet) that stacks dilated convolutions with different dilation rates upon ResNet-50 to capture wider spatial information. It achieves state-of-the-art performance on various benchmarks. A widely-used practice to transfer the knowledge learned from image classification to saliency prediction is by using the weights pre-trained on ImageNet as model initialization <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b36">37]</ref>. In contrast, this work studies the few-shot cross-domain transfer learning problem, which takes place between two domains. Without loss of generality, we follow <ref type="bibr" target="#b32">[33]</ref> to adopt both ResNet-50 and DINet as the baseline models in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Few-shot Learning</head><p>Few-shot learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44]</ref> aims to study how to learn classifiers for unseen visual concepts with only a few samples per class. Lake et al. <ref type="bibr" target="#b27">[28]</ref> introduce a Bayesian program learning framework that can learn from one example for predicting character strokes. Matching networks <ref type="bibr" target="#b45">[46]</ref> use an attention mechanism that is analogous to a kernel density estimator so that it can learn from a few examples rapidly. Sung et al. <ref type="bibr" target="#b43">[44]</ref> propose a relation network to learn a transferable deep metric to compare the relation between the small number samples. In <ref type="bibr" target="#b28">[29]</ref>, Lee et al. study how to learn feature embeddings with a few samples that can minimize generalization error across a distribution of tasks. As the process of collecting human fixations is prohibitive <ref type="bibr" target="#b19">[20]</ref>, learning with very few samples is promising for saliency prediction to overcome the need for big data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Transfer Learning</head><p>Transfer learning, a.k.a. domain adaptation or domain transfer, is a paradigm to utilize training data in the source domain to solve the problem in the target domain <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b37">38]</ref>. In general, it can be seen as a two-stage learning framework, i.e., first training a model with source domain data and then fine-tuning the pre-trained model with target domain data. There are many DNN-based works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b48">49]</ref> that use this learning framework for classification tasks. Specifically, Guo et al. <ref type="bibr" target="#b12">[13]</ref> study and design a variant of the standard fine-tuning method for better transferability. However, it requires many training samples to determine whether it should fine-tune or freeze the parameters in a particular layer. Recently, Bäuml and Tulbure <ref type="bibr" target="#b0">[1]</ref> introduce a learning framework that transfers the knowledge learned from the source domain to the target domain with a few samples for tactile material classification. As saliency prediction is by nature class-agnostic, learning to predict human fixations with very few samples (e.g., ≤ 10) in the target domain is more challenging than the same paradigm for classification and has not been explored yet. Different from the aforementioned methods, we propose the first model-agnostic few-shot transfer learning framework for saliency prediction and conduct comprehensive study on multiple combinations of source domain datasets and target domain datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first formulate the problem and discuss its theoretical generalization bound. Then, we delve into the details of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>In this work, we denote the images as I S , I T ∈ R m and the human fixation maps as</p><formula xml:id="formula_0">y S , y T ∈ Y (Y ≡ [0, 1] m ⊆ R m ),</formula><p>where m is the dimensions of the image and S (T ) indicates the source (target) domain. In general, given an image I, the prediction function f : R m θ − → Y with parameters θ will predict z and then the loss function : Y × Y → R + will evaluate the discrepancy between z and y. Transfer learning for saliency prediction task can be considered as a two-stage learning problem. First, the model's parameters are learned with the source domain data through the training process, i.e.,</p><formula xml:id="formula_1">θ TR = arg min θ 1 |D S | (Ii,yi)∈D S (f (I i ; θ), y i )| θ0<label>(1)</label></formula><p>where D S is the source domain dataset, |D S | is the number of the samples, and TR stands for training. θ 0 are the initialized parameters and the model is usually pre-trained on ImageNet <ref type="bibr" target="#b9">[10]</ref>. Then, θ TR is taken as the initialization for further fine-tuning on the target domain data, i.e.,</p><formula xml:id="formula_2">θ * FT = arg min θ 1 |D T | (Ii,yi)∈D T (f (I i ; θ), y i )| θ0=θ TR<label>(2)</label></formula><p>In this work, we aim to learn a better initialization by the first stage objective (1), which is in favor of the target domain data. Such initialized parameters (i.e., θ TR ) are expected to further achieve better performance by fine-tuning on D T . To this end, we introduce a referencing mechanism that allows the training process fed with D S to reference the model update w.r.t. the referenced samples</p><formula xml:id="formula_3">(I R , y R ) ∈ D T (|D S | |D T |).</formula><p>Mathematically, this can be formulated as</p><formula xml:id="formula_4">θ TR−Ref = arg min θ 1 |D S | (Ii,yi)∈D S (I R j ,y R j )∈D T (f Ref (I i ; θ, (I R j , y R j )), y i )| θ0<label>(3)</label></formula><p>where We denote the resulting parameters as θ FT|Ref .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generalization Bound of Saliency Prediction</head><p>Here, we discuss the theoretical guarantee of saliency prediction. Following the setting used in <ref type="bibr" target="#b33">[34]</ref>, given training data (</p><formula xml:id="formula_5">I 1 , y 1 ), (I 2 , y 2 ), . . . ∈ X × Y, where Y ∈ [0, 1] m ⊆ R m , we use the L p loss, i.e., p : Y ×Y → R + , p ≥ 1. The prediction function f (·; θ)</formula><p>is denoted as f (·) for simplicity. I is drawn i.i.d. according to the unknown distribution D and y = f * (I) where f * is the target labeling function. Saliency prediction can be considered as a mathematical problem that finds</p><formula xml:id="formula_6">hypothesis f : R m → [0, 1] m in a set H with small generalization error w.r.t. f * , R D (f ) = E I∼D [ (f (I), f * (I))].</formula><p>In practice, as D is unknown, we use empirical error for approximation, i.e.,</p><formula xml:id="formula_7">R D (f ) = 1 |D| |D| i=1 (f (I i ), y i ),</formula><p>where |D| is the sample number in dataset D for training. We introduce the generalization bound of saliency prediction as follows. The proof is provided in the supplementary document.</p><p>Theorem 1 (Saliency generalization bound). Denote H as a finite hypothesis set. Given p and y ∈ [0, 1] m , for any δ &gt; 0, with probability at least 1 − δ, the following inequality holds for all f ∈ H:</p><formula xml:id="formula_8">|R D (f ) −R D (f )| ≤ m 1 p log |H| + log 2 δ 2|D|</formula><p>Remark 1. Theorem 1 shows how the training set scale influences the generalization bound. When |D| tends towards infinity, R D (f ) ≡R D (f ). This conforms to the general intuition that it can train a more general model with more data. Contrarily, when |D| = 1, it leads to the largest bound for |R D (f ) −R D (f )|. Moreover, it demonstrates the task is challenging with small number of samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overall Framework</head><p>In this subsection, we introduce the few-shot transfer learning framework that solves the objective function <ref type="formula" target="#formula_2">(2)</ref> and <ref type="formula" target="#formula_4">(3)</ref>. The overall workflow of the proposed n-reference transfer learning framework is shown in <ref type="figure">Fig. 2</ref>.</p><p>Similar to classification model <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b47">48]</ref>, state-of-the-art saliency models tend to be large. For example, DINet <ref type="bibr" target="#b49">[50]</ref> and SAM-ResNet-50 <ref type="bibr" target="#b6">[7]</ref> consist of 26M and 70M parameters, respectively. Therefore, instead of inefficiently applying the proposed framework to the whole saliency model, we only apply it to a few downstream layers which are close to the output. The downstream layers produce discriminative features used for prediction with a small number of parameters,  <ref type="figure">Figure 2</ref>: Proposed n-reference transfer learning framework. Note that we assume that only very few samples from the target domain are available, i.e., n ≤ 10 and it makes the transfer learning process more cost-effective. Consequently, we split the model into two parts, i.e., the model body θ body and the model head θ head . This split would be only effective in the training stage and the two parts will be integrated again as they always are in the inference stage. Note that the split is flexible. The effective scope of the proposed framework could cover the whole model and the model body would correspondingly turn to be an empty set. As we only focus on θ head , we simplify it as θ in the following text.</p><p>In the forward propagation, as the training image I S ∈ D S and the reference image I R ∈ D T are fed to the model body, the discriminative feature x S and x R are generated, respectively. Then, the model head would take x S and x R as input to produce prediction z S and z R , respectively. Specifically, z S = f (x S ; θ). A similar process applies to z R . The loss function is used to compute the distance between z S and y S (and between y R and y R as well). In the backward propagation, two gradients are computed by the chain rule</p><formula xml:id="formula_9">∂ S ∂θ = ∂ (f (x S ; θ), y S ) ∂z S ∂z S ∂θ , ∂ R ∂θ = ∂ (f (x R ; θ), y R ) ∂z R ∂z R ∂θ .</formula><p>Specifically, ∂ S ∂θ indicates the model update towards a local minimum θ * (S) which is learned from the samples from D S , while ∂ R ∂θ indicates the model update towards a local minimum θ * (T ) which is learned from the samples from D T .</p><p>As shown in <ref type="figure">Fig. 2</ref>, θ head are updated by the proposed reference process and θ body are updated with the standard gradients in the training stage. During fine-tuning, θ head and θ body are updated with the standard gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Reference Process</head><p>Here, we delve into the formulation of the proposed reference process <ref type="figure">(Fig. 3)</ref>. The cosine similarity between ∂ S ∂θ and ∂ R ∂θ can evaluate the difference of the two  <ref type="figure">Figure 3</ref>: The reference process computing the gradient that better adapts to the target domain data. θ * (S) is a local minimum trained by sufficient source domain samples, while θ * (T ) is a local minimum trained by sufficient target domain samples. Given a pre-defined threshold , if the cosine similarity between the gradient ( ∂ S ∂θ ) generated by the source sample and the gradient ( ∂ R ∂θ ) generated by the reference sample is smaller than , it will compute a corrected gradient by optimizing the cosine similarity. It retains ∂ S ∂θ otherwise gradients. Accordingly, we pre-define a threshold to determine if the difference is considered as minor and the update with ∂ S ∂θ will be close to both θ * (S) and θ * (T ) . If the difference is significant, the proposed reference process will adjust ∂ S ∂θ so that it will move more towards θ * (T ) . This process is defined as follows</p><formula xml:id="formula_10">g = arg max g cos(g, ∂ R ∂θ ) − λ g 2 2 | g0= ∂ S ∂θ if cos( ∂ S ∂θ , ∂ R ∂θ ) &lt; , ∂ S ∂θ otherwise,<label>(4)</label></formula><p>where λ is the regularization parameter and cos(·, ·) is the cosine similarity, i.e., cos(a, b) = a b/|a||b| (a and b are the input vectors), andg is the output gradient. The embedded optimization problem in Eq. (4) aims to find ag, which is with an initial point g 0 = ∂ S ∂θ , to be consistent with the reference gradient ∂ R ∂θ in terms of cosine similarity. In other words, the reference gradient ∂ R ∂θ provides a reference so thatg is able to be aware of a rough direction towards the underlying θ * (T ) . In this way, the knowledge learned from D S is transferred to the target domain. We solve the embedded optimization problem with the gradient ascent method because our goal is to maximize the cosine similarity betweeng and ∂ R ∂θ . Subsequently, θ would be updated withg, i.e., θ ← θ − ηg, where η is a learning rate. Note that ∂ S ∂θ is generated by randomly selected training samples and is the initial point forg. As a result, the process of optimizing cosine similarity in the training stage is almost surely stochastic. This can effectively preventg from overfitting ∂ R ∂θ . The proposed reference process yieldsg to update the model so that the parameters are close to the underlying θ * (T ) . As θ is learned with the references from the target domain, by the chain rule, ∂ S ∂θ body = ∂ S ∂x S ∂x S ∂θ body and ∂ S ∂x S can be considered as a function of θ. So θ b will be affected by the references as well.</p><p>As the number of references is expected to be far smaller than the training data, we follow a similar idea of the stochastic process to randomly draw a reference from the reference pool at each iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we introduce the experimental protocol, present the experimental results, and then have a discussion about the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. We adopt the large-scale saliency prediction dataset SALICON <ref type="bibr" target="#b19">[20]</ref> (the 2017 version) and the MIT1003 <ref type="bibr" target="#b21">[22]</ref> as the source domain datasets. Accordingly, we adopt WebSal <ref type="bibr" target="#b41">[42]</ref> and the art subset in CAT2000 <ref type="bibr" target="#b2">[3]</ref> as the target domain datasets. Specifically, SALICON consists of 10000 real-world images, MIT1003 consists of 1003 natural scene images, and WebSal consists of 149 webpage screenshots. CAT2000 includes 20 categories and each category has 100 images. Art is one of the most common categories, whose images are the pictures of human-made works, like the paintings, handcrafts, and etc.</p><p>Baseline Models. To study how well the proposed method would generalize to different models, we use two baseline models, i.e., DINet <ref type="bibr" target="#b50">[51]</ref> and ResNet-50 <ref type="bibr" target="#b14">[15]</ref>.</p><p>Settings. There are three dimensions to the experiments in this work, i.e., source domain samples, baseline model, and target domain samples. Specifically, the baseline model is trained with the source domain samples. The learned model is further fine-tuned with the target domain samples. This setting is similar in the case of the proposed method. For convenience, we denote the setting as a combination of the initials of the datasets or the models, e.g., S, D, W indicates that we use SALICON as the source domain dataset, DINet as the baseline model, and WebSal as the target domain dataset. Similarly, we use initials M, R, and A to represent MIT1003, ResNet-50, and Art, respectively.</p><p>To understand how the number of references affects the performance, we evaluate the proposed method with n = 1, 5, 10. Moreover, to provide a benchmark of the performance w.r.t. more references, a paradigm that is similar to 3-fold cross validation is applied with more references. For instance, given WebSal as the target domain datasets, we divide it into three subsets, which contain 50, 50, and 49 images, respectively. Then, we alternately use any two subsets as the reference samples and the rest as the validation set. The process is repeated 3 times. We denote the results of this process as an empirical upper bound.</p><p>Evaluation Metrics. We adopt the common metrics used in <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b19">[20]</ref>, i.e., normalized scanpath saliency (NSS) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b39">40]</ref>, area under curve (AUC) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21]</ref>, and correlation coefficient (CC) <ref type="bibr" target="#b35">[36]</ref>. Higher scores indicate better performance. We use the public implementation 3 provided by <ref type="bibr" target="#b19">[20]</ref>. Each experiment is repeated 10 times and the mean metric scores are reported. Due to the space limits, we report the corresponding standard deviation in the supplementary document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Training Scheme</head><p>We follow the widely-used two-stage transfer learning framework <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b48">49]</ref>, i.e., first train a model with the source domain data and fine-tune with the target domain data. We denote the trained model as TR and the fine-tuned model as FT. In the proposed framework, the n-reference training stage first trains a model with the source domain data and n target domain references (denoted as TR−Ref), and then further fine-tune with the references (denoted as FT|Ref).</p><p>Regarding the experimental details, we follow DINet <ref type="bibr" target="#b49">[50]</ref> to use Adam optimizer <ref type="bibr" target="#b22">[23]</ref> with learning rate η = 5e-5 and weight decay 1e-4. We use batch size 10 for all the experiments. The number of epochs is 10 and we decrease the learning rate for every 3 epochs by multiplying with 0.2. In TR−Ref, we randomly sample 10 training data without replacement as the training sample at each iteration. Meanwhile, we randomly sample n r references with replacement as the reference. In this way, the difference between the number of training samples and references will not cause a problem. n r are 1, 3 and 5 in the experiments with n = 1, 5, 10, respectively. This process is the same for the one of FT. We select the model with the best performance over epochs for further fine-tuning. The normalized l 1 loss <ref type="bibr" target="#b49">[50]</ref> is used and the threshold is set to 0 for all the experiments. We implement the proposed framework with PyTorch <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance</head><p>The experimental results with the following settings, i.e., S, D, W , S, R, W , M, D, W , and S, D, A , are shown in <ref type="table">Table 1</ref>. Within setting S, D, W , the proposed framework (i.e., FT|Ref) achieves better performance than FT over all metrics. Particularly, as the number of references increases, the consequently trained models provide better initializations for fine-tuning. In other words, FT|Ref yields better performance when the dependent trained model uses more reference samples. Using a different baseline model, we experiment it with setting S, R, W which FT|Ref achieves consistent improvement. Moreover, using DINet as the baseline model leads to better performance than using ResNet-50.</p><p>We study how well the proposed framework generalizes to different target domain data using setting S, D, A . As seen in <ref type="table">Table 1</ref>, similar performance improvement can be found, which implies the proposed framework can generalize to a different target domain. Furthermore, the study with MIT1003 as the source domain dataset, i.e., setting M, D, W , shows consistent improvement. The overall performance within setting M, D, W is slightly lower than the one within setting S, D, W . This implies that SALICON is more efficient than MIT1003 to transfer the knowledge to WebSal. On the other hand, models trained with one sample in target domain have noticeable gaps w.r.t. EUB, and is improved with more training samples. This is consistent with the implication of Theorem 1.</p><p>We perform paired t-test and permutation test over images within setting S, D, W to evaluate the difference between TR−Ref and FT|Ref. Both corresponding p are less than 0.001. This implies that TR−Ref significantly provides a good initialization to FT|Ref to yield high performance. To validate the effect <ref type="table">Table 1</ref>: Performance with various settings of source, model, target . Here, S is SALICON, M is MIT1003, W is WebSal, A is Art subset, D is DINet, and R is ResNet. ↑ implies that a higher score is better. The score in bold font indicates the best result under the respective metric. We report the mean score from 10 runs for conventional training (i.e., n = 0) and the proposed method. The empirical upper bound (EUB) is generated by <ref type="bibr">3-</ref>  The results of c and d are generated with n = 1. determines whether the gradient needs to be corrected or not (see <ref type="figure">Fig. 3</ref>). Comparing to TR, FT, and FT|Ref, only TR−Ref is able to evaluate the cosine similarity between the samples from the source domain and target domain (see <ref type="figure">Fig. 2</ref>) of knowledge transfer in saliency prediction, we conduct the experiment where models are learned using only the target domain samples, i.e., FT w/o TR in <ref type="table">Table 1</ref>. We set n = 10 as n = 1, 5 will yield much worse performance. In all settings, the performance of FT w/o TR significantly drops when compare to FT|Ref. These results are even lower than TR and FT, which indicate the importance of efficient initialization with a source domain dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Analysis</head><p>We study the influences of the number of references, the threshold , and the layers updated by the proposed framework. All analysis are within setting S, D, W , where the mean score and standard deviation from 3 runs are reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablation Study</head><p>Effect of Number of References. As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, as the number of references increases, the performance of TR−Ref keeps flat or even slightly drops, but the performance of FT|Ref is significantly improved. This implies that the proposed reference process with more reference samples can yield better initialization for fine-tuning. Moreover, the average cosine similarity is increased with (a) (b) (c) <ref type="figure">Figure 5</ref>: Ablation study of downstream layers updated within setting S, D, W with n = 1. Note that when 0 layer is updated, it turns to be TR and FT more references. This implies that the number of references is helpful to adapt the training process with source domain data to the target domain data.</p><p>Effect of Threshold . We experiment with the proposed framework with n = 1, which is more representative and challenging than cases with more references, with various thresholds. An interesting observation in <ref type="figure" target="#fig_3">Fig. 4c</ref> is that although = 1 achieves best performance on TR−Ref, it deteriorates the performance of FT|Ref. This shows that when = 1, all the gradients at each iteration need to be corrected because the cosine similarity between any two gradients is equal or less than 1. As a result, the reference process enforces the training process to overfit the reference samples. This can be verified in <ref type="figure" target="#fig_3">Fig. 4d</ref> where the average cosine similarity is roughly increased as is increasing.</p><p>Effect of Updated Layers. To understand the effect of layers updated by the proposed 1-reference transfer learning, we experiment with various downstream layers. Consequently, the performance is shown in <ref type="figure">Fig. 5a</ref>, while the number of parameters and the computational cost are reported in <ref type="figure">Fig. 5b</ref> and <ref type="figure">Fig. 5c</ref>, respectively. The layers are downstream layers, which are close to the output. When 0 layer is updated, TR−Ref and FT|Ref are equivalent to TR and FT, respectively. The baseline model in this experiment is DINet. <ref type="figure">Fig. 5a</ref> shows that using the last 2 layers achieves slightly better performance in NSS than using the other numbers of the last layers. However, it takes 69 milliseconds longer in the training process than using the last layers. In light of the trade-off, we use the last layer of the baseline model in Section 4.  the source domain, which are based on natural scene images, to subtly identify salience in the new domain. Taking the example in the first row, FT|Ref predicts that the people is salient, which takes the learned knowledge into account, whereas FT predicts that the people is less salient than the text. <ref type="figure" target="#fig_5">Fig. 7</ref> shows more references lead to better prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This work studies how to leverage the knowledge learned from a source domain that has adequate images and corresponding human fixations and very few samples (i.e., references) from a new domain (i.e., target domain) to predict saliency maps in the target domain. We propose an n-reference transfer learning framework to guide the training process to converge to a local minimum in favor of the target domain. The proposed framework is gradient-based and model-agnostic. Comprehensive experiments and ablation studies to evaluate the proposed framework are reported. Results show the effectiveness of the framework with a significant performance improvement. 1908711, 1849107, in part by the University of Minnesota Department of Computer Science and Engineering Start-up Fund (QZ), and in part supported by the National Research Foundation, Singapore under its Strategic Capability Research Centres Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>r kn ow led ge T ra ns fe r kn ow le dg e Input Input ImageNet pre-trained model s a m p le s a r e r e f e r r e d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The effect of the number of references (a, b) and threshold (c, d) on NSS metric and average cosine similarity within setting S, D, W . n = 0 indicates that no reference sample is used. Hence, TR−Ref and FT|Ref turn to be TR and FT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 Figure 6 :</head><label>66</label><figDesc>shows the comparison between the predicted saliency maps generated by TR, TR−Ref, FT, and FT|Ref. It can be observed that with the reference process, the proposed framework efficiently leverages the knowledge learned from Qualitative results with human fixations and maps generated by the models trained by the four procedures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Qualitative results w.r.t. different n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>TR−Ref indicates the training process references target domain samples when updating the model. f Ref is a variant of f which has the same forward propagation as f but has more complicated backward propagation. θ TR−Ref is taken as the initialization in the second stage objective (2) for further fine-tuning.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>fold cross validation on the target domain. The experimental details are provided in Section 4.1 and 4.2</figDesc><table><row><cell></cell><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>S, D, W</cell><cell></cell><cell></cell><cell>S, R, W</cell></row><row><cell></cell><cell></cell><cell>NSS↑</cell><cell>AUC↑</cell><cell>CC↑</cell><cell>NSS↑</cell><cell>AUC↑</cell><cell>CC↑</cell></row><row><cell>FT w/o TR</cell><cell>n = 10</cell><cell cols="5">0.8252 0.7430 0.3635 0.8846 0.7455 0.3852</cell></row><row><cell>TR</cell><cell>n = 0</cell><cell cols="5">1.3330 0.7796 0.5515 1.2950 0.7749 0.5358</cell></row><row><cell>TR−Ref</cell><cell>n = 1</cell><cell cols="5">1.3621 0.7848 0.5628 1.3569 0.7864 0.5611</cell></row><row><cell>FT</cell><cell>n = 1</cell><cell cols="5">1.4731 0.8005 0.5976 1.3722 0.7923 0.5627</cell></row><row><cell>FT|Ref</cell><cell>n = 1 (c)</cell><cell cols="5">1.5077 0.8051 0.6121 1.4272 0.7983 0.5817 (d)</cell></row><row><cell>TR−Ref</cell><cell>n = 5</cell><cell cols="5">1.3683 0.7874 0.5659 1.3535 0.7837 0.5593</cell></row><row><cell>FT</cell><cell>n = 5</cell><cell cols="5">1.5803 0.8161 0.6355 1.5043 0.8131 0.6139</cell></row><row><cell>FT|Ref</cell><cell>n = 5</cell><cell cols="5">1.6085 0.8200 0.6468 1.5491 0.8149 0.6281</cell></row><row><cell>TR−Ref</cell><cell>n = 10</cell><cell cols="5">1.3647 0.7839 0.5633 1.3583 0.7857 0.5612</cell></row><row><cell>FT</cell><cell>n = 10</cell><cell cols="5">1.6290 0.8247 0.6531 1.5164 0.8103 0.6200</cell></row><row><cell>FT|Ref</cell><cell>n = 10</cell><cell cols="5">1.6439 0.8276 0.6605 1.5829 0.8143 0.6414</cell></row><row><cell>TR−Ref</cell><cell>EUB</cell><cell cols="5">1.3822 0.7910 0.5708 1.3626 0.7864 0.5645</cell></row><row><cell>FT</cell><cell>EUB</cell><cell cols="5">1.8695 0.8488 0.7389 1.8325 0.8462 0.7275</cell></row><row><cell>FT|Ref</cell><cell>EUB</cell><cell cols="5">1.8831 0.8494 0.7442 1.8500 0.8480 0.7321</cell></row><row><cell></cell><cell></cell><cell></cell><cell>M, D, W</cell><cell></cell><cell></cell><cell>S, D, A</cell></row><row><cell></cell><cell></cell><cell>NSS↑</cell><cell>AUC↑</cell><cell>CC↑</cell><cell>NSS↑</cell><cell>AUC↑</cell><cell>CC↑</cell></row><row><cell>FT w/o TR</cell><cell>n = 10</cell><cell cols="5">0.8252 0.7430 0.3635 1.2183 0.8339 0.5161</cell></row><row><cell>TR</cell><cell>n = 0</cell><cell cols="5">1.3905 0.7991 0.5700 1.5172 0.8225 0.6003</cell></row><row><cell>TR−Ref</cell><cell>n = 1</cell><cell cols="5">1.4405 0.8085 0.5902 1.5651 0.8287 0.6211</cell></row><row><cell>FT</cell><cell>n = 1</cell><cell cols="5">1.4410 0.8023 0.5784 1.6255 0.8324 0.6449</cell></row><row><cell>FT|Ref</cell><cell>n = 1</cell><cell cols="5">1.4575 0.8070 0.5838 1.6523 0.8380 0.6564</cell></row><row><cell>TR−Ref</cell><cell>n = 5</cell><cell cols="5">1.4452 0.8064 0.5908 1.5870 0.8304 0.6274</cell></row><row><cell>FT</cell><cell>n = 5</cell><cell cols="5">1.5795 0.8217 0.6395 1.8049 0.8480 0.7185</cell></row><row><cell>FT|Ref</cell><cell>n = 5</cell><cell cols="5">1.6136 0.8269 0.6515 1.8314 0.8503 0.7274</cell></row><row><cell>TR−Ref</cell><cell>n = 10</cell><cell cols="5">1.4330 0.8060 0.5872 1.5704 0.8288 0.6204</cell></row><row><cell>FT</cell><cell>n = 10</cell><cell cols="5">1.6462 0.8261 0.6660 1.8325 0.8474 0.7288</cell></row><row><cell>FT|Ref</cell><cell>n = 10</cell><cell cols="5">1.6691 0.8283 0.6730 1.8584 0.8503 0.7366</cell></row><row><cell>TR−Ref</cell><cell>EUB</cell><cell cols="5">1.4402 0.8087 0.5905 1.5980 0.8340 0.6331</cell></row><row><cell>FT</cell><cell>EUB</cell><cell cols="5">1.8450 0.8466 0.7330 2.1595 0.8636 0.8464</cell></row><row><cell>FT|Ref</cell><cell>EUB</cell><cell cols="5">1.8507 0.8478 0.7344 2.1874 0.8649 0.8519</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/NUS-VIP/salicon-evaluation</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments. This research was funded in part by the NSF under Grants</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep n-shot transfer learning for tactile material classification with a flexible pressure-sensitive skin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bäuml</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulbure</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4262" to="4268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning of representations for unsupervised and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML workshop on unsupervised and transfer learning</title>
		<meeting>ICML workshop on unsupervised and transfer learning</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="17" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CAT2000: A large scale fixation dataset for boosting saliency research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2015 workshop on</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="69" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">What do different evaluation metrics tell us about saliency models?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="740" to="757" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A deep multi-level network for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3488" to="3493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting human eye fixations via an lstm-based saliency attentive model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="5142" to="5154" />
			<date type="published" when="2018-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A comprehensive survey on domain adaptation for visual applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="35" />
		</imprint>
	</monogr>
	<note>Domain Adaptation in Computer Vision Applications</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain adaptation for statistical classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Marcu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="101" to="126" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ImageNet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="594" to="611" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Borrowing treasures from the wealthy: Deep transfer learning through selective joint fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1086" to="1095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">SpotTune: Transfer learning through adaptive fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rosing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4805" to="4814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image signature: Highlighting sparse salient regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="194" to="201" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Realistic Avatar Eye and Head Animation Using a Neurobiological Model of Visual Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dhavale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SPIE 48th Annual International Symposium on Optical Science and Technology</title>
		<meeting>SPIE 48th Annual International Symposium on Optical Science and Technology</meeting>
		<imprint>
			<date type="published" when="2003-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SALICON: Saliency in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1072" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A benchmark of computational models of saliency to predict human fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MIT Technical Report</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">DeepFix: A fully convolutional neural network for predicting human eye fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4446" to="4456" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep gaze i: Boosting saliency prediction with feature maps trained on imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kümmerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Attention transfer from web images for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Multimedia. pp</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">GradMix: Multi-source transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Revisiting local descriptor based image-to-class measure for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7260" to="7268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Direction concentration learning: Enhancing congruency in machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Foundations of Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scene context guides eye movements during visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Neider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Zelinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="614" to="621" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Empirical validation of the saliency-based model of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ouerhani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Von Wartburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hugli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Müri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Letters on Computer Vision and Image Analysis</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="13" to="24" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Ferrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Torres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01081</idno>
		<title level="m">SalGAN: Visual saliency prediction with generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Attention links sensing to recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Rothenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="114" to="126" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Two-stage transfer learning of end-to-end convolutional neural networks for webpage saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Science and Big Data Engineering</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="316" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Webpage saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8695</biblScope>
			<biblScope unit="page" from="33" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Data-driven visual similarity for cross-domain image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">154</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Castelhano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">766</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Five factors that guide attention in visual search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Wolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Horowitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Human Behaviour</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">58</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5987" to="5995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Explicit inductive bias for transfer learning with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xuhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Davoine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2830" to="2839" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A dilated inception network for visual saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Feature selection for multimedia analysis by sharing information among multiple tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="661" to="669" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Saliency detection: A boolean map approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
