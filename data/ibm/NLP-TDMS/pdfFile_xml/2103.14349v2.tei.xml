<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DAGN: Discourse-Aware Graph Network for Logical Reasoning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinya</forename><surname>Huang</surname></persName>
							<email>yinya.huang@hotmail</email>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Campus of Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Fang</surname></persName>
							<email>mfang@tencent.com</email>
							<affiliation key="aff1">
								<orgName type="department">Tencent Robotics X</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cao</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
							<email>lwwang@cse.cuhk.edu.hk</email>
							<affiliation key="aff3">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xdliang328@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Shenzhen Campus of Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DAGN: Discourse-Aware Graph Network for Logical Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent QA with logical reasoning questions requires passage-level relations among the sentences. However, current approaches still focus on sentence-level relations interacting among tokens. In this work, we explore aggregating passage-level clues for solving logical reasoning QA by using discourse-based information. We propose a discourse-aware graph network (DAGN) that reasons relying on the discourse structure of the texts. The model encodes discourse information as a graph with elementary discourse units (EDUs) and discourse relations, and learns the discourseaware features via a graph network for downstream QA tasks. Experiments are conducted on two logical reasoning QA datasets, Re-Clor and LogiQA, and our proposed DAGN achieves competitive results. The source code is available at https://github.com/</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A variety of QA datasets have promoted the development of reading comprehensions, for instance, SQuAD <ref type="bibr" target="#b13">(Rajpurkar et al., 2016)</ref>, HotpotQA <ref type="bibr" target="#b22">(Yang et al., 2018)</ref>, <ref type="bibr">DROP (Dua et al., 2019)</ref>, and so on. Recently, QA datasets with more complicated reasoning types, i.e., logical reasoning, are also introduced, such as ReClor <ref type="bibr" target="#b23">(Yu et al., 2020)</ref> and LogiQA . The logical questions are taken from standardized exams such as GMAT and LSAT, and require QA models to read complicated argument passages and identify logical relationships therein. For example, selecting a correct assumption that supports an argument, or finding out a claim that weakens an argument in a passage. Such logical reasoning is beyond the capability of most of the previous QA models which focus on reasoning with entities or numerical keywords.</p><p>A main challenge for the QA models is to uncover the logical structures under passages, such as identifying claims or hypotheses, or pointing out flaws in arguments. To achieve this, the QA models should first be aware of logical units, which can be sentences or clauses or other meaningful text spans, then identify the logical relationships between the units. However, the logical structures are usually hidden and difficult to be extracted, and most datasets do not provide such logical structure annotations.</p><p>An intuitive idea for unwrapping such logical information is using discourse relations. For instance, as a conjunction, "because" indicates a causal relationship, whereas "if" indicates a hypothetical relationship. However, such discourse-based information is seldom considered in logical reasoning tasks. Modeling logical structures is still lacking in logical reasoning tasks, while current opened methods use contextual pre-trained models <ref type="bibr" target="#b23">(Yu et al., 2020)</ref>. Besides, previous graph-based methods <ref type="bibr" target="#b14">(Ran et al., 2019;</ref><ref type="bibr">Chen et al., 2020a</ref>) that construct entity-based graphs are not suitable for logical reasoning tasks because of different reasoning units.</p><p>In this paper, we propose a new approach to solve logical reasoning QA tasks by incorporating discourse-based information. First, we construct discourse structures. We use discourse relations from the Penn Discourse TreeBank 2.0 (PDTB 2.0)   Theoretically, analog systems are superior to digital systems. A signal in a pure analog system can be infinitely detailed, while digital systems cannot produce signals that are more precise than their digital units. With this theoretical advantage there is a practical disadvantage. Since there is no limit on the potential detail of the signal, the duplication of an analog representation allows tiny variations from the original, which are errors.</p><p>Context:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Question:</head><p>The statements above, if true, most strongly support which one of the following?</p><p>Digital systems are the best information systems because error cannot occur in the emission of digital signals. Option <ref type="bibr">(1)</ref> (2)</p><p>Context Option <ref type="figure">Figure 1</ref>: The architecture of our proposed method with an example below.</p><p>soning QA datasets, ReClor and LogiQA. Our main contributions are three-fold:</p><p>• We propose to construct logic graphs from texts by using discourse relations as edges and elementary discourse units as nodes.</p><p>• We obtain discourse features via graph neural networks to facilitate logical reasoning in QA models.</p><p>• We show the effectiveness of using logic graph and feature enhancement by noticeable improvements on two datasets, ReClor and LogiQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>Our intuition is to explicitly use discourse-based information to mimic the human reasoning process for logical reasoning questions. The questions are in multiple choices format, which means given a triplet (context, question, answer options), models answer the question by selecting the correct answer option. Our framework is shown in <ref type="figure">Figure 1</ref>. We first construct a discourse-based logic graph from the raw text. Then we conduct reasoning via graph networks to learn and update the discoursebased features, which are incorporated with the contextual token embeddings for downstream answer prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Graph Construction</head><p>Our discourse-based logic graph is constructed via two steps: delimiting text into elementary discourse units (EDUs) and forming the graph using their relations as edges, as illustrated in <ref type="figure">Figure 1</ref>(1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discourse Units Delimitation</head><p>It is studied that clause-like text spans delimited by discourse relations can be discourse units that reveal the rhetorical structure of texts <ref type="bibr" target="#b6">(Mann and Thompson, 1988;</ref>. We further observe that such discourse units are essential units in logical reasoning, such as being assumptions or opinions. As the example shown in <ref type="figure">Figure 1</ref>, the "while" in the context indicates a comparison between the attributes of "pure analog system" and that of "digital systems". The "because" in the option provides evidence "error cannot occur in the emission of digital signals" to the claim "digital systems are the best information systems". We use PDTB 2.0  to help drawing discourse relations. PDTB 2.0 contains discourse relations that are manually annotated on the 1 million Wall Street Journal (WSJ) corpus and are broadly characterized into "Explicit" and "Implicit" connectives. The former apparently presents in sentences such as discourse adverbial "instead" or subordinating conjunction "because", whereas the latter are inferred by annotators between successive pairs of text spans split by punctuation marks such as "." or ";". We simply take all the "Explicit" connectives as well as common punctuation marks to form our discourse delimiter library (details are given in Appendix A), with which we delimit the texts into EDUs. For each data sample, we segment the context and options, ignoring the question since the question usually does not carry logical content.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discourse Graph Construction</head><p>We define the discourse-based graphs with EDUs as nodes, the "Explicit" connectives as well as the punctuation marks as two types of edges. We assume that each connective or punctuation mark connects the EDUs before and after it. For example, the option sentence in <ref type="figure">Figure 1</ref> is delimited into two EDUs, EDU 7 ="digital systems are the best information systems" and EDU 8 ="error cannot occur in the emission of digital signals" by the connective r ="because". Then the returned triplets are (EDU 7 , r, EDU 8 ) and (EDU 8 , r, EDU 7 ). For each data sample with the context and multiple answer options, we separately construct graphs corresponding to each option, with EDUs in the same context and every single option. The graph for the single option k is denoted by G k = (V k , E k ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Discourse-Aware Graph Network</head><p>We present the Discourse-Aware Graph Network (DAGN) that uses the constructed graph to exploit discourse-based information for answering logical questions. It consists of three main components: an EDU encoding module, a graph reasoning module, and an answer prediction module. The former two are demonstrated in <ref type="figure">Figure 1</ref>(2), whereas the final component is in <ref type="figure">Figure 1</ref>(3).</p><p>EDU Encoding An EDU span embedding is obtained from its token embeddings. There are two steps. First, similar to previous works <ref type="bibr" target="#b23">(Yu et al., 2020;</ref>, we encode such input sequence "&lt;s&gt; context &lt;/s&gt; question || option &lt;/s&gt;" into contextual token embeddings with pre-trained language models, where &lt;s&gt; and &lt;/s&gt; are the special tokens for RoBERTa  model, and || denotes concatenation. Second, given the token embedding sequence {t 1 , t 2 , ..., t L }, the n-th EDU embedding is obtained by e n = l∈Sn t l , where S n is the set of token indices belonging to n-th EDU.</p><p>Graph Reasoning After EDU encoding, DAGN performs reasoning over the discourse graph. Inspired by previous graph-based models <ref type="bibr" target="#b14">(Ran et al., 2019;</ref><ref type="bibr">Chen et al., 2020a)</ref>, we also learn graph node representations to obtain higher-level features. However, we consider different graph construction and encoding. Specifically, let G k = (V k , E k ) denote a graph corresponding to the k-th option in answer choices. For each node</p><formula xml:id="formula_1">v i ∈ V, the node embedding v i is initialized with the correspond- ing EDU embedding e i . N i = {j|(v j , v i ) ∈ E k }</formula><p>indicates the neighbors of node v i . W r ji is the adjacency matrix for one of the two edge types, where r E indicates graph edges corresponding to the explicit connectives, and r I indicates graph edges corresponding to punctuation marks.</p><p>The model first calculates weight α i for each node with a linear transformation and a sigmoid function</p><formula xml:id="formula_2">α i = σ(W α (v i ) + b α )</formula><p>, then conducts message propagation with the weights:</p><formula xml:id="formula_3">v i = 1 |N i | ( j∈N i α j W r ji v j ), r ji ∈ {r E , r I } (1)</formula><p>whereṽ i is the message representation of node v i . α j and v j are the weight and the node embedding of v j respectively. After the message propagation, the node representations are updated with the initial node embeddings and the message representations by</p><formula xml:id="formula_4">v i = ReLU(W u v i +ṽ i + b u ),<label>(2)</label></formula><p>where W u and b u are weight and bias respectively. The updated node representations v i will be used to enhance the contextual token embedding via summation in corresponding positions. Thus t l = t l + v n , where l ∈ S n and S n is the corresponding token indices set for n-th EDU.</p><p>Answer Prediction The probabilities of options are obtained by feeding the discourse-enhanced token embeddings into the answer prediction module. The model is end-to-end trained using cross entropy loss. Specifically, the embedding sequence first goes through a layer normalization <ref type="bibr" target="#b0">(Ba et al., 2016)</ref>, then a bidirectional GRU <ref type="bibr">(Cho et al., 2014)</ref>. The output embeddings are then added to the input ones as the residual structure <ref type="bibr">(He et al., 2016)</ref>. We finally obtain the encoded sequence after another layer normalization on the added embeddings.</p><p>We then merge the high-level discourse features and the low-level token features. Specifically, the variant-length encoded context sequence, questionand-option sequence are pooled via weighted summation wherein the weights are softmax results of  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We evaluate the performance of DAGN on two logical reasoning datasets, ReClor <ref type="bibr" target="#b23">(Yu et al., 2020)</ref> and LogiQA , and conduct ablation study on graph construction and graph network. The implementation details are shown in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets</head><p>ReClor   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Ablation Study</head><p>We conduct ablation study on graph construction details as well as the graph reasoning module. The results are reported in <ref type="table" target="#tab_5">Table 3</ref>.</p><p>Varied Graph Nodes We first use clauses or sentences in substitution for EDUs as graph nodes. For clause nodes, we simply remove "Explicit" connectives during discourse unit delimitation. So that the texts are just delimited by punctuation marks. For sentence nodes, we further reduce the delimiter library to solely period ("."). Using the modified graphs with clause nodes or coarser sentence nodes, the accuracy of DAGN drops to 64.40%. This indicates that clause or sentence nodes carry less discourse information and act poorly as logical reasoning units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Varied Graph Edges</head><p>We make two changes of the edges: (1) modifying the edge type, (2) modifying the edge linking. For edge type, all edges are regarded as a single type. For edge linking, we ignore discourse relations and connect every pair of nodes, turning the graph into fully-connected. The resulting accuracies drop to 64.80% and 61.60% respectively. It is proved that in the graph we built, edges link EDUs in reasonable manners, which properly indicates the logical relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation on Graph Reasoning</head><p>We remove the graph module from DAGN and give a comparison. This model solely contains an extra prediction module than the baseline. The performance on ReClor dev set is between the baseline model and DAGN. Therefore, despite the prediction module benefits the accuracy, the lack of graph reasoning leads to the absence of discourse features and degenerates the performance. It demonstrates the necessity of discourse-based structure in logical reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Works</head><p>Recent datasets for reading comprehension tend to be more complicated and require models' capability of reasoning. For instance, HotpotQA <ref type="bibr" target="#b22">(Yang et al., 2018)</ref>, WikiHop <ref type="bibr" target="#b19">(Welbl et al., 2018)</ref>, Open-BookQA <ref type="bibr" target="#b7">(Mihaylov et al., 2018)</ref>, and MultiRC <ref type="bibr" target="#b1">(Khashabi et al., 2018)</ref> require the models to have multi-hop reasoning. DROP <ref type="bibr">(Dua et al., 2019)</ref> and MA-TACO  need the models to have numerical reasoning. WIQA <ref type="bibr" target="#b16">(Tandon et al., 2019)</ref> and <ref type="bibr">CosmosQA (Huang et al., 2019)</ref> require causal reasoning that the models can understand the counterfactual hypothesis or find out the causeeffect relationships in events. However, the logical reasoning datasets <ref type="bibr" target="#b23">(Yu et al., 2020;</ref> require the models to have the logical reasoning capability of uncovering the inner logic of texts.</p><p>Deep neural networks are used for reasoningdriven RC. Evidence-based methods <ref type="bibr" target="#b5">(Madaan et al., 2020;</ref><ref type="bibr" target="#b3">Huang et al., 2020;</ref> generate explainable evidence from a given context as the backup of reasoning. Graph-based methods <ref type="bibr">De Cao et al., 2019;</ref><ref type="bibr">Cao et al., 2019;</ref><ref type="bibr" target="#b14">Ran et al., 2019;</ref><ref type="bibr" target="#b21">Chen et al., 2020b;</ref><ref type="bibr" target="#b21">Xu et al., 2020b;</ref> explicitly model the reasoning process with constructed graphs, then learn and update features through message passing based on graphs. There are also other methods such as neuro-symbolic models <ref type="bibr" target="#b15">(Saha et al., 2021)</ref> and adversarial training <ref type="bibr" target="#b9">(Pereira et al., 2020)</ref>. Our paper uses a graph-based model. However, for uncovering logical relations, graph nodes and edges are customized with discourse information.</p><p>Discourse information provides a high-level understanding of texts and hence is beneficial for many of the natural language tasks, for instance, text summarization <ref type="bibr">(Cohan et al., 2018;</ref><ref type="bibr">Joty et al., 2019;</ref><ref type="bibr" target="#b20">Xu et al., 2020a;</ref><ref type="bibr" target="#b23">Feng et al., 2020)</ref>, neural machine translation <ref type="bibr" target="#b17">(Voita et al., 2018)</ref>, and coherent text generation <ref type="bibr">Bosselut et al., 2018)</ref>. There are also discourse-based applications for reading comprehension. <ref type="bibr">DISCERN (Gao et al., 2020)</ref> segments texts into EDUs and learns interactive EDU features. <ref type="bibr" target="#b8">Mihaylov and Frank (2019)</ref> provide additional discourse-based annotations and encodes them with discourseaware self-attention models. Unlike previous works, DAGN first uses discourse relations as graph edges connecting EDUs for texts, then learns the discourse features via message passing with graph neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we introduce a Discourse-Aware Graph Network (DAGN) to addressing logical reasoning QA tasks. We first treat elementary discourse units (EDUs) that are split by discourse relations as basic reasoning units. We then build discourse-based logic graphs with EDUs as nodes and discourse relations as edges. DAGN then learns the discourse-based features and enhances them with contextual token embeddings. DAGN reaches competitive performances on two recent logical reasoning datasets ReClor and LogiQA.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>We fine-tune RoBERTa-Large  as the backbone pre-trained language model for DGAN, which contains 24 hidden layers with hidden size 1024. The overall model is end-to-end trained and updated by Adam (Kingma and <ref type="bibr" target="#b2">Ba, 2015)</ref> optimizer with an overall learning rate of 5e-6 and a weight decay of 0.01. The overall dropout rate is 0.1. The maximum sequence length is 256. We tune the model on the dev set to obtain the best iteration steps of graph reasoning, which is 2 for ReClor data, and 3 for LogiQA data. The model is trained for 10 epochs with a batch size of 16 on Nvidia Tesla V100 GPU. For the answer prediction module, the hidden size of GRU is the same as the token embeddings in the pre-trained language model, which is 1024. The two-layer perceptron first projects the concatenated vectors with a hidden size of 1024 × 3 to 1024, then project 1024 to 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>DAGN ranks the 1st on the public ReClor leaderboard 1 until 17th Nov., 2020 before submitting it to NAACL. Until now, we find that several better results appeared in the leaderboard and they are not opened.</figDesc><table><row><cell>Methods</cell><cell>Dev</cell><cell>Test</cell><cell cols="2">Test-E Test-H</cell></row><row><cell>BERT-Large</cell><cell>53.80</cell><cell>49.80</cell><cell>72.00</cell><cell>32.30</cell></row><row><cell>XLNet-Large</cell><cell>62.00</cell><cell>56.00</cell><cell>75.70</cell><cell>40.50</cell></row><row><cell>RoBERTa-Large</cell><cell>62.60</cell><cell>55.60</cell><cell>75.50</cell><cell>40.00</cell></row><row><cell>DAGN</cell><cell>65.20</cell><cell>58.20</cell><cell>76.14</cell><cell>44.11</cell></row><row><cell>DAGN (Aug)</cell><cell>65.80</cell><cell>58.30</cell><cell>75.91</cell><cell>44.46</cell></row></table><note>* The results are taken from the ReClor paper.*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>: Experimental results (accuracy %) of DAGN</cell></row><row><cell>compared with baseline models on ReClor dataset.</cell></row><row><cell>Test-E = Test-EASY, Test-H = Test-HARD.</cell></row></table><note>a linear transformation of the sequence, resulting in single feature vectors separately. We concate- nate them with "&lt;s&gt;" embedding from the back- bone pre-trained model, and feed the new vector into a two-layer perceptron with a GELU activa- tion (Hendrycks and Gimpel, 2016) to get the out- put features for classification.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Experimental results (accuracy %) of DAGN compared with baseline models on LogiQA dataset.</figDesc><table><row><cell>Methods</cell><cell>Dev</cell></row><row><cell>DAGN</cell><cell>65.20</cell></row><row><cell>ablation on nodes</cell><cell></cell></row><row><cell>DAGN -clause nodes</cell><cell>64.40</cell></row><row><cell>DAGN -sentence nodes</cell><cell>64.40</cell></row><row><cell>ablation on edges</cell><cell></cell></row><row><cell>DAGN -single edge type</cell><cell>64.80</cell></row><row><cell>DAGN -fully connected edges</cell><cell>61.60</cell></row><row><cell>ablation on graph reasoning</cell><cell></cell></row><row><cell>DAGN w/o graph module</cell><cell>64.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell>: Ablation study results (accurcy %) on ReClor</cell></row><row><cell>development set.</cell></row><row><cell>models. As for DAGN, we fine-tune RoBERTa-</cell></row><row><cell>Large as the backbone. DAGN (Aug) is a variant</cell></row><row><cell>that augments the graph features.</cell></row><row><cell>DAGN reaches 58.20% of test accuracy on</cell></row><row><cell>ReClor. DAGN (Aug) reaches 58.30%, therein</cell></row><row><cell>75.91% on EASY subset, and 44.46% on HARD</cell></row><row><cell>subset. Compared with RoBERTa-Large, the</cell></row><row><cell>improvement on the HARD subset is remark-</cell></row><row><cell>ably 4.46%. This indicates that the incorporated</cell></row><row><cell>discourse-based information supplements the short-</cell></row><row><cell>coming of the baseline model, and that the dis-</cell></row><row><cell>course features are beneficial for such logical rea-</cell></row><row><cell>soning. Besides, DAGN and DAGN (Aug) also</cell></row><row><cell>outperform the baseline models on LogiQA, espe-</cell></row><row><cell>cially showing 4.01% improvement over RoBERTa-</cell></row><row><cell>Large on the test set.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Lifu Huang, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2019. Cosmos qa: Machine reading comprehension with contextual commonsense reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 2391-2401. Shafiq Joty, Giuseppe Carenini, Raymond Ng, and Gabriel Murray. 2019. Discourse analysis and its applications. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Tutorial Abstracts, pages 12-17, Florence, Italy. Association for Computational Linguistics.</figDesc><table><row><cell>Antoine Bosselut, Asli Celikyilmaz, Xiaodong He, Jianfeng Gao, Po-Sen Huang, and Yejin Choi. 2018. Discourse-aware neural rewards for coherent text generation. In Proceedings of the 2018 Conference</cell><cell cols="2">Jacob Devlin, Computational Linguistics: Human Language Tech-nologies, Volume 1 (Long and Short Papers), pages 4171-4186.</cell></row><row><cell>of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 173-184.</cell><cell cols="2">Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019. Drop: A reading comprehension benchmark requir-ing discrete reasoning over paragraphs. In Proceed-</cell></row><row><cell>Yu Cao, Meng Fang, and Dacheng Tao. 2019. Bag: Bi-directional attention entity graph convolutional network for multi-hop reasoning question answering. In Proceedings of the 2019 Conference of the North</cell><cell cols="2">ings of the 2019 Conference of the North American Chapter of the Association for Computational Lin-guistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368-2378.</cell></row><row><cell>American Chapter of the Association for Computa-tional Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 357-362.</cell><cell cols="2">Xiachong Feng, Xiaocheng Feng, Bing Qin, Xin-wei Geng, and Ting Liu. 2020. Dialogue discourse-aware graph convolutional networks for</cell></row><row><cell>Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xi-aochuan, Yuyu Zhang, Le Song, Taifeng Wang,</cell><cell cols="2">abstractive meeting summarization. arXiv preprint arXiv:2012.03502.</cell></row><row><cell>Yuan Qi, and Wei Chu. 2020a. Question directed graph attention network for numerical reasoning over text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6759-6768.</cell><cell cols="2">Yifan Gao, Chien-Sheng Wu, Jingjing Li, Shafiq Joty, Steven CH Hoi, Caiming Xiong, Irwin King, and Michael Lyu. 2020. Discern: Discourse-aware en-tailment reasoning network for conversational ma-chine reading. In Proceedings of the 2020 Confer-</cell></row><row><cell>Kunlong Chen, Weidi Xu, Xingyi Cheng, Zou Xi-aochuan, Yuyu Zhang, Le Song, Taifeng Wang,</cell><cell cols="2">ence on Empirical Methods in Natural Language Processing (EMNLP), pages 2439-2449.</cell></row><row><cell>Yuan Qi, and Wei Chu. 2020b. Question directed graph attention network for numerical reasoning over text. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process-ing (EMNLP), pages 6759-6768, Online. Associa-tion for Computational Linguistics.</cell><cell cols="2">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recog-nition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778.</cell></row><row><cell>Kyunghyun Cho, Bart van Merriënboer, Dzmitry Bah-danau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder-decoder ap-</cell><cell cols="2">Dan Hendrycks and Kevin Gimpel. 2016. Gaus-sian error linear units (gelus). arXiv preprint arXiv:1606.08415.</cell></row><row><cell>proaches. In Proceedings of SSST-8, Eighth Work-</cell><cell></cell><cell></cell></row><row><cell>shop on Syntax, Semantics and Structure in Statisti-</cell><cell></cell><cell></cell></row><row><cell>cal Translation, pages 103-111.</cell><cell></cell><cell></cell></row><row><cell>Arman Cohan, Franck Dernoncourt, Doo Soon Kim,</cell><cell></cell><cell></cell></row><row><cell>Trung Bui, Seokhwan Kim, Walter Chang, and Na-</cell><cell></cell><cell></cell></row><row><cell>zli Goharian. 2018. A discourse-aware attention</cell><cell></cell><cell></cell></row><row><cell>model for abstractive summarization of long docu-</cell><cell></cell><cell></cell></row><row><cell>ments. In Proceedings of the 2018 Conference of</cell><cell></cell><cell></cell></row><row><cell>the North American Chapter of the Association for</cell><cell></cell><cell></cell></row><row><cell>Computational Linguistics: Human Language Tech-</cell><cell cols="2">Yinya Huang, Meng Fang, Xunlin Zhan, Qingxing</cell></row><row><cell>nologies, Volume 2 (Short Papers), pages 615-621,</cell><cell cols="2">Cao, Xiaodan Liang, and Liang Lin. 2020. Rem-</cell></row><row><cell>New Orleans, Louisiana. Association for Computa-</cell><cell cols="2">net: Recursive erasure memory network for com-</cell></row><row><cell>tional Linguistics.</cell><cell>monsense evidence refinement.</cell><cell>arXiv preprint</cell></row><row><cell></cell><cell>arXiv:2012.13185.</cell><cell></cell></row><row><cell>Nicola De Cao, Wilker Aziz, and Ivan Titov. 2019.</cell><cell></cell><cell></cell></row><row><cell>Question answering by reasoning across documents</cell><cell></cell><cell></cell></row><row><cell>with graph convolutional networks. In Proceed-</cell><cell></cell><cell></cell></row><row><cell>ings of the 2019 Conference of the North American</cell><cell></cell><cell></cell></row><row><cell>Chapter of the Association for Computational Lin-</cell><cell></cell><cell></cell></row><row><cell>guistics: Human Language Technologies, Volume 1</cell><cell></cell><cell></cell></row><row><cell>(Long and Short Papers), pages 2306-2317.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>The discourse delimiter library in our implementation.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://bit.ly/2UOQfaS</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors would like to thank Wenge Liu, Jianheng Tang, Guanlin Li and Wei Wang for their support and useful discussions. </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Layer normalization. stat</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Looking beyond the surface: A challenge set for reading comprehension over multiple sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Snigdha</forename><surname>Chaturvedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shyam</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="252" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Logiqa: A challenge dataset for machine reading comprehension with logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leyang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanmeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yile</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Eigen: Event influence generation using pre-trained language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Madaan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeraj</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhilasha</forename><surname>Ravichander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shrimai</forename><surname>Prabhumoye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11764</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rhetorical structure theory: Toward a functional theory of text organization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandra</forename><forename type="middle">A</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Text</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="243" to="281" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Can a suit of armor conduct electricity? a new dataset for open book question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2381" to="2391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discourseaware semantic self-attention for narrative reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Todor</forename><surname>Mihaylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anette</forename><surname>Frank</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1257</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2541" to="2552" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial training for commonsense inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lis</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayuki</forename><surname>Asahara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ichiro</forename><surname>Kobayashi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.repl4nlp-1.8</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Representation Learning for NLP</title>
		<meeting>the 5th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Dinesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Miltsakaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Robaldo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aravind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bonnie</forename><forename type="middle">L</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Webber</surname></persName>
		</author>
		<editor>LREC. Citeseer</editor>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dynamically fused graph network for multi-hop reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1617</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6140" to="6150" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What-if i ask you to explain: Explaining the effects of perturbations in procedural text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeraj</forename><surname>Rajagopal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavana</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: Findings</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3345" to="3355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Numnet: Machine reading comprehension with numerical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Qiu Ran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2474" to="2484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Weakly supervised neuro-symbolic module networks for numerical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amrita</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11802</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">reasoning over procedural text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niket</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhavana</forename><surname>Dalvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keisuke</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6078" to="6087" />
		</imprint>
	</monogr>
	<note>Wiqa: A dataset for &quot;what if</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context-aware neural machine translation learns anaphora resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Serdyukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1264" to="1274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piji</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08822</idno>
		<title level="m">Consistency and coherency enhanced story generation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="287" to="302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discourse-aware neural extractive text summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.451</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5021" to="5031" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning with stacked hierarchical attention for text-based games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqiu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joey</forename><forename type="middle">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16495" to="16507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reclor: A reading comprehension dataset requiring logical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2020 : Eighth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graph-totree learning for solving math word problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jipeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy Ka-Wei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Bin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ee-Peng</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3928" to="3937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">as well as a set of punctuation marks. The overall discourse delimiters used in our method are presented in Table 4. Explicit Connectives &apos;once&apos;, &apos;although&apos;, &apos;though&apos;, &apos;but&apos;, &apos;because&apos;, &apos;nevertheless&apos;, &apos;before&apos;, &apos;for example&apos;, &apos;until&apos;, &apos;if&apos;, &apos;previously&apos;, &apos;when&apos;, &apos;and&apos;, &apos;so&apos;, &apos;then&apos;, &apos;while&apos;, &apos;as long as&apos;, &apos;however&apos;, &apos;also&apos;, &apos;after&apos;, &apos;separately&apos;, &apos;still&apos;, &apos;so that&apos;, &apos;or&apos;, &apos;moreover&apos;, &apos;in addition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Prasad</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1332</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3363" to="3369" />
		</imprint>
	</monogr>
	<note>yet&apos;, &apos;since&apos;, &apos;rather&apos;, &apos;in fact. either or&apos;, &apos;therefore&apos;, &apos;in turn&apos;, &apos;thus&apos;, &apos;in particular&apos;, &apos;further&apos;, &apos;afterward&apos;, &apos;next&apos;, &apos;similarly&apos;, &apos;besides&apos;, &apos;if and when&apos;, &apos;nor&apos;, &apos;alternatively&apos;, &apos;whereas&apos;, &apos;overall&apos;, &apos;by comparison&apos;, &apos;till&apos;, &apos;in contrast&apos;, &apos;finally&apos;, &apos;otherwise&apos;, &apos;as if&apos;, &apos;thereby&apos;, &apos;now that&apos;, &apos;before and after&apos;, &apos;additionally&apos;, &apos;meantime&apos;, &apos;by contrast&apos;, &apos;if then&apos;, &apos;likewise&apos;, &apos;in the end&apos;, &apos;regardless&apos;, &apos;thereafter&apos;, &apos;earlier&apos;, &apos;in other words&apos;, &apos;as soon as&apos;, &apos;except&apos;, &apos;in short&apos;, &apos;neither nor&apos;, &apos;furthermore&apos;, &apos;lest&apos;, &apos;as though&apos;, &apos;specifically&apos;, &apos;conversely&apos;, &apos;consequently&apos;, &apos;as well. much as. insofar as&apos;, &apos;else&apos;, &apos;as an alternative</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
