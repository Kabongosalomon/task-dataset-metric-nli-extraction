<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Dependency Parsing: Let&apos;s Use Supervised Parsers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
							<email>p.le@uva.nl</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
							<email>zuidema@uva.nl</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Institute for Logic</orgName>
								<address>
									<settlement>Language</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Computation University of Amsterdam</orgName>
								<address>
									<country key="NL">the Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Dependency Parsing: Let&apos;s Use Supervised Parsers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a self-training approach to unsupervised dependency parsing that reuses existing supervised and unsupervised parsing algorithms. Our approach, called 'iterated reranking' (IR), starts with dependency trees generated by an unsupervised parser, and iteratively improves these trees using the richer probability models used in supervised parsing that are in turn trained on these trees. Our system achieves 1.8% accuracy higher than the stateof-the-part parser of <ref type="bibr" target="#b32">Spitkovsky et al. (2013)</ref> on the WSJ corpus.</p><p>However, applying existing supervised parsing</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised dependency parsing and its supervised counterpart have many characteristics in common: they take as input raw sentences, produce dependency structures as output, and often use the same evaluation metric (DDA, or UAS, the percentage of tokens for which the system predicts the correct head). Unsurprisingly, there has been much more research on supervised parsing -producing a wealth of models, datasets and training techniques -than on unsupervised parsing, which is more difficult, much less accurate and generally uses very simple probability models. Surprisingly, however, there have been no reported attempts to reuse supervised approaches to tackle the unsupervised parsing problem (an idea briefly mentioned in <ref type="bibr" target="#b30">Spitkovsky et al. (2010b)</ref>).</p><p>There are, nevertheless, two aspects of supervised parsers that we would like to exploit in an unsupervised setting. First, we can increase the model ex-pressiveness in order to capture more linguistic regularities. Many recent supervised parsers use thirdorder (or higher order) features <ref type="bibr" target="#b16">(Koo and Collins, 2010;</ref><ref type="bibr" target="#b20">Martins et al., 2013;</ref><ref type="bibr" target="#b18">Le and Zuidema, 2014)</ref> to reach state-of-the-art (SOTA) performance. In contrast, existing models for unsupervised parsing limit themselves to using simple features (e.g., conditioning on heads and valency variables) in order to reduce the computational cost, to identify consistent patterns in data <ref type="bibr">(Naseem, 2014, page 23)</ref>, and to avoid overfitting <ref type="bibr" target="#b3">(Blunsom and Cohn, 2010)</ref>. Although this makes learning easier and more efficient, the disadvantage is that many useful linguistic regularities are missed: an upper bound on the performance of such simple models -estimated by using annotated data -is 76.3% on the WSJ corpus <ref type="bibr" target="#b32">(Spitkovsky et al., 2013)</ref>, compared to over 93% actual performance of the SOTA supervised parsers.</p><p>Second, we would like to make use of information available from lexical semantics, as in <ref type="bibr" target="#b0">Bansal et al. (2014)</ref>, <ref type="bibr" target="#b18">Le and Zuidema (2014)</ref>, and <ref type="bibr" target="#b5">Chen and Manning (2014)</ref>. Lexical semantics is a source for handling rare words and syntactic ambiguities. For instance, if a parser can identify that "he" is a dependent of "walks" in the sentence "He walks", then, even if "she" and "runs" do not appear in the training data, the parser may still be able to recognize that "she" should be a dependent of "runs" in the sentence "she runs". Similarly, a parser can make use of the fact that "sauce" and "John" have very different meanings to decide that they have different heads in the two phrases "ate spaghetti with sauce" and "ate spaghetti with John". techniques to the task of unsupervised parsing is, unfortunately, not trivial. The reason is that those parsers are optimally designed for being trained on manually annotated data. If we use existing unsupervised training methods (like EM), learning could be easily misled by a large amount of ambiguity naturally embedded in unannotated training data. Moreover, the computational cost could rapidly increase if the training algorithm is not designed properly.</p><p>To overcome these difficulties we propose a framework, iterated reranking (IR), where existing supervised parsers are trained without the need of manually annotated data, starting with dependency trees provided by an existing unsupervised parser as initialiser. Using this framework, we can employ the work of <ref type="bibr" target="#b18">Le and Zuidema (2014)</ref> to build a new system that outperforms the SOTA unsupervised parser of <ref type="bibr" target="#b32">Spitkovsky et al. (2013)</ref> on the WSJ corpus.</p><p>The contribution of this paper is twofold. First, we show the benefit of using lexical semantics for the unsupervised parsing task. Second, our work is a bridge connecting the two research areas unsupervised parsing and its supervised counterpart. Before going to the next section, in order to avoid confusion introduced by names, it is worth noting that we use un-trained existing supervised parsers which will be trained on automatically annotated treebanks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unsupervised Dependency Parsing</head><p>The first breakthrough was set by <ref type="bibr" target="#b15">Klein and Manning (2004)</ref> with their dependency model with valence (DMV), the first model to outperform the right-branching baseline on the DDA metric: 43.2% vs 33.6% on sentences up to length 10 in the WSJ corpus. Nine years later, <ref type="bibr" target="#b32">Spitkovsky et al. (2013)</ref> achieved much higher DDAs: 72.0% on sentences up to length 10, and 64.4% on all sentences in section 23. During this period, many approaches have been proposed to attempt the challenge. <ref type="bibr" target="#b24">Naseem and Barzilay (2011)</ref>, <ref type="bibr" target="#b33">Tu and</ref><ref type="bibr">Honavar (2012), Spitkovsky et al. (2012)</ref>, <ref type="bibr" target="#b32">Spitkovsky et al. (2013)</ref>, and <ref type="bibr" target="#b19">Marecek and Straka (2013)</ref> employ extensions of the DMV but with different learning strategies. Naseem and Barzilay (2011) use semantic cues, which are event annotations from an outof-domain annotated corpus, in their model during training. Relying on the fact that natural language grammars must be unambiguous in the sense that a sentence should have very few correct parses, <ref type="bibr" target="#b33">Tu and Honavar (2012)</ref> incorporate unambiguity regularisation to posterior probabilities. <ref type="bibr" target="#b31">Spitkovsky et al. (2012)</ref> bootstrap the learning by slicing up all input sentences at punctuation. <ref type="bibr" target="#b32">Spitkovsky et al. (2013)</ref> propose a complete deterministic learning framework for breaking out of local optima using count transforms and model recombination. <ref type="bibr" target="#b19">Marecek and Straka (2013)</ref> make use of a large raw text corpus (e.g., Wikipedia) to estimate stop probabilities, using the reducibility principle.</p><p>Differing from those works, Bisk and Hockenmaier (2012) rely on Combinatory Categorial Grammars with a small number of hand-crafted general linguistic principles; whereas <ref type="bibr" target="#b3">Blunsom and Cohn (2010)</ref> use Tree Substitution Grammars with a hierarchical non-parametric Pitman-Yor process prior biasing the learning to a small grammar.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Reranking</head><p>Our work relies on reranking which is a technique widely used in (semi-)supervised parsing. Reranking requires two components: a k-best parser and a reranker. Given a sentence, the parser generates a list of k best candidates, the reranker then rescores those candidates and picks the one that has the highest score. Reranking was first successfully applied to supervised constituent parsing <ref type="bibr" target="#b6">(Collins, 2000;</ref><ref type="bibr" target="#b4">Charniak and Johnson, 2005)</ref>. It was then employed in the supervised dependency parsing approaches of <ref type="bibr" target="#b27">Sangati et al. (2009</ref><ref type="bibr" target="#b14">), Hayashi et al. (2013</ref>, and <ref type="bibr" target="#b18">Le and Zuidema (2014)</ref>.</p><p>Closest to our work is the work series on semisupervised constituent parsing of McClosky and colleagues, e.g. <ref type="bibr" target="#b21">McClosky et al. (2006)</ref>, using selftraining. They use a k-best generative parser and a discriminative reranker to parse unannotated sentences, then add resulting parses to the training treebank and re-train the reranker. Different from their work, our work is for unsupervised dependency parsing, without manually annotated data, and uses iterated reranking instead of single reranking. In addition, both two components, k-best parser and reranker, are re-trained after each iteration.</p><p>Existing training methods for the unsupervised dependency task, such as <ref type="bibr" target="#b3">Blunsom and Cohn (2010)</ref>, <ref type="bibr" target="#b13">Gillenwater et al. (2011)</ref>, and <ref type="bibr" target="#b33">Tu and Honavar (2012)</ref>, are hypothesis-oriented search with the EM algorithm or its variants: training is to move from a point which represents a model hypothesis to another point. This approach is feasible for optimising models using simple features since existing dynamic programming algorithms can compute expectations, which are sums over all possible parses, or to find the best parse in the whole parse space with low complexities. However, the complexity increases rapidly if rich, complex features are used. One way to reduce the computational cost is to use approximation methods like sampling as in <ref type="bibr" target="#b3">Blunsom and Cohn (2010)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Treebank-oriented Greedy Search</head><p>Believing that the difficulty of using EM is from the fact that treebanks are 'hidden', leading to the need of computing sum (or max) overall possible treebanks, we propose a greedy local search scheme based on another training philosophy: treebankoriented search. The key idea is to explicitly search for concrete treebanks which are used to train parsing models. This scheme thus allows supervised parsers to be trained in an unsupervised parsing setting since there is a (automatically annotated) treebank at any time.</p><p>Given S a set of raw sentences, the search space consists of all possible treebanks D = {d(s)|s ∈ S} where d(s) is a dependency tree of sentence s. The target of search is the optimal treebank D * that is as good as human annotations. Greedy search with this philosophy is as follows: starting at an initial point D 1 , we pick up a point D 2 among its neighbours N(D 1 ) such that</p><formula xml:id="formula_0">D 2 = arg max D∈N(D 1 ) f D 1 (D) (1) where f D 1 (D)</formula><p>is an objective function measuring the goodness of D (which may or may not be conditioned on D 1 ). We then continue this search until some stop criterion is satisfied. The crucial factor here is to define N(D i ) and f D i (D). Below are two special cases of this scheme.</p><p>Semi-supervised parsing using reranking <ref type="bibr">(Mc-Closky et al., 2006)</ref>. This reranking is indeed onestep greedy local search. In this scenario, N(D 1 ) is the Cartesian product of k-best lists generated by a k-best parser, and f D i (D) is a reranker.</p><p>Unsupervised parsing with hard-EM (Spitkovsky et al., 2010b) In hard-EM, the target is to maximise the following objective function with respect to a parameter set Θ</p><formula xml:id="formula_1">L(S|Θ) = s∈S max d∈Dep(s) log P Θ d<label>(2)</label></formula><p>where Dep(s) is the set of all possible dependency structures of s. The two EM steps are thus</p><formula xml:id="formula_2">• Step 1: D i+1 = arg max D P Θ i (D) • Step 2: Θ i+1 = arg max Θ P Θ (D i+1 ) In this case, N(D i ) is the whole treebank space and f D i (D) = P Θ i (D) = P arg max Θ P Θ (D i ) (D).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Iterated Reranking</head><p>We instantiate the greedy search scheme by iterated reranking which requires two components: a k-best parser P , and a reranker R. Firstly, D 1 is used to train these two components, resulting in P 1 and R 1 . The parser P 1 then generates a set of lists of k candidates k D 1 (whose Cartesian product results in N(D 1 )) for the set of training sentences S. The best candidates, according to reranker R 1 , are collected to form D 2 for the next iteration. This process is halted when a pre-defined stop criterion is met. 1 It is certain that we can, as in the work of Spitkovsky et al. (2010b) and many bootstrapping approaches, employ only parser P . Reranking, however, brings us two benefits. First, it allows us to employ very expressive models like the ∞-order generative model proposed by <ref type="bibr" target="#b18">Le and Zuidema (2014)</ref>. Second, it embodies a similar idea to co-training <ref type="bibr" target="#b2">(Blum and Mitchell, 1998)</ref>: P and R play roles as two views of the data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Multi-phase Iterated Reranking</head><p>Training in machine learning often uses starting big which is to use up all training data at the same time. However, Elman <ref type="bibr">(1993)</ref> suggests that in some cases, learning should start by training simple models on small data and then gradually increase the model complexity and add more difficult data. This is called starting small.</p><p>In unsupervised dependency parsing, starting small is intuitive. For instance, given a set of long sentences, learning the fact that the head of a sentence is its main verb is difficult because a long sentence always contains many syntactic categories. It would be much easier if we start with only lengthone sentences, e.g "Look!", since there is only one choice which is usually a verb. This training scheme was successfully applied by <ref type="bibr" target="#b29">Spitkovsky et al. (2010a)</ref> under the name: Baby Step.</p><p>We adopt starting small to construct the multiphase iterated reranking (MPIR) framework. In phase 0, a parser M with a simple model is trained on a set of short sentences S (0) as in traditional approaches. This parser is used to parse a larger set of sentences S (1) ⊇ S (0) , resulting in D (1)</p><formula xml:id="formula_3">1 . D (1)</formula><p>1 is then used as the starting point for the iterated reranking in phase 1. We continue this process until phase N finishes, with S (i) ⊇ S (i−1) (i = 1..N ). In general, we use the resulting reranker in the previous phase to generate the starting point for the iterated reranking in the current phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Le and Zuidema (2014)'s Reranker</head><p>Le and Zuidema (2014)'s reranker is an exception among supervised parsers because it employs an extremely expressive model whose features are ∞order 2 . To overcome the problem of sparsity, they introduced the inside-outside recursive neural network (IORNN) architecture that can estimate treegenerating models including those proposed by <ref type="bibr" target="#b11">Eisner (1996)</ref> and <ref type="bibr" target="#b7">Collins (2003a)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The ∞-order Generative Model</head><p>Le and Zuidema (2014)'s reranker employs the generative model proposed by <ref type="bibr" target="#b11">Eisner (1996)</ref>. Intuitively, this model is top-down: starting with ROOT, 2 In fact, the order is finite but unbound. we generate its left dependents and its right dependents. We then generate dependents for each ROOT's dependent. The generative process recursively continues until there is no dependent to generate. Formally, this model is described by the following formula</p><formula xml:id="formula_4">P (d(H)) = L l=1 P H L l |C(H L l ) P d(H L l ) × R r=1 P H R r |C(H R r ) P d(H R r )<label>(3)</label></formula><p>where H is the current head, d(N ) is the fragment of the dependency parse rooted at N , and C(N ) is the context to generate N . H L , H R are respectively H's left dependents and right dependents, plus EOC (End-Of-Children), a special token to inform that there are no more dependents to generate. Thus, P (d(ROOT )) is the probability of generating the entire dependency structure d. Le and Zuidema's ∞-order generative model is defined as Eisner's model in which the context C ∞ (D) to generate D contains all of D's generated siblings, its ancestors and their siblings. Because of very large fragments that contexts are allowed to hold, traditional count-based methods are impractical (even if we use smart smoothing techniques). They thus introduced the IORNN architecture to estimate the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Estimation with the IORNN</head><p>An IORNN <ref type="figure" target="#fig_0">(Figure 1)</ref> is a recursive neural network whose topology is a tree. What make this network different from traditional RNNs <ref type="bibr" target="#b28">(Socher et al., 2010)</ref> is that each tree node u caries two vectors: i u -the inner representation, represents the content of the phrase covered by the node, and o u -the outer representation, represents the context around that phrase. In addition, information in an IORNN is allowed to flow not only bottom-up as in RNNs, but also topdown. That makes IORNNs a natural tool for estimating top-down tree-generating models.</p><p>Applying the IORNN architecture to dependency parsing is straightforward, along the generative story of the ∞-order generative model. First of all, the "inside" part of this IORNN is simpler than what is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>: the inner representation of a phrase is assumed to be the inner representation of its head. This approximation is plausible since the meaning of a phrase is often dominated by the meaning of its head. The inner representation at each node, in turn, is a function of a vector representation for the word (in our case, the word vectors are initially borrowed from <ref type="bibr" target="#b9">Collobert et al. (2011)</ref>), the POS-tag and capitalisation feature.</p><p>Without loss of generality and ignoring directions for simplicity, they assume that the model is generating dependent u for node h conditioning on context C ∞ (u) which contains all of u's ancestors (including h) and theirs siblings, and all of previously generated u's sisters. Now there are two types of contexts: full contexts of heads (e.g., h) whose dependents are being generated, and contexts to generate nodes (e.g., C ∞ (u)). Contexts of the first type are clearly represented by outer representations. Contexts of the other type are represented by partial outer representations, denoted byō u . Because the context to generate a node can be constructed recursively by combining the full context of its head and its previously generated sisters, they can computeō u as a function of o h and the inner representations of its previously generated sisters. On the top ofō u , they put a softmax layer to estimate the probability P (x|C ∞ (u)).</p><p>Training this IORNN is to minimise the cross entropy over all dependents. This objective function is indeed the negative log likelihood P (D) of training treebank D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Reranker</head><p>Le and Zuidema's (generative) reranker is given by</p><formula xml:id="formula_5">d * = arg max d∈ k Dep(s) P (d)</formula><p>where P (Equation 3) is computed by the ∞-order generative model which is estimated by an IORNN; and k Dep(s) is a k-best list.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Complete System</head><p>Our system is based on the multi-phase IR. In general, any third-party parser for unsupervised dependency parsing can be used in phase 0, and any thirdparty parser that can generate k-best lists can be used in the other phases. In our experiments, for phase 0, we choose the parser using an extension of the DMV model with stop-probability estimates computed on a large corpus proposed by <ref type="bibr" target="#b19">Marecek and Straka (2013)</ref>. This system has a moderate performance 3 on the WSJ corpus: 57.1% vs the SOTA 64.4% DDA of <ref type="bibr" target="#b32">Spitkovsky et al. (2013)</ref>. For the other phases, we use the MSTParser 4 (with the second-order feature mode) <ref type="bibr" target="#b22">(McDonald and Pereira, 2006)</ref>.</p><p>Our system uses Le and Zuidema (2014)'s reranker (Section 4.3). It is worth noting that, in this case, each phase with iterated reranking could be seen as an approximation of hard-EM (see Equation 2) where the first step is replaced by</p><formula xml:id="formula_6">D i+1 = arg max D∈N(D i ) P Θ i (D)<label>(4)</label></formula><p>In other words, instead of searching over the treebank space, the search is limited in a neighbour set N(D i ) generated by k-best parser P i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tuning Parser P</head><p>Parser P i trained on D i defines neighbour set N(D i ) which is the Cartesian product of the k-best lists in k D i . The position and shape of N(D i ) is thus determined by two factors: how well P i can fit D i , and k. Intuitively, the lower the fitness is, the more N(D i ) goes far away from D i ; and the larger k is, the larger <ref type="bibr">3</ref> Marecek and Straka (2013) did not report any experimental result on the WSJ corpus. We use their source code at http: //ufal.mff.cuni.cz/udp with the setting presented in Section 6.1. Because the parser does not provide the option to parse unseen sentences, we merge the training sentences (up to length 15) to all the test sentences to evaluate its performance. Note that this result is close to the DDA (55.4%) that the authors reported on CoNLL 2007 English dataset, which is a portion of the WSJ corpus. 4 http://sourceforge.net/projects/ mstparser/ N(D i ) is. Moreover, the diversity of N(D i ) is inversely proportional to the fitness. When the fitness decreases, patterns existing in the training treebank become less certain to the parser, patterns that do not exist in the training treebank thus have more chances to appear in k-best candidates. This leads to high diversity of N(D i ). We blindly set k = 10 in all of our experiments. With the MSTParser, there are two hyperparameters: iters MST , the number of epochs, and training-k MST , the k-best parse set size to create constraints during training. training-k MST is always 1 because constraints from k-best parses with almost incorrect training parses are useless.</p><p>Because iters MST controls the fitness of the parser to training treebank D i , it, as pointed out above, determines the distance from N(D i ) to D i and the diversity of the former. Therefore, if we want to encourage the local search to explore more distant areas, we should set iters MST low. In our experiments, we test two strategies: (i) MaxEnc, iters MST = 1, maximal encouragement, and (ii) MinEnc, iters MST = 10, minimal encouragement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Tuning Reranker R</head><p>Tuning the reranker R is to set values for dim IORNN , the dimensions of inner and outer representations, and iters IORNN , the number of epochs to train the IORNN. Because the ∞-order model is very expressive and feed-forward neural networks are universal approximators <ref type="bibr" target="#b10">(Cybenko, 1989)</ref>, the reranker is capable of perfectly remembering all training parses. In order to avoid this, we set dim IORNN = 50, and set iters IORNN = 5 for very early stopping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Tuning multi-phase IR</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Because</head><p>Marecek and Straka (2013)'s parser does not distinguish training data from test data, we postulate S 0 = S 1 . Our system has N phases such that S 0 , S 1 contain all sentences up to length l 1 = 15, S i (i = 2..N ) contains all sentences up to length l i = l i−1 + 1, and S N contains all sentences up to length 25. Phase 1 halts after 100 iterations whereas all the following phases run with one iteration. Note that we force the local search in phase 1 to run intensively because we hypothesise that most of the important patterns for dependency parsing can be found within short sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Setting</head><p>We use the Penn Treebank WSJ corpus: sections 02-21 for training, and section 23 for testing. We then apply the standard pre-processing 5 for unsupervised dependency parsing task <ref type="bibr" target="#b15">(Klein and Manning, 2004)</ref>: we strip off all empty sub-trees, punctuation, and terminals (tagged # and $) not pronounced where they appear; we then convert the remaining trees to dependencies using Collins's head rules <ref type="bibr" target="#b8">(Collins, 2003b)</ref>. Both word forms and gold POS tags are used. The directed dependency accuracy (DDA) metric is used for evaluation.</p><p>The vocabulary is taken as a list of words occurring more than two times in the training data. All other words are labelled 'UNKNOWN' and every digit is replaced by '0'. We initialise the IORNN with the 50-dim word embeddings from <ref type="bibr">Collobert et al. (2011) 6</ref> , and train it with the learning rate 0.1,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Results</head><p>We compare our system against recent systems (Table 1 and Section 2.1). Our system with the two encouragement levels, MinEnc and MaxEnc, achieves the highest reported DDAs on section 23: 1.8% and 1.2% higher than Spitkovsky et al. (2013) on all sentences and up to length 10, respectively. Our improvements over the system's initialiser <ref type="bibr" target="#b19">(Marecek and Straka, 2013)</ref> are 9.1% and 4.4%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Analysis</head><p>In this section, we analyse our system along two aspects. First, we examine three factors which determine the performance of the whole system: encouragement level, lexical semantics, and starting point. We then search for what IR (with the MaxEnc option) contributes to the overall performance by comparing the quality of the treebank resulted in the end of phase 1 against the quality of the treebank given by its initialier, i.e. <ref type="bibr" target="#b19">Marecek and Straka (2013)</ref>.  pute DDA MaxEnc − DDA MinEnc of each phase on its training set (e.g., phase 3 with S (3) containing all training sentences up to length 17). MinEnc outperforms MaxEnc within phases 1, 2, 3, and 4. However, from phase 5, the latter surpasses the former. It suggests that exploring areas far away from the current point with long sentences is risky. The reason is that long sentences contain more ambiguities than short ones; thus rich diversity, high difference from the current point, but small size (i.e., small k) could easily lead the learning to a wrong path. The performance of the system with the two encouragement levels on section 23 <ref type="table">(Table 1)</ref> also suggests the same. MaxEnc strategy helps the system achieve the highest accuracy on short sentences (up to length 10). However, it is less helpful than Mi-nEnc when performing on long sentences.   <ref type="bibr" target="#b13">(Gillenwater et al., 2011)</ref>, and Harmonic <ref type="bibr" target="#b15">(Klein and Manning, 2004)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The effect of encouragement level</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The role of lexical semantics</head><p>We examine the role of the lexical semantics, which is given by the word embeddings. <ref type="figure" target="#fig_3">Figure 3</ref> shows DDAs on training sentences up to length 15 (i.e. S (1) ) of phase 1 (MaxEnc) with and without the word-embeddings. With the wordembeddings, phase 1 achieves 71.11%. When the word-embeddings are not given, i.e. the IORNN uses randomly generated word vectors, the accuracy drops 4.2%. It shows that lexical semantics plays a decisive role in the performance of the system. However, it is worth noting that, even without that knowledge (i.e., with the ∞-order generative model alone), the DDA of phase 1 is 2% higher than before being trained (66.89% vs 64.9%). It suggests that phase 1 is capable of discovering some useful dependency patterns that are invisible to the parser in phase 0. This, we conjecture, is thanks to high-order features captured by the IORNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The importance of the starting point</head><p>Starting point is claimed to be important in local search. We examine this by using three different parsers in phase 0: (i) MS <ref type="bibr">(Marecek and Straka,</ref><ref type="bibr">Figure 5: Precision (top)</ref> and recall (bottom) over binned HEAD distance of iterated reranking (IR) and its initializer (MS) on the training sentences in phase 1 (≤ 15 words). 2013), the parser used in the previous experiments, (ii) GGGPT <ref type="bibr">(Gillenwater et al., 2011) 7</ref> employing an extension of the DMV model and posterior regularization framework for training, and (iii) Harmonic, the harmonic initializer proposed by <ref type="bibr" target="#b15">Klein and Manning (2004)</ref>. <ref type="figure" target="#fig_4">Figure 4</ref> shows DDAs of phase 1 (MaxEnc) on training sentences up to length 15 with three starting-points given by those parsers. Starting point is clearly very important to the performance of the iterated reranking: the better the starting point is, the higher performance phase 1 has. However, a remarkable point here is that the iterated reranking of phase 1 always finds out more useful patterns for parsing whatever the starting point is in this experiment. It is certainly due to the high order features and lexical semantics, which are not exploited in those parsers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The contribution of Iterated Reranking</head><p>We compare the quality of the treebank resulted in the end of phase 1 against the quality of the treebank given by the initialier <ref type="bibr" target="#b19">Marecek and Straka (2013)</ref>. <ref type="figure">Figure 5</ref> shows precision (top) and recall (bottom) 7 code.google.com/p/pr-toolkit over binned HEAD distance. IR helps to improve the precision on all distance bins, especially on the bins corresponding to long distances (≥ 3). The recall is also improved, except on the bin corresponding to ≥ 7 (but the F1-score on this bin is increased). We attribute this improvement to the ∞-order model which uses very large fragments as contexts thus be able to capture long dependencies. <ref type="figure" target="#fig_5">Figure 6</ref> shows the correct-head accuracies over POS-tags. IR helps to improve the accuracies over almost all POS-tags, particularly nouns (e.g. NN, NNP, NNS), verbs (e.g. VBD, VBZ, VBN, VBG) and adjectives (e.g. JJ, JJR). However, as being affected by the initializer, IR performs poorly on conjunction (CC) and modal auxiliary (MD). For instance, in the treebank given by the initializer, almost all modal auxilaries are dependents of their verbs instead of the other way around.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion</head><p>Our system is different from the other systems shown in <ref type="table">Table 1</ref> as it uses an extremely expressive model, the ∞-order generative model, in which conditioning contexts are very large fragments. Only the work of <ref type="bibr" target="#b3">Blunsom and Cohn (2010)</ref>, whose resulting grammar rules can contain large tree fragments, shares this property. The difference is that their work needs a pre-defined prior, namely hierarchical non-parametric Pitman-Yor process prior, to avoid large, rare fragments and for smoothing. The IORNN of our system, in contrast, does that automatically. It learns by itself how to deal with distant conditioning nodes, which are often less informative than close conditioning nodes on computing P (x|C ∞ (u)). In addition, smoothing is given free: recursive neural nets are able to map 'similar' fragments onto close points <ref type="bibr" target="#b28">(Socher et al., 2010)</ref> thus an unseen fragment tends to be mapped onto a point close to points corresponding to 'similar' seen fragments.</p><p>Another difference is that our system exploits lexical semantics via word embeddings, which were learnt unsupervisedly. By initialising the IORNN with these embeddings, the use of this knowledge turns out easy and transparent. <ref type="bibr" target="#b32">Spitkovsky et al. (2013)</ref> also exploit lexical semantics but in a limited way, using a context-based polysemous unsuper- vised clustering method to tag words. Although their approach can distinguish polysemes (e.g., 'cool' in 'to cool the selling panic' and in 'it is cool'), it is not able to make use of word meaning similarities (e.g., the meaning of 'dog' is closer to 'animal' than to 'table'). Naseem and Barzilay (2011)'s system uses semantic cues from an out-of-domain annotated corpus, thus is not fully unsupervised.</p><p>We have showed that IR with a generative reranker is an approximation of hard-EM (see Equation 4). Our system is thus related to the works of <ref type="bibr" target="#b32">Spitkovsky et al. (2013)</ref> and <ref type="bibr" target="#b33">Tu and Honavar (2012)</ref>. However, what we have proposed is more than that: IR is a general framework that we can have more than one option for choosing k-best parser and reranker. For instance, we can make use of a generative k-best parser and a discriminative reranker that are used for supervised parsing. Our future work is to explore this.</p><p>The experimental results reveal that starting point is very important to the iterated reranking with the ∞-order generative model. On the one hand, that is a disadvantage compared to the other systems, which use uninformed or harmonic initialisers. But on the other hand, that is an innovation as our approach is capable of making use of existing systems. The results shown in <ref type="figure" target="#fig_4">Figure 4</ref> suggest that if phase 0 uses a better parser which uses less expressive model and/or less external knowledge than our model, such as the one proposed by <ref type="bibr" target="#b32">Spitkovsky et al. (2013)</ref>, we can expect even a higher performance. The other systems, except <ref type="bibr" target="#b3">Blunsom and Cohn (2010)</ref>, however, might not benefit from using good existing parsers as initializers because their models are not significantly more expressive than others 8 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We have proposed a new framework, iterated reranking (IR), which trains supervised parsers without the need of manually annotated data by using a unsupervised parser as an initialiser. Our system, employing Marecek and Straka (2013)'s unsupervised parser as the initialiser, the k-best MSTParser, and Le and Zuidema (2014)'s reranker, achieved 1.8% DDA higher than the SOTA parser of Spitkovsky et al. (2013) on the WSJ corpus. Moreover, we also showed that unsupervised parsing benefits from lexical semantics through using word-embeddings.</p><p>Our future work is to exploit other existing supervised parsers that fit our framework. Besides, taking into account the fast development of the word embedding research <ref type="bibr" target="#b23">(Mikolov et al., 2013;</ref><ref type="bibr" target="#b26">Pennington et al., 2014)</ref>, we will try different word embeddings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Inside-Outside Recursive Neural Network (IORNN). Black/white rectangles correspond to inner/outer representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>shows the differences in DDA between using MaxEnc and MinEnc in each phase: we com-Performance on section 23 of the WSJ corpus (all sentences and up to length 10) for recent systems and our system. MinEnc and MaxEnc denote iters MST = 10 and iters MST = 1 respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>DDA MaxEnc − DDA MinEnc of all phases on the their training sets (e.g., phase 3 with S (3) containing all training sentences up to length 17).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>DDA of phase 1 (MaxEnc), with and without the word embeddings (denoted by w/ sem and wo/ sem, respectively), on training sentences up to length 15 (i.e. S (1) ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>DDA of phase 1 (MaxEnc) before and after training with three different starting points provided by three parsers used in phase 0: MS (Marecek and Straka, 2013), GGGPT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Correct-head accuracies over POS-tags (sorted in the descending order by frequency) of iterated reranking (IR) and its initializer (MS) on the training sentences in phase 1 (≤ 15 words).</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It is worth noting that, although N(Di) has the size O(k n ) where n is the number of sentences, reranking only needs to process O(k × n) parses if these sentences are assumed to be independent.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">http://www.cs.famaf.unc.edu.ar/ francolq/en/proyectos/dmvccm 6 http://ml.nec-labs.com/senna/. These word embeddings were unsupervisedly learnt from Wikipedia.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank Remko Scha and three anonymous reviewers for helpful comments. Le thanks Milos Stanojević for helpful discussion. 8  In an experiment, we used the Marecek and Straka (2013)'s parser as an initializer for the <ref type="bibr" target="#b13">Gillenwater et al. (2011)</ref>'s parser. As we expected, the latter was not able to make use of this.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tailoring continuous word representations for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simple robust grammar induction with combinatory categorial grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Combining labeled and unlabeled sata with co-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="92" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised induction of tree substitution grammars for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2010 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1204" to="1213" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Coarse-tofine n-best parsing and maxent discriminative reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="175" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="589" to="637" />
		</imprint>
	</monogr>
	<note>Computational linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Head-driven statistical models for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="589" to="637" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of control, signals and systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Three new probabilistic models for dependency parsing: An exploration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th conference on Computational linguistics</title>
		<meeting>the 16th conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1996" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="340" to="345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning and development in neural networks: The importance of starting small</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeffrey L Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="71" to="99" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Posterior sparsity in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Gillenwater</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">João</forename><surname>Graça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="455" to="490" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient stacked dependency parsing by forest reranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katsuhiko</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhei</forename><surname>Kondo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="139" to="150" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Corpusbased induction of syntactic structure: Models of dependency and constituency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="478" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient thirdorder dependency parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 48th</title>
		<meeting>the 48th</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The insideoutside recursive neural network model for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phong</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Stopprobability estimates computed on a large corpus improve unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Marecek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Milan</forename><surname>Straka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL (1)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="281" to="290" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order non-projective turbo parsers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">T</forename><surname>André</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><forename type="middle">B</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Effective self-training for parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mcclosky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Charniak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics</title>
		<meeting>the main conference on human language technology conference of the North American Chapter of the Association of Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online learning of approximate dependency parsing algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EACL</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Using semantic cues to learn syntax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Regina</forename><surname>Barzilay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Linguistically Motivated Models for Lightly-Supervised Dependency Parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Empiricial Methods in Natural Language Processing</title>
		<meeting>the Empiricial Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A generative re-ranking model for dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Sangati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Willem</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rens</forename><surname>Bod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Parsing Technologies</title>
		<meeting>the 11th International Conference on Parsing Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="238" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning continuous phrase representations and syntactic parsing with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop</title>
		<meeting>the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">From Baby Steps to Leapfrog: How &quot;Less is More&quot; in unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of NAACL-HLT</title>
		<meeting>of NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Viterbi training improves unsupervised dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Computational Natural Language Learning</title>
		<meeting>the Fourteenth Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bootstrapping dependency grammar inducers from incomplete sentence fragments via austere models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Grammatical Inference</title>
		<meeting>the 11th International Conference on Grammatical Inference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Breaking out of local optima with count transforms and model recombination: A study in grammar induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Valentin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiyan</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1983" to="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unambiguity regularization for unsupervised learning of probabilistic grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasant</forename><surname>Honavar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</title>
		<meeting>the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1324" to="1334" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
