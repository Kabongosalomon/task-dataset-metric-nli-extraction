<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Conceptual Compression Ivo Danihelka</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederic</forename><surname>Besse</surname></persName>
							<email>fbesse@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">Jimenez</forename><surname>Rezende</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">KAROLG@GOOGLE.COM</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">DANILOR@GOOGLE.COM</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">WIERSTRA@GOOGLE.COM Google DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Conceptual Compression Ivo Danihelka</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce a simple recurrent variational autoencoder architecture that significantly improves image modeling. The system represents the stateof-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Images contain a large amount of information that is a priori stored independently in the pixels. In the semisupervised learning regime where a large number of images is available but only a small number of labels, one would like to leverage this information to create representations that allow for better (and especially faster) generalization. Intuitively one expects such representations to explicitly extract global conceptual aspects of an image.</p><p>In this paper we propose a method that is able to transform an image into a progression of increasingly detailed representations, ranging from global conceptual aspects to low level details (see <ref type="figure" target="#fig_0">Figures 1 &amp; 2)</ref>. At the same time, our model greatly improves latent variable image modeling compared to earlier implementations of deep variational auto-encoders <ref type="bibr" target="#b13">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b18">Rezende et al., 2014;</ref><ref type="bibr" target="#b6">Gregor et al., 2014)</ref>. Furthermore, it has the advantage of being a simple homogeneous architecture not requiring complex design choices, which is similar to the recurrent structure of DRAW <ref type="bibr" target="#b7">(Gregor et al., 2015)</ref>. Last, it provides an important insight into building good variational auto-encoder models of images: the use of multiple layers of stochastic variables that are all 'close' to the pix-els significantly improves performance.  <ref type="bibr">[Omniglot]</ref>. The top row shows full reconstructions from the model. The subsequent rows were obtained by storing the first t groups of latent variables and generating the remaining ones from the model <ref type="bibr">(t = 1, 4, 7, 10, 13, 16, 19, 22, 25, 28</ref> are shown, out of a total of 30 steps, from top to bottom). Each group of four columns shows different samples at a given compression level. We see that variations in later samples lie in small details, such as the precise placement of strokes. Reducing the number of stored bits tends to preserve the overall shape, but increases the symbol variation. Eventually a varied set of symbols are generated. Nevertheless even in the first row there is a clear difference between variations produced from a given symbol and those between different symbols.</p><p>The system's ability to stratify information enables it to perform high quality lossy compression, by storing only a subset of latent variables, starting with the high level ones, and generating the remainder during decompression (see <ref type="figure" target="#fig_3">Figure 4</ref>).</p><p>Currently the ultimate arbiter of lossy compression remains human evaluation. Other simple measures such as the L2 distance between compressed and original images are inappropriate -for example if a particular generated grass texture is sharp, but different from the one in the original image, it will yield a large L2 distance yet should, at the  <ref type="bibr">2,</ref><ref type="bibr">4,</ref><ref type="bibr">6,</ref><ref type="bibr">8,</ref><ref type="bibr">10,</ref><ref type="bibr">14,</ref><ref type="bibr">18,</ref><ref type="bibr">25</ref>, 32 of the model with 32 steps are shown. same time, be considered conceptually close to the original.</p><p>Achieving good lossy compression while storing only high level latent variables would imply that representations learned at a high level contain information similar to that used by humans to judge images. As humans outperform the best machines at learning abstract representations, human evaluation of lossy compression obtained by these generative models constitutes a reasonable test of the quality of representations learned by these models.</p><p>In the following we discuss variational auto-encoders and compression in more detail, present the algorithm and demonstrate the results on generation quality and compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Variational Auto-Encoders</head><p>Numerous techniques exist for unsupervised learning in deep networks, e.g. sparse auto-encoders and sparse coding <ref type="bibr" target="#b11">(Kavukcuoglu et al., 2010;</ref><ref type="bibr" target="#b16">Le, 2013)</ref>, denoising autoencoders <ref type="bibr" target="#b23">(Vincent et al., 2010)</ref>, deconvolutional networks <ref type="bibr" target="#b25">(Zeiler et al., 2010)</ref>, restricted Boltzmann machines <ref type="bibr" target="#b8">(Hinton &amp; Salakhutdinov, 2006)</ref>, deep Boltzmann machines <ref type="bibr" target="#b19">(Salakhutdinov &amp; Hinton, 2009</ref>), generative adversarial networks <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref> and variational autoencoders <ref type="bibr" target="#b13">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b18">Rezende et al., 2014;</ref><ref type="bibr" target="#b6">Gregor et al., 2014)</ref>.</p><p>In this paper we focus on the class of models in the variational auto-encoding framework.</p><p>Since we are also interested in compression, we present them from an information-theoretic perspective. Variational autoencoders typically consist of two neural networks: one that generates samples from latent variables ('imagination'), and one that infers latent variables from observations ('recognition'). The two networks share the latent variables. Intuitively speaking one might think of these variables as specifying, for a given image, at different levels of abstraction, whether a particular object such as a cat or a dog is present in the input, or perhaps what the exact position and intensity of an edge at a given location might be.</p><p>During the recognition phase the network acquires information about the input and stores it in the latent variables, reducing their uncertainty. For example, at first not knowing whether a cat or a dog is present in the image, the network observes the input and becomes nearly certain that it is a cat. The reduction in uncertainty is quantitatively equal to the amount of information the network acquired about the input. During generation the network starts with uncertain latent variables and selects their values from a prior distribution that specifies this uncertainty (e.g. it chooses a dog). Different choices will produce different samples.</p><p>Variational auto-encoders provide a natural framework for unsupervised learning -we can build networks with layers of stochastic variables and expect that, after learning, the representations become increasingly more abstract for higher levels of the hierarchy. The questions then are: can such a framework indeed discover such representations both in principle and in practice, are such networks powerful enough for modeling real data, and what techniques one needs to make it work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Conceptual Compression</head><p>Variational auto-encoders can not only be used for representation learning but also for compression. The training objective of variational auto-encoders is to compress the total amount of information needed to encode the input. They achieve this by using information-carrying latent variables that express what, before compression, was encoded using a larger amount of information in the input. The information in the layers and the remaining information in the input can be encoded in practice as explained later in this paper.</p><p>The amount of lossless compression one is able to achieve is bounded by the underlying entropy of the image distribution. Additionally, most image information as measured in bits is contained in the fine details of the image. Thus we might reasonably expect that lossless compression will never improve by more than a factor of two in comparison to current performance. Lossy compression, on the other hand, holds much more potential for improvement. In this case we want to compress an image by a certain amount, allowing some information loss, while maximizing both quality and similarity to the original image. As an example, at a low level of compression (close to lossless compression), we could start by reducing pixel precision, e.g. from 8 bits to 7 bits. Then, as in JPEG, we could express a local 8x8 neighborhood in a discrete cosine transform basis and store only the most significant components. This way, instead of introducing quantization artifacts in the image that would appear if we kept decreasing pixel precision, we preserve higher level structures but to a lower level of precision. However, what can we do beyond that as we keep pushing the compression? We would like to preserve the most important aspects of the image. What determines what is important?</p><p>Let us imagine that we are compressing images of cats and dogs and would like to compress an image down to one bit. What would that bit be? One would imagine that it should represent whether the image contains either a cat or a dog. How would we then get an image out of this single bit? If we have a good generative model, we can simply generate the entire image from this one latent variable, an image of a cat if the bit corresponds to 'cat', and an image of a dog otherwise. Now let us imagine that instead of compressing to one bit we wanted to compress down to ten bits. Now we can store the most important properties of the animal as well -e.g. its type, color, and basic pose. The rest would be 'filled in' by the generative model that is conditioned on this information. If we increase the number of bits further we can preserve more and more about the image, while generating the fine details such as hair, or the exact pattern of the floor, etc. Most bits are in fact about such low level details. We call this kind of compression -compressing by giving priority to higher levels of representation and generating the remainder -'conceptual compression'. We suggest that this should be the ultimate objective of lossy compression.</p><p>Importantly, if we solve deep representation learning with latent variable generative models that generate high quality samples, we achieve the objective of lossy compression mentioned above. We can see this as follows. Assume that the network has learned a hierarchy of progressively more abstract representations. Then, to get different levels of compression, we can store only the corresponding number of topmost layers and generate the rest. By solving unsupervised deep learning, the network would order information according to its importance and store it with that priority.</p><p>While the ultimate goal of unsupervised learning remains elusive, we make a step in this direction, and show that our network learns to order information from a rather global level to precise details in images, without being hand-engineered to do this explicitly, as illustrated in <ref type="figure">Figures</ref> 1 and 2. This information separation already allows us to achieve better compression quality than JPEG and JPEG2000 as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. While we are not bound by the same constraints as these algorithms, such as speed and memory, these results demonstrate the potential of this method, which will get better as latent variable generative models improve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">The Importance of Recurrent Feedback</head><p>What are the challenges involved in turning latent variable models into state-of-the-art generative models of images? Many successful vision architectures (e.g. <ref type="bibr" target="#b20">Simonyan &amp; Zisserman, 2014)</ref> have highly over-complete representations that contain many more neurons in hidden layers than pixels. These representations need to be combined to get a very sharp distribution at the pixel level if the pixels are modeled independently. This distribution corresponds to salt and pepper noise which is not present in natural images to a perceptible level. This poses a major challenge.</p><p>After experimenting with deep variational auto-encoders we concluded that it was exceedingly difficult to obtain satisfactory results with a single computational pass through the network. Instead we propose that the network needs the ability to correct itself over a number of time steps. Thus, sharp reconstructions should not be a property of high-precision values in the network, but should rather be the result of an iterative feedback mechanism that is robust to network parameter change.</p><p>Such a mechanism is provided by the DRAW algorithm <ref type="bibr" target="#b7">(Gregor et al., 2015)</ref>, which is a recurrent type of variational auto-encoder. At each time step, DRAW maintains a provisionary reconstruction, takes in information about a given image, stores it in latent variables and updates the reconstruction. Keeping track of the reconstruction aids the iterative feedback mechanism which is learned by backpropagation. Computation is both deep -in iterations -and close to the pixels.</p><p>We introduce convolutional DRAW. It features convolutions, latent prior modeling, a Gaussian input distribution (for natural images) and, in some experiments, a multilayer architecture. However, it does not use an explicit attentional mechanism. We note that even the single-layer version is already a deep generative model which can decide to process higher level information first before focusing on details, as we demonstrate to some degree. We also experiment with making convolutional DRAW hierarchical in a similar way that we would build conventional deep variational auto-encoders <ref type="bibr" target="#b6">(Gregor et al., 2014)</ref> -stacking more layers of latent and deterministic variables. We believe that the recurrence is important not just for accurate pixel reconstructions, but also at higher levels. For example, when the network decides to generate edges at different locations, it needs to make sure that they are aligned. It is hard to imagine this happening in a single computational pass through the network. Similarly at higher levels, when it decides to generate objects, they need to be generated with the right relationship to one another. And finally at the scene level, one probably does not generate entire scenes at once, but rather one step at a time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Comparison to Non-variational Models</head><p>Let us relate this discussion to two other families of generative models, specifically generative adversarial networks (GANs; <ref type="bibr" target="#b3">Goodfellow et al. (2014)</ref>) and auto-regressive pixel models. GANs have been demonstrated to be able to generate realistic looking images <ref type="bibr" target="#b1">(Denton et al., 2015;</ref><ref type="bibr" target="#b17">Radford et al., 2015)</ref>, with properly aligned edges, using a simple feedforward generative network <ref type="bibr" target="#b17">(Radford et al., 2015)</ref>. GANs also contain two networks -a generative network that is the same as in variational auto-encoders, and a classification network. The classification network is presented with both real and model-generated images and tries to classify them according to their true nature -real or modelgenerated. The generative network gets gradients from the classification network, changing its weights in an attempt to make the generated images be judged as real ones by the classification network. This makes the generation network produce realistic images that 'fool' the classification network. It needs to produce a wide diversity of images, not just one realistic image, because if it produced only one (or a small number of them), the classification network would classify that image as generated and others as realistic, and be almost always correct. This actually happens in practice, and one has to apply a variety of techniques, e.g. as in <ref type="bibr" target="#b17">(Radford et al., 2015)</ref>, to obtain sufficient image diversity. However the extent of GANs' sampling diversity is unknown and currently there is no satisfactory measure for it. So while a given network doesn't produce just one image, it is possible that it produces only a tiny subset of pos-sible realistic images, as it simply competes with the power of the classifier. For example if it generates a sharp image, it is unclear whether the system is also capable of generating its translated version, or simply a slightly distorted version.</p><p>Finally there is another way to get low uncertainty at the pixel level: instead of predicting pixels independently given the latents, we can decide not to use latents and iterate sequentially over the pixels, predicting a given pixel from the previous ones (from top left to bottom right) in an autoregressive fashion <ref type="bibr" target="#b0">(Bengio &amp; Bengio, 1999;</ref><ref type="bibr" target="#b4">Graves &amp; Schmidhuber, 2009;</ref><ref type="bibr" target="#b15">Larochelle &amp; Murray, 2011;</ref><ref type="bibr" target="#b5">Gregor &amp; LeCun, 2011;</ref><ref type="bibr" target="#b22">van den Oord et al., 2016)</ref>. This is as 'close' to the pixels as one can possibly be, and furthermore the procedure is purely deterministic. The disadvantage is conceptual -the information and decisions are not done at a conceptual level but at the pixel level. For example when generating cats vs dogs the decisions at the first set of pixels (top left of the image) will contain no information about a hypothetical cat or dog. But as we get to the region where these objects are, we start choosing pixels that will slowly tip the probability of generating a cat vs a dog one way or the other. As we start generating an ear, it will more likely be a cat's or a dog's and so on. However this pixel level approach and our approach are orthogonal and can be easily combined, for example by feeding the output of convolutional DRAW into the conditional computation of a pixel level model. In this paper we study the latent variable approach and make the pixels independent given the latents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Convolutional DRAW</head><p>In this section we describe the details of a single-layer version of the algorithm. Convolutional DRAW contains the following variables: input x, reconstruction r, reconstruction error , the state of the encoder recurrent net h e , the state of the decoder recurrent net h d and latent variable z. The variables h e , h d and r are recurrent (passed between different time steps) and are initialized with learned biases. Then at each time step t ∈ {1, T }, convolutional DRAW performs the following updates:</p><formula xml:id="formula_0">= x − r (1) h e = Rnn(x, , h e , h d ) (2) z ∼ q = q(z|h e ) (3) p = p(z|h d ) (4) h d = Rnn(z, h d , r) (5) r = r + W h d (6) L z t = KL(q|p)<label>(7)</label></formula><p>where W denotes a convolution and Rnn denotes a recurrent network. We use LSTM <ref type="bibr" target="#b10">(Hochreiter &amp; Schmidhuber, 1997)</ref> with convolutional operators instead of the usual linear ones. The final value of r f inal = r T contains the parameters of the input distribution. For binary images we use the Bernoulli distribution. For natural images we use the Gaussian distribution with mean and log variance given by splitting the vector r T to obtain the input cost L x and the total cost L:</p><formula xml:id="formula_1">q x = U(x − s/2, x + s/2) (8) p x = N (r µ T , exp(r α T )) (9) L x = log(q x /p x )<label>(10)</label></formula><formula xml:id="formula_2">L = βL x + T t=1 L z t<label>(11)</label></formula><p>where the handling of real valued-ness of the inputs (8, 10) is explained below, and β = 1 being the standard setting. The algorithm is schematically illustrated in the first layer of <ref type="figure" target="#fig_2">Figure 3</ref>. The network is trained by calculating the gradient of this loss and using a stochastic gradient descent algorithm. Stochastic back-propagation through a sampling function is done as in variational auto-encoders <ref type="bibr" target="#b13">(Kingma &amp; Welling, 2014;</ref><ref type="bibr" target="#b18">Rezende et al., 2014)</ref>. Both the approximate posterior q and the prior p are Gaussian, with mean and log variance being linear functions of h e or h d , respectively.</p><p>Let us discuss how we handle the input distribution for natural images. Each pixel (per color) is one of 256 values. We could use a soft-max distribution to model it. This would result in a rather large output vector at every time step and also does not take advantage of the underlying real valuedness of intensities and therefore we opted for a Gaussian distribution. However this still needs to be converted to a discrete distribution over 256 values to calculate the negative likelihood loss L x = − log p(x|z). Instead of this, we add uniform noise to the input with width corresponding to the spacing between discrete values and calculate L x = − log p(x|z)/q 0 (x) where q 0 (x) = U(x − s/2, x + s/2) with s = 1/256 if inputs are scaled to the interval [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-layer Architectures</head><p>Next we explain how we can stack convolutional DRAW with a two layer example. The first layer is the same as the one just introduced. The second layer has the same structure: recurrent encoder, recurrent decoder and a stochastic layer. The input to the second layer is the mean µ of the approximate posterior of the first layer. The output of the second layer biases the prior of the latent variable of the first layer and is also passed as input into the first layer decoder recurrent net. This is illustrated in <ref type="figure" target="#fig_2">Figure 3</ref>. We don't use any reconstruction or error in the second layer.</p><p>Here we describe a given computational step in detail. Indices 1 and 2 denote the variables of layers 1 and 2, respectively. In addition, let µ 1 (q 1 ) be the mean of q 1 . Then, the update at a given time step is given by</p><formula xml:id="formula_3">= x − r (12) h e 1 = Rnn(x, , h e 1 , h d 1 ) (13) z 1 ∼ q 1 = q 1 (z 1 |h e 1 ) (14) h e 2 = Rnn(µ 1 (q 1 ), h e 2 , h d 2 ) (15) z 2 ∼ q 2 = q 2 (z 2 |h e 2 ) (16) p 2 = p(z 2 |h d 2 ) (17) h d 2 = Rnn(z 2 , h d 2 ) (18) p 1 = p(z 1 |h d 1 , h d 2 ) (19) h d 1 = Rnn(z 1 , h d 1 , h d 2 , r) (20) r = r + W h d 1 (21) L z t = KL(q 1 |p 1 ) + KL(q 2 |p 2 )<label>(22)</label></formula><p>Systems with more layers can be built analogously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Compression</head><p>Here we show how one can turn variational auto-encoders including convolutional DRAW into compression algorithms. We have not built the actual compressor, however, as we explain, we have strong reasons to believe it would perform as well as calculated here. Two basic approaches exist. The first one is less convenient because it needs to add extra data to the bitstream when compressing an image but has essentially a guaranteed compression rate. The other one may require some experimentation but is expected to yield a similar compression rate and can be used on a given image without needing extra data.</p><p>The underlying compression mechanism for all cases is arithmetic coding <ref type="bibr" target="#b24">(Witten et al., 1987)</ref>. Arithmetic coding takes as input a sequence of discrete variables x 1 , . . . , x t and a set of probabilities p(x t |x 1 , . . . , x t−1 ) that predict the variable at time t from previous variables. It then compresses this sequence to L = − t log 2 (p(x t |x 1 , . . . , x t−1 )) bits plus a constant of order one.</p><p>Compressing inputs using variational auto-encoders proceeds as follows: discretize each latent variable in each layer using the width of q (eq. 3), treat the resulting variables as a sequence with predictions p (eq. 4) and compress using arithmetic coding.</p><p>For this to work as explained, several things are needed. First, the discretization should be independent of the input. This can be achieved by training the network with the variance of q being a learned constant that does not depend on the input. We found that this does not have much effect on the likelihood. Second, one should evaluate the log likelihood using this discretization. One has to decide on the exact manner in which p should be computed for each discrete value. Significant tuning might be required here, for  <ref type="bibr">1, 0.15, 0.2, 0.4, 0.8 (bits per image: 153, 307, 460, 614, 1228, 2457)</ref>. In the first block, JPEG was left gray because it does not compress to this level. Images are of size 32 × 32. See appendix for 64 × 64 images.</p><p>the obtained likelihoods to be as good as the ones obtained with sampling. However this is likely to be fruitful since there exists a second, less convenient way to compress that is guaranteed to achieve this rate.</p><p>This second approach uses bits-back coding <ref type="bibr" target="#b9">(Hinton &amp; Van Camp, 1993)</ref>. We explain only the basic idea here. We discretize the latents down to a very high level of precision and use p to transmit the information. Because the discretization precision is high, the probabilities for discrete values are easily assigned. That will preserve the information but it will cost many bits, namely − log 2 (p d ) where p d is a probability under that discretization. Now, instead of sampling from the approximate posterior q when encoding an input, we encode − log 2 q(z) bits of other information into the choice of z, that we also want to transmit. When z is recovered at the receiving end, both the information about the current input and the other information is recovered and thus the information needed to encode the current input is − log 2 (p) + log 2 (q) = − log 2 (p/q). The expectation of this quantity is the KL-divergence in <ref type="formula" target="#formula_0">(7)</ref>, which therefore measures the amount of information stored in a given latent layer. The disadvantage of this approach is that we cannot encode a given input without also having some other information we want to transmit. However, this coding scheme works even if the variance of the approximate posterior is dependent on the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>For natural images, all models except otherwise specified were single-layer, with n t = 32, a kernel size of 5 × 5, and stride 2 convolutions between input layers and hidden layers with 12 latent feature maps. We trained the models on Cifar-10 and ImageNet with 320 and 160 LSTM feature maps respectively. We use the version of ImageNet presented in (van den Oord et al., 2016) that will soon be released as a standard dataset. We train the network with the Adam algorithm <ref type="bibr" target="#b12">(Kingma &amp; Ba, 2014)</ref> with learning rate 5 × 10 −4 . Occasionally, we find that the cost suddenly increases dramatically. This is probably due to the Gaussian nature of the distribution, when a given variable is produced too far from the mean relative to sigma. We observed this happening approximately once per run. To be able to keep training we store older parameters, detect such jumps and revert to the old parameters when they occur. The network then just keeps training as if nothing had happened.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Modeling Quality</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">OMNIGLOT</head><p>The recently introduced Omniglot dataset <ref type="bibr" target="#b14">(Lake et al., 2015)</ref> is comprised of 1628 character classes drawn from multiple alphabets with just 20 samples per class. Referred to by many as the 'inverse of MNIST', it was designed to study conceptual representations and generative models in a low-data regime. <ref type="table" target="#tab_0">Table 1</ref> shows likelihoods of different models compared to ours. For our model, we only calculate the upper bound (variational bound) and therefore the true likelihood is actually better. Samples generated by the model are shown in <ref type="figure" target="#fig_4">Figure 5</ref>. <ref type="table">Table 2</ref> shows likelihoods of different models on Cifar-10. We see that our method outperforms previous methods except for the just released Pixel RNN model of (van den <ref type="bibr" target="#b22">Oord et al., 2016)</ref>. As mentioned, the advantage of our model compared to such auto-regressive models is that it is a latent variable model that can be used for representation learning and lossy compression. At the same time, the two approaches are orthogonal and can be combined, for example by feeding the output of convolutional DRAW into the recurrent network of Pixel RNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">CIFAR-10</head><p>We also report the likelihood for a (non-recurrent) variational auto-encoder and standard DRAW. For the variational auto-encoder we tested architectures with multiple layers, both deterministic and stochastic but with standard functional forms, and this was the best result that we obtained. Convolutional DRAW performs significantly better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3.">IMAGENET</head><p>Additionally, we trained on the version of ImageNet prepared in <ref type="bibr" target="#b22">(van den Oord et al., 2016)</ref> which was created with the aim of making a standardized dataset to test generative models. The results are in <ref type="table" target="#tab_1">Table 3</ref>. Note that being a new dataset, no other methods have been reported on it.</p><p>In <ref type="figure" target="#fig_5">Figure 6</ref> and <ref type="figure" target="#fig_6">Figure 7</ref> we show generations from the model. We trained networks with varying input cost scales as explained in the next section. The generations are sharp and contain many details, unlike previous versions of variational auto-encoder that tend to generate blurry images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Input Cost Scaling</head><p>Each pixel (and color channel) of the data consists of 256 values, and as such, likelihood and lossless compression are well defined. When compressing the image there is much to be gained in capturing precise correlations between nearby pixels. There are a lot more bits in these low level details than in the higher level structure that we are actually interested in when learning higher level represen- tations. The network might focus on these details, ignoring higher level structure.</p><p>One way to make it focus less on the details is to scale down the cost of the input relative to the latents, that is, setting β &lt; 1 in (11). Generations for different cost scalings are shown in <ref type="figure" target="#fig_5">Figure 6</ref>, with the original objective being scale β = 1. We see that lower scales indeed have a 'cleaner' high level structure. Scale 1 contains a lot of information at the precise pixel values and the network tries to capture that, while not being good enough to properly align details and produce realistic patterns. This might be simply a matter of scaling, making layers larger, networks deeper, using more iterations, or using better functional forms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">The Dependence on Computational Depth</head><p>Convolutional DRAW uses many iterations and might be considered expensive. However we found that networks with a larger number of time steps train faster per data example as shown in <ref type="figure">Figure 8</ref> (left). To study how they train with respect to real time, we multiply the time scale of each input by the number of iterations as seen in <ref type="figure">Figure 8 (right)</ref>. We see that despite having to do several iterations, up to about n t &lt; 16, convolutional DRAW does not take more wall clock time to train than the same architecture with smaller n t . For larger n t , the training slows down, but it does eventually reach better performance than at lower n t .  <ref type="figure">Figure 8</ref>. Dependence of training times for different number of DRAW iterations. Convolutional DRAW performs several recurrent steps nt for a given input image. The left graph shows training curves for different nt as a function of the number of data presentations, and the right graph displays the same as a function of real training time. We see that despite having to do several iterations, up to about nt &lt; 16, DRAW does not take more wall clock time to train than DRAW with smaller nt. For larger nt, the training slows down, however it does eventually reach better performance than lower nt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Information Distribution</head><p>We look at how much information different levels and time steps contain. This information is simply the KL divergence in <ref type="formula" target="#formula_0">(7)</ref> and <ref type="formula" target="#formula_3">(22)</ref>. For a two layer system with one convolutional and one fully connected layer, this is shown in <ref type="figure">Figure 9</ref>. We see that the higher level contains information mainly at the beginning of the computation, whereas the lower layer starts with low information which then gradually increases. This is desirable from a conceptual point of view. It suggests that the network first finds out about an overall structure of the image and then explains the details contained within that structure. Understanding the overall structure rapidly is also convenient if the algorithm needs to respond to observations in a timely manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Lossy Compression</head><p>We can compress an image with loss of information by storing only a subset of the latent variables, typically the high levels of the hierarchy. We can do this in multilayer convolutional DRAW, storing only higher levels. However we can also store only a subset of time steps, specifically a given number of time steps at the beginning, and let the network generate the rest.</p><p>The units not stored should be generated from the prior Figure 9. Amount of information at different layers and time steps. A two-layer convolutional DRAW was trained on ImageNet, with a convolutional first layer and a fully connected second layer. The amount of information at a given layer and iteration is measured by the KL-divergence between the prior and the posterior <ref type="formula" target="#formula_3">(22)</ref>. When presented with an image, first the top layer acquires information and then the second slowly increases, suggesting that the network first acquires information about 'what is in the image' and subsequently encodes the details.</p><p>distribution <ref type="formula">(Equation 4</ref>). However we can also generate a more likely image by lowering the variance of the prior Gaussian. We show generations with full variance in row 3 of each block of <ref type="figure" target="#fig_3">Figure 4</ref> and with variance zero in row 4 of that figure. We see that using the original variance, the network generates sharp details. Because the generative model is not perfect, the resulting images are less realistic looking as we lower the number of stored time steps. For zero variance we see that the network starts with rough details making a smooth image and then refines it with more time steps. All these generations are produced with a singlelayer convolutional DRAW, and thus, despite being singlelayer, it achieves some level of 'conceptual compression' by first capturing the global structure of the image and then focusing on details.</p><p>There is another dimension we can vary for lossy compression -the input scale introduced in subsection 4.2. Even if we store all the latent variables (but not the input bits), the reconstructed images will get less detailed as we scale down the input cost.</p><p>To build a really good compressor, at each compression rate, we need to find which of the networks, input scales and number of time steps would produce visually good images. For several compression levels, we have looked at images produced by different methods and selected qualitatively which network gave the best looking images. We have not done this per image, just per compression level. We then display compressed images that we have not seen with this selection.</p><p>We compare our results to JPEG and JPEG2000 compres-sion which we obtained using ImageMagick. We found however that these compressors are unable to produce reasonable results for small images (3 × 32 × 32) at high compression rates. Instead, we concatenated 100 images into one 3 × 320 × 320 image, compressed that and extracted back the compressed small images. The number of bits per image reported is then the number of bits of this image divided by 100. This is actually unfair to our algorithm since any correlations between nearby images can be exploited. Nevertheless we show the comparison in <ref type="figure" target="#fig_3">Figure 4</ref>. Our algorithm shows better quality than JPEG and JPEG 2000 at all levels where a corruption is easily detectable. Note that even when our algorithm is trained on one specific image size, it can be used on arbitrarily sized images for those networks that contain only convolutional operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduced convolutional DRAW, a stateof-the-art generative model which demonstrates the potential of sequential computation and recurrent neural networks in scaling up latent variable models. During inference, the algorithm sequentially arrives at a natural stratification of information, ranging from global aspects to lowlevel details. An interesting feature of the method is that, when we restrict ourselves to storing just the high level latent variables, we arrive at a 'conceptual compression' algorithm that rivals the quality of JPEG2000. As a generative model, it outperforms earlier latent variable models on both the Omniglot and ImageNet datasets. <ref type="figure" target="#fig_0">Figure 10</ref>. Generated samples from a network trained on 64 × 64 ImageNet with input scaling β = 0.4. Qualitatively asking the model to be less precise seems to lead to visually more appealing samples. <ref type="figure" target="#fig_0">Figure 11</ref>. Generated samples from a network trained on 64 × 64 ImageNet with input scaling β = 1. For this value of β, the system dedicates a lot of resources to explain details, losing higher level coherence. As models get better, this problem might disappear. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Conceptual Compression</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Conceptual Compression [ImageNet] Analogous to Figure 1 but applied to natural images. Originals are placed on the bottom to compare more easily to the final reconstructions, which are nearly perfect. Here the latent variables were generated with zero variance. Iterations t =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Schematic depiction of one time slice in convolutional DRAW. X and R denote input and reconstruction respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Lossy Compression. Example images for various methods and levels of compression. Top block: original images. Each subsequent block has four rows corresponding to four methods of compression: JPEG, JPEG2000, convolutional DRAW with full prior variance for generation and convolutional DRAW with zero prior variance. Each block corresponds to a different compression level; from top to bottom, average number of bits per input dimension are: 0.05, 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Generated samples for Omniglot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Generated samples for different input cost scales. Convolutional DRAW is trained on 32 × 32 ImageNet. The scale of the input cost β in (11) is {0.2, 0.4, 0.6, 0.8, 1} for each respective block of two rows, with standard maximum likelihood being β = 1. For smaller values of β the network is less compelled to explain very fine details of images and produces cleaner larger structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Generated samples for different input cost scales on 64 × 64 ImageNet. The input cost scale β is {0.4, 0.5, 0.6, 0.8, 1} for each row respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 12 .</head><label>12</label><figDesc>Lossy Compression, Part 1 Analogous to Figure 4 of the main paper but for 64 × 64 inputs. Example images for various methods and amounts of compression. Top block: original images. Each subsequent block has four methods of compression: JPEG, JPEG2000, convolutional DRAW with full prior variance for generation and convolutional DRAW with zero prior variance. Different blocks correspond to different compression levels, from top to bottom with bits per input dimension: 0.05, 0.1, 0.15, 0.2, 0.4, 0.8. In the first block, JPEG was left gray because it does not compress to this level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Test set performance of different models on 28 × 28 Omniglot in nats.</figDesc><table><row><cell>Model</cell><cell></cell><cell>NLL Test</cell></row><row><cell cols="2">VAE (2 layer, 5 samples)</cell><cell>106.31</cell></row><row><cell cols="2">IWAE (2 layer, 50 samples)</cell><cell>103.38</cell></row><row><cell>RBM (500 hidden)</cell><cell></cell><cell>100.46</cell></row><row><cell>DRAW</cell><cell></cell><cell>&lt; 96.5</cell></row><row><cell>Conv DRAW</cell><cell></cell><cell>&lt; 91.0</cell></row><row><cell cols="3">Table 2. Test set performance of different models on CIFAR-10</cell></row><row><cell cols="3">in bits/dim. For our models we give the training performance in</cell></row><row><cell cols="3">brackets. [1] (Dinh et al., 2014), [2] (Sohl-Dickstein et al., 2015),</cell></row><row><cell cols="3">[3] (van den Oord &amp; Schrauwen, 2014), [4] (van den Oord et al.,</cell></row><row><cell>2016).</cell><cell></cell></row><row><cell>Model</cell><cell cols="2">NLL Test (Train)</cell></row><row><cell>Uniform Distribution</cell><cell></cell><cell>8.00</cell></row><row><cell>Multivariate Gaussian</cell><cell></cell><cell>4.70</cell></row><row><cell>NICE [1]</cell><cell></cell><cell>4.48</cell></row><row><cell>Deep Diffusion [2]</cell><cell></cell><cell>4.20</cell></row><row><cell>Deep GMMs [3]</cell><cell></cell><cell>4.00</cell></row><row><cell>Pixel RNN [4]</cell><cell cols="2">3.00 (2.93)</cell></row><row><cell>Deep VAE</cell><cell></cell><cell>&lt; 4.54</cell></row><row><cell>DRAW</cell><cell></cell><cell>&lt; 4.13</cell></row><row><cell>Conv DRAW</cell><cell cols="2">&lt; 3.58 (3.57)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Performance of different models on ImageNet in bits/dim. Note that the Pixel RNN method has just been released and no other methods have been tested on this dataset.</figDesc><table><row><cell></cell><cell cols="2">32 × 32</cell><cell></cell><cell cols="3">Method</cell><cell cols="7">NLL Validation (Train)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Conv DRAW</cell><cell></cell><cell></cell><cell cols="3">&lt; 4.40 (4.35)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Pixel RNN</cell><cell></cell><cell></cell><cell cols="3">3.86 (3.83)</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">64 × 64</cell><cell></cell><cell cols="3">Method</cell><cell cols="7">NLL Validation (Train)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Conv DRAW</cell><cell></cell><cell></cell><cell cols="3">&lt; 4.10 (4.04)</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Pixel RNN</cell><cell></cell><cell></cell><cell cols="3">3.63 (3.57)</cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.85 2.9 2.95 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>nt=2 nt=3 nt=4 nt=6 nt=8 nt=12 nt=16 nt=24 nt=32</cell><cell></cell><cell>2.85 2.9 2.95 3</cell><cell></cell><cell></cell><cell></cell><cell cols="2">nt=2 nt=3 nt=4 nt=6 nt=8 nt=12 nt=16 nt=24 nt=32</cell></row><row><cell>cost</cell><cell>2.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>cost</cell><cell>2.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.75</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.65</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>2.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>2.6</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>200</cell><cell>400</cell><cell>600</cell><cell>800</cell><cell>1000</cell><cell>1200</cell><cell>0</cell><cell>500</cell><cell>1000</cell><cell>1500</cell><cell>2000</cell><cell>2500</cell><cell>3000</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>data iterations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">training time = data iterations * nt</cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Aaron van den Oord, Diederik Kingma and Koray Kavukcuoglu for fruitful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head> <ref type="bibr">Figures 10,</ref><ref type="bibr">11</ref> <p>show 32 × 32 image generations for input scaling β = 0.4 and β = 1 of (11), while <ref type="figure">Figures 12, 13</ref> show 64 × 64 generations, also for β = 0.4 and β = 1.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling highdimensional discrete data with multi-layer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="400" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep generative image models using a Laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename><surname>Soumith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sherjil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Offline handwriting recognition with multidimensional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning representations by maximizing compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1108.1169</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep autoregressive networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andriy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ivo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Keeping the neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Camp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the sixth annual conference on Computational learning theory</title>
		<meeting>the sixth annual conference on Computational learning theory</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long shortterm memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning convolutional feature hierarchies for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>-Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michaël</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1090" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brenden</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="29" to="37" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="8595" to="8598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soumith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1278" to="1286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="448" to="455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jascha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Surya</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03585</idno>
		<title level="m">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factoring variations in natural images with deep gaussian mixture models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schrauwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Benjamin ; Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3518" to="3526" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Pixel recurrent neural networks</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hugo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lajoie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Isabelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="3371" to="3408" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Arithmetic coding for data compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Witten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">G</forename><surname>Cleary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="520" to="540" />
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dilip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2528" to="2535" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
