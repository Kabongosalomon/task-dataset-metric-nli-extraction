<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HYBRID COARSE-FINE CLASSIFICATION FOR HEAD POSE ESTIMATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haofan</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<address>
									<country>Horizon Robotics</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghua</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<address>
									<country>Horizon Robotics</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Information Science and Technology</orgName>
								<orgName type="institution">Dalian Maritime University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HYBRID COARSE-FINE CLASSIFICATION FOR HEAD POSE ESTIMATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T11:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Head pose estimation, which computes the intrinsic Euler angles (yaw, pitch, roll) from the human, is crucial for gaze estimation, face alignment and 3D reconstruction. Traditional approaches heavily relies on the accuracy of facial landmarks. It limits their performances, especially when the visibility of face is not in good conditions. In this paper, to do the estimation without facial landmarks, we combine the coarse and fine regression output together for a deep network. Utilizing more quantization units for the angles, a fine classifier is trained with the help of other auxiliary coarse units. Integrating regression is adopted to get the final prediction. The proposed approach is evaluated on three challenging benchmarks. It achieves the state-of-the-art on AFLW2000, BIWI and performs favorably on AFLW. Code has been released on Github. *</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Facial expression recognition is one of the most successful applications of convolutional neural network in the past few years. Recently, more and more attentions have been posed on 3D facial understanding. Most of existed methods of 3D understanding require extracting 2D facial landmarks. Establishing the corresponding relationship between 2D landmarks and a standardized 3D head model, 3D pose estimation of the head can be viewed as a by-product of the 3D understanding. While facial landmark detection has been improved by large scale, thanks to the deep neural network, the two-step based head pose estimation may have two extra errors. For example, poor facial landmark detection in the bad visual conditions harms the precision of angle estimation. Also, the precision of ad-hoc fitting of the 3D head model affects the accuracy of the pose estimation. Recently, a fined-grained head pose estimation method <ref type="bibr" target="#b0">[1]</ref> without landmarks has been proposed. It predict head pose Euler angle directly from image using a multi-loss network, where three angles are trained together and each angle loss is of two parts: a bin classification and regression component. Classification and regression components are connected through multi-loss training. However, they do not deal with the quantization errors brought by coarse bin classification.</p><p>In our proposed method, we pose higher restriction for bin classification, in order to get better result of regression. Based on our observation, the classification converges much faster than regression, which weakens the usefulness of multi-loss training scheme. But a direct refined bin classification may counteract the benefit of problem reduction. Therefore, we introduce a hybrid coarse-fine classification framework, which proves to not only be helpful to refined bin classification but also improve the performance of prediction. Proposed network is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. The main contributions of our work are summarized as below:</p><p>• Use stricter fine bin classification to reduce the error brought by coarse bin classification.</p><p>• Propose our hybrid coarse-fine classification scheme to make better refined classification.</p><p>• State-of-the-art performance for head pose estimation using CNN-based method on AFLW2000 and BIWI datasets, and close the gap with state-of-the-art on AFLW. arXiv:1901.06778v2 [cs.CV] 2 Oct 2019</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>Head pose estimation has been widely studied and diverse traditional approaches have been proposed, including Appearance Template Models <ref type="bibr" target="#b1">[2]</ref>, Detector Arrays <ref type="bibr" target="#b2">[3]</ref> and Mainfold Embedding <ref type="bibr" target="#b3">[4]</ref>. Until now, approaches to head pose estimation have adapted to deep neural network and been divided into two camps: landmark-based and landmark-free.</p><p>Landmark-based methods utilize facial landmarks to fit a standard 3D face. 3DDFA <ref type="bibr" target="#b4">[5]</ref> directly fits a 3D face model to RGB image via convolutional neural networks, and aligns facial landmarks using a dense 3D model, 3D head pose is produced in the 3D fitting process. SolvePnP tool <ref type="bibr" target="#b5">[6]</ref> also produces head pose in analogous way. However, this method usually use a mean 3D human face model which introduces intrinsic error during the fitting process.</p><p>Another recent work done by Aryaman et al. <ref type="bibr" target="#b6">[7]</ref> achieves great performance on public datasets. They propose to use a higher level representation to regress the head pose while using deep learning architectures. They use the uncertainty maps in the form of 2D soft localization heatmap images over selected 5 facial landmarks, and pass them through an convolutional neural network as input channels to regress the head pose. However, this approach still cannot avoid problem of landmark invisibility even though they use coarse location, especially when considering that their method only involves with five landmarks which make their method very fragile to invisible condition.</p><p>Landmark-free methods treat head pose estimation as a sub-problem of multi-task learning process. M. Patacchiola <ref type="bibr" target="#b7">[8]</ref> proposes a shallow network to estimate head pose, and provide a detailed analysis on AFLW dataset. KEPLER <ref type="bibr" target="#b8">[9]</ref> uses a modified GoogleNet and adopts multi-task learning to learn facial landmarks and head pose jointly. Hyperface[10] also follows multi-task learning framework, which detect faces and gender, predict facial landmarks and head pose at once. All-In-One <ref type="bibr" target="#b10">[11]</ref> adds smile prediction and age estimation to the former method.</p><p>Chang et al. <ref type="bibr" target="#b11">[12]</ref> regresses 3D head pose by a simple convolutional neural network. However, they focus on face alignment and do not explicitly evaluate their method on public datasets. Ruiz et al. <ref type="bibr" target="#b0">[1]</ref> is another landmark-free work which performs well recently, they divide three branches to predict each angle jointly, each branch is combined by classification and integral regression. Lathuiliere et. al <ref type="bibr" target="#b12">[13]</ref> proposed a CNN-based model with a Gaussian mixture of linear inverse regressions to regress head pose. Drouard et. al <ref type="bibr" target="#b13">[14]</ref> further exploits to deal with issues in <ref type="bibr" target="#b12">[13]</ref>, including illumination, variability in face orientation and in appearance, etc. by combining the qualities of unsupervised manifold learning and inverse regressions.</p><p>Although recent state-of-the-art landmark-based method has better prediction given the ground truth, they suffer from landmark invisibility and landmark inaccuracy under real scene. Robust landmark-free method introduces extra error which limits its performance. In our work, we follow the landmark-free scheme and propose hybrid coarse-fine classification scheme which intends to solve the problem of extra error introduced by coarse classification in <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hybrid coarse-fine classification</head><p>Although <ref type="bibr" target="#b0">[1]</ref> contribute great work on head pose estimation using landmark-free method, but their work still meet with some issues. They do coarse bin classification before integrate regression. Bin classification relaxes a strict regression problem into a coarse classification problem, meanwhile, it introduces extra error which limits the performance of precise prediction.</p><p>Multi-task learning (MTL) has led to success in many applications of machine learning. <ref type="bibr" target="#b14">[15]</ref> demonstrates that MTL can be treated as implicit data augmentation, representation bias, regularization and etc. Hard parameter sharing which is common in MTL, shares low representation but has task specific layer for high representation. Most of existed multi-task methods combine several related but different tasks together, such as age, gender and emotion, to learn a more general high level representation. However, hybrid classification scheme for single same task with different dimension still does not receive enough attention as far as we know.</p><p>Here, we introduce our general hybrid coarse-fine classification scheme into network, architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Hybrid scheme can be regarded as a new type of hard parameters sharing, but unlike former methods which are of different tasks, each classification branch is a related and same task with specific restriction scale. It shares advantages of MTL. First, it is helpful to reduce the risk of overfitting, as the more tasks we are learning simultaneously, the more our model has to find a universal representation that captures all of the tasks and the less is our chance of overfitting on a single fine classification task. Besides, coarse classification poses with less restriction can converge faster, thus, it can help avoid some flagrant mistakes, e.g. predict a wrong sign symbol, and make the prediction more stable.</p><p>We use more refined classification at the highest level, which can improve the regression accuracy to bits in theory, but this operation may counteract the benefit of problem reduction. Thus, we propose our hybrid coarse-fine classification scheme to offset the influence of refined classification. The problem is relaxed multiple times on different scales in order to ensure precise prediction under different classification scale. We take both coarse bin classification and relatively fine bin classification into account, each FC layer represents a different classification scale and compute its own cross-entropy loss. In the integrate regression component, we only utilize result of the most refined bin classification to compute expectation and regression loss. One regression loss and multiple classification losses are combined as a total loss. Each angle has such a combined loss and share the previous convolutional layers of the network.</p><p>Our proposed hybrid coarse-fine classification scheme can be easily added into former framework and bring performance up without much extra computing resources. The final loss for each angle is the following:</p><formula xml:id="formula_0">Loss = α * M SE(y, y * ) + num i=1 β i * H(y i , y * i ) (1)</formula><p>where H and MSE respectively designate the crossentropy loss and mean squared error loss functions, num means the number of classification branch which is set to 5 in our case.</p><p>We have tested different coefficients for the regression component and hybrid classification component, our results are presented in <ref type="table" target="#tab_2">Table 4</ref>. and <ref type="table">Table 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Integrate regression</head><p>Xiao et al. <ref type="bibr" target="#b15">[16]</ref> introduces integrate regression into human pose estimation to cope with non-differentiable post-processing and quantization error. Their work shows that a simple integral operation relates and unifies the heat map representation and joint regression. Ruiz et al. <ref type="bibr" target="#b0">[1]</ref> utilizes integrate regression in head pose estimation.</p><p>This scheme treats a direct regression problem as two steps process, a multi-class classification followed by integrate regression, by modifying the "taking-maximum"of classification to "taking-expectation", and a fine-grained predictions is obtained by computing the expectation of each output probability for the binned output.</p><p>We follow this setting in our network and use the same backbone network as <ref type="bibr" target="#b0">[1]</ref> in order to fairly compare. Intuitively, such scheme can be seen as a way of problem reduction, as bin classification is a coarse annotation rather than precise label, classification and the output are connected through multi-loss learning which makes the classification also sensitive the output. Another explanation is that bin classification uses the very stable softmax layer and cross-entropy loss, thus the network learns to predict the neighborhood of the pose in a robust fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets for Pose Estimation</head><p>We demostrate that datasets under real scene with precise head pose annotations, numerous variation on pose scale and lighting condition, is essential to make progress in this filed. Three benchmarks are used in our experiments. 300W-LP <ref type="bibr" target="#b4">[5]</ref>: is a synthetically expanded dataset, and a collection of popular in-the-wild 2D landmark datasets which have been re-annotated. It contains 61,225 samples across large poses, which is further expanded to 122,450 samples with flipping.</p><p>AFLW2000 <ref type="bibr" target="#b16">[17]</ref>: contains the first 2000 identities of the in-the-wild AFLW dataset, all of them have been re-annotated with 68 3D landmarks.</p><p>AFLW <ref type="bibr" target="#b16">[17]</ref>: contains 21,080 in-the-wild faces with largepose variations (yaw from -90 • to 90 • ).</p><p>BIWI <ref type="bibr" target="#b17">[18]</ref>: is captured in well controlled laboratory environment by record RGB-D video of different people across different head pose range using a Kinect v2 device and has better pose annotations. It contains about 15, 000 images with ±75 • for yaw, ±60 • for pitch and ±50 • for roll.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Yaw Pitch Roll MAE 3DDFA <ref type="bibr" target="#b4">[5]</ref> 5 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pose Estimation on the AFLW2000</head><p>Same backbone network as [1] is adopted. Network was trained for 25 epochs on 300L-WP using Adam optimization <ref type="bibr" target="#b18">[19]</ref> with a learning rate of 10 −6 and β 1 = 0.9, β 2 = 0.999 and ε= 10 −8 . We normalize the data before training by using the ImageNet mean and standard deviation for each color channel. Our method bins angles in the ±99 • range we discard images with angles outside of this range. Results can be seen in <ref type="table">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pose Estimation on the AFLW and BIWI Datasets</head><p>We also test our method on AFLW and BIWI dataset with same parameter setting as 4.2. Results can be seen in <ref type="table">Table 2</ref>. and <ref type="table" target="#tab_1">Table 3</ref>. Our method achieves state-of-the-art on BIWI, MAE of our method is lower the base network <ref type="bibr" target="#b0">[1]</ref> by large scale, and also achieves better performance than the recent landmark-based CNN + Heatmap <ref type="bibr" target="#b6">[7]</ref> method. We demonstrate that our method perform well on BIWI as it is captured in controlled environment, and has better ground truth annotations, but this verifies the usefulness of our hybrid coarse-fine classification scheme when the annotation is precise. We also surpass all landmark-free methods, and achieve competing performance over all methods on AFLW, following testing protocol in <ref type="bibr" target="#b8">[9]</ref>   <ref type="table">Table 5</ref>. Ablation analysis: MAE across different regression loss weights on the AFLW2000 dataset.</p><p>In this part, we present an ablation study of the hybrid coarse-fine classification. We train ResNet50 using different coefficient setting. Results can be seen in <ref type="table" target="#tab_2">Table 4</ref>. and <ref type="table">Table  5</ref>. We observe the best results on the AFLW2000 dataset when the coefficient is 2,7,5,3,1,1 by order. α,β 1 ,β 2 ,β 3 ,β 4 ,β 5 correspond to the weight of regression, 198 classes, 66 classes, 18 classes, 6 classes and 2 classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>We present a hybrid classification scheme for precise head pose estimation without facial landmarks. Our proposed method achieves state-of-the-art on BIWI and AFLW2000 dataset, and also achieves promising performance on AFLW dataset. The hybrid coarse-fine classification framework is proved to be beneficial for head pose estimating and we believe that it is not only beneficial for single specific task, it may be also helpful to other classification problem such as digits recognition, we will work on it later.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Example pose estimation using our method. The blue axis points towards the front of the face, green pointing downward and red pointing to the side.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Hybrid Coarse-fine Classification for head pose estimation. The green cude is backbone network used to extract feature, the number e.g. 198, 66, 2 in the parallelogram (FC layer) is the number of node. The upper classification branch is fine classification and used for get final prediction through integrate regression. Other branches are coarse classification and used to assist the learning. The total loss is combined by several classification loss and one regression loss as shown in equation(1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Mean average error of Euler angles across different methods on the AFLW dataset.</figDesc><table><row><cell></cell><cell>Yaw</cell><cell>Pitch</cell><cell>Roll</cell><cell>MAE</cell></row><row><cell>Liu et al.[20]</cell><cell>6.0</cell><cell>6.1</cell><cell>5.7</cell><cell>5.94</cell></row><row><cell>Ruiz et al.[1]</cell><cell>4.810</cell><cell>6.606</cell><cell>3.269</cell><cell>4.895</cell></row><row><cell>Drounard[14]</cell><cell>4.24</cell><cell>5.43</cell><cell>4.13</cell><cell>4.60</cell></row><row><cell>DMLIR[13]</cell><cell>3.12</cell><cell>4.68</cell><cell>3.07</cell><cell>3.62</cell></row><row><cell cols="2">MLP + Location[7] 3.64</cell><cell>4.42</cell><cell>3.19</cell><cell>3.75</cell></row><row><cell cols="2">CNN + Heatmap[7] 3.46</cell><cell>3.49</cell><cell>2.74</cell><cell>3.23</cell></row><row><cell>Ours</cell><cell cols="4">3.4273 2.6437 2.9811 3.0174</cell></row><row><cell cols="5">Table 2. Mean average error of Euler angles across different</cell></row><row><cell cols="5">methods on the BIWI dataset with 8-fold cross-validation.</cell></row><row><cell>Method</cell><cell>Yaw</cell><cell cols="3">Pitch Roll MAE</cell></row><row><cell cols="3">Patacchiola et al.[8] 11.04 7.15</cell><cell cols="2">4.40 7.530</cell></row><row><cell>KEPLER[9]</cell><cell>6.45</cell><cell>5.85</cell><cell cols="2">8.75 7.017</cell></row><row><cell>Ruiz et al.[1]</cell><cell>6.26</cell><cell>5.89</cell><cell cols="2">3.82 5.324</cell></row><row><cell cols="2">MLP + Location[7] 6.02</cell><cell>5.84</cell><cell cols="2">3.56 5.14</cell></row><row><cell>Ours</cell><cell>6.18</cell><cell>5.38</cell><cell cols="2">3.71 5.090</cell></row><row><cell cols="2">CNN + Heatmap[7] 5.22</cell><cell>4.43</cell><cell cols="2">2.53 4.06</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Ablation analysis: MAE across different classification loss weights on the AFLW2000 dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="4">(i.e. selecting 1000 images from</cell></row><row><cell cols="5">testing and remaining for training.)</cell><cell></cell><cell></cell></row><row><cell cols="7">4.4. AFLW2000 Multi-Classification Ablation</cell></row><row><cell cols="7">α β 1 β 2 β 3 β 4 β 5 MAE</cell></row><row><cell cols="2">2 1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>5.7062</cell></row><row><cell cols="2">2 3</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>5.6270</cell></row><row><cell cols="2">2 5</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>5.6898</cell></row><row><cell cols="2">2 7</cell><cell>5</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>5.3953</cell></row><row><cell cols="2">2 9</cell><cell>7</cell><cell>5</cell><cell>3</cell><cell>1</cell><cell>5.5149</cell></row><row><cell>α</cell><cell cols="6">β 1 β 2 β 3 β 4 β 5 MAE</cell></row><row><cell cols="2">0.1 7</cell><cell>5</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>5.4834</cell></row><row><cell>1</cell><cell>7</cell><cell>5</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>5.6160</cell></row><row><cell>2</cell><cell>7</cell><cell>5</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>5.3953</cell></row><row><cell>4</cell><cell>7</cell><cell>5</cell><cell>3</cell><cell>1</cell><cell>1</cell><cell>5.6255</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Finegrained head pose estimation without keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nataniel</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunji</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00925</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face pose discrimination using support vector machines (svm),&quot; in Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuhui</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Wechsler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings. Fourteenth International Conference on</title>
		<meeting>Fourteenth International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="154" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Training support vector machines: an application to face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Osuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Girosit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="130" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Biased manifold embedding: A framework for person-independent head pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieping</forename><surname>Vineeth Nallure Balasubramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Face alignment across large poses: A 3d solution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="146" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Complete solution classification for the perspective-three-point problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Shan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Rong</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang-Fei</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="930" to="943" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Nose, eyes and ears: Head pose estimation by locating facial keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aryaman</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalpit</forename><surname>Thakkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vineet</forename><surname>Gandhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Narayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.00739</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Head pose estimation in the wild using convolutional neural networks and adaptive gradient methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Cangelosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page" from="132" to="143" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kepler: Keypoint and pose estimation of unconstrained faces by learning efficient h-cnn regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azadeh</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Hyperface: A deep multi-task learning framework for face detection,&quot; landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An all-in-one convolutional neural network for face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajeev</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Carlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Faceposenet: Making a case for landmark-free face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng-Ju</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><forename type="middle">Tuan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iacopo</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshop</title>
		<imprint>
			<publisher>ICCVW</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1599" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep mixture of linear inverse regressions applied to head-pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Lathuilière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Juge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Mesejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Muñoz-Salinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust head-pose estimation based on partially-latent mixture of linear regressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Drouard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Horaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Deleforge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1428" to="1440" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Sileye Ba, and Georgios Evangelidis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">An overview of multi-task learning in deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05098</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Annotated facial landmarks in the wild: A large-scale, real-world database for facial landmark localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Workshops (ICCV Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2144" to="2151" />
		</imprint>
	</monogr>
	<note>2011 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Real time head pose estimation from consumer depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Weise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kinga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d head pose estimation with convolutional neural network trained on synthetic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiabing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yumeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingtao</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Processing (ICIP), 2016 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1289" to="1293" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
