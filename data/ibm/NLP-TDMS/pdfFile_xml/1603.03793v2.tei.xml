<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Training with Exploration Improves a Greedy Stack LSTM Parser</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2016-09-13">13 Sep 2016</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
							<email>miguel.ballesteros@upf.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
							<email>yoav.goldberg@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">Computer Science Department</orgName>
								<orgName type="institution">Bar-Ilan University</orgName>
								<address>
									<settlement>Ramat Gan</settlement>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
							<email>cdyer@google.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Google DeepMind</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
							<email>nasmith@cs.washington.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">♦ NLP Group</orgName>
								<orgName type="institution">Pompeu Fabra University</orgName>
								<address>
									<settlement>Barcelona</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Training with Exploration Improves a Greedy Stack LSTM Parser</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2016-09-13">13 Sep 2016</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We adapt the greedy stack LSTM dependency parser of <ref type="bibr" target="#b2">Dyer et al. (2015)</ref> to support a training-with-exploration procedure using dynamic oracles <ref type="bibr" target="#b2">(Goldberg and Nivre, 2013)</ref> instead of assuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure <ref type="bibr" target="#b13">(Yamada and Matsumoto, 2003;</ref><ref type="bibr" target="#b7">Nivre, 2003;</ref><ref type="bibr" target="#b8">Nivre, 2004;</ref><ref type="bibr" target="#b9">Nivre, 2008)</ref>. The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state 1 and predicts the next action to take conditioned on the state. The state is of unbounded size. <ref type="bibr" target="#b2">Dyer et al. (2015)</ref> presented a parser in which the parser's unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able to capture information from the entirety of the state, without resorting to locality assumptions that were common in most other transition-based parsers. The use of a novel stack LSTM data structure allows the parser to maintain a constant time per-state update, and retain an overall linear parsing time.</p><p>The <ref type="bibr">Dyer et al. parser</ref> was trained to maximize the likelihood of gold-standard transition sequences, given words. At test time, the parser makes greedy decisions according to the learned model. Although this setup obtains very good performance, the training and testing conditions are mismatched in the following way: at training time the historical context of an action is always derived from the gold standard (i.e., perfectly correct past actions), but at test time, it will be a model prediction.</p><p>In this work, we adapt the training criterion so as to explore parser states drawn not only from the training data, but also from the model as it is being learned. To do so, we use the method of Goldberg and <ref type="bibr" target="#b3">2013)</ref> to dynamically chose an optimal (relative to the final attachment accuracy) action given an imperfect history. By interpolating between algorithm states sampled from the model and those sampled from the training data, more robust predictions at test time can be made. We show that the technique can be used to improve the strong parser of Dyer et al.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Parsing Model and Parameter Learning</head><p>Our departure point is the parsing model described by <ref type="bibr" target="#b2">Dyer et al. (2015)</ref>. We do not describe the model in detail, and refer the reader to the original work. At each stage t of the parsing process, the parser state is encoded into a vector p t , which is used to compute the probability of the parser action at time t as:</p><formula xml:id="formula_0">p(z t | p t ) = exp g ⊤ zt p t + q zt z ′ ∈A(S,B) exp g ⊤ z ′ p t + q z ′ ,<label>(1)</label></formula><p>where g z is a column vector representing the (output) embedding of the parser action z, and q z is a bias term for action z. The set A(S, B) represents the valid transition actions that may be taken in the current state. Since p t encodes information about all previous decisions made by the parser, the chain rule gives the probability of any valid sequence of parse transitions z conditional on the input:</p><formula xml:id="formula_1">p(z | w) = |z| t=1 p(z t | p t ).<label>(2)</label></formula><p>The parser is trained to maximize the conditional probability of taking a "correct" action at each parsing state. The definition of what constitutes a "correct" action is the major difference between a static oracle as used by <ref type="bibr" target="#b2">Dyer et al. (2015)</ref> and the dynamic oracle explored here.</p><p>Regardless of the oracle, our training implementation constructs a computation graph (nodes that represent values, linked by directed edges from each function's inputs to its outputs) for the negative log probability for the oracle transition sequence as a function of the current model parameters and uses forward-and backpropagation to obtain the gradients respect to the model parameters <ref type="bibr">(Lecun et al., 1998, section 4)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Training with Static Oracles</head><p>With a static oracle, the training procedure computes a canonical reference series of transitions for each gold parse tree. It then runs the parser through this canonical sequence of transitions, while keeping track of the state representation p t at each step t, as well as the distribution over transitions p(z t | p t ) which is predicted by the current classifier for the state representation. Once the end of the sentence is reached, the parameters are updated towards maximizing the likelihood of the reference transition sequence (Equation 2), which equates to maximizing the probability of the correct transition, p(z gt | p t ), at each state along the path.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Training with Dynamic Oracles</head><p>In the static oracle case, the parser is trained to predict the best transition to take at each parsing step, assuming all previous transitions were correct. Since the parser is likely to make mistakes at test time and encounter states it has not seen during training, this training criterion is problematic <ref type="bibr" target="#b1">(Daumé III et al., 2009;</ref><ref type="bibr">Ross et al., 2011;</ref><ref type="bibr" target="#b2">Goldberg and Nivre, 2012;</ref><ref type="bibr">Goldberg and Nivre, 2013, inter alia)</ref>. Instead, we would prefer to train the parser to behave optimally even after making a mistake (under the constraint that it cannot backtrack or fix any previous decision). We thus need to include in the training examples states that result from wrong parsing decisions, together with the optimal transitions to take in these states. To this end we reconsider which training examples to show, and what it means to behave optimally on these training examples. The framework of training with exploration using dynamic oracles suggested by <ref type="bibr" target="#b2">Goldberg and Nivre (2012;</ref><ref type="bibr" target="#b3">2013)</ref> provides answers to these questions. While the application of dynamic oracle training is relatively straightforward, some adaptations were needed to accommodate the probabilistic training objective. These adaptations mostly follow <ref type="bibr" target="#b3">Goldberg (2013)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dynamic Oracles.</head><p>A dynamic oracle is the component that, given a gold parse tree, provides the optimal set of possible actions to take for any valid parser state. In contrast to static oracles that derive a canonical state sequence for each gold parse tree and say nothing about states that deviate from this canonical path, the dynamic oracle is well defined for states that result from parsing mistakes, and they may produce more than a single gold action for a given state. Under the dynamic oracle framework, an action is said to be optimal for a state if the best tree that can be reached after taking the action is no worse (in terms of accuracy with respect to the gold tree) than the best tree that could be reached prior to taking that action. <ref type="bibr" target="#b2">Goldberg and Nivre (2013)</ref> define the arcdecomposition property of transition systems, and show how to derive efficient dynamic oracles for transition systems that are arc-decomposable. 2 Un-fortunately, the arc-standard transition system does not have this property. While it is possible to compute dynamic oracles for the arc-standard system <ref type="bibr" target="#b2">(Goldberg et al., 2014)</ref>, the computation relies on a dynamic programming algorithm which is polynomial in the length of the stack. As the dynamic oracle has to be queried for each parser state seen during training, the use of this dynamic oracle will make the training runtime several times longer. We chose instead to switch to the arc-hybrid transition system <ref type="bibr">(Kuhlmann et al., 2011)</ref>, which is very similar to the arc-standard system but is arc-decomposable and hence admits an efficient O(1) dynamic oracle, resulting in only negligible increase to training runtime. We implemented the dynamic oracle to the arc-hybrid system as described by <ref type="bibr" target="#b3">Goldberg (2013)</ref>.</p><p>Training with Exploration. In order to expose the parser to configurations that are likely to result from incorrect parsing decisions, we make use of the probabilistic nature of the classifier. During training, instead of following the gold action, we sample the next transition according to the output distribution the classifier assigns to the current configuration. Another option, taken by Goldberg and Nivre, is to follow the one-best action predicted by the classifier. However, initial experiments showed that the onebest approach did not work well. Because the neural network classifier becomes accurate early on in the training process, the one-best action is likely to be correct, and the parser is then exposed to very few error states in its training process. By sampling from the predicted distribution, we are effectively increasing the chance of straying from the gold path during training, while still focusing on mistakes that receive relatively high parser scores. We believe further formal analysis of this method will reveal connections to reinforcement learning and, perhaps, other methods for learning complex policies.</p><p>Taking this idea further, we could increase the number of error-states observed in the training process by changing the sampling distribution so as to bias it toward more low-probability states. We do this by raising each probability to the power of arcs A, if each arc in A can be derived from p, then a valid tree structure containing all of the arcs in A can also be derived from p. This is a sufficient condition, but whether it is necessary is unknown; hence the question of an efficient, O(1) dynamic oracle for the augmented system is open. α (0 &lt; α ≤ 1) and re-normalizing. This transformation keeps the relative ordering of the events, while shifting probability mass towards less frequent events. As we show below, this turns out to be very beneficial for the configurations that make use of external embeddings. Indeed, these configurations achieve high accuracies and sharp class distributions early on in the training process.</p><p>The parser is trained to maximize the likelihood of a correct action z g at each parsing state p t according to Equation 1. When using the dynamic oracle, a state p t may admit multiple correct actions z g = {z g i , . . . , z g k }. Our objective in such cases is the marginal likelihood of all correct actions, 3</p><formula xml:id="formula_2">p(z g | p t ) = zg i ∈zg p(z g i | p t ).</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Following the same settings of Chen and Manning (2014) and <ref type="bibr" target="#b2">Dyer et al (2015)</ref> we report results 4 in the English PTB and Chinese CTB-5. The score achieved by the dynamic oracle for English is 93.56 UAS. This is remarkable given that the parser uses a completely greedy search procedure. Moreover, the Chinese score establishes the state-of-the-art, using the same settings as Chen and Manning (2014).   <ref type="formula" target="#formula_0">(2015)</ref> and Andor et al. <ref type="bibr" target="#b10">(2016)</ref>, respectively.</p><p>A'16-beam is the parser with beam larger than 1 by Andor et al. <ref type="bibr" target="#b10">(2016)</ref>. Bold numbers indicate the best results among the greedy parsers.</p><p>The error-exploring dynamic-oracle training always improves over static oracle training controlling for the transition system, but the arc-hybrid system slightly under-performs the arc-standard system when trained with static oracle. Flattening the sampling distribution (α = 0.75) is especially beneficial when training with pretrained word embeddings.</p><p>In order to be able to compare with similar greedy parsers <ref type="bibr">(Yazdani and Henderson, 2015;</ref><ref type="bibr">Andor et al., 2016) 5</ref> we report the performance of the parser on the multilingual treebanks of the CoNLL 2009 shared task <ref type="bibr" target="#b4">(Hajič et al., 2009)</ref>. Since some of the treebanks contain nonprojective sentences and arc-hybrid does not allow nonprojective trees, we use the pseudo-projective approach <ref type="bibr">(Nivre and Nilsson, 2005)</ref>. We used predicted partof-speech tags provided by the CoNLL 2009 shared task organizers. We also include results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training setup as <ref type="bibr" target="#b2">Dyer et al. (2015)</ref>; for English and Chinese we used the same pretrained word embeddings as in Table 1, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3. See <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways <ref type="bibr" target="#b2">(Goldberg and Nivre, 2012;</ref><ref type="bibr" target="#b2">Goldberg and Nivre, 2013;</ref><ref type="bibr" target="#b2">Goldberg et al., 2014;</ref><ref type="bibr">Honnibal et al., 2013;</ref><ref type="bibr">Honnibal and Johnson, 2014;</ref><ref type="bibr">5</ref> We report the performance of these parsers in the most comparable setup, that is, with beam size 1 or greedy search. <ref type="bibr" target="#b4">Gómez-Rodríguez et al., 2014;</ref><ref type="bibr" target="#b0">Björkelund and Nivre, 2015;</ref><ref type="bibr" target="#b12">Tokgöz and Eryigit, 2015;</ref><ref type="bibr" target="#b4">Gómez-Rodríguez and Fernández-González, 2015;</ref><ref type="bibr">Vaswani and Sagae, 2016)</ref>. More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learningto-search frameworks <ref type="bibr" target="#b0">(Abbeel and Ng, 2004;</ref><ref type="bibr">Daumé III and Marcu, 2005;</ref><ref type="bibr" target="#b13">Vlachos, 2012;</ref><ref type="bibr">He et al., 2012;</ref><ref type="bibr" target="#b1">Daumé III et al., 2009;</ref><ref type="bibr">Ross et al., 2011;</ref><ref type="bibr">Chang et al., 2015)</ref>. Directly modeling the probability of making a mistake has also been explored for parsing <ref type="bibr">(Yazdani and Henderson, 2015)</ref>.</p><p>Generally, the use of RNNs to conditionally predict actions in sequence given a history is spurring increased interest in training regimens that make the learned model more robust to test-time prediction errors.</p><p>Solutions based on curriculum learning <ref type="bibr">(Bengio et al., 2015)</ref>, expected loss training <ref type="bibr" target="#b11">(Shen et al., 2015)</ref>, and reinforcement learning have been proposed <ref type="bibr">(Ranzato et al., 2016)</ref>. Finally, abandoning greedy search in favor of approximate global search offers an alternative solution to the problems with greedy search <ref type="bibr" target="#b0">(Andor et al., 2016)</ref>, and has been analyzed as well <ref type="bibr" target="#b4">(Kulesza and Pereira, 2007;</ref><ref type="bibr" target="#b2">Finley and Joachims, 2008)</ref>, including for parsing <ref type="bibr" target="#b5">(Martins et al., 2009</ref>). <ref type="bibr" target="#b2">Dyer et al. (2015)</ref> presented stack LSTMs and used them to implement a transition-based dependency parser. The parser uses a greedy learning strategy which potentially provides very high parsing speed while still achieving state-of-the-art results. We have demonstrated that improvement by training the greedy parser on non-gold outcomes; dynamic oracles improve the stack LSTM parser, achieving 93.56 UAS for English, maintaining greedy search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 93.56 91.42 87.65 86.21Table 1 :</head><label>11</label><figDesc>shows the results of the parser in its different configurations. The table also shows the best result obtained with the static oracle (obtained by rerunning Dyer et al. parser) for the sake of comparison between static and dynamic training strategies. Dependency parsing: English (SD) and Chinese.</figDesc><table><row><cell></cell><cell>English</cell><cell>Chinese</cell></row><row><cell>Method</cell><cell cols="2">UAS LAS UAS LAS</cell></row><row><cell>Arc-standard (Dyer et al.)</cell><cell cols="2">92.40 90.04 85.48 83.94</cell></row><row><cell>Arc-hybrid (static)</cell><cell cols="2">92.08 89.80 85.66 84.03</cell></row><row><cell>Arc-hybrid (dynamic)</cell><cell cols="2">92.66 90.43 86.07 84.46</cell></row><row><cell cols="3">Arc-hybrid (dyn., α = 0.75) 92.73 90.60 86.13 84.53</cell></row><row><cell>+ pre-training:</cell><cell></cell></row><row><cell>Arc-standard (Dyer et al.)</cell><cell cols="2">93.04 90.87 86.85 85.36</cell></row><row><cell>Arc-hybrid (static)</cell><cell cols="2">92.78 90.67 86.94 85.46</cell></row><row><cell>Arc-hybrid (dynamic)</cell><cell cols="2">93.15 91.05 87.05 85.63</cell></row><row><cell>Arc-hybrid (dyn., α = 0.75)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS UAS LAS Arc-standard, static + PP 89.60 85.45 79.68 75.08 77.96 71.06 91.12 88.69 88.09 85.24 93.10 92.28 89.08 85.03 Arc-hybrid, dyn. + PP 90.45 86.38 80.74 76.52 85.68 79.38 91.62 89.23 89.80 87.29 93.47 92.70 89.53 85.69</figDesc><table><row><cell></cell><cell cols="2">Catalan</cell><cell cols="2">Chinese</cell><cell cols="2">Czech</cell><cell>English</cell><cell cols="2">German</cell><cell cols="2">Japanese</cell><cell>Spanish</cell></row><row><cell cols="3">Method UAS LAS + pre-training --</cell><cell cols="2">82.45 78.55</cell><cell>-</cell><cell>-</cell><cell cols="3">91.59 89.15 88.56 86.15</cell><cell>-</cell><cell>-</cell><cell>90.76 87.48</cell></row><row><cell>+ pre-training</cell><cell>-</cell><cell>-</cell><cell cols="2">83.54 79.66</cell><cell>-</cell><cell>-</cell><cell cols="3">92.22 89.87 90.34 88.17</cell><cell>-</cell><cell>-</cell><cell>91.09 87.95</cell></row><row><cell>Y'15</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>85.2</cell><cell cols="3">77.5 90.75 88.14 89.6</cell><cell>86.0</cell><cell>-</cell><cell>-</cell><cell>88.3</cell><cell>85.4</cell></row><row><cell>A'16 + pre-training</cell><cell cols="12">91.24 88.21 81.29 77.29 85.78 80.63 91.44 89.29 89.12 86.95 93.71 92.85 91.01 88.14</cell></row><row><cell>A'16-beam</cell><cell cols="12">92.67 89.83 84.72 80.85 88.94 84.56 93.22 91.23 90.91 89.15 93.65 92.84 92.62 89.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; seeTable 1). PP refers to pseudoprojective parsing. Y'15 and A'16 are beam = 1 parsers from Yazdani and Henderson</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The term "state" refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are typically stored in a stack data structure, and the words remaining to be processed.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Specifically: for every parser configuration p and group of</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">A similar objective was used by<ref type="bibr" target="#b10">Riezler et al (2000)</ref>,Charniak and Johnson (2005)  and<ref type="bibr" target="#b3">Goldberg (2013)</ref> in the context of log-linear probabilistic models.4  The results on the development sets are similar and only used for optimization and validation.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work was sponsored in part by the U. S. Army Research Laboratory and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533, and in part by NSF CAREER grant IIS-1054319. Miguel Ballesteros was supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA). Yoav Goldberg is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI), a Google Research Award and the Israeli Science Foundation (grant number 1555/15).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Non-deterministic oracles for unrestricted non-projective transition-based dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ng2004] Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng ; Andor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03099</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP. [Daumé III and Marcu2005] Hal Daumé III and Daniel Marcu. 2005. Learning as search optimization: Approximate large margin methods for structured prediction</title>
		<editor>Manning2014] Danqi Chen and Christopher D. Manning</editor>
		<meeting>of EMNLP. [Daumé III and Marcu2005] Hal Daumé III and Daniel Marcu. 2005. Learning as search optimization: Approximate large margin methods for structured prediction</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
	<note>Proc. of ICML</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Search-based structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename><surname>Daumé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="297" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Transition-based dependency parsing with stack long short-term memory</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of COLING. [Goldberg and Nivre2013] Yoav Goldberg and Joakim Nivre</title>
		<meeting>of COLING. [Goldberg and Nivre2013] Yoav Goldberg and Joakim Nivre</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="403" to="414" />
		</imprint>
	</monogr>
	<note>A tabular method for dynamic oracles. in transition-based parsing. Transactions of the association for Computational Linguistics, 2</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dynamic-oracle transition-based parsing with calibrated probabilistic output</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWPT</title>
		<meeting>of IWPT</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note>Gómez-Rodríguez and Fernández-González2015</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-Rodríguez</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fernández-González</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Gómez-Rodríguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of EMNLP</title>
		<editor>Kuhlmann et al.2011] Marco Kuhlmann, Carlos Gómez-Rodríguez, and Giorgio Satta</editor>
		<meeting>of EMNLP</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
	<note>Léon Bottou. and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Polyhedral outer approximations with application to natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ICML</title>
		<meeting>of ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Pseudo-projective dependency parsing</title>
	</analytic>
	<monogr>
		<title level="m">Nivre and Nilsson2005] Joakim Nivre and Jens Nilsson</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>Proc. of ACL</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An efficient algorithm for projective dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of IWPT</title>
		<meeting>of IWPT</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Incrementality in deterministic dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</title>
		<meeting>the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Algorithms for deterministic incremental dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joakim</forename><surname>Nivre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<editor>Marc&apos;Aurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba</editor>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="513" to="553" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Ranzato et al.2016</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Lexicalized stochastic modeling of constraint-based grammars using loglinear measures and em training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Riezler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sequence level training with recurrent neural networks. In Proc. of ICLR</title>
		<editor>Ross, Geoffrey J. Gordon, and J. Andrew Bagnell</editor>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>Proc. of AISTAT</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Minimum risk training for neural machine translation</title>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL</title>
		<meeting>of ACL</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient structured inference for transition-based parsing with neural networks and error states</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eryigit2015] Alper</forename><surname>Tokgöz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gülşen</forename><surname>Eryigit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of ACL SRW. [Vaswani and Sagae2016] Ashish Vaswani and Kenji Sagae</title>
		<meeting>of ACL SRW. [Vaswani and Sagae2016] Ashish Vaswani and Kenji Sagae</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="183" to="196" />
		</imprint>
	</monogr>
	<note>Transition-based dependency DAG parsing using dynamic oracles</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Incremental recurrent neural network dependency parser with search-based discriminative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Vlachos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. ; Hiroyasu</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Workshop on Reinforcement Learning</title>
		<editor>Henderson2015] Majid Yazdani and James Henderson</editor>
		<meeting>of the European Workshop on Reinforcement Learning</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
	<note>Proc. of CoNLL</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
