<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Point2Sequence: Learning the Shape Representation of 3D Point Clouds with an Attention-based Sequence to Sequence Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2018-11-15">15 Nov 2018</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinhai</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Shen</forename><surname>Liu</surname></persName>
							<email>liuyushen@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
							<email>zwicker@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software</orgName>
								<orgName type="department" key="dep2">Beijing National Research Center for Information Science and Technology (BNRist)</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<postCode>100084</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Point2Sequence: Learning the Shape Representation of 3D Point Clouds with an Attention-based Sequence to Sequence Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2018-11-15">15 Nov 2018</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T14:03+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Exploring contextual information in the local region is important for shape understanding and analysis. Existing studies often employ hand-crafted or explicit ways to encode contextual information of local regions. However, it is hard to capture fine-grained contextual information in hand-crafted or explicit manners, such as the correlation between different areas in a local region, which limits the discriminative ability of learned features. To resolve this issue, we propose a novel deep learning model for 3D point clouds, named Point2Sequence, to learn 3D shape features by capturing fine-grained contextual information in a novel implicit way. Point2Sequence employs a novel sequence learning model for point clouds to capture the correlations by aggregating multi-scale areas of each local region with attention. Specifically, Point2Sequence first learns the feature of each area scale in a local region. Then, it captures the correlation between area scales in the process of aggregating all area scales using a recurrent neural network (RNN) based encoder-decoder structure, where an attention mechanism is proposed to highlight the importance of different area scales. Experimental results show that Point2Sequence achieves state-of-the-art performance in shape classification and segmentation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>3D point clouds, also called point sets, are considered as one of the simplest 3D shape representations, since they are composed of only raw coordinates in 3D space. A point cloud can be acquired expediently by popular sensors such as LiDAR, conventional cameras, or RGB-D cameras. Furthermore, this kind of 3D data is widely used in 3D modeling <ref type="bibr" target="#b4">(Golovinskiy, Kim, and Funkhouser 2009)</ref>, autonomous driving <ref type="bibr" target="#b14">(Qi et al. 2017a</ref>), indoor navigation <ref type="bibr" target="#b28">(Zhu et al. 2017)</ref> and robotics <ref type="bibr" target="#b22">(Wang and Posner 2015)</ref>. However, learning features or shape representations based on point clouds by deep learning models remains a challenging problem due to the irregular nature of point clouds <ref type="bibr" target="#b15">(Qi et al. 2017b)</ref>.</p><p>As a pioneering approach, PointNet <ref type="bibr" target="#b15">(Qi et al. 2017b</ref>) resolves this challenge by directly applying deep learning on point sets. PointNet individually computes a feature of each point and then aggregates all the point features into a global feature by a pooling layer. This leads to PointNet being limited by capturing contextual information of local regions. Attempting to address this issue, several researches take the aggregation of local regions into consideration. KC-Net <ref type="bibr" target="#b19">(Shen et al. 2018</ref>) employs a kernel correlation layer and a graph-based pooling layer to capture the local information of point clouds. SO-Net <ref type="bibr" target="#b11">(Li, Chen, and Lee 2018)</ref> and DGCNN <ref type="bibr" target="#b23">(Wang et al. 2018)</ref> further explore the local structures by building k-nearest neighbors (kNN) graphs and integrating neighbors of a given point using learnable edge attributes in the graph. PointNet++ <ref type="bibr" target="#b16">(Qi et al. 2017c</ref>) first extracts features for multi-scale local regions individually and aggregates these features by concatenation, where the two steps are repeated to complete the hierarchical feature extraction. Similarly, ShapeContextNet <ref type="bibr" target="#b26">(Xie et al. 2018</ref>) segments the local region of a given point into small bins, then extracts the feature for each bin individually, and finally concatenates the features of all bins as the updated feature for the point. However, most of these previous methods employ hand-crafted or explicit ways for encoding contextual information in local regions, which makes it hard to fully capture fine-grained contextual information, such as the correlation between different areas in the feature space. However, the correlation between different areas in a local region is an important contextual information. Fully exploiting this information might enhance the discriminability of learned features and improve the performance in shape analysis tasks.</p><p>We address this issue by proposing a novel deep learning model for 3D point clouds, called Point2Sequence, to encode fine-grained contextual information in local regions in a novel implicit way. Point2Sequence employs a novel RNN-based sequence model for local regions in point clouds to capture the correlation by aggregating multi-scale areas with attention. Specifically, each local region is first separated into multi-scale areas. Then, the feature of each area scale is extracted by a shared Multi-Layer-Perceptron (MLP) layer. Finally, our novel encoder-decoder based sequence model aggregates the features of all area scales, where an attention mechanism is involved to highlight the importance of different scale areas. Experimental results show that Point2Sequence is able to learn more discriminative features from point clouds than existing methods in shape classification and part segmentation tasks.</p><p>Our contributions are summarized as follows.</p><p>• We propose Point2Sequence to learn features from point clouds by capturing correlations between different areas in a local region, which takes full advantage of encoding fine-grained contextual information in local regions. • We introduce an attention mechanism to highlight the importance of different scale areas, and the feature extraction of local regions is enhanced by leveraging the correlation between different area scales. • To the best of our knowledge, Point2Sequence is the first RNN-based model for capturing correlations between different areas in local regions of point clouds, and our outperforming results verify the feasibility of RNNs to effectively understand point clouds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Learning from point clouds by rasterization. As an irregular type of 3D data, it is intuitive to rasterize the point clouds into uniform sparse 3D grids and then apply volumetric convolutional neural networks. Some approaches <ref type="bibr" target="#b13">Maturana and Scherer 2015)</ref> represent each voxel with a binary representation which indicates whether it is occupied in the space. Nevertheless, the performance is largely limited by the time and memory consuming due to the data sparsity of 3D shapes. Several improvements <ref type="bibr" target="#b9">(Li et al. 2016;</ref><ref type="bibr" target="#b22">Wang and Posner 2015)</ref> have been proposed to relief the sparsity problem of the volumetric representation. However, the sparsity of 3D shapes is an inherent drawback, which still makes it difficult to process very large point clouds. Some recent methods <ref type="bibr" target="#b20">(Su et al. 2015;</ref><ref type="bibr" target="#b24">Wang, Pelillo, and Siddiqi 2017)</ref> have tried to project the 3D point clouds or 3D shapes into 2D views and then apply 2D CNNs to recognize them. Influenced by the great success of 2D CNNs for images, such methods have achieved dominant results in 3D shape classification and retrieval tasks <ref type="bibr" target="#b6">(Kanezaki, Matsushita, and Nishida 2016)</ref>. Due to the lack of depth information, it is nontrivial to extend view-based methods to per-point processing tasks such as point classification and shape classification. Compared with uniform 3D grids, some latest studies utilize more scalable indexing techniques such as kd-tree and octree to generate regular structures which can facilitate the use of deep learning functions. To enable 3D convolutional networks, OctNet (Riegler, Ulusoy, and Geiger 2017) build a hierarchically partition of the space by generating a set of unbalanced octrees in the regular grids, where each leaf node stores a pooled feature representation. Kd-Net (Klokov and Lempitsky 2017) performs multiplicative transformations according to the subdivisions of point clouds based on the kd-trees. However, in order to obtain rotation invariant of shapes, these methods usually require extra operations such as pre-alignment or excessive data argumentations.</p><p>Different from the above-mentioned methods, our method directly learns from point clouds without pre-alignment and voxelization.</p><p>Learning from point clouds directly. As a pioneer, PointNet <ref type="bibr" target="#b15">(Qi et al. 2017b</ref>) achieves satisfactory performance  <ref type="figure">Figure 1</ref>: The left is an airplane point cloud with two sampled centroids p ′ 1 and p ′ 2 in the red color. The right is the corresponding local regions of p ′ 1 (below) and p ′ 2 (above), where different colors represent different area scales within the local region. For example, there are four different scale</p><formula xml:id="formula_0">areas {A 1 1 , A 2 1 , A 3 1 , A 4 1 } in the local region R 1 centered by p ′ 1 .</formula><p>by directly applying deep learning methods on point sets. PointNet individually computes the feature of each point and then aggregates all the point features into a global feature by pooling. This leads to PointNet limited by capturing contextual information in local regions. Some enhancements are proposed to address this problem by combining contextual information in local regions by hand-crafted or explicit ways. PointNet++ <ref type="bibr" target="#b16">(Qi et al. 2017c</ref>) has been proposed to group points into several clusters in pyramid-like layers, where the feature of multi-scale local regions can be extracted hierarchically. PointCNN ) and SpiderCNN <ref type="bibr" target="#b27">(Xu et al. 2018</ref>) investigate the convolution-like operations which aggregate the neighbors of a given point by edge attributes in the local region graph. However, with hand-crafted or explicit ways of encoding contextual information in local regions, it is hard for these methods to capture fine-grained contextual information. In particular, the correlation between different areas in feature space is an important contextual information, which limits the discriminative ability of learned features for point cloud understanding.</p><p>Correlation learning with RNN-based models. To aggregate sequential feature vectors, recurrent neural networks <ref type="bibr" target="#b3">(Elman 1990</ref>) have shown preeminent performance in many popular tasks such as speech recognition <ref type="bibr" target="#b8">(Li and Wu 2015)</ref> or handwriting recognition <ref type="bibr" target="#b1">(Bertolami et al. 2009</ref>). Inspired by the sequence to sequence architecture ), RNN-based seq2seq models can capture the finegrained contextual information from the input sequence and effectively convert it to another sequence. Furthermore, Sutskever, Vinyals, and Le (2014) engages an attention mechanism that intuitively allows neural networks to focus on different parts of the input sequences, which is more in line with the actual situation. In addition, several kinds of attention mechanism <ref type="bibr" target="#b12">Luong, Pham, and Manning 2015)</ref> have been presented to enhance the performance of networks in machine transla- The learned global feature can be used not only for shape classification shown in (e) but also for part segmentation with some extension network shown in (f).</p><p>tion (MT). To utilize the powerful ability of correlation learning from RNN-based sequence to sequence structure, Point2Sequence adopts a seq2seq encoder-decoder architecture to learn the correlation of different areas in each local region with attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The Point2Sequence Model</head><p>In this section, we first overview our Point2Sequence model, and then detail the model including multi-scale area establishment, multi-scale area feature extraction, attentionbased sequence to sequence structure and model adjustments for segmentation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Overview</head><formula xml:id="formula_1">= {p i ∈ R 3 , i = 1, 2, · · · , N }. We first select M points P ′ = {p ′ j ∈ R 3 , j = 1, 2, · · · , M } from P to define the centroids of local re- gions {R 1 , · · · , R j , · · · , R M }. As illustrated in Figure 1, T different scale areas {A 1 j , · · · , A t j , · · · , A T j } in each lo- cal region R j centered by p ′</formula><p>j are established in the multiscale area establishment part, where T different scale areas contain [K 1 , · · · , K t , · · · , K T ] points, respectively. We then extract a D-dimensional feature s t j for each scale area A t j through the scale feature extraction part. In each local region R j , a feature sequence S j = {s 1 j , · · · , s t j , · · · , s T j } of multi-scale areas is aggregated into a D-dimensional feature vector r j by the encoder-decoder feature aggregation part. Finally, a 1024-dimensional global feature g is aggregated from the features r j of all local regions by the local region feature aggregation part. Our model can be trained for shape classification or shape part segmentation by minimizing the error according to the ground truth class labels or per point part labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-scale Area Establishment</head><p>Similar to PointNet++ <ref type="bibr" target="#b16">(Qi et al. 2017c</ref>) and ShapeCon-textNet <ref type="bibr" target="#b26">(Xie et al. 2018)</ref>, we build the structure of multiscale areas in each local region on the point cloud. This part is formed by three layers: sampling layer, searching layer and grouping layer. The sampling layer selects M points from the input point cloud as the centroids of local regions. In each local region, searching layer searches the K t points in the input points and return the corresponding point indexes. According to the point indexes, grouping layer groups the K t points from P to form each multi-scale area.</p><p>As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, a certain amount of points are first selected as the centroids of local regions by the sampling layer. We adopt the farthest point sampling (FPS) to iteratively select M (M &lt; N ) points. The new added point p ′ j is always the farthest point from points {p ′ 1 , p ′ 2 , · · · , p ′ j−1 } in the 3D space. Although random sampling is an optional choice, the FPS can achieves a more uniform coverage of the entire point cloud in the case of the same centroids.</p><p>Then the searching layer respectively finds the top [K 1 , · · · , K t , · · · , K T ] nearest neighbors for each local region R j from the input points and returns the corresponding indexes of these points. In the searching layer, we adopt kNN to find the neighbors of centroids according to the sorted Euclidean distance in the 3D space. Another optional search method is ball search <ref type="bibr" target="#b16">(Qi et al. 2017c</ref>) which selects all points within a radius around the centroid. Compared with ball search, kNN search guarantees the size of local regions and makes it insensitive to the sampling density of point clouds.</p><p>Finally, by the grouping layer, these indexes of points are used to extract the points in each area of local region R j , where we obtain the points with the size of M × K t × 3 in the scale area A t j of all local regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-scale Area Feature Extraction</head><p>To extract the feature for each multi-scale area, a simple and effective MLP layer is employed in our network. The MLP layer first abstracts the points in each scale area A t j into the feature space and then the point features are aggregated into a D-dimensional feature s t j by max pooling. Therefore, in each local region R j , a T ×D feature sequence S j of the multi-scale areas is acquired.</p><p>In addition, similar to prior studies such as SO-Net (Li, Chen, and Lee 2018), the coordinates of points in each area A t j are converted to the relative coordinate system of the centroid p ′ j by a subtraction operation: p l = p l − p ′ j , where l is the index of point in the area. By using the relative coordinate, the learned features of point sets can be invariant to transformations such as rotation and translation, which is crucial for improving the learning ability of networks. Moreover, we also combine the area feature s t j with the coordinates of area centroid p ′ j to enhance the association between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention-based Sequence to Sequence Structure</head><p>In this subsection, we propose an attention-based encoder-decoder network to capture the fine-grained contextual information of local regions. Through the multi-scale area feature extraction, each local region R j is abstracted into a feature sequence r j . To learn from the feature sequence, we adopt a RNN-based model to aggregate the features S j of size T × D into a D dimension feature vector s j in each local region by an implicit way. To further promote the correlation learning between different areas in local regions, we employ an attention mechanism to focus on items of the feature sequence.</p><p>Multi-scale area encoder. To learn the correlation between different areas, a RNN is employed as encoder to integrate the features of multi-scale areas in a local region. The RNN encoder is consisted by a hidden state h and an optional output y, which operates on the input feature sequence S j = {s 1 j , . . . , s t j , . . . , s T j } of multi-scale areas in each local region. Each item s t j of the sequence S j is a Ddimensional feature vector and the length of S j is T which is also the steps of the encoder. At each time step t, the hidden state h t of the RNN encoder is updated by</p><formula xml:id="formula_2">h t = f (h t−1 , s t j ),<label>(1)</label></formula><p>where f is a non-linear activation function which can be a long short-term memory (LSTM) unit <ref type="bibr" target="#b5">(Hochreiter and Schmidhuber 1997)</ref> or a gated recurrent unit . A RNN can learn the probability distribution over a sequence by being trained to predict the next item in the sequence. Similarly, at time t, the output y t of the encoder can be represented as</p><formula xml:id="formula_3">y t = W a h t ,<label>(2)</label></formula><p>where W a is a learnable weight matrix.</p><p>After forwarding the entire input feature sequence at step T , the last-step hidden state h T of the encoder is acquired, which contains the context information of the entire input sequence.</p><p>Local region feature decoder. To obtain the feature r j for each local region R j , we employ a RNN decoder to translate the contextual information from the multi-scale areas in R j . Different from the models in machine translation, there is no decoding target for the decoder in our case. To address this issue, we employ h T as the decoding target which contains contextual information of the entire input feature sequence. Therefore, a one-step decoding process is adopt in our network to decode the feature r j for each local region R j . Similar to the encoder, the decoder is also consisted by a hidden stateh and outputȳ. We initialize theh 0 with a zero state z 0 and the current hidden state of the decoder at step one can be updated byh</p><formula xml:id="formula_4">1 = f (z 0 , h T ),<label>(3)</label></formula><p>where f is an activation function as shown in Eq. (1). Similarly, the output of decoderȳ 1 is computed bȳ</p><formula xml:id="formula_5">y 1 = W bh1 ,<label>(4)</label></formula><p>where W b is a learnable matrix in the training. To further enhance the decoding of the contextual information in the input feature sequence S j , a context vector c is generated to help the predict of featureỹ 1 of local region with attention. Therefore, a new hidden stateh 1 and output y 1 are computed in the decoder as introduced later, where we employ the outputỹ 1 to be the feature r j for each local region.</p><p>Attention mechanism in decoder. Inspired by the thought of focusing on parts of the source sentence in the machine translation, we adopt an attention mechanism to highlight the importance of different areas in each local region. The goal is to utilize the context vector c which is generated by</p><formula xml:id="formula_6">c = T t=1 α(t)h t ,<label>(5)</label></formula><p>where α is the attention vector and t is the time step. The idea of our attention model is to consider all the hidden states of the encoder when generating the context vector c. In this model, a fixed-length attention vector α, whose  size is equal to the sequence length T of the input side, is generated by comparing the current hidden stateh 1 with each source hidden state h t as</p><formula xml:id="formula_7">α(t) = exp(score(h 1 , h t )) T t ′ =1 exp(score(h 1 , h t ′ )) .<label>(6)</label></formula><p>Here, score is referred as</p><formula xml:id="formula_8">score(h 1 , h t ) =h ⊤ 1 W c h t ,<label>(7)</label></formula><p>which is a content-based function to show the correlation between these two vectors. Here, W c is also a learnable weight matrix in the training.</p><p>With the help of Eq. (5), we can acquire the new hidden stateh 1 based on the current hidden stateh 1 in the decoder. Specifically, with the current hidden stateh 1 and the context vector c, a simple concatenation layer combines the contextual information of the two vectors to generate the attentional new hidden state as follows,</p><formula xml:id="formula_9">h 1 = tanh(W d [c;h 1 ]).</formula><p>(8) And the outputỹ 1 is similarly computed bỹ</p><formula xml:id="formula_10">y 1 = W sh1 ,<label>(9)</label></formula><p>where W d and W s are variables to be learned.</p><p>Our attention-based sequence to sequence structure aggregates the sequential features S j of multi-scale areas in each local region R j by an implicit way. So far, the features r j of all local regions with size of M × D are acquired. In the subsequent network, a 1024-dimensional global feature g of the input point cloud is extracted from the features of all local regions by the global feature extraction part. As depicted in <ref type="figure" target="#fig_1">Figure 2</ref>, we apply the global feature g to shape classification and part segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Adjustments for Segmentation</head><p>The goal of part segmentation is to predict a semantic label for each point in the point cloud. In Point2Sequence, a global feature of the input point cloud is generated. Therefore, we need to acquire the per-point feature for each point in the point cloud from the global feature. There are two optional implementations, one is to duplicate the global feature with N times <ref type="bibr" target="#b15">(Qi et al. 2017b;</ref><ref type="bibr" target="#b23">Wang et al. 2018)</ref>, and the other is to perform upsampling by interpolation <ref type="bibr" target="#b16">(Qi et al. 2017c;</ref><ref type="bibr" target="#b11">Li, Chen, and Lee 2018)</ref>. In this paper, two interpolate layers are equipped in our networks as shown in <ref type="figure" target="#fig_1">Figure  2</ref>, which propagate the feature from shape level to point level by upsampling. Compared with shape classification, it is a challenge task to distinguish the parts in the object, which requires more fine-grained contextual information of local regions. We implement the feature φ propagation according to the Euclidean distance between points in 3D space. The feature is interpolated by inverse square Euclidean distance wighted average based on k nearest neighbors as</p><formula xml:id="formula_11">φ(p) = k i=1 w(p i )φ(p i ) k i=1 w(p i ) ,<label>(10)</label></formula><p>where w(p i ) = 1 (p−pi) 2 is the inverse square Euclidean distance between p and p i .</p><p>To guide the interpolation process, the interpolated features are concatenated with the corresponding point features in the abstraction side and several MLP layers are equipped in our network to enhance the performance. Several shared fully connected layers and ReLU layers are also applied to promote the extraction of point features, like the branch in shape classification. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>In this section, we first investigate how some key parameters affect the performance of Point2Sequence in the shape classification task. Then, we compare our Point2Sequence with several state-of-the-art methods in shape classification on ModelNet10 and ModelNet40 , respectively. Finally, the performance of our Point2Sequence is evaluated in the part segmentation task on ShapeNet part dataset <ref type="bibr" target="#b18">(Savva et al. 2016</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>Network configuration. In Point2Sequence, we first sample M = 384 points as the centroids of local regions by the sampling layer. Then the searching layer and grouping layer select T = 4 scale of areas with <ref type="bibr">[16,</ref><ref type="bibr">32,</ref><ref type="bibr">64,</ref><ref type="bibr">128]</ref> points in each area of a local region. The points in each area are abstracted into a D = 128 dimensional feature by a 3-layer MLP and then these abstracted features are aggregated by max pooling. And the feature sequence of different areas in each local region is aggregated by the RNN-based encoderdecoder structure with attention. Here, we initialize the RNN encoder and decoder with h=128 dimensional hidden state, where LSTM is used as the RNN cell. The rest setting of our network is the same as in <ref type="figure" target="#fig_1">Figure 2</ref>. In addition, ReLU is used after each fully connected layer with Batch-normalization, and Dropout is also applied with drop ratio 0.4 in the fully connected layers. In the experiment, we trained our network on a NVIDIA GTX 1080Ti GPU using ADAM optimizer with initial learning rate 0.001, batch size of 16 and batch normalization rate 0.5. The learning rate and batch normalization rate are decreased by 0.3 and 0.5 for every 20 epochs, respectively.</p><p>Parameters. All the experiments in the ablation study are evaluated on ModelNet40. ModelNet40 contains 12311 CAD models from 40 categories and is split into 9843 for training and 2468 for testing. For each model, we adopt 1024 points which are uniformly sampled from mesh faces and are normalized into a unit ball as input.</p><p>We first explore the number of sampled points M which influences the distribution of local regions in point clouds.</p><p>In the experiment, we keep the settings of our network as depicted in the network configuration section and modifies the number of sampled points M from 128 to 512. The results are shown in Table3, where the instance accuracies on the benchmark of ModelNet40 have a tendency to rise first and then fall. The highest accuracy of 92.54% is reached at M = 384 sampled points. This comparison implies that Point2Sequence can extract the contextual information from local regions effectively and M = 384 is a optimum number of sampled points to coverage the whole point cloud.</p><p>Therefore, we employ the sampled points M = 384 as the  We also discuss the impact of the attention mechanism, the decoder and the encoder-decoder structure on our network. We evaluate the performance of our network without the attention mechanism (No Att), without decoder (No Dec) and without encoder-decoder structure (No ED). In No ED, we remove the encoder-decoder from our network and aggregate the features of multi-scale areas in each local region by concatenating (Con) or max pooling (MP). In <ref type="table" target="#tab_4">Table 5</ref>, the result of the attention mechanism with encoder-decoder (Att+ED) is better than the results of removing parts of the attention mechanism and encoder-decoder. This comparison shows that the decoder works better with the attention mechanism, and the decoder without attention will decrease the capability of the network. And our RNN-based sequence to sequence structure outperforms hand-crafted manners such as concatenate (Con) and max pooling (MP) in aggregating feature of multi-scale areas. Moreover, we reduce the scale of local regions T from 4 to 1 and remains the largest scale with 128 points. As depicted in <ref type="table" target="#tab_5">Table 6</ref>, we obtain a even better result of 92.62% with T = 2 (K 1 = 64, K 2 = 128). The results with T &gt; 1 are better than the result with T = 1, which shows that the strategy of multi-scale areas can be better in capturing contextual information from local regions. Therefore, the number of areas T affects the performance of Point2Sequence in extracting the information from local regions.</p><p>Finally, based on the setting of multi-scale areas T = 2, we explore the effect of learning rate (LR) by setting it to GT Ours GT Ours GT Ours GT Ours <ref type="figure">Figure 3</ref>: Visualization of part segmentation results. In each shape pair, first column is the ground truth (GT), and second column is our predicted result, where parts with the same color have a consistent meaning. From left to right: bag, airplane, car, and cup. 0.0005 and 0.002. As shown in <ref type="table" target="#tab_6">Table 7</ref>, the highest accuracy is reached at LR = 0.001. Therefore, LR = 0.001 is the optimal choice of Point2Sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shape Classification</head><p>In this subsection, we evaluate the performance of Point2Sequence on ModelNet10 and ModelNet40 benchmarks, where ModelNet10 contains 4899 CAD models which is split into 3991 for training and 908 for testing. <ref type="table" target="#tab_0">Table 1</ref> compares Point2Sequence with the state-of-the-art methods in the shape classification task on ModelNet10 and ModelNet40. We compare our method with the results of eight recently ranked methods on each benchmark in terms of class average accuracy and instance average accuracy. For fair comparison, all the results in <ref type="table" target="#tab_0">Table 1</ref> are obtained under the same condition, which handles with raw point sets without the normal vector. By optimizing the cross entropy loss function in the training process, on both benchmarks, Point2Sequence outperforms other methods in class average accuracies and instance average accuracies. In ModelNet40 shape classification, our method achieves the instance accuracy of 92.6% which is 1.9% and 0.2% higher than Point-Net++ <ref type="bibr" target="#b16">(Qi et al. 2017c</ref>) and DGCNN <ref type="bibr" target="#b23">(Wang et al. 2018</ref>), respectively. Experimental results show that Point2Sequence outperforms other methods by extracting the contextual information of local regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part Segmentation</head><p>To further validate that our approach is qualified for point cloud analysis, we evaluate Point2Sequence on the semantic part segmentation task. The goal of part segmentation is to predict semantic part label for each point of the input point cloud. As dipected in <ref type="figure" target="#fig_1">Figure 2</ref>, we build the part segmentation branch to implement the per-point classification.</p><p>In part segmentation, ShapeNet part dataset is used as our benchmark for the part segmentation task, the dataset con-tains 16881 models from 16 categories and is spit into train set, validation set and test set following PointNet++. There are 2048 points sampled from each 3D shape, where each point in a point cloud object belongs to a certain one of 50 part classes and each point cloud contains 2 to 5 parts.</p><p>We employ the mean Intersection over Union (IoU) proposed in <ref type="bibr" target="#b15">(Qi et al. 2017b</ref>) as the evaluation metric. For each shape, the IoU is computed between groundtruth and the prediction for each part type in the shape category. To calculate the mIoU for each shape category, we compute the average of all shape mIoUs in the shape category. Overall mIoU is also calculated as the average mIoUs over all test shapes. Similar to the shape classification task, we optimized the cross entropy loss in the training process. We compare our results with PointNet <ref type="bibr" target="#b15">(Qi et al. 2017b</ref>), PointNet++ <ref type="bibr" target="#b16">(Qi et al. 2017c</ref>), Kd-Net (Klokov and Lempitsky 2017), SO-Net (Li, Chen, and Lee 2018), KC-Net <ref type="bibr" target="#b19">(Shen et al. 2018)</ref>, ShapeContextNet <ref type="bibr" target="#b26">(Xie et al. 2018</ref>) and DGCNN <ref type="bibr" target="#b23">(Wang et al. 2018)</ref>. In <ref type="table" target="#tab_1">Table 2</ref>, we report the performance of Point2Sequence in each category and the mean IoU of all testing shapes. Compared with the stated-of-the-art methods, Point2Sequence acquires the best mean instance IoU of 85.2% and achieves the comparable performances on many categories. <ref type="figure">Figure 3</ref> visualizes some examples of our predicted results, where our results are highly consistent with the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>In this paper, we propose a novel representation learning framework for point cloud processing in the shape classification and part segmentation tasks. An attention-based sequence to sequence model is proposed to utilize a sequence of multi-scale areas, which focuses on learning the correlation of different areas in a local region. To enhance the performance, an attention mechanism is adopted to highlight the importance of multi-scale areas in the local region. Experimental results show that our method achieves competitive performances with the state-of-the-art methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Our Point2Sequence architecture. Point2Sequence first samples local regions from an input point cloud and establishes multi-scale areas in each local region in (a). Then, MLP layer is employed to extract the feature of each multi-scale area in (b). Subsequently, the feature of each local region is extracted by attention-based seq2seq structure in (c). Finally, the global feature of the point cloud is obtained by aggregating the features of all sampled local regions in (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2</head><label>2</label><figDesc>illustrates our Point2Sequence architecture. Our model is formed by six parts: (a) Multi-scale area establishment, (b) Area feature extraction, (c) Encoder-decoder feature aggregation, (d) Local region feature aggregation, (e) Shape classification and (f) Shape part segmentation. The input of our network is a raw point set P</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The shape classification accuracy (%) comparison on ModelNet10 and ModelNet40.</figDesc><table><row><cell>Method</cell><cell>Input</cell><cell cols="4">ModelNet10 Class Instance Class Instance ModelNet40</cell></row><row><cell>PointNet (Qi et al. 2017b)</cell><cell>1024 × 3</cell><cell>-</cell><cell>-</cell><cell>86.2</cell><cell>89.2</cell></row><row><cell>PointNet++ (Qi et al. 2017c)</cell><cell>1024 × 3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>90.7</cell></row><row><cell>ShapeContextNet (Xie et al. 2018)</cell><cell>1024 × 3</cell><cell>-</cell><cell>-</cell><cell>87.6</cell><cell>90.0</cell></row><row><cell>Kd-Net (Klokov and Lempitsky 2017)</cell><cell>2 15 × 3</cell><cell>93.5</cell><cell>94.0</cell><cell>88.5</cell><cell>91.8</cell></row><row><cell>KC-Net (Shen et al. 2018)</cell><cell>1024 × 3</cell><cell>-</cell><cell>94.4</cell><cell>-</cell><cell>91.0</cell></row><row><cell>PointCNN (Li et al. 2018)</cell><cell>1024 × 3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>91.7</cell></row><row><cell>DGCNN (Wang et al. 2018)</cell><cell>1024 × 3</cell><cell>-</cell><cell>-</cell><cell>90.2</cell><cell>92.2</cell></row><row><cell>SO-Net (Li, Chen, and Lee 2018)</cell><cell cols="2">2048 × 3 93.9</cell><cell>94.1</cell><cell>87.3</cell><cell>90.9</cell></row><row><cell>Ours</cell><cell cols="2">1024 × 3 95.1</cell><cell>95.3</cell><cell>90.4</cell><cell>92.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The accuracies (%) of part segmentation on ShapeNet part segmentation dataset.</figDesc><table><row><cell></cell><cell>mean</cell><cell>air.</cell><cell>bag</cell><cell>cap</cell><cell>car</cell><cell>cha.</cell><cell cols="3">Intersection over Union (IoU) ear. gui. kni. lam. lap. mot. mug pis. roc. ska.</cell><cell>tab.</cell></row><row><cell># SHAPES</cell><cell></cell><cell>2690</cell><cell>76</cell><cell>55</cell><cell cols="2">898 3758</cell><cell>69</cell><cell>787 392 1547 451 202 184 283</cell><cell>66</cell><cell>152 5271</cell></row><row><cell>PointNet</cell><cell>83.7</cell><cell cols="8">83.4 78.7 82.5 74.9 89.6 73.0 91.5 85.9 80.8 95.3 65.2 93.0 81.2 57.9 72.8 80.6</cell></row><row><cell>PointNet++</cell><cell>85.1</cell><cell cols="8">82.4 79.0 87.7 77.3 90.8 71.8 91.0 85.9 83.7 95.3 71.6 94.1 81.3 58.7 76.4 82.6</cell></row><row><cell cols="2">ShapeContextNet 84.6</cell><cell cols="8">83.8 80.8 83.5 79.3 90.5 69.8 91.7 86.5 82.9 96.0 69.2 93.8 82.5 62.9 74.4 80.8</cell></row><row><cell>Kd-Net</cell><cell>82.3</cell><cell cols="8">80.1 74.6 74.3 70.3 88.6 73.5 90.2 87.2 81.0 94.9 57.4 86.7 78.1 51.8 69.9 80.3</cell></row><row><cell>KCNet</cell><cell>84.7</cell><cell cols="8">82.8 81.5 86.4 77.6 90.3 76.8 91.0 87.2 84.5 95.5 69.2 94.4 81.6 60.1 75.2 81.3</cell></row><row><cell>DGCNN</cell><cell>85.1</cell><cell cols="8">84.2 83.7 84.4 77.1 90.9 78.5 91.5 87.3 82.9 96.0 67.8 93.3 82.6 59.7 75.5 82.0</cell></row><row><cell>SO-Net</cell><cell>84.9</cell><cell cols="8">82.8 77.8 88.0 77.3 90.6 73.5 90.7 83.9 82.8 94.8 69.1 94.2 80.9 53.1 72.9 83.0</cell></row><row><cell>Ours</cell><cell>85.2</cell><cell cols="8">82.6 81.8 87.5 77.3 90.8 77.1 91.1 86.9 83.9 95.7 70.8 94.6 79.3 58.1 75.2 82.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The effect of the number of sampled points M on ModelNet40. ) 91.86 92.34 92.54 91.86</figDesc><table><row><cell>M</cell><cell>128</cell><cell>256</cell><cell>384</cell><cell>512</cell></row><row><cell>Acc (%</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The effects of the type of RNN cell (CT) and hidden state dimension h on ModelNet40. network in the following experiments. Then, as shown inTable 4, we show the effect of the type of the RNN cell (RT) and the dimension of the RNN hidden state h, respectively. The accuracy degenerates to 92.18% when replacing the LSTM cell as the GRU cell. Based on the setting of h = 128, we set h to 64 and 256, which reduce the accuracy of h = 128 to 92.46% and 92.18%. The above results suggest that dimension of hidden state h = 128 and the type of hidden state RT=LSTM is more suitable for our network.</figDesc><table><row><cell>Metric</cell><cell cols="2">RT=LSTM GRU h=64</cell><cell>256</cell></row><row><cell>Acc (%)</cell><cell>92.54</cell><cell cols="2">92.18 92.46 92.18</cell></row><row><cell>setting of our</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="6">: The effects of the attention mechanism (Att) and</cell></row><row><cell cols="3">decoder (Dec) on ModelNet40.</cell><cell></cell><cell></cell></row><row><cell>Metric</cell><cell cols="4">Att+ED No Att No Dec Con</cell><cell>MP</cell></row><row><cell>Acc (%)</cell><cell>92.54</cell><cell>92.26</cell><cell>92.42</cell><cell cols="2">92.06 91.73</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The effect of the number of scales T on Model-Net40.</figDesc><table><row><cell>T</cell><cell>4</cell><cell>3</cell><cell>2</cell><cell>1</cell></row><row><cell cols="5">Acc (%) 92.54 92.46 92.63 91.94</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="2">: The effect of learning rate on ModelNet40.</cell></row><row><cell>LR</cell><cell>0.0005 0.001 0.002</cell></row><row><cell cols="2">Acc (%) 91.94 92.63 91.86</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Yu-Shen Liu is the corresponding author. This work was supported by National Key R&amp;D Program of China (2018YFB0505400), the National Natural Science Foundation of China (61472202), and Swiss National Science Foundation grant (169151). We thank all anonymous reviewers for their constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A novel connectionist system for improved unconstrained handwriting recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bertolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bunke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<title level="m">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Finding structure in time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Science</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="211" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Shape-based recognition of 3D point clouds in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Golovinskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2154" to="2161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">RotationNet: Joint object categorization and pose estimation using multiviews from unsupervised viewpoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanezaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nishida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.06208</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3D point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="863" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Constructing long short-term memory based deep recurrent neural networks for large vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4520" to="4524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">FPNN: Field probing neural networks for 3D data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07791</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">PointCNN.</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">SO-Net: Selforganizing network for point cloud analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9397" to="9406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.04025</idno>
		<title level="m">Effective approaches to attention-based neural machine translation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Voxnet: A 3D convolutional neural network for real-time object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maturana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Scherer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="922" to="928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.08488</idno>
		<title level="m">Frustum pointnets for 3D object detection from RGB-D data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Point-Net: Deep learning on point sets for 3D classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Point-Net++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5099" to="5108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">OctNet: Learning deep 3D representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Shrec16 track large-scale 3D shape retrieval from ShapeNet core55</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eurographics Workshop on 3D Object Retrieval</title>
		<meeting>the Eurographics Workshop on 3D Object Retrieval</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Mining point cloud local structures by kernel correlation and graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-view convolutional neural networks for 3D shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07829</idno>
		<title level="m">Dynamic graph CNN for learning on point clouds</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dominant set clustering and pooling for multi-view 3D object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Siddiqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attentional ShapeContextNet for point cloud recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4606" to="4615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11527</idno>
		<title level="m">Spider-CNN: Deep Learning on Point Sets with Parameterized Convolutional Filters</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Target-driven visual navigation in indoor scenes using deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kolve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICRA</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3357" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
