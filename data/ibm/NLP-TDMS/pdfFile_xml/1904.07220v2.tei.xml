<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Discriminative Model Prediction for Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goutam</forename><surname>Bhat</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Luc</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Gool</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>CVL, ETH</roleName><surname>Timofte</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>ZÃ¼rich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Switzerland</surname></persName>
						</author>
						<title level="a" type="main">Learning Discriminative Model Prediction for Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The current strive towards end-to-end trainable computer vision systems imposes major challenges for the task of visual tracking. In contrast to most other vision problems, tracking requires the learning of a robust target-specific appearance model online, during the inference stage. To be end-to-end trainable, the online learning of the target model thus needs to be embedded in the tracking architecture itself. Due to the imposed challenges, the popular Siamese paradigm simply predicts a target feature template, while ignoring the background appearance information during inference. Consequently, the predicted model possesses limited target-background discriminability.</p><p>We develop an end-to-end tracking architecture, capable of fully exploiting both target and background appearance information for target model prediction. Our architecture is derived from a discriminative learning loss by designing a dedicated optimization process that is capable of predicting a powerful model in only a few iterations. Furthermore, our approach is able to learn key aspects of the discriminative loss itself. The proposed tracker sets a new state-of-the-art on 6 tracking benchmarks, achieving an EAO score of 0.440 on VOT2018, while running at over 40 FPS. The code and models are available at https: //github.com/visionml/pytracking.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generic object tracking is the task of estimating the state of an arbitrary target in each frame of a video sequence. In the most general setting, the target is only defined by its initial state in the sequence. Most current approaches address the tracking problem by constructing a target model, capable of differentiating between the target and background appearance. Since target-specific information is only available at test-time, the target model cannot be learned in an offline training phase, as in for instance object detection. Instead, the target model must be constructed during the inference stage itself by exploiting the target information given * Both authors contributed equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image</head><p>Siamese based Ours <ref type="figure">Figure 1</ref>. Confidence maps of the target object (red box) provided by the target model obtained using i) a Siamese approach (middle), and ii) Our approach (right). The model predicted in a Siamese fashion, using only target appearance, struggles to distinguish the target from distractor objects in the background. In contrast, our model prediction architecture also integrates background appearance, providing superior discriminative power. at test-time. This unconventional nature of the visual tracking problem imposes significant challenges when pursuing an end-to-end learning solution.</p><p>The aforementioned problems have been most successfully addressed by the Siamese learning paradigm <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23]</ref>. These approaches first learn a feature embedding, where the similarity between two image regions is computed by a simple cross-correlation. Tracking is then performed by finding the image region most similar to the target template. In this setting, the target model simply corresponds to the template features extracted from the target region. Consequently, the tracker can easily be trained end-to-end using pairs of annotated images.</p><p>Despite its recent success, the Siamese learning framework suffers from severe limitations. Firstly, Siamese trackers only utilize the target appearance when inferring the model. This completely ignores background appearance information, which is crucial for discriminating the target from similar objects in the scene (see <ref type="figure">figure 1</ref>). Secondly, the learned similarity measure is not necessarily reliable for objects that are not included in the offline training set, leading to poor generalization. Thirdly, the Siamese formulation does not provide a powerful model update strategy. Instead, state-of-the-art approaches resort to simple template averaging <ref type="bibr" target="#b45">[46]</ref>. These limitations result in inferior robustness <ref type="bibr" target="#b19">[20]</ref> compared to other state-of-the-art tracking approaches.</p><p>In this work, we introduce an alternative tracking architecture, trained in an end-to-end manner, that directly addresses all aforementioned limitations. In our design, we take inspiration from the discriminative online learning procedures that have been successfully applied in recent trackers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30]</ref>. Our approach is based on a target model prediction network, which is derived from a discriminative learning loss by applying an iterative optimization procedure. The architecture is carefully designed to enable effective end-to-end training, while maximizing the discriminative ability of the predicted model. This is achieved by ensuring a minimal number of optimization steps through two key design choices. First, we employ a steepest descent based methodology that computes an optimal step length in each iteration. Second, we integrate a module that effectively initializes the target model. Furthermore, we introduce significant flexibility into our final architecture by learning the discriminative learning loss itself.</p><p>Our entire tracking architecture, along with the backbone feature extractor, is trained using annotated tracking sequences by minimizing the prediction error on future frames. We perform comprehensive experiments on 7 tracking benchmarks: VOT2018 <ref type="bibr" target="#b19">[20]</ref>, LaSOT <ref type="bibr" target="#b9">[10]</ref>, Track-ingNet <ref type="bibr" target="#b26">[27]</ref>, GOT10k <ref type="bibr" target="#b15">[16]</ref>, NFS <ref type="bibr" target="#b11">[12]</ref>, OTB-100 <ref type="bibr" target="#b42">[43]</ref>, and UAV123 <ref type="bibr" target="#b25">[26]</ref>. Our approach achieves state-of-the-art results on all 7 datasets, while running at over 40 FPS. We also provide an extensive experimental analysis of the proposed architecture, showing the impact of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Generic object tracking has undergone astonishing progress in recent years, with the development of a variety of approaches. Recently, methods based on Siamese networks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref> have received much attention due to their end-to-end training capabilities and high efficiency. The name derives from the deployment of a Siamese network architecture in order to learn a similarity metric offline. Bertinetto et al. <ref type="bibr" target="#b1">[2]</ref> utilize a fully-convolutional architecture for similarity prediction, thereby attaining high tracking speeds of over 100 FPS. Wang et al. <ref type="bibr" target="#b41">[42]</ref> learn a residual attention mechanism to adapt the tracking model to the current target. Li et al. <ref type="bibr" target="#b22">[23]</ref> employ a region proposal network <ref type="bibr" target="#b33">[34]</ref> to obtain accurate bounding boxes.</p><p>A key limitation in Siamese approaches is their inability to incorporate information from the background region or previous tracked frames into the model prediction. A few recent attempts aim to address these issues. Guo et al. <ref type="bibr" target="#b12">[13]</ref> learn a feature transformation to handle the target appearance changes and to suppress background. Zhu et al. <ref type="bibr" target="#b45">[46]</ref> handle background distractors by subtracting corresponding image features from the target template during online tracking. Despite these attempts, the Siamese trackers are yet to reach high level of robustness attained by state-of-the-art trackers employing online learning <ref type="bibr" target="#b19">[20]</ref>.</p><p>In contrast to Siamese methods, another family of trackers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b29">30]</ref> learn a discriminative classifier online to distinguish the target object from the background. These approaches can effectively utilize background information, thereby achieving impressive robustness on multiple tracking benchmarks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43]</ref>. However, such methods rely on more complicated online learning procedures that cannot be easily formulated in an end-to-end learning framework. Thus, these approaches are often restricted to features extracted from deep networks pre-trained for image classification <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25]</ref> or hand-crafted alternatives <ref type="bibr" target="#b7">[8]</ref>.</p><p>A few recent works aim to formulate existing discriminative online learning based trackers as a neural network component in order to benefit from end-to-end training. Valmadre et al. <ref type="bibr" target="#b40">[41]</ref> integrate the single-sample closedform solution of the correlation filter (CF) <ref type="bibr" target="#b14">[15]</ref> into a deep network. Yao et al. <ref type="bibr" target="#b44">[45]</ref> unroll the ADMM iterations in BACF <ref type="bibr" target="#b17">[18]</ref> tracker to learn the feature extractor and a few tracking hyper-parameters in a complex multi-stage training procedure. The BACF model learning is however restricted to the single-sample variant of the Fourier-domain CF formulation which cannot exploit multiple samples, requiring ad-hoc linear combination of filters for model adaption.</p><p>The problem of learning to predict a target model using only a few images is closely related to meta-learning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref>. A few works have already pursued this direction for tracking. Bertinetto et al. <ref type="bibr" target="#b0">[1]</ref> meta-train a network to predict the parameters of the tracking model. Choi et al. <ref type="bibr" target="#b4">[5]</ref> utilize a meta-learner to predict a targetspecific feature space to complement the general targetindependent feature space used for estimating the similarity in Siamese trackers. Park et al. <ref type="bibr" target="#b31">[32]</ref> develop a meta-learning framework employing an initial target independent model, which is then refined using gradient descent with learned step-lengths. However, constant step-lengths are only suitable for fast initial adaption of the model and does not provide optimal convergence when applied iteratively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this work, we develop a discriminative model prediction architecture for tracking. As in Siamese trackers, our approach benefits from end-to-end training. However, unlike Siamese, our architecture can fully exploit background information and provides natural and powerful means of updating the target model with new data. Our model prediction network is derived from two main principles: (i) A discriminative learning loss promoting robustness in the learned target model; and (ii) a powerful optimization strat-  <ref type="figure">Figure 2</ref>. An overview of the target classification branch in our tracking architecture. Given an annotated training set (top left), we extract deep feature maps using a backbone network followed by an additional convolutional block (Cls Feat). The feature maps are then input to the model predictor D, consisting of the initializer and the recurrent optimizer module. The model predictor outputs the weights of the convolutional layer which performs target classification on the feature map extracted from the test frame. egy ensuring rapid convergence. By such careful design, our architecture can predict the target model in only a few iterations, without compromising its discriminative power.</p><p>In our framework, the target model constitutes the weights of a convolutional layer, providing target classification scores as output. Our model prediction architecture computes these weights by taking a set of bounding-box annotated image samples as input. The model predictor includes an initializer network that efficiently provides an initial estimate of the model weights, using only the target appearance. These weights are then processed by the optimizer module, taking both target and background appearance into account. By design, our optimizer module possesses few learnable parameters in order to avoid overfitting to certain classes and scenes during offline training. Our model predictor can thus generalize to unseen objects, which is crucial in generic object tracking.</p><p>Our final tracking architecture consists of two branches: a target classification branch (see <ref type="figure">figure 2</ref>) for distinguishing the target from background, and a bounding box estimation branch for predicting an accurate target box. Both branches input deep features from a common backbone network. The target classification branch contains a convolutional block, extracting features on which the classifier operates. Given a training set of samples and corresponding target boxes, the model predictor generates the weights of the target classifier. These weights are then applied to features extracted from the test frame, in order to compute the target confidence scores. For the bounding box estimation branch, we utilize the overlap maximization based architecture introduced in <ref type="bibr" target="#b5">[6]</ref>. The entire tracking network, including the target classification, bounding box estimation and backbone modules, is trained offline on tracking datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Discriminative Learning Loss</head><p>In this section, we describe the discriminative learning loss used to derive our model prediction architecture. The input to our model predictor D consists of a training set S train = {(x j , c j )} n j=1 of deep feature maps x j â X generated by the feature extractor network F . Each sample is paired with the corresponding target center coordinate c j â R 2 . Given this data, our aim is to predict a target model f = D(S train ). The model f is defined as the filter weights of a convolutional layer tasked with discriminating between target and background appearance in the feature space X . We gather inspiration from the least-squaresbased regression take on the tracking problem, that has seen tremendous success in the recent years <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15]</ref>. However, in this work we generalize the conventional least-squares loss applied for tracking in several directions, allowing the final tracking network to learn the optimal loss from data.</p><p>In general, we consider a loss of the form,</p><formula xml:id="formula_0">L(f ) = 1 |S train | (x,c)âStrain r(x * f, c) 2 + Î»f 2 . (1)</formula><p>Here, * denotes convolution and Î» is a regularization factor. The function r(s, c) computes the residual at every spatial location based on the target confidence scores s = x * f and the ground-truth target center coordinate c. The most common choice is r(s, c) = s â y c , where y c are the desired target scores at each location, popularly set to a Gaussian function centered at c <ref type="bibr" target="#b3">[4]</ref>. However, simply taking the difference forces the model to regress calibrated confidence scores, usually zero, for all negative samples. This requires substantial model capacity, forcing the learning to focus on the negative data samples instead of achieving the best discriminative abilities. Furthermore, taking the naÃ¯ve difference does not address the problem of data imbalance between target and background.</p><p>To alleviate the latter issue of data imbalance, we use a spatial weight function v c . The subscript c indicates the dependence on the center location of the target, as detailed in section 3.4. To accommodate the first issue, we modify the loss following the philosophy of Support Vector Machines. We employ a hinge-like loss in r, clipping the scores at zero as max(0, s) in the background region. The model is thus free to predict large negative values for easy samples in the background without increasing the loss. For the target region on the other hand, we found it disadvantageous to add an analogous hinge loss max(0, 1âs). Although contradictory at a first glance, this behavior can be attributed to the fundamental asymmetry between the target and background class, partially due to the numerical imbalance. Moreover, accurately calibrated target confidences are indeed advantageous in the tracking scenario, e.g. for detecting target loss. We therefore desire the properties of standard least-squares regression in the target neighborhood.</p><p>To accommodate the advantages of both least-squares regression and the hinge loss, we define the residual function,</p><formula xml:id="formula_1">r(s, c) = v c Â· (m c s + (1 â m c ) max(0, s) â y c ) . (2)</formula><p>The target region is defined by the mask m c , having values in the interval m c (t) â [0, 1] at each spatial location t â R 2 . Again, the subscript c indicate the dependence on the target center coordinate. The formulation in <ref type="formula">(2)</ref> is capable of continuously changing the behavior of the loss from standard least squares regression to a hinge loss depending on the image location relative to the target center c. Setting m c â 1 at the target and m c â 0 in the background region yields the desired behavior described above. However, how to optimally set m c is not clear, in particular at the transition region between target and background. While the classical strategy is to manually set the mask parameters using trial and error, our end-to-end formulation allows us to learn the mask in a data-driven manner. In fact, as detailed in section 3.4, our approach learns all free parameters in the loss: the target mask m c , the spatial weight v c , the regularization factor Î», and even the regression target y c itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Optimization-Based Architecture</head><p>Here, we derive the network architecture D that predicts the filter f = D(S train ) by implicitly minimizing the error (1). The network is designed by formulating an optimization procedure. From eqs. <ref type="formula">(1)</ref> and <ref type="formula">(2)</ref> we can easily derive a closed-form expression for the gradient of the loss âL with respect to the filter f 2 . The straight-forward option is to then employ gradient descent using a step length Î±,</p><formula xml:id="formula_2">f (i+1) = f (i) â Î±âL(f (i) ) .<label>(3)</label></formula><p>However, we found this simple approach to be insufficient, even if the learning rate Î± (either a scalar or coefficientspecific) is learned by the network itself (see section 4.1). It experiences slow adaption of the filter parameters f , requiring a vast increase in the number of iterations. This harms efficiency and complicates offline learning. The slow convergence of gradient descent is largely due to the constant step length Î±, which does not depend on data or the current model estimate. We solve this issue by deriving a more elaborate optimization approach, requiring only a handful of iterations to predict a strong discriminative filter f . The core idea is to compute the step length Î± based on the steepest descent methodology, which is a common optimization technique <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b36">37]</ref>. We first approximate the loss with a quadratic function at the current estimate f (i) ,</p><formula xml:id="formula_3">L(f ) âL(f ) = 1 2 (f â f (i) ) T Q (i) (f â f (i) )+ (4) (f â f (i) ) T âL(f (i) ) + L(f (i) ) .</formula><p>Here, the filter variables f and f (i) are seen as vectors and Q (i) is positive definite square matrix. The steepest descent then proceeds by finding the step length Î± that minimizes the approximate loss (4) in the gradient direction (3). This is found by</p><formula xml:id="formula_4">solving d dÎ±L f (i) â Î±âL(f (i) ) = 0, as Î± = âL(f (i) ) T âL(f (i) ) âL(f (i) ) T Q (i) âL(f (i) )</formula><p>.</p><p>In steepest descent, the formula <ref type="formula" target="#formula_5">(5)</ref> is used to compute the scalar step length Î± in each iteration of the filter update (3). The quadratic model <ref type="formula">(4)</ref>, and consequently the resulting step length <ref type="bibr" target="#b4">(5)</ref>, depends on the choice of Q (i) . For example, by using a scaled identity matrix Q (i) = 1 Î² I we retrieve the standard gradient descent algorithm with a fixed step length Î± = Î². On the other hand, we can now integrate second order information into the optimization procedure. The most obvious choice is setting Q (i) = â 2 L âf 2 (f (i) ) to the Hessian of the loss <ref type="bibr" target="#b0">(1)</ref>, which corresponds to a second order Taylor approximation <ref type="bibr" target="#b3">(4)</ref>. For our least-squares formulation (1) however, the Gauss-Newton method <ref type="bibr" target="#b30">[31]</ref> provides a powerful alternative, with significant computational benefits since it only involves first-order derivatives. We thus set</p><formula xml:id="formula_6">Q (i) = (J (i) ) T J (i) , where J (i)</formula><p>is the Jacobian of the residuals at f (i) . In fact, neither the matrix Q (i) or Jacobian J (i) need to be constructed explicitly, but rather implemented as a sequence of neural network operations. See the supplementary material (section S2) for details. Algorithm 1 describes our target model predictor D. Note that our optimizer module can easily be employed for online model adaption as well. This is achieved by continuously extending the training set S train with new samples from the previously tracked frames. The optimizer module is then applied on this extended training set, using the current target model as the initialization f (0) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Initial Filter Prediction</head><p>To further reduce the number of optimization recursions required in D, we introduce a small network module that predicts an initial model estimate f (0) . Our initializer network consists of a convolutional layer followed by a precise ROI pooling <ref type="bibr" target="#b16">[17]</ref>. The latter extracts features from the Algorithm 1 Target model predictor D.</p><formula xml:id="formula_7">Input: Samples S train = {(x j , c j )} n j=1 , iterations N iter 1: f (0) â ModelInit(S train ) # Initialize filter (sec 3.3) 2: for i = 0, . . . , N iter â 1 do # Optimizer module loop 3: âL(f (i) ) â FiltGrad(f (i) , S train ) # Using (1)-(2) 4: h â J (i) âL(f (i) )</formula><p># Apply Jacobian of (2) 5:</p><formula xml:id="formula_8">Î± â âL(f (i) ) 2 / h 2 # Compute step length (5) 6: f (i+1) â f (i) â Î±âL(f (i) )</formula><p># Update filter 7: end for target region and pools them to the same size as the target model f . The pooled feature maps are then averaged over all the samples in S train to obtain the initial model f (0) . As in Siamese trackers, this approach only utilizes the target appearance. However, rather than predicting the final model, our initializer network is tasked with only providing a reasonable initial estimate, which is then processed by the optimizer module to provide the final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Learning the Discriminative Learning Loss</head><p>Here, we describe how the free parameters in the residual function <ref type="formula">(2)</ref>, defining the loss (1), are learned. Our residual function includes the label confidence scores y c , the spatial weight function v c and the target mask m c . While such variables are constructed by hand in current discriminative online learning based trackers, our approach in fact learns these functions from data. We parametrize them based on the distance from the target center. This is motivated by the radial symmetry of the problem, where the direction to the sample location relative to the target is of little significance. In contrast, the distance to the sample location plays a crucial role, especially in the transition from target to background. Thus, we parameterize y c , m c and v c using radial basis functions Ï k and learn their coefficients Ï k . For instance, the label y c at position t â R 2 is given by</p><formula xml:id="formula_9">y c (t) = N â1 k=0 Ï y k Ï k ( t â c ) .<label>(6)</label></formula><p>We use triangular basis functions Ï k , defined as</p><formula xml:id="formula_10">Ï k (d) = max(0, 1 â |dâkâ| â ), k &lt; N â 1 max(0, min(1, 1 + dâkâ â )), k = N â 1<label>(7)</label></formula><p>The above formulation corresponds to a continuous piecewise linear function with a knot displacement of â. Note that the final case k = N â1 represents all locations that are far away from the target center and thus can be treated identically. We use a small â to enable accurate representation of the regression label at the target-background transition. The functions v c and m c are parameterized analogously using coefficients Ï v k and Ï m k respectively in <ref type="bibr" target="#b5">(6)</ref>. For the tar- get mask m c , we constrain the values to the interval [0, 1] by passing the output from (6) through a Sigmoid function. We use N = 100 basis functions and set the knot displacement to â = 0.1 in the resolution of the deep feature space X . For offline training, the regression label y c is initialized to the same Gaussian z c used in the offline classification loss, described in section 3.6. The weight function v c is initialized to constant v c (t) = 1. Lastly, we initialize the target mask m c using a scaled tanh function. The coefficients Ï k , along with Î», are learned as part of the model prediction network D (see section 3.6). The initial and learned values for y c , m c and v c are visualized in <ref type="figure" target="#fig_0">figure 3</ref>. Notably, our network learns to increase the weight v c at the target center and reduce it in the ambiguous transition region.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Bounding Box Estimation</head><p>We utilize the overlap maximization strategy introduced in [6] for the task of accurate bounding box estimation. Given a reference target appearance, the bounding box estimation branch is trained to predict the IoU overlap between the target and a set of candidate boxes on a test image. The target information is integrated into the IoU prediction by computing a modulation vector from the reference appearance of the target. The computed vector is used to modulate the features from the test image, which are then used for IoU prediction. The IoU prediction network is differentiable w.r.t. the input box co-ordinates, allowing the candidates to be refined during tracking by maximizing the predicted IoU. We use the same network architecture as in <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Offline Training</head><p>Here, we describe our offline training procedure. In Siamese approaches, the network is trained with image pairs, using one image to predict the target template and the other for evaluating the tracker. In contrast, our model prediction network D inputs a set S train of multiple data samples from the sequence. To better exploit this advantage, we train our full tracking architecture on pairs of sets</p><formula xml:id="formula_11">(M train , M test ). Each set M = {(I j , b j )} Nframes j=1</formula><p>consists of images I j paired with their corresponding target bounding boxes b j . The target model is predicted using M train and then evaluated on the test frames M test . Uniquely, our train-ing allows the model predictor D to learn how to better utilize multiple samples. The sets are constructed by sampling a random segment of length T ss in the sequence. We then construct M train and M test by sampling N frames frames each from the first and second halves of the segment respectively.</p><p>Given the pair (M train , M test ), we first pass the images through the backbone feature extractor to construct the train S train and test S test samples for our target model. Formally, the train set is obtained as S train = {(F (I j ), c j ) : (I j , b j ) â M train }, where c j is the center coordinate of the box b j . This is input to the target predictor f = D(S train ). The aim is to predict a model f that is discriminative and that generalizes well to future unseen frames. We therefore only evaluate the predicted model f on the test samples S test , obtained analogously using M test . Following the discussion in section 3.1, we compute the regression errors using a hinge for the background samples,</p><formula xml:id="formula_12">(s, z) = s â z , z &gt; T max(0, s) , z â¤ T .<label>(8)</label></formula><p>Here, the threshold T defines the target and background region based on the label confidence value z. For the target region z &gt; T we take the difference between the predicted confidence score s and the label z, while we only penalize positive confidence values for the background z â¤ T . The total target classification loss is computed as the mean squared error (8) over all test samples. However, instead of only evaluating the final target model f , we average the loss over the estimates f (i) obtained in each iteration i by the optimizer (see alg. 1). This introduces intermediate supervision to the target prediction module, benefiting training convergence. Furthermore, we do not aim to train for a specific number of recursions, but rather be free to set the desired number of optimization recursions online. It is thus natural to evaluate each iterate f (i) equally. The target classification loss used for offline training is given by,</p><formula xml:id="formula_13">L cls = 1 N iter Niter i=0 (x,c)âStest x * f (i) , z c 2 .<label>(9)</label></formula><p>Here, regression label z c is set to a Gaussian function centered as the target c. Note that the output f (0) from the filter initializer (section 3.3) is also included in the above loss.</p><p>Although not denoted explicitly to avoid clutter, both x and f (i) in (9) depend on the parameters of the feature extraction network F . The model iterates f (i) additionally depend on the parameters in the model predictor network D.</p><p>For bounding box estimation, we extend the training procedure in <ref type="bibr" target="#b5">[6]</ref> to image sets by computing the modulation vector on the first frame in M train and sampling candidate boxes from all images in M test . The bounding box estimation loss L bb is computed as the mean squared error between the predicted IoU overlaps in M test and the ground truth. We train the full tracking architecture by combining this with the target classification loss (9) as L tot = Î²L cls + L bb . Training details: We use the training splits of the Track-ingNet <ref type="bibr" target="#b26">[27]</ref>, LaSOT <ref type="bibr" target="#b9">[10]</ref>, GOT10k <ref type="bibr" target="#b15">[16]</ref> and COCO <ref type="bibr" target="#b23">[24]</ref> datasets. The backbone network is initialized with the ImageNet weights. We train for 50 epochs by sampling 20,000 videos per epoch, giving a total training time of less than 24 hours on a single Nvidia TITAN X GPU. We use ADAM <ref type="bibr" target="#b18">[19]</ref> with learning rate decay of 0.2 every 15th epoch. The target classification loss weight is set to Î² = 10 2 and we use N iter = 5 optimizer module recursions in (9) during training. The image patches in (M train , M test ) are extracted by sampling a random translation and scale relative to the target annotation. We set the base scale to 5 times the target size to incorporate significant background information. For each sequence, we sample N frames = 3 test and train frames, using a segment length of T ss = 60. The label scores z c are constructed using a standard deviation of 1/4 relative to the base target size, and we use T = 0.05 for the regression error <ref type="bibr" target="#b7">(8)</ref>. We employ the ResNet architecture for the backbone. For the model predictor D, we use features extracted from the third block, having a spatial stride of 16. We set the kernel size of the target model f to 4 Ã 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Online Tracking</head><p>Given the first frame with annotation, we employ data augmentation strategies <ref type="bibr" target="#b2">[3]</ref> to construct an initial set S train containing 15 samples. The target model is then obtained using our discriminative model prediction architecture f = D(S train ). For the first frame, we employ 10 steepest descent recursions, after the initializer module. Our approach allows the target model to be easily updated by adding a new training sample to S train whenever the target is predicted with sufficient confidence. We ensure a maximum memory size of 50 by discarding the oldest sample. During tracking, we refine the target model f by performing two optimizer recursions every 20 frames, or a single recursion whenever a distractor peak is detected. Bounding box estimation is performed using the same settings as in <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Our approach is implemented in Python using PyTorch, and operates at 57 FPS with a ResNet-18 backbone and 43 FPS with ResNet-50 on a single Nvidia GTX 1080 GPU. Detailed results are provided in the supplementary material (section S3-S6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Analysis of our Approach</head><p>Here, we perform an extensive analysis of the proposed model prediction architecture. Experiments are performed on a combined dataset containing the entire OTB-100 <ref type="bibr" target="#b42">[43]</ref>, NFS (30 FPS version) <ref type="bibr" target="#b11">[12]</ref> and UAV123 <ref type="bibr" target="#b25">[26]</ref>  pooled dataset contains 323 diverse videos to enable thorough analysis. The trackers are evaluated using the AUC <ref type="bibr" target="#b42">[43]</ref> metric. Due to the stochastic nature of the tracker, we always report the average AUC score over 5 runs. We employ ResNet-18 as the backbone network for this analysis. Impact of optimizer module: We compare our proposed method, utilizing the steepest descent (SD) based architecture, with two alternative approaches. Init: Here, we only use the initializer module to predict the final target model, which corresponds to removing the optimizer module in our approach. Thus, similar to the Siamese approaches, only target appearance information is used for model prediction, while background information is discarded. GD: In this approach, we replace steepest descent with the gradient descent (GD) algorithm using learned coefficient-wise steplengths Î± in (3). All networks are trained using the same settings. The results for this analysis are shown in table 1. The model predicted by the initializer network, which uses only target information, achieves an AUC score of 58.2%. The gradient descent approach, which can exploit background information, provides a substantial improvement, achieving an AUC score of 61.6%. This highlights the importance of employing discriminative learning for model prediction. Our steepest descent approach obtains the best results, outperforming GD by 2.2%. This is due to the superior convergence properties of steepest descent, important for offline learning and fast online tracking. Analysis of model prediction architecture: Here, we analyze the impact of key aspects of the proposed discriminative online learning architecture, by incrementally adding them one at a time. The results are shown in table 2. The baseline SD constitutes our steepest descent based optimizer module along with a fixed ResNet-18 network trained on ImageNet. That is, similar to the current state-of-the-art discriminative approaches, we do not fine-tune the backbone. Instead of learning the discriminative loss, we employ the regression error <ref type="bibr" target="#b7">(8)</ref>   baseline approach achieves an AUC score of 58.7%. By adding the model initializer module (+Init), we achieve a significant gain of 1.3% in AUC score. Further training the entire network, including backbone feature extractor, (+FT) leads to a major improvement of 2.6% in AUC score. This demonstrates the advantages of learning specialized features suitable for tracking through end-to-end learning.</p><p>Using an additional convolutional block to extract classification specific features (+Cls) yields a further improvement of 0.7% AUC score. Finally, learning the discriminative loss (2) itself (+Loss), as described in section 3.4, improves the AUC score by another 0.5%. This shows the benefit of learning the implicit online loss by maximizing the generalization capabilities of the model on future frames. Impact of online model update: Here, we analyze the impact of updating the target model online, using information from previous tracked frames. We compare three different model update strategies. i) No update: The model is not updated during tracking. Instead, the model predicted in the first frame by our model predictor D, is employed for the entire sequence. ii) Model averaging: In each frame, the target model is updated using the linear combination of the current and newly predicted model, as commonly employed in tracking <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b40">41]</ref>. iii) Ours: The target model is obtained using the training set constructed online, as described in section 3.7. The naÃ¯ve model averaging fails to improve over the baseline method with no updates (see table 3). In contrast, our approach obtains a significant gain of about 2% in AUC score over both methods, indicating that our approach can effectively adapt the target model online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">State-of-the-art Comparison</head><p>We compare our proposed approach DiMP with the state-of-the-art methods on seven challenging tracking benchmarks. Results for two versions of our approach are shown: DiMP-18 and DiMP-50 employing ResNet-18 and ResNet-50 respectively as the backbone network. VOT2018 <ref type="bibr" target="#b19">[20]</ref>: We evaluate our approach on the 2018 version of the Visual Object Tracking (VOT) challenge consisting of 60 challenging videos. Trackers are evaluated using the measures accuracy (average overlap over successfully tracked frames) and robustness (failure rate). Both these measures are combined to get the EAO (Expected   Average Overlap) score used to rank trackers. The results are shown in table 4. Among previous approaches, SiamRPN++ achieves the best accuracy and EAO. However, it attains much inferior robustness compared to the discriminative learning based approaches, such as MFT and LADCF. Similar to the aforementioned approaches, SiamRPN++ employs ResNet-50 for feature extraction. Our approach DiMP-50, employing the same backbone network, significantly outperforms SiamRPN++ with a relative gain of 6.3% in terms of EAO. Further, compared to SiamRPN++, our approach has a 34% lower failure rate, while achieving similar accuracy. This shows that discriminative model prediction is crucial for robust tracking.</p><p>LaSOT <ref type="bibr" target="#b9">[10]</ref>: We evaluate our approach on the test set consisting of 280 videos. The success plots are shown in <ref type="figure" target="#fig_2">figure 4</ref>. Compared to other datasets, LaSOT has longer sequences, with an average of 2500 frames per sequence. Thus, online model adaption is crucial for this dataset. The previous best approach ATOM <ref type="bibr" target="#b5">[6]</ref> employs online discriminative learning with with pre-trained ResNet-18 features.</p><p>Our end-to-end trained approach, using the same backbone architecture, outperforms ATOM with a relative gain of 3.3%, showing the impact of end-to-end training. DiMP-50 further improves the results with an AUC score of 56.9%.</p><p>These results demonstrate the powerful model adaption capabilities of our method on long sequences.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We propose a tracking architecture that is trained offline in an end-to-end manner. Our approach is derived from a discriminative learning loss by applying an iterative optimization procedure. By employing a steepest descent based optimizer and an effective model initializer, our approach can predict a powerful model in only a few optimization steps. Further, our approach learns the discriminative loss during offline training by minimizing the prediction error on unseen test frames. Our approach sets a new state-of-the-art on 6 tracking benchmarks, while operating at over 40 FPS.  <ref type="figure">Figure S1</ref>. Expected average overlap curve on the VOT2018 dataset, showing the expected overlap between tracker prediction and ground truth for different sequence lengths. The EAO measure, computed as the average of the expected average overlap over typical sequence lengths (grey region in the plot), is shown in the legend. Our approach achieves the best EAO score, outperforming the previous best approach SiamRPN++ <ref type="bibr" target="#b21">[22]</ref> with a relative gain of 6.3% in terms of EAO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S3. Detailed Results on VOT2018</head><p>In this section, we provide detailed results on the VOT2018 <ref type="bibr" target="#b19">[20]</ref> dataset. The VOT protocol evaluates the expected average overlap (EAO) between the tracker predictions and the ground truth bounding boxes for different sequence lengths. The trackers are then ranked using the EAO measure, which computes the average of the expected average overlaps over typical sequence lengths. We refer to <ref type="bibr" target="#b20">[21]</ref> for further details about the EAO computation. Figure S1 plots the expected average overlap for different sequence lengths on VOT2018 dataset. Our approach DiMP-50 achieves the best EAO score of 0.44.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S4. Detailed Results on LaSOT</head><p>Here, we provide the normalized precision plots on the LaSOT <ref type="bibr" target="#b9">[10]</ref> dataset. These are obtained in the following manner. First, the normalized precision score P norm is computed as the percentage of frames in which the distance between the target location predicted by the tracker and the ground truth, relative to the target size, is less than a certain threshold. The normalized precision score over all the the videos are then plotted over a range of thresholds [0, 0.5] to obtain the normalized precision plots. The trackers are ranked using the area under the resulting curve.   <ref type="figure">Figure S2</ref>. Normalized precision plot on the LaSOT dataset. Both our ResNet-18 and ResNet-50 versions outperform all previous methods by significant margins. over the previous best method, ATOM <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S5. Detailed Results on NFS, OTB-100, and UAV123</head><p>Here, we provide detailed results on NFS <ref type="bibr" target="#b11">[12]</ref>, OTB-100 <ref type="bibr" target="#b42">[43]</ref>, and UAV123 <ref type="bibr" target="#b25">[26]</ref> datasets. We use the overlap precision (OP) metric for evaluating the trackers. The OP score denotes the percentage of frames in a video for which the intersection-over-union (IoU) overlap between the tracker prediction and the ground truth bounding box exceeds a certain threshold. The mean OP score over all the videos in a dataset are plotted over a range of thresholds [0, 1] to obtain the success plot. The area under this plot provides the AUC score, which is used to rank the trackers. We refer to <ref type="bibr" target="#b42">[43]</ref> for further details. The success plots over the entire NFS, OTB-100, and UAV123 datasets are shown in <ref type="figure" target="#fig_0">figure S3</ref>. Our tracker using ResNet-50 backbone, denoted DiMP-50, achieves the best results on both NFS and UAV123 datasets, while obtaining results competitive with the state-of-the-art on the, now saturated, OTB-100 dataset. On the challenging NFS dataset, our approach achieves an absolute gain of 3.5% AUC score over the previous best method ATOM <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S6. Impact of Training Data</head><p>Here, we investigate the impact of the number of videos used for training on the tracking performance. We train different versions of our tracker using the same datasets as in the main paper, i.e. TrackingNet <ref type="bibr" target="#b26">[27]</ref>, LaSOT <ref type="bibr" target="#b9">[10]</ref>, GOT10k <ref type="bibr" target="#b15">[16]</ref>, and COCO <ref type="bibr" target="#b23">[24]</ref>, but using only a sub-set of videos from each dataset. The results on the combined OTB-100, NFS, and UAV123 datasets are shown in figure   <ref type="figure" target="#fig_0">Figure S3</ref>. Success plots on NFS (a), OTB-100 (b), and UAV123 (c) datasets. The area-under-the-curve (AUC) scores are shown in the legend. Our approach achieves the best scores on both the NFS and UAV123 datasets. <ref type="figure" target="#fig_2">Figure S4</ref>. Impact of the percentage of total videos used for offline training (log x-axis). Results are shown on the combined OTB-100, NFS, and UAV123 datasets.</p><p>S4. Observe that the performance degrades by only 1.5% when the model is trained with only 10% of the total videos. Even when using only 1% of videos, our approach still obtains a respectable AUC score of around 58%.</p><p>S7. Visualizations of learned y c , m c , and v c A 2D visualization of the learned regression label (y c ), target mask (m c ), and spatial weight (v c ) is provided in figure S5. Note that each of these quantities are in fact continuous and are here sampled at the discrete feature grid points. In this example, that target (red box) is centered in the image patch. From the figure, we can see that the network learns to give the samples in the target-background transition region less weight due to their ambiguous nature.  <ref type="figure">Figure S5</ref>. Visualization of the learned label yc, spatial weight vc, and target mask mc. The red box denotes the target object.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Plot of the learned regression label (yc), target mask (mc), and spatial weight (vc). The markers show the knot locations. The initialization of each quantity is shown in dotted lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Success plot on the LaSOT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure S2 shows the normalized precision plots over all 280 videos in the LaSOT dataset. Both our ResNet-18 (DiMP-18) and ResNet-50 (DiMP-50) versions outperform all previous methods, achieving relative gains of 5.9% and 12.8%</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Analysis of different model prediction architectures on the combined OTB-100, NFS and UAV123 datasets. The architecture using only the target information for model prediction (Init) achieves an AUC score of 58.2%. The proposed steepest descent based architecture (SD) provides the best results, outperforming the gradient descent method (GD) by over 2.2% AUC score.</figDesc><table><row><cell>datasets. This</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>in the optimizer module. This Comparison of different model update strategies on the combined OTB-100, NFS and UAV123 datasets.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">No update Model averaging Ours</cell></row><row><cell></cell><cell>AUC</cell><cell>61.7</cell><cell></cell><cell>61.7</cell><cell>63.8</cell></row><row><cell></cell><cell cols="5">DRT RCO UPDT DaSiam-MFT LADCF ATOM SiamRPN++ DiMP-18 DiMP-50</cell></row><row><cell></cell><cell>[38] [20]</cell><cell>[3] RPN [46] [20]</cell><cell>[44]</cell><cell>[6]</cell><cell>[22]</cell></row><row><cell>EAO</cell><cell cols="4">0.356 0.376 0.378 0.383 0.385 0.389 0.401</cell><cell>0.414</cell><cell>0.402</cell><cell>0.440</cell></row><row><cell cols="5">Robustness 0.201 0.155 0.184 0.276 0.140 0.159 0.204</cell><cell>0.234</cell><cell>0.182</cell><cell>0.153</cell></row><row><cell cols="5">Accuracy 0.519 0.507 0.536 0.586 0.505 0.503 0.590</cell><cell>0.600</cell><cell>0.594</cell><cell>0.597</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>State-of-the-art comparison on the VOT2018 dataset in terms of expected average overlap (EAO), accuracy &amp; robustness.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>State-of-the-art comparison on the TrackingNet test set in terms of precision, normalized precision, and success.</figDesc><table><row><cell></cell><cell cols="8">MDNet CF2 ECO CCOT GOTURN SiamFC SiamFCv2 ATOM DiMP-18 DiMP-50</cell></row><row><cell></cell><cell cols="2">[30] [25] [7]</cell><cell>[9]</cell><cell>[14]</cell><cell>[2]</cell><cell>[41]</cell><cell>[6]</cell></row><row><cell cols="4">SR0.50 (%) 30.3 29.7 30.9 32.8</cell><cell>37.5</cell><cell>35.3</cell><cell>40.4</cell><cell>63.4</cell><cell>67.2</cell><cell>71.7</cell></row><row><cell cols="2">SR0.75 (%) 9.9</cell><cell cols="2">8.8 11.1 10.7</cell><cell>12.4</cell><cell>9.8</cell><cell>14.4</cell><cell>40.2</cell><cell>44.6</cell><cell>49.2</cell></row><row><cell>AO (%)</cell><cell cols="3">29.9 31.5 31.6 32.5</cell><cell>34.7</cell><cell>34.8</cell><cell>37.4</cell><cell>55.6</cell><cell>57.9</cell><cell>61.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>State-of-the-art comparison on the GOT10k test set in terms of average overlap (AO), and success rates (SR) at overlap thresholds 0.5 and 0.75.</figDesc><table><row><cell></cell><cell cols="9">ECOhc DaSiam-ATOM CCOT MDNet ECO SiamRPN++ UPDT DiMP-18 DiMP-50</cell></row><row><cell></cell><cell cols="3">[7] RPN [46] [6]</cell><cell>[9]</cell><cell>[30]</cell><cell>[7]</cell><cell>[22]</cell><cell>[3]</cell></row><row><cell>NFS</cell><cell>-</cell><cell>-</cell><cell cols="2">58.4 48.8</cell><cell cols="2">41.9 46.6</cell><cell>-</cell><cell>53.6</cell><cell>61.0</cell><cell>61.9</cell></row><row><cell cols="2">OTB-100 64.3</cell><cell>65.8</cell><cell cols="2">66.3 68.2</cell><cell cols="2">67.8 69.1</cell><cell>69.6</cell><cell>70.4</cell><cell>66.0</cell><cell>68.4</cell></row><row><cell cols="2">UAV123 51.2</cell><cell>57.7</cell><cell cols="2">64.2 51.3</cell><cell>-</cell><cell>53.2</cell><cell>-</cell><cell>54.5</cell><cell>64.3</cell><cell>65.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>State-of-the-art comparison on the NFS, OTB-100 and UAV123 datasets in terms of AUC score. achieves the best AO score of 61.1%, verifying the strong generalization abilities of our tracker. Need for Speed<ref type="bibr" target="#b11">[12]</ref>: We evaluate our approach on the 30 FPS version of the dataset, containing challenging videos with fast-moving objects. The AUC scores over all the 100 videos are shown in table 7. The previous best method ATOM achieves an AUC score of 58.4% . Our approach outperforms ATOM with relative gains of 4.4% and 6.0% using ResNet-18 and ResNet-50 respectively. OTB-100<ref type="bibr" target="#b42">[43]</ref>:Table 7shows the AUC scores over all the 100 videos in the dataset. Among the compared methods, UPDT achieves the best results with an AUC score of 70.4%. Our DiMP-50 achieves an AUC score of 68.4%, competitive with the other state-of-the-art approaches. UAV123<ref type="bibr" target="#b25">[26]</ref>: This dataset consists of 123 low altitude aerial videos captured from a UAV. Results in terms of AUC are shown in table 7. Among previous methods, ATOM achieves an AUC score of 64.2%. Both DiMP-18 and DiMP-50 outperform ATOM, achieving AUC scores of 64.3% and 65.4%, respectively.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">See supplementary material (section S1) for details.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: This work was supported by ETH General Fund (OK), and Nvidia through a hardware grant.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>This supplementary material provides additional details and results. Section S1 derives the closed form expression of the filter gradient, employed in the optimizer module. In section S2 we derive the application of the Jacobian in order to compute the quantity h, employed in algorithm 1 in the paper. In section S3 we provide detailed results on the VOT2018 dataset, while in section S4, we provide detailed results on the LaSOT dataset. We also provide additional details on the NFS, OTB100 and UAV123 datasets in section S5. We analyze the impact when training with less data in section S6. Finally, we provide a 2d visualization of the learned functions parametrizing the discriminative loss in section S7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S1. Closed-Form Expression for âL</head><p>Here, we derive a closed-form expression for the gradient of the loss (1) in the main paper, also restated here,</p><p>(S1)</p><p>Here, s = x * f is the score map obtained after convolving the deep feature map x with the target model f . The training set is given by</p><p>The residual function r(s, c) is defined as (also eq. (2) in the paper),</p><p>The gradient âL(f ) of the loss (S1) w.r.t. the filter coefficients f is then computed as,</p><p>Here, we have defined r s,c = r(s, c) and ârs,c âf corresponds to the Jacobian of the residual function (S2) w.r.t. the filter coefficients f . Using eq. (S2) we obtain,</p><p>Here, diag(q c ) denotes a diagonal matrix containing the el- </p><p>Here, Â· denotes the element-wise product. The multiplication with the transposed Jacobian âs âf T corresponds to backpropagation of the input q c Â· r s,c through the convolution layer f â x * f . This is implemented as a transposed convolution with x. The closed-form expression (S5) is thus easily implemented using standard operations in a deep learning library like PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S2. Calculation of h in Algorithm 1</head><p>In this section, we show the calculation of h = J (i) âL(f (i) ), used when determining the optimal step length Î± in Algorithm 1 in the main paper. Since we only need the squared L 2 norm of h in step length calculation, we will directly derive an expression for h 2 = J (i) âL(f (i) ) 2 . Here, J (i) = âÎ¾ âf f (i) is the Jacobian of the residual vector Î¾ of loss (S1), evaluated at the filter estimate f (i) . Not to be confused with the residual function (S2), the residual vector Î¾ is obtained as the concatenation of individual residuals Î¾ j = r(x j * f, c j )/ â n for j â {1, . . . , n} and Î¾ j = Î»f for j = n + 1. Here, n = |S train | is the number of samples in S train . Consequently, we get,</p><p>Using eqs. (S6) and (S4) we finally obtain,</p><p>As described in section S1, âL(f (i) ) is computed using the closed-form expression (S5). The term âs âf f (i) âL(f (i) ) corresponds to convolution of x with âL(f (i) ), i.e. âs âf f (i) âL(f (i) ) = x * âL(f (i) ). Thus, h 2 is computed easily using standard operations from deep learning libraries.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fully-convolutional siamese networks for object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unveiling the power of deep tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual object tracking using adaptive correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Bolme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Beveridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Draper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Lui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Deep meta learning for real-time visual tracking based on target-specific feature space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
		<idno>abs/1712.09153</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">ATOM: Accurate tracking by overlap maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ECO: efficient convolution operators for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning spatially regularized correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>HÃ¤ger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Beyond correlation filters: Learning continuous convolution operators for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">Shahbaz</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Lasot: A high-quality benchmark for large-scale single object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<idno>abs/1809.07845</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-agnostic metalearning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Need for speed: A benchmark for higher frame rate object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">K</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning dynamic siamese network for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to track at 100 fps with deep regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Held</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Highspeed tracking with kernelized correlation filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caseiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Batista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Got-10k: A large highdiversity benchmark for generic object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.11981</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning background-aware correlation filters for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Galoogahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fagg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The sixth visual object tracking vot2018 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pfugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Zajc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vojir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bhat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lukezic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eldesokey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fernandez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV workshop</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The visual object tracking vot2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Äehovin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>FernÃ¡ndez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>VojÃ­r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Nebehay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pflugfelder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Siamrpn++: Evolution of siamese visual tracking with very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High performance visual tracking with siamese region proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hierarchical convolutional features for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A benchmark and simulator for uav tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Trackingnet: A large-scale dataset and benchmark for object tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Giancola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Al-Subaihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<title level="m">Meta networks. Proceedings of machine learning research</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2554" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta-neural networks that learn by learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Mammone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 1992] IJCNN International Joint Conference on Neural Networks</title>
		<meeting>1992] IJCNN International Joint Conference on Neural Networks</meeting>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="437" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Numerical Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Wright</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Meta-tracker: Fast and robust online adaptation for visual object trackers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Optimization as a model for fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning. on learning now to learn: The meta-meta-meta...-hook. Diploma thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987-05-14" />
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Technische Universitat Munchen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to control fast-weight memories: An alternative to dynamic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="139" />
			<date type="published" when="1992-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">An introduction to the conjugate gradient method without the agonizing pain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Shewchuk</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994" />
			<pubPlace>Pittsburgh, PA, USA</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Correlation tracking via joint discrimination and reliability learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Siamese instance search for tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning to Learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pratt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>Kluwer Academic Publishers</publisher>
			<pubPlace>Norwell, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">End-to-end representation learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning attentions: Residual attentional siamese network for high performance online visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<title level="m">Object tracking benchmark. TPAMI</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Learning adaptive discriminative correlation filters via temporal consistency preserving spatial feature selection for robust visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<idno>abs/1807.11348</idno>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint representation and truncated inference learning for correlation filter based tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Distractor-aware siamese networks for visual object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
