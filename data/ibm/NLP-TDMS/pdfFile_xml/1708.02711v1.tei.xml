<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
							<email>damien.teney@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Centre for Visual Technologies</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
							<email>peter.anderson@anu.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Deep Learning Technology Center</orgName>
								<orgName type="institution">Microsoft Research</orgName>
								<address>
									<settlement>Redmond</settlement>
									<region>WA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den Hengel</surname></persName>
							<email>anton.vandenhengel@adelaide.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Australian Centre for Visual Technologies</orgName>
								<orgName type="institution">The University of Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a state-of-the-art model for visual question answering (VQA), which won the first place in the 2017 VQA Challenge. VQA is a task of significant importance for research in artificial intelligence, given its multimodal nature, clear evaluation protocol, and potential real-world applications. The performance of deep neural networks for VQA is very dependent on choices of architectures and hyperparameters. To help further research in the area, we describe in detail our high-performing, though relatively simple model. Through a massive exploration of architectures and hyperparameters representing more than 3,000 GPU-hours, we identified tips and tricks that lead to its success, namely: sigmoid outputs, soft training targets, image features from bottom-up attention, gated tanh activations, output embeddings initialized using GloVe and Google Images, large mini-batches, and smart shuffling of training data. We provide a detailed analysis of their impact on performance to assist others in making an appropriate selection.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of Visual Question Answering (VQA) involves an image and a related text question, to which the machine must determine the correct answer (see <ref type="figure">Fig. 1</ref>). The task lies at the intersection of the fields of computer vision, natural language processing, and artificial intelligence. This paper presents a relatively simple model for VQA that achieves state-of-the-art results. It is based on a deep neural network that implements the well-known joint embedding approach. The details of its architecture and hyperparameters were carefully selected for optimal performance 1 Work performed while interning at Microsoft.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What is on the coffee table ? What color is the hydrant ? candles black and yellow</head><p>What is on the bed ? What is the long stick for ? books whipping <ref type="figure">Figure 1</ref>. The task of visual question answering (VQA) relates visual concepts with elements of language and, occasionally, common-sense or general knowledge. Examples of training questions and their correct answer from the VQA v2 dataset <ref type="bibr" target="#b15">[14]</ref>.</p><p>on the VQA v2 benchmark <ref type="bibr" target="#b15">[14]</ref>. Admittedly, a large part of such a search is necessarily guided by empirical exploration and validation. Given the limited understanding of neural networks trained on a task as complex as VQA, small variations of hyperparameters and of network architectures may have significant and sometimes unpredictable effects on final performance <ref type="bibr" target="#b23">[22]</ref>. The aim of this paper is to share the details of a successful model for VQA. The findings reported herein may serve as a basis for future development of VQA systems and multimodal reasoning algorithms in general.</p><p>The proposed model is based on the principle of a joint 1 embedding of the input question and image, followed by a multi-label classifier over a set of candidate answers. While this general approach forms the basis of many modern VQA methods <ref type="bibr" target="#b34">[33]</ref>, the details of the model are critical to achieving a high quality result. We also complement our model with a few key technical innovations that greatly enhance its performance. We have conducted an extensive empirical study to explore the space of architectures and hyperparameters to determine the importance of the various components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Summary of findings</head><p>Our key findings are summarized with the following characteristics of the proposed model, which enable its high performance (see also <ref type="table" target="#tab_1">Table 2</ref>).</p><p>-Using a sigmoid output that allows multiple correct answers per question, instead of a common single-label softmax.</p><p>-Using soft scores as ground truth targets that cast the task as a regression of scores for candidate answers, instead of a traditional classification.</p><p>-Using gated tanh activations in all non-linear layers.</p><p>-Using image features from bottom-up attention <ref type="bibr" target="#b4">[3]</ref> that provide region-specific features, instead of traditional grid-like feature maps from a CNN.</p><p>-Using pretrained representations of candidate answers to initialize the weights of the output layer.</p><p>-Using large mini-batches and smart shuffling of training data during stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>The task of VQA has gathered increasing interest in the past few years since the seminal paper of Antol et al. <ref type="bibr" target="#b7">[6]</ref>. Even though the task straddles the fields of computer vision and natural language processing, it has primarily been a focus of the former. This is partly because VQA constitutes a practical setting to evaluate deep visual understanding, itself considered the over-arching goal of computer vision. The task of VQA is extremely challenging, since it requires the comprehension of a text question, the parsing of visual elements of an image, and reasoning over those modalities, sometimes on the basis of external or common-sense knowledge (see <ref type="figure">Fig. 1</ref>). The increasing interest in VQA parallels a similar trend for other tasks involving vision and language, such as image captioning <ref type="bibr" target="#b13">[12,</ref><ref type="bibr" target="#b33">32]</ref> and visual dialog <ref type="bibr" target="#b11">[10]</ref>. Datasets A number of large-scale datasets for VQA have been created (e.g. <ref type="bibr" target="#b7">[6,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b39">38]</ref>; see <ref type="bibr" target="#b34">[33]</ref> for a survey). Each dataset contains various images, typically from Flickr and/or from the COCO dataset <ref type="bibr" target="#b26">[25]</ref>, together with humanproposed questions and ground truth answers. The VQAreal dataset of Antol et al.. <ref type="bibr" target="#b7">[6]</ref> has served as the de facto benchmark since its introduction in 2015. As the performance of methods improved, however, it became apparent that language-based priors and rote-learning of example questions/answers were overly effective ways to obtain good performance <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b38">37]</ref>. That fact hinders the effective evaluation and comparison of competing methods. The observation led to the introduction of a new version of the dataset, referred to as VQA v2 <ref type="bibr" target="#b15">[14]</ref>. It associates two images to every question. Crucially, the two images are chosen so as to each lead to different answers. This obviously discourages blind guesses, i.e. inferring the answer from the question alone. This new setting and dataset were the basis of the 2017 VQA challenge [1] and of the experiments presented in this paper.</p><p>Another dataset used in this paper is the Visual Genome <ref type="bibr" target="#b25">[24]</ref>. This multipurpose dataset contains annotations of images in the form of scene graphs. Those constitute fine-grained descriptions of the image contents. They provide a set of visual elements appearing in the scene (e.g. objects, persons), together with their attributes (e.g. color, appearance) and the relations between them. We do not use these annotations directly, but they serve in <ref type="bibr" target="#b4">[3]</ref> to train a Faster R-CNN model <ref type="bibr" target="#b29">[28]</ref>, which we use here to obtain object-centric image features. We directly use other annotations of the Visual Genome dataset which are simply questions relating to the images. In comparison to the questions of VQA v2, these have more diverse formulations and a more varied set of answers. Those answers are also often longer, i.e. short phrases, whereas most answers in VQA v2 are usually 1 to 3-words long. As described in Sect.3.8, we only use the subset of questions whose answers overlap those in VQA v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>The prevailing approach to VQA is based on three components. (1) Posing question answering as a classification problem, solved with (2) a deep neural network that implements a joint embedding model, (3) trained end-to-end with supervision of example questions/answers. First, question-answering is posed as a classification over a set of candidate answers. Questions in the current VQA datasets are mostly visual in nature, and the correct answers therefore only span a small set of words and phrases. Practically, correct answers are concentrated in a small set of words and phrases (typically a few hundreds to a few thousands). Second, most VQA models are based on a deep neural network that implements a joint embedding of the image and of the question. The two inputs are mapped into fixed-size vector representations with convolutional and recurrent neural networks, respectively. Further non-linear mappings of those representations are usually interpreted as projections into a joint "semantic" space. They can then be  combined by means of concatenation of element-wise multiplication, before feeding the classifier mentioned above. Third, owing to the success of deep learning on supervised learning problems, this whole neural network is trained endto-end from questions, images, and their ground truth answers. Note that this constitutes a relatively sparse training signal considering the huge dimensionality of the input space of possible images and questions. This in turn requires massive amounts of training examples. This has driven the efforts in collecting large-scale datasets such as VQA v2. It contains in the order of 650,000 questions with ground-truth answers, relating to about 120,000 different images.</p><p>The majority of methods proposed for VQA in the past few years have built on the basic joint embedding approach with more complex mechanisms for attention (see <ref type="bibr" target="#b34">[33]</ref> for a recent survey). Interestingly, recent studies have shown that very simple models can also achieve strong performance <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b23">22]</ref> given careful implementation and/or selection of hyperparameters. Our work follows a similar line. Our extensive series of experiments show that a few keys choices in the implementation (e.g. gated activations, regression output, smart shuffling, etc.) dramatically improve the performance of a relatively simple model. We also show how the performance of a naive implementation gradually improves with each of those choices. We therefore hope that the proposed model can serve as a strong basis for other future incremental developments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed model</head><p>This section presents the proposed model, based on a deep neural network. For the sake of transparency, we pragmatically describe the model with the specific choices and values of hyperparameters that lead to its best performance. Section 4 will examine variations of architecture and hyperparameters and their influence on performance.</p><p>The complete model is summarized in <ref type="figure">Fig. 1</ref>.1. As a onesentence summary, it implements the well-known joint RN-N/CNN embedding of the question/image, with questionguided attention over the image <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b23">22</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Question embedding</head><p>The input for each instance -whether during training or test time -is a text question and an image. The question is tokenized, i.e. first split into words using spaces and punctuation. Any number or number-based word (e.g. 10,000 or 2:15pm) is also considered as a word. Questions are trimmed to a maximum of 14 words for computational efficiency. The extra words are then simply discarded. but only about 0.25% of questions in the dataset are longer than 14 words. Each word is turned into a vector representation with a look-up table, whose entries are 300-dimensional vectors learned along other parameters during training. Those vectors are however initialized with pretrained GloVe word embeddings <ref type="bibr" target="#b28">[27]</ref> (Global Vectors for Word Representation). We use the publicly available version of GloVe pretrained on the Wikipedia/Gigaword corpus 1 . The words not present in the pretrained word embdding are initialized with vectors of zeros (subsequenty optimized during training). The questions shorter than 14 words are end-padded with vectors of zeros (frozen during training). The resulting sequence of word embeddings is of size 14 × 300 and it is passed through a Recurrent Gated Unit (GRU <ref type="bibr" target="#b10">[9]</ref>). The recurrent unit has an internal state of dimension 512, and we use its final state, i.e. after processing the 14 word embeddings, as our question embedding q. Note that we do not use sentinel start or end tokens, nor do we trim sequences, or process the strict number of tokens in the given sentence (also known as per-example dynamic unrolling in <ref type="bibr" target="#b23">[22]</ref>, or TrimZero in <ref type="bibr" target="#b24">[23]</ref>). We rather found it more effective to always run the recurrent units for the same number of iterations, including entries containing zero-padding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Image features</head><p>The input image is passed through a Convolutional Neural Network (CNN) to obtain a vector representation of size K × 2048, where K is a number of image locations. Each location is thus represented by a 2048-dimensional vector that encodes the appearance of the image in that region. Our evaluation in Section 4 compares two main options with different trade-offs in commodity and performance: a standard pretrained CNN, or a better-performing option. The first, lower-performance option is a 200-layer ResNet (Residual Network <ref type="bibr" target="#b16">[15]</ref>) pretrained on ImageNet and publicly available <ref type="bibr" target="#b17">[16]</ref>. This gives feature maps of size 14×14 that we resize by average pooling (i.e. bilinear interpolation) to 7×7 (i.e. K=49). The second, higher-performance option is to use the method proposed in <ref type="bibr" target="#b4">[3]</ref> which provides image features using bottom-up attention. The method is based on a ResNet CNN within a Faster R-CNN framework <ref type="bibr" target="#b29">[28]</ref>. It is trained to focus on specific elements in the given image, using annotations from the Visual Genome dataset <ref type="bibr" target="#b25">[24]</ref>. The resulting features can be interpreted as ResNet features centered on the top-K objects in the image. Our experiments evaluate both a fixed K=36, and an adaptive K that uses a fixed threshold for the detected elements in the image, allowing the number of regions K to vary with the complexity of each image, up to a maximum of 100. The images used by the VQA v2 dataset yield in that case an average of about K=60 per image.</p><p>In all cases, the CNN is pretrained and held fixed during the training of the VQA model. The features can therefore be extracted from the input images as a preprocessing step for efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Image attention</head><p>Our model implements a classical question-guided attention mechanism common to most modern VQA models (see e.g. <ref type="bibr" target="#b39">[38,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b9">8,</ref><ref type="bibr" target="#b20">19,</ref><ref type="bibr" target="#b6">5,</ref><ref type="bibr" target="#b36">35]</ref>). We refer to this stage as the topdown attention, as opposed to the model of Anderson et al.. <ref type="bibr" target="#b4">[3]</ref> that provides image features from bottom-up attention.</p><p>For each location i = 1...K in the image, the feature vector v i is concatenated with the question embedding q (see <ref type="figure">Fig. 1</ref>.1). They are both passed through a non-linear layer f a (see Section 3.7) and a linear layer to obtain a scalar attention weight α i,t associated with that location. Formally,</p><formula xml:id="formula_0">a i = w a f a ([v i , q]) (1) α = softmax (a) (2) v = Σ K i=1 α i v i (3)</formula><p>where w a is a learned parameter vector. The attention weights are normalized over all locations with a softmax function (Eq. 2). The image features from all locations are then weighted by the normalized values and summed (Eq. 3) to obtain a single 2048-sized vectorv representing the attended image. Note that this attention mechanism is a simple oneglimpse, one-way attention, as opposed to more complex schemes of recent models (e.g. stacked, multi-headed, or bidirectional attention <ref type="bibr" target="#b36">[35,</ref><ref type="bibr" target="#b19">18,</ref><ref type="bibr" target="#b23">22,</ref><ref type="bibr" target="#b27">26]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multimodal fusion</head><p>The representations of the question (q) and of the image (v) are passed through non-linear layers and then combined with a simple Hadamard product (i.e. element-wise multiplication):</p><formula xml:id="formula_1">h = f q (q) • f v (v)<label>(4)</label></formula><p>The resulting vector h is referred to as the joint embedding of the question and of the image, and is then fed to the output classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Output classifier</head><p>A set of candidate answers, that we refer to as the output vocabulary, is predetermined from all the correct answers in the training set that appear more than 8 times. This amounts to N =3129 candidate answers. We treat VQA as a multi-label classification task. Indeed, each training question in the VQA v2 dataset is associated with one or several answers, each labeled with soft accuracies in [0, 1]. Multiple answers and accuracies in (0, 1) arise in case of disagreement between human annotators, particularly with ambiguous questions and multiple or synonymous correct answers <ref type="bibr" target="#b15">[14]</ref>. Moreover, in our case, some training questions (about 7%) have no correct answer within the selected output vocabulary. Those questions are not discarded however. We find them to provide a useful training signal, by driving towards zero the scores predicted for all candidates of the output vocabulary (see Section 4.1).</p><p>Our multi-label classifier passes the joint embedding h through a non-linear layer f o then through a linear mapping w o to predict a scoreŝ for each of the N candidates:</p><formula xml:id="formula_2">s = σ w o f o (h)<label>(5)</label></formula><p>where σ is a sigmoid (logistic) activation function, and w o ∈ R N ×512 is a learned weight matrix initialized as described below. The sigmoid normalizes the final scores to (0, 1), which are followed by a loss similar to a binary cross-entropy, although we use soft target scores. This final stage can be seen as a logistic regression that predicts the correctness of each candidate answer. Our objective function is</p><formula xml:id="formula_3">L = − M i N j s ij log(ŝ ij ) − (1−s ij ) log(1−ŝ ij ) (6)</formula><p>where the indices i and j run respectively over the M training questions and N candidate answers. The ground-truth scores s are the aforementioned soft accuracies of ground truth answers. The above formulation proved to be much more effective than a softmax classifier as commonly used in other VQA models. The advantage of the above formulation is two-fold. First, the sigmoid outputs allow optimization for multiple correct answers per question <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b32">31]</ref> as is occasionally the case in the VQA v2 dataset. Second, the use of soft scores as targets provides a slightly richer training signal than binary targets, as they capture the occasional uncertainty in ground truth annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Pretraining the classifier</head><p>In the output stage described above (Eq. 5), the score of a candidate answer j is effectively determined by a dot product between our joint image-question representation f o (h) and the jth row of w o . During training, an appropriate representation for each candidate answer is thus learned as a row of w o . We propose to use prior information about the candidate answers from two sources to initialize the rows of w o . On the one hand, we use linguistic information in the form of the GloVe word embeddings of the answer word (as described above for Question embedding). When the answer cannot be matched exactly with a pretrained embedding, we use the closest match after spell checking, removing hyphenation, or keeping a single term from multi-word expressions. The corresponding vectors are placed in the matrix w text o . We also exploit visual information gathered from images representing the candidate answers. We use Google Images to automatically retrieve 10 photographs associated with each candidate answer. Those photographs are passed through a ResNet-101 CNN pretrained on ImageNet <ref type="bibr" target="#b16">[15]</ref>. The final mean-pooled features are extracted and averaged over the 10 photographs. The resulting 2048-sized vector of each candidate answer is placed in the corresponding row of a matrix w img o . Those visual representations are complementary to the linguistic ones obtained through word embeddings. They can also be obtained for any candidate answer, including multi-word expressions and rare words for which no word embeddings are available. On the downside, abstract words and expressions may not lead to informative visual representations (see Section 4.6 and <ref type="figure" target="#fig_3">Fig. 4.6</ref>).</p><p>We combine the prior representations w text o and w img o as follows, decomposing Eq. 5 intô</p><formula xml:id="formula_4">s = σ w text o f text o (h) + w img o f img o (h)<label>(7)</label></formula><p>where the non-linear transformations f text o and f img o bring h to the appropriate dimensions, i.e. 300 and 2048 respectively (see <ref type="figure">Fig. 1</ref>.1). The matrices w text o and w img o are finetuned with the remainder of the network using smaller relative learning rates, respectively 0.5 and 0.01 (determined through cross-validation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Non-linear layers</head><p>The network described above uses multiple learned nonlinear layers (see <ref type="figure">Fig. 1</ref>.1). A common implementation for such a layer would be an affine transformation followed by a Rectified Linear Unit (ReLU). In our implementation, each non-linear layer uses a gated hyperbolic tangent activation. That is, each of those layers implements a function f a : x ∈ R m → y ∈ R n with parameters a defined as follows:</p><formula xml:id="formula_5">y = tanh (W x + b) (8) g = σ(W x + b ) (9) y =ỹ • g<label>(10)</label></formula><p>where σ is the sigmoid activation function, W, W ∈ R n×m are learned weights, b, b ∈ R n are learned biases, and • is the Hadamard (element-wise) product. The vector g acts multiplicatively as a gate on the intermediate activationỹ.</p><p>That formulation is inspired by similar gating operations within recurrent units such as LSTMs and GRUs <ref type="bibr" target="#b10">[9]</ref>. This can also be seen as a special case of highway networks <ref type="bibr" target="#b30">[29]</ref> and has been mentioned in other work in natural language processing <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b31">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.8.">Training</head><p>We train the network using stochastic gradient descent. We use the AdaDelta algorithm <ref type="bibr" target="#b37">[36]</ref>, which does not require fixing learning rates and is very insensitive to the initialization of the parameters. The model is prone to overfitting, which we prevent by early stopping as follows. We first train leaving out the official validation set of the VQA v2 dataset for monitoring, and identify the epoch yielding the best performance (highest overall VQA score). The training is then repeated for the same number of epochs, now also using the validation set as training data (as in <ref type="bibr" target="#b15">[14]</ref>).</p><p>We use questions/answers from the Visual Genome <ref type="bibr" target="#b25">[24]</ref> as additional training data. We only use questions whose correct answers overlap the output vocabulary determined on the VQA v2 dataset. This amounts to only about 30% or 485,000 questions from the Visual Genome.</p><p>During training with stochastic gradient descent, we enforce the shuffling of training instance to keep balanced pairs of VQA v2 in the same mini-batches. Those pairs correspond to identical questions with different images and answers. Our intuitive motivation is that such pairs likely lead to gradients pulling the network parameters in different directions. Keeping an example and its balanced counter-part in a same mini-batch is expected to make the learning more stable, and encourage the network to discern the subtle differences between the paired instances <ref type="bibr" target="#b31">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.9.">Implementation</head><p>Our model is implemented entirely in Matlab using custom deep learning libraries, with the exception of some Java code for multithreaded loading of input data. Training one network usually converges in 12-18 epochs, which takes in the order of 12-18 hours with K = 36 on a single Nvidia K40 GPU, or about twice as long on a CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Ablative experiments</head><p>We present an extensive set of experiments that compare our model as presented above (referred to as the reference model) with alternative architecture and hyperparameter values. The objective is to show that the proposed model corresponds to a local optimum in the space of architectures and parameters, and to evaluate the sensitivity of the final performance to each design choice. The following discussion follows the structure of Tables 1 and 2. Note the significant breadth and exhaustivity of the following experiments, which represent more than 3,000 GPU-hours of training time.</p><p>Experimental setup Each experiment in this section uses a single network (i.e. no ensemble) that is a variation of our reference model (first row of <ref type="table" target="#tab_3">Table 1</ref>). Each network is trained on the official training set of VQA v2 and on the additional questions from the Visual Genome unless specified. Results are reported on the validation test VQA v2 at the best epoch (highest overall VQA score). Each experiment (i.e. each row of Tables 1 and 2) is repeated 3 times, training the same network with different random seeds. We report the average and standard deviation over those three runs. The main performance metric is the standard VQA accuracy <ref type="bibr" target="#b7">[6]</ref>, i.e. the average ground truth score of the answers predicted for all questions 2 . We additionally report the metric of Accuracy over pairs <ref type="bibr" target="#b38">[37]</ref> (last column of Tables 1 and 2). It is the ratio of balanced questions (i.e. questions associated with two images leading to two different answers) that are answered perfectly, i.e. with both predicted answers having a ground truth score of 1.0. This metric is significantly harder than the standard per-question score since it requires a correct answer to both images of the pair, discouraging blind guesses and reliance on language priors <ref type="bibr" target="#b15">[14,</ref><ref type="bibr" target="#b38">37]</ref>.</p><p>We now discuss the results of each ablative experiment from Tables 1 and 2 in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training data</head><p>The use of additional training questions/answers from the Visual Genome (VG) <ref type="bibr" target="#b25">[24]</ref> increase the performance on VQA v2 <ref type="bibr" target="#b15">[14]</ref> in all question types. As mentioned above, we only use VG instances with a correct answer appearing the output vocabulary determined on VQA v2, and which use an image also used in VQA v2. Note that the +0.67% increase in performance is modest relative to the amount of additional training questions (an additional 485,000 over the 650,000 of VQA v2). Note that including VG questions relating to MS COCO images not used in VQA v2 resulted <ref type="bibr" target="#b3">2</ref> The ground truth score of a candidate answer j to a question i is a value s ij ∈ [0, 1] provided with the dataset. It accounts for possible disagreement between annotators: s ij = 1.0 if provided by m ≥ 3 annotators, and s = m/3 otherwise. Those scores are further averaged in a 10-choose-9 manner <ref type="bibr" target="#b7">[6]</ref>. in slightly lower final performance (not reported in <ref type="table" target="#tab_3">Table 1</ref>). We did not further investigate this issue.</p><p>We compare the proposed shuffling of training data which keeps balanced pairs in the same mini-batches, with a standard, arbitrary random shuffling. The results in <ref type="table" target="#tab_3">Table 1</ref> are inconclusive: the overall VQA score is virtually identical either way. The accuracy over pairs however improves with the proposed shuffling. This is to be expected, as the purpose of the proposed method is to improve the learning of differentiating between balanced pairs. The cumulative ablation (row (5) in <ref type="table" target="#tab_1">Table 2</ref>) confirms this advantage more clearly.</p><p>We evaluate discarding the training questions that do not have their ground truth answer within the selected candidates. Early experiments have shown that those instances do still carry a useful training signal by drawing the predicted scores of the selected candidate answers towards zero. In <ref type="table" target="#tab_3">Table 1</ref>, the VQA score remains virtually identical, but a very slight benefit is observed on the accuracy over pairs by retaining those instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Question embedding</head><p>Our reference model uses pretrained GloVe word embeddings of dimension 300 followed by a one-layer GRU processing words in forward order. We compare this choice to a series of more simple and more advanced options.</p><p>Learning the word embeddings from scratch, i.e. from random initializations reduces performance by 0.87%. The gap with pretrained embeddings is even greater as the model is trained on less training data <ref type="figure" target="#fig_3">(Fig. 4.9</ref> and Section 4.9). This is consistent with findings previously reported in <ref type="bibr" target="#b32">[31]</ref>. This experiment shows, on the one hand, a benefit of leveraging non-VQA training data. On the other hand, it suggests that a sufficiently large VQA training set may remove this benefit altogether. Using GloVe vectors of smaller dimension (100 or 200) also give lower performance than those in the reference model (300).</p><p>We investigate whether the pretrained word embeddings (GloVe vectors) really capture word-specific information. The alternative, null hypothesis is that the simple spreading of vectors in the word embedding space is a benefit in itself. To test this, we randomly shuffle the same GloVe vectors, i.e. we associate them with words from the input dictionary chosen at random. The shuffled GloVe vectors perform even worse than word embeddings learned from scratch. This shows that GloVe vectors indeed capture word-specific information.</p><p>In other experiments (not reported in <ref type="table" target="#tab_3">Table 1</ref>), we experimented with a tanh activation following the word embeddings as in <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b23">22]</ref>. This had no significant effect, either with random or pretrained initializations.</p><p>Replacing our GRU with advanced options (backward, bidirectional, or two-layer GRU) gives lower performance. Simpler options (bag-of-words, simply summing or averaging word embeddings) also give lower performance but are still a surprisingly strong baseline, as previously reported in <ref type="bibr" target="#b19">[18,</ref><ref type="bibr" target="#b32">31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Image features</head><p>Our best model uses the images features from bottomup attention of Anderson et al. <ref type="bibr" target="#b4">[3]</ref>. These are obtained through a Faster R-CNN framework and an underlying 101layer ResNet that focuses on specific image regions. The method uses a fixed threshold on object detections, and the number of features K is therefore adaptive to the contents of the image. It is capped at a maximum of 100, and yields an average in the order of K=60. We experiment with a fixed number of features K=36. The performance degrades only slightly. This option may be a reasonable alternative considering the lower implementation and computational costs.</p><p>We also experiment with mainstream image features from a 200-layer ResNet <ref type="bibr" target="#b16">[15,</ref><ref type="bibr" target="#b17">16]</ref>. As per the common practice, we use the last feature map of the network, of dimensions 14×14 and 2,048 channels. The performance with ResNet features drops dramatically from 63.15% to 57.52%. A global average pooling of those features, i.e. collapsing to a 1×1 map and discarding the attention mechanism, is expectedly even worse. A surprising finding however is that coarser 7×7 ResNet feature maps give a reasonable performance of 59.24. Those are obtained by linear interpolation of the 14×14 maps, equivalent to a 2×2 average pooling of stride 2. As an explanation, we hypothesize that the resolution of the coarser maps better correspond to the scale of objects in our images.</p><p>Note that the proposed model was initially developed with standard ResNet features, and is not specifically optimized for the features of <ref type="bibr" target="#b4">[3]</ref>. On the contrary, we found that optimal choices were stable across types of image features (including for the attention mechanism, see Section 4.4). In all cases, we also observed that an L2 normalization of the image features was crucial for good performance -at least with our choices of architecture and optimizer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Image attention</head><p>Our reference model uses a single set of K attention weights normalized with a softmax. Some previous studies have reported better performance with multiple sets of attention weights (also known as multiple "glimpses" or "attentions heads") <ref type="bibr" target="#b14">[13]</ref> and/or with a sigmoid normalization over the weights <ref type="bibr" target="#b31">[30]</ref> (which allows multiple glimpses in one pass). In our case, none of these options proved beneficial, whether with ResNet features or with features from bottom-up attention. This confirms that the latter image features are suited to a simple drop-in replacement in lieu of traditional feature maps, as argued in <ref type="bibr" target="#b4">[3]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Output vocabulary</head><p>We determine the output vocabulary, i.e. the set of candidate answers from those appearing in the training set more than times. We cross-validate this threshold and find a relatively broad optimum around = 8-12 occurrences. This corresponds to about N =2, 400 to 3, 800 candidate answers, which is of the same order as the values typically reported in the literature (without cross-validation) of 2, 000 or 3, 000. Note however a higher (i.e. lower N ) still gives reasonable performance with a lower number of parameters and computational complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Output classifier</head><p>Our reference model uses sigmoid outputs and the soft scores s i j as ground truth targets. We first compare the soft scores with two binarized versions:</p><formula xml:id="formula_6">s ij = (s ij ? &gt; 0.0) (11a) s ij = (s ij ? = 1.0) .<label>(11b)</label></formula><p>The proposed soft scores perform significantly better than either of the binarized versions ( <ref type="table" target="#tab_1">Table 2)</ref>.</p><p>We then compare the sigmoid outputs of our reference model to the common choice of a softmax. Both use a crossentropy loss. The softmax uses the single ground truth answer provided in the dataset, whereas the sigmoid uses the complete annotation data, occasionally with multiple correct answers marked for one question, due to disagreement between multiple annotators. The sigmoid performs significantly better than a softmax. This observation is confirmed in the cumulative ablations ( <ref type="table" target="#tab_1">Table 2)</ref> We now evaluate the proposed pretraining of the parameters w text o and/or w img o of the classifiers. We consider two baselines for those two matrices: random initialization and random shuffling. The former simply initializes and learns the weights like any others in the network. The latter uses the proposed pretraining as initializations but assigns then to the candidate answers randomly by shuffling the rows. This should reveal whether the proposed initialization provides answer-specific information, or whether it simply helps the numerical optimization because of better conditioned initial values. The proposed initialization performs consistently better than those baselines. The random initialization of w text o may seem surprisingly good as it even surpasses the reference model on the overall VQA score. It does not, however, on the metric of Accuracy over pairs. The experiments with reduced training data (Section 4.9 and <ref type="figure" target="#fig_3">Fig. 4.9</ref>) confirm even more clearly the benefits of the proposed initialization.</p><p>We take a closer look at the actual answers that improve with the proposed initialization ( <ref type="figure" target="#fig_3">Fig. 4.6)</ref>. We look at the recall of each candidate answer j, defined as</p><formula xml:id="formula_7">recall i = M i (s ij ? = 1.0 ∧ŝ ij ? = 1.0) M i (s ij ? = 1.0)<label>(12)</label></formula><p>where M is the number of evaluated questions, s ij is a ground truth score, andŝ ij a predicted score. In <ref type="figure" target="#fig_3">Fig. 4.6</ref>, we plot the recall of a random selection of answers with and without pretraining the classifier. It is expected that pretraining w text o improves a variety of answers while w img o improves those with clearer visual representation. That intuition is hard to evaluate subjectively and is easy to confirm or disprove by cherry-picking examples. Note that, despite the overall benefit for the proposed approach, the recall of many answers is affected negatively. Other architectures may be necessary to obtain the full benefits of the approach. Another observation -not directly inferable from <ref type="figure" target="#fig_3">Fig. 4.6</ref> is that the recall the most influenced by pretraining -pos-itively or negatively -is of answers with few training occurrences. This confirms the potential of the approach for better handling rare answers <ref type="bibr" target="#b32">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">General architecture</head><p>All of our non-linear layers are implemented as gated tanh <ref type="figure" target="#fig_2">(Section 3.7)</ref>. These show a clear benefit over the gated ReLU, and even more so over simple ReLU or tanh activations. Note that we also experimented, without success, with various other combinations of highway <ref type="bibr" target="#b30">[29]</ref>, residual <ref type="bibr" target="#b16">[15]</ref> and gating connections (not reported in Table 1). One benefit of gated layers is to double the number of learned parameters without increasing the dimension of the hidden states.</p><p>We cross-validate the dimension of hidden states among the values {256, 384, 512, 768, 1024, 1280}. We settled on 512 as a reasonable sweet spot. Larger dimensions (e.g. 1280) can be better but without guarantees. The variance across repeated experiments is larger, likely due to overfitting and unstable training.</p><p>Our architecture uses a simple element-wise product to combine the question and image representations. This proved far superior to a concatenation (not reported in Table 1), but we did not experiment with the various advanced forms of pooling proposed in the recent literature <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b8">7]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Mini-batch size</head><p>The size of mini-batches during the optimization proves to have a strong influence on the final performance. Midrange values in {128, 256, 384, 512, 768} proved superior to smaller mini-batches (including even smaller values), although they require significantly more memory and highend GPUs. We observed the optimum mini-batch size to be stable across variations of the network architecture, through other experiments not reported in <ref type="table" target="#tab_3">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Training set size</head><p>We investigate the relation between performance and the quantity of training data. We create random subsets of our training data and train four different models on it.</p><p>(1) Our best reference model. We plot in <ref type="figure" target="#fig_3">Fig. 4.9</ref> their performance against the amount of training data and make the following observations.    logarithmic trend. Remarkably, we already obtain reasonable performance with only 10% of the data. Consequently, the gain when training on the whole dataset appears small relative to the ten-fold increase in data. That observation is common among natural language tasks in which the data typically follows a Zipf law <ref type="bibr" target="#b3">[2]</ref> and in other domains with long-tail distributions. In those cases,</p><p>few training examples are sufficient to learn the most common cases, but an exponentially larger dataset is required for covering more and more of the rare concepts.</p><p>-The use of extra data to pretrain word embeddings and classifiers is always beneficial. The gap with the baselines models learned from scratch shrinks as more VQAspecific training data is used. It could suggest that a sufficiently large VQA training set would remove the benefit altogether. An alternative view however, is that those other sources of information are most useful for representing rare words and concepts <ref type="bibr" target="#b32">[31]</ref> which would require an impractically large dataset to be learned from VQA-specific examples alone. That view then suggests that extra data is necessary in practice.</p><p>-Pretrained word embeddings and pretrained classifiers each provide a benefit of the same order of magnitude. Importantly, the two techniques are clearly complementary and the best performance is obtained by combining them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.10.">Ensembling</head><p>We use the common practice of ensembling several networks to obtain better performance. We use the most basic form of ensembling: multiple instances of the same model  <ref type="figure">Figure 5</ref>. Performance of our best model (last row of <ref type="table" target="#tab_5">Table 3</ref>) as a function of the ensemble size. The ensemble uses several instances of a same network trained with different random seeds. Their predicted scores are combined additively. Even small ensembles provide a significant increase in performance over a single network.</p><p>(same network architecture, same hyperparameters, same data) is trained with different initial random seeds. This affects the initialization of the learned parameters and the stochastic gradient descent optimization. At test time, the scores predicted for the candidates answers by all instances are summed, and the final answer is determined from the highest summed score. As reported in <ref type="figure">Fig. 5</ref>, the performance increases monotonically with the size of the ensemble, i.e. the number of network instances. We obtained our final, best results, with an ensemble of 30 networks. The training of multiple instances is independent and obviously parallelizeable on multiples CPUs or GPUs. Interestingly, even small ensembles of 2-5 instances provide a significant increase in performance over a single network.</p><p>Note that those experiments include the validation split of VQA v2 for training and use its test-dev split for evaluation, hence the higher overall performance compared to Tables 1 and 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Cumulative ablations</head><p>All ablative experiments presented above consider one or two modifications of the reference model at a time. It is important to note that the cumulative effect of several modifications is not necessarily additive. In practice, this complicates the search for optimal architectures and hyperparameters. Some choices that appear promising at first may not pan out when combined with other optimizations. Conversely, some options discarded early on the search may prove effective once other hyperparameters have been tuned.</p><p>We report in <ref type="table" target="#tab_1">Table 2</ref> a series of cumulative ablations of our reference model. We consider a series of characteristics of our model in the inverse of the order in which they could be incorporated into other VQA models. The results follow the trends observed with the individual ablations. Removing each proposed contribution steadily decreases the performance of the model. This set of experiments reveals that the most critical components of our model are the sigmoid outputs instead of a softmax, the soft scores used as ground truth targets, the image features from bottomup attention <ref type="bibr" target="#b4">[3]</ref>, the gated tanh activations, the output layers initialized using GloVE and Google Images, and the smart shuffling of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Comparison with existing methods</head><p>We compare in <ref type="table" target="#tab_5">Table 3</ref> the performance of our best model with existing methods. Ours is an ensemble of 30 networks identical to the reference model (first row of Table 1) with the exception of the dimension of the hidden states, increased here to 1, 500. The issue of overfitting (Section 4.7) is mitigated by the large ensemble size. Compared to <ref type="table" target="#tab_3">Table 1</ref>, this model also includes here the validation split of VQA v2 for training. Our model obtained the first place at the 2017 VQA Challenge <ref type="bibr">[1]</ref>. It still surpasses all competing methods by a significant margin at the time of writing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion and conclusions</head><p>This paper presented a model for VQA based on a deep neural network that significantly outperforms all other approaches proposed to date. Importantly, we reported an extensive suite of experiments that identify the contribution of each design choice and the performance of alternative designs. The general take-away from this study is that the performance is very dependent on design choices and on various details of the implementation. We attribute the success of our model to a number of points. Some are seemingly minor and easily implemented (e.g. large mini-batch size, sigmoid output) while others are clearly non-trivial (e.g. gated non-linear activations, image features from bottom-up attention, pretraining the output classifier). This paper does not claim to make breakthrough advances in the field of VQA, which remains a largely unsolved problem. We hope that our model may however serve as a solid basis on which to make future progress. Our extensive analysis and exploration of designs is unprecedented in scale for VQA, and is intended as a significant contribution to the field. It provides indicators on the importance of various components of a VQA model. It also allows us to point at several promising directions for future developments.</p><p>We showed that significant gains were still achievable through better image features, in particular with the regionspecific features of <ref type="bibr" target="#b4">[3]</ref> that use bottom-up attention. We also measured that gains from additional VQA training data had not reached a clear plateau yet. However, the trend of collecting larger datasets is unlikely to bring significant breakthroughs. We believe that incorporating other sources of information and leveraging non-VQA datasets is a promising direction.</p><p>Our evaluation of simple baselines have shown surpris-  ingly strong performances. For example, encoding questions as a simple bag-of-words performs almost as well as state-of-the-art recurrent encoders. It suggests that the word ordering in questions may not convey much informationwhich is plausible for the most basic ones -or, more realistically, that our current models are still unable to understand and make effective use of language structure. Recent works on compositional models for VQA are a promising direction to address this issue <ref type="bibr" target="#b5">[4,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b22">21]</ref>. Finally, we must be reminded to look at performance measures with a critical eye. The commonly reported metrics such as our &gt; 70% per-question accuracy appear encouraging, but it is valuable to keep an eye on failure cases and alternative performance measures. We reported throughout this study the accuracy over balanced pairs of questions. This stricter measure requires accurate answers to two complimentary versions of a same question relating to different images. It better reflects the ability of the method for visual understanding and for making out subtle differences between images. In that case, the performance drops to the order 35%. While far less impressive, that figure is more representative of our current state of progress on VQA. We hope that keeping such a critical outlook will encourage more radical innovation and breakthroughs in the near future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed model. A deep neural network implements a joint embedding of the input question and image, followed by a multi-label classifier over a fixed set of candidate answers. Gray numbers indicate the dimensions of the vector representations between layers. Yellow elements use learned parameters. The elements w represent linear layers, and w non-linear layers (gated tanh).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 2 ) 3 ) 4 )</head><label>234</label><figDesc>The ablation that uses word embeddings learned from scratch, instead of GloVe vectors. (The ablation with the output classifier learned from scratch, instead of pretrained w text o and w img o . (The conjunction of (2) and (3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Effect of pretraining the output classifier on specific answers. We compare the per-candidate-answer recall of three models: a baseline using a classifier trained from scratch or pretrained (in black), and models using pretrained wtext o and/or w img o . (Leftmost chart) Random selection of answers sorted by their recall in the baseline model. (Right three charts) Top-60 answers with the largest improvement in recall by pretraining the classifier. See discussion in Section 4.6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Performance of our reference model (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Image-based w img</figDesc><table><row><cell>Question</cell><cell>Word embedding</cell><cell>GRU</cell><cell></cell><cell></cell><cell>w w</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Image</cell><cell>CNN/</cell><cell>L2 Norm.</cell><cell cols="2">Top-down attention weights Σ Σ w w w softmax</cell><cell>w w</cell><cell>w w w w</cell><cell>w w</cell><cell>σ</cell><cell>candidate answers Predicted scores of</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Concatenation</cell><cell>Weighted sum over image locations</cell><cell cols="2">Element-wise product</cell><cell cols="2">Pretrained linear classifiers</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Training data (reference: with Visual Genome data; shuffling keeps balanced pairs in the same minibatches) Output vocabulary (reference: &gt;8 occurrences as correct answers in the training data, N = 3, 129 Cumulative ablations of a single network, evaluated on the VQA v2 validation set. The ablations of table rows are cumulative from top to bottom. The experimental setup is identical to the one used forTable 1.</figDesc><table><row><cell></cell><cell cols="3">VQA v2 validation</cell><cell></cell></row><row><cell></cell><cell cols="2">VQA Score</cell><cell></cell><cell>Accuracy over</cell></row><row><cell>All</cell><cell>Yes/no</cell><cell>Numbers</cell><cell>Other</cell><cell>balanced pairs</cell></row></table><note>Table 1. Ablations of a single network, evaluated on the VQA v2 validation set. We evaluate variations of our best "reference" model (first row), and show that it corresponds to a local optimum in the space of architectures and hyperparameters. Every row corresponds to one variation of that reference model. We train each variation with three different random seeds and report averages and standard deviations (±).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1</head><label>1</label><figDesc>, first row) trained on a subset of the training data. The use of additional non-VQA data for pretraining the word embeddings and the output classifiers is significant, especially when training on a reduced training set. Also not the tendency performance to plateau, and the small gain in performance relative to a 10-fold increase of training data. See discussion in Section 4.9.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Comparison of our best model with competing methods. Excerpt from the official VQA v2 Leaderboard [1].</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://nlp.stanford.edu/projects/glove/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Image features from bottom-up attention, adaptive K, single network</title>
		<idno>65.32 81.82 44.21 56.05 65.67 82.20 43.90 56.26</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<idno>66.34 83.38 43.17 57.10 66.73 83.71 43.77 57.20</idno>
		<title level="m">ResNet features 7×7</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Image features from bottom-up attention, adaptive K, ensemble 69</title>
		<idno>86.08 48.99 60.80 70.34 86.60 48.64 61.15</idno>
		<imprint>
			<biblScope unit="volume">87</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Zipfs law and the internet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Huberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Glottometrics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="143" to="150" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07998</idno>
		<title level="m">Bottom-up and top-down attention for image captioning and vqa</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural Module Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comp. Vis</title>
		<meeting>IEEE Int. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">MU-TAN: multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadène</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06676</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abc-Cnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05960</idno>
		<title level="m">An Attention Based Convolutional Neural Network for Visual Question Answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning phrase representations using RNN encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merrienboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Empirical Methods in Natural Language Processing</title>
		<meeting>Conf. Empirical Methods in Natural Language essing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Visual Dialog</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08083</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00837</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05526</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05676</idno>
		<title level="m">Compositional Memory for Visual Question Answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06890</idno>
		<title level="m">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03633</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Show, ask, attend, and answer: A strong baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elqursh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03162</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">TrimZero: A Torch Recurrent Module for Efficient Natural Language Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KIIS Spring Conference</title>
		<meeting>KIIS Spring Conference</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="165" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07332</idno>
		<title level="m">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comp. Vis</title>
		<meeting>Eur. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00061</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Inf. Process. Syst. 2015</title>
		<meeting>Advances in Neural Inf. ess. Syst. 2015</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.00387v1</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Zero-shot visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Visual question answering: A survey of methods and datasets. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ask</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05234</idno>
		<title level="m">Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Stacked Attention Networks for Image Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">ADADELTA: an adaptive learning rate method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Yin and yang: Balancing and answering binary visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Visual7W: Grounded Question Answering in Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
