<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Continuous Adaptation for Interactive Object Segmentation by Learning from Corrections</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodora</forename><surname>Kontogianni</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>‡2</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">RWTH Aachen University</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gygli</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<settlement>Zurich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<settlement>Zurich</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<settlement>Zurich</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Continuous Adaptation for Interactive Object Segmentation by Learning from Corrections</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T12:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In interactive object segmentation a user collaborates with a computer vision model to segment an object. Recent works employ convolutional neural networks for this task: Given an image and a set of corrections made by the user as input, they output a segmentation mask. These approaches achieve strong performance by training on large datasets but they keep the model parameters unchanged at test time. Instead, we recognize that user corrections can serve as sparse training examples and we propose a method that capitalizes on that idea to update the model parameters on-the-fly to the data at hand. Our approach enables the adaptation to a particular object and its background, to distributions shifts in a test set, to specific object classes, and even to large domain changes, where the imaging modality changes between training and testing. We perform extensive experiments on 8 diverse datasets and show: Compared to a model with frozen parameters, our method reduces the required corrections (i) by 9%-30% when distribution shifts are small between training and testing; (ii) by 12%-44% when specializing to a specific class; (iii) and by 60% and 77% when we completely change domain between training and testing.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In interactive object segmentation a human collaborates with a computer vision model to segment an object of interest <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b10">11]</ref>. The process iteratively alternates between the user providing corrections on the current segmentation and the model refining the segmentation based on these corrections. The objective of the model is to infer an accurate segmentation mask from as few user corrections as possible (typically point clicks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref> or strokes <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b23">24]</ref> on mislabeled pixels). This enables fast and accurate object segmentation, which is indispensable for image editing <ref type="bibr" target="#b1">[2]</ref> and collecting ground-truth segmentation masks at scale <ref type="bibr" target="#b10">[11]</ref>.</p><p>Current state-of-the-art methods train a convolutional neural network (CNN) which takes an image and user corrections as input and predicts a foreground / background segmentation <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>. At test time, the model parameters</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Adaptation</head><p>Sequence Adaptation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fixed Model</head><p>Ours Sequence Adaptation</p><p>Frozen model <ref type="bibr" target="#b37">[38]</ref> Ours <ref type="figure">Fig. 1</ref>: Example results for a frozen model (top) and our adaptive methods (bottom). A frozen model performs poorly when foreground and background share similar appearance (left), when it is used to segment new object classes absent in the training set (center, donut class), or when the model is tested on a different image domain (aerial) than it is trained on (consumer) (right). By using corrections to adapt the model parameters to a specific test image, or to the test image sequence, our method substantially improves segmentation quality. The input is four corrections in all cases shown.</p><p>are frozen and corrections are only used as additional input to guide the model predictions. But in fact, user corrections directly specify the ground-truth labelling of the corrected pixels. In this paper we capitalize on this observation: we treat user corrections as training examples to adapt our model on-the-fly. We use these user corrections in two ways: <ref type="bibr" target="#b0">(1)</ref> in single image adaptation we iteratively adapt model parameters to one specific object in an image, given the corrections produced while segmenting that object; <ref type="bibr" target="#b1">(2)</ref> in image sequence adaptation we adapt model parameters to a sequence of images with an online method, given the set of corrections produced on these images. Each of these leads to distinct advantages over using a frozen model: During single image adaptation our model learns the specific appearance of the current object instance and the surrounding background. This allows the model to adapt even to subtle differences between foreground and background for that specific example. This is necessary when the object to be segmented has similar color to the background <ref type="figure">(Fig. 1, 1st</ref> column), has blurry object boundaries, or low contrast. In addition, a frozen model can sometimes ignore the user corrections and overrule them in its next prediction. We avoid this undesired behavior by updating the model parameters until its predictions respect the user corrections.</p><p>During image sequence adaptation we continuously adapt the model to a sequence of segmentation tasks. Through this, the model parameters are optimized to the image and class distribution in these tasks, which may consist of different types of images or a set of new classes which are unseen during training. An important case of this is specializing the model for segmenting objects of a single class. This is useful for collecting many examples in high-precision domains, such as pedestrians for self-driving car applications. <ref type="figure">Fig. 1</ref>, middle column shows an example of specializing to the single, unseen class donut. Furthermore, an important property of image sequence adaptation is that it enables us to handle large domain changes, where the imaging modality changes dramatically between training and testing. We demonstrate this by training on consumer photos while testing on medical and aerial images ( <ref type="figure">Fig. 1, right column)</ref>.</p><p>Naturally, single image adaptation and image sequence adaptation can be used jointly, leading to a method that combines their advantages.</p><p>In summary: Our innovative idea of treating user corrections as training examples allows to update the parameters of an interactive segmentation model at test time. To update the parameters we propose a practical online adaptation method. Our method operates on sparse corrections, balances adaptation vs. retaining old knowledge and can be applied to any CNN-based interactive segmentation model. We perform extensive experiments on 8 diverse datasets and show: Compared to a model with frozen parameters, our method reduces the required corrections (i) by 9%-30% when distribution shifts are small between training and testing; (ii) by 12%-44% when specializing to a specific class; (iii) and by 60% and 77% when we completely change domain between training and testing. (iv) Finally, we evaluate on four standard datasets where distribution shifts between training and testing are minimal. Nevertheless, our method did set a new state-of-the-art on all of them, when it was initially released <ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Interactive Object Segmentation. Traditional methods approach interactive segmentation via energy minimization on a graph defined over pixels <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b43">44]</ref>. User inputs are used to create an image-specific appearance model based on low-level features (e.g. color), which is then used to predict foreground and background probabilities. A pairwise smoothness term between neighboring pixels encourages regular segmentation outputs. Hence these classical methods are based on a weak appearance model which is specialized to one specific image.</p><p>Recent methods rely on Convolutional Neural Networks (CNNs) to interactively produce a segmentation mask <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b2">3]</ref>. These methods take the image and user corrections (transformed into a guidance map) as input and map them to foreground and background probabilities. This mapping is optimized over a training dataset and remains frozen at test time. Hence these models have a strong appearance model but it is not optimized for the test image or dataset at hand.</p><p>Our method combines the advantages of traditional and recent approaches: We use a CNN to learn a strong initial appearance model from a training set. During segmentation of a new test image, we adapt the model to it. It thus learns an appearance model specifically for that image. Furthermore, we also continuously adapt the model to the new image and class distribution of the test set, which may be significantly different from the one the model is originally trained on. Gradient Descent at test time. Several methods iteratively minimize a loss at test time. The concurrent work of <ref type="bibr" target="#b53">[54]</ref> uses self-supervision to adapt the feature extractor of a multi-tasking model to the test distribution. Instead, we directly adapt the full model by minimizing the task loss. Others iteratively update the inputs of a model <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>, e.g. for style transfer <ref type="bibr" target="#b22">[23]</ref>. In the domain of interactive segmentation, <ref type="bibr" target="#b28">[29]</ref> updates the guidance map which encodes the user corrections and is input to the model. <ref type="bibr" target="#b51">[52]</ref> made this idea more computationally efficient by updating intermediate feature activations, rather than the guidance maps. Instead, our method updates the model parameters, making it more general and allowing it to adapt to individual images as well as sequences.</p><p>In-domain Fine-Tuning. In other applications it is common practice to finetune on in-domain data when transferring a model to a new domain <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b60">61]</ref>. For example, when supervision for the first frame of a test video is available <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b12">13]</ref>, or after annotating a subset of an image dataset <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b60">61]</ref>. In interactive segmentation the only existing attempt is <ref type="bibr" target="#b0">[1]</ref>, which performs polygon annotation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b36">37]</ref>. However, it does not consider adapting to a particular image; their process to fine-tune on a dataset involves 3 different models, so they do it only a few times per dataset; they cannot directly train on user corrections, only on complete masks from previous images; finally, they require a bounding box on the object as input. Few-shot and Continual Learning. Our method automatically adapts to distribution shifts and domain changes. It performs domain adaptation from limited supervision, similar to few-shot learning <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b44">45]</ref>. It also relates to continual learning <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b20">21]</ref>, except that the output label space of the classifier is fixed. As in other works, our method needs to balance between preserving existing knowledge and adapting to new data. This is often done by fine-tuning on new tasks while discouraging large changes in the network parameters, either by penalizing changes to important parameters <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref> orchanging predictions of the model on old tasks <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b40">41]</ref>. Alternatively, some training data of the old task is kept and the model is trained on a mixture of the old and new task data <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b8">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>We adopt a typical interactive object segmentation process <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>: the model is given an image and makes an initial foreground / background prediction for every pixel. The prediction is then overlaid on the image and presented to the user, who is asked to make a correction. The user clicks on a single pixel to mark that it was incorrectly predicted to be foreground instead of background or vice versa. The model then updates the predicted segmentation based on all corrections received so far. This process iterates until the segmentation reaches a desired quality level.</p><p>We start by describing the model we build on (Sec. 3.1). Then, we describe our core contribution: treating user corrections as training examples to adapt the model on-the-fly at test-time (Sec. 3.2). Lastly, we describe how we simulate user corrections to train and test our method (Sec. 3.3). For learning the initial model parameters, full supervision is available, allowing to compute a loss over all the pixels in the image. At test time, the user provides sparse supervision in the form of corrections. We use these to adapt the model parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Interactive Segmentation Model</head><p>As the basis of our approach, we use a strong re-implementation of <ref type="bibr" target="#b37">[38]</ref>, an interactive segmentation model based on a convolutional neural network. The model takes an RGB image and the user corrections as input and produces a segmentation mask. As in <ref type="bibr" target="#b10">[11]</ref> we encode the position of user corrections by placing binary disks into a guidance map. This map has the same resolution as the image and consists of two channels (one channel for foreground and one for background corrections). The guidance map is concatenated with the RGB image to form a 5-channel map x which is provided as input to the network.</p><p>We use DeepLabV3+ <ref type="bibr" target="#b16">[17]</ref> as our network architecture, which has demonstrated good performance on semantic segmentation. However, we note that our method does not depend on a specific architecture and can be used with others as well.</p><p>For training the model we need a training dataset D with ground-truth object segmentations, as well as user corrections which we simulate as in <ref type="bibr" target="#b37">[38]</ref> (Sec. 3.3). We train the model using the cross-entropy loss over all pixels in an image:</p><formula xml:id="formula_0">L CE (x, y; θ) = 1 |y| {−y log f (x; θ) − (1 − y) log(1 − f (x; θ))} (1)</formula><p>where x is the 5-channel input defined above (image plus guidance maps), y ∈ {0, 1} H×W are the pixel labels of the ground-truth object segmentations, and f (x; θ) represents the mapping of the convolutional network parameterized by θ. | · | denotes the l 1 norm. We produce the initial parameters θ * of the segmentation model by minimizing</p><formula xml:id="formula_1">(xi,y i )∈D L CE (x i , y i ; θ)</formula><p>over the training set using stochastic gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning from Corrections at Test-Time</head><p>Previous interactive object segmentation methods do not treat corrections as training examples. Thus, the model parameters remain unchanged/frozen at test time <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref> and corrections are only used as inputs to guide the predictions. Instead, we treat corrections as ground-truth labels to adapt the model at test time. We achieve this by minimizing the generalized cross-entropy loss over the corrected pixels:</p><formula xml:id="formula_2">L GCE (x, c; θ) = 1[c =−1] T |1[c =−1]| − c log f (x; θ) − (1 − c) log (1 − f (x; θ))<label>(2)</label></formula><p>where 1 is an indicator function and c is a vector of values {1, 0, −1}, indicating what pixels were corrected to what label. Pixels that were corrected to be positive are set to 1 and negative pixels to 0. The remaining ones are set to −1, so that they are ignored in the loss. As there are very few corrections available at test time, this loss is computed over a sparse set of pixels. This is in contrast to the initial training which had supervision at every pixel (Sec. 3.1). We illustrate the contrast between the two forms of supervision in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p><p>Dealing with label sparsity. In practice, corrections c are extremely sparse and consist of just a handful of scattered points ( <ref type="figure" target="#fig_1">Fig. 3</ref>). Hence, they offer limited information on the spatial extent of objects and special care needs to be taken to make this form of supervision useful in practice. As our model is initially trained with full supervision, it has learned strong shape priors. Thus, we propose two auxiliary losses to prevent forgetting these priors as the model is adapted. First, we regularize the model by treating the initial mask prediction p as ground-truth and making it a target in the cross-entropy loss, i.e. L CE (x, p; θ). This prevents the model from focusing only on the user corrections while forgetting the initially good predictions on pixels for which no corrections were given. Second, inspired by methods for class-incremental learning <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b4">5]</ref>, we minimize unnecessary changes to the network parameters to prevent it from forgetting crucial patterns learned on the initial training set. Specifically, we add a cost for changing important network parameters:</p><formula xml:id="formula_3">L F (θ) = Ω T (θ − θ * ) 2 (3)</formula><p>where θ * are the initial model parameters, θ are the updated parameters and Ω is the importance of each parameter. (·) 2 is the element-wise square (Hadamard square). Intuitively, this loss penalizes changing the network parameters away from their initial values, where the penalty is higher for important parameters. We compute Ω using Memory-Aware Synapses (MAS) <ref type="bibr" target="#b4">[5]</ref>, which estimates importance based on how much changes to the parameters affect the prediction of the model. Combined loss. Our full method uses a linear combination of the above losses:</p><formula xml:id="formula_4">L ADAPT (x, p, c; θ) = λL GCE (x, c; θ) + (1 − λ)L GCE (x, p; θ) + γL F (θ) (4)</formula><p>where λ balances the importance of the user corrections vs. the predicted mask, and γ defines the strength of parameter regularization. Next, we introduce single image adaptation and image sequence adaptation, which both minimize Eq. (4). Their difference lies in how the model parameters θ are updated: individually for each object or over a sequence.</p><p>Adapting to a single image. We adapt the segmentation model to a particular object in an image by training on the click corrections. We start from the segmentation model with parameters θ * fit to the initial training set (Sec. 3.1). Then we update them by running several gradient descent steps to minimize our combined loss Eq. (4) every time the user makes a correction (Algo. in supp. material). We choose the learning rate and the number of update steps such that the updated model adheres to the user corrections. This effectively turns corrections into constraints. This process results in a segmentation mask p, predicted using the updated parameters θ.</p><p>Adapting the model to the current test image brings two core advantages. First, it learns about the specific appearance of the object and background in the current image. Hence corrections have a larger impact and can also improve the segmentation of distant image regions which have similar appearance. The model can also adapt to low-level photometric properties of this image, such as overall illumination, blur, and noise, which results in better segmentation in general. Second, our adaptation step makes the corrections effectively hard constraints, so the model will preserve the corrected labeling in later iterations too.</p><p>This adaptation is done for each object separately, and the updated θ is discarded once an object is segmented.</p><p>Adapting to an image sequence. Here we describe how to continuously adapt the segmentation model to a sequence of test images using an online algorithm. Again, we start from the model parameters θ * fit to the initial training set (Sec. 3.1). When the first test image arrives, we perform interactive segmentation using these initial parameters. Then, after segmenting each image I t = (x t , c t ), the model parameters are updated to θ t+1 by doing a single gradient descent step to minimize Eq. (4) for that image. Thereby we subsample the corrections in the guidance maps to avoid trivial solutions (predict the corrections given the corrections themselves, see supp. material). The updated model parameters are used to segment the next image I t+1 .</p><p>Through the method described above our model adapts to the whole test image sequence, but does so gradually, as objects are segmented in sequence. As a consequence, this process is fast, does not require storing a growing number of images, and can be used in a online setting. In this fashion it can adapt to changing appearance properties, adapt to unseen classes, and specialize to one particular class. It can even adapt to radically different image domains as we demonstrate in Sec. 4.3.</p><p>Combined adaptation. For a test image I t , we segment the object using single image adaptation (Algo. in supp. material). After segmenting a test image, we gather all corrections provided for that image and apply a image sequence adaptation step to update the model parameters from θ t to θ t+1 . At the next image, the image adaptation process will thus start from parameters θ t+1 better suited for the test sequence. This combination allows to leverage the distinct advantages of the two types of adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Simulating user corrections</head><p>To train and test our method we rely on simulated user corrections, as is common practice <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b28">29]</ref>. Test-time corrections. When interactively segmenting an object, the user clicks on a mistake in the predicted segmentation. To simulate this we follow <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38]</ref>, which assume that the user clicks on the largest error region. We obtain this error region by comparing the model predictions with the ground-truth and select its center pixel. Train-time corrections. Ideally one wants to train with the same user model that is used at test-time. To make this computationally feasible, we train the model in two stages as in <ref type="bibr" target="#b37">[38]</ref>. First, we sample corrections using ground-truth segmentations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b55">56]</ref>. Positive user corrections are sampled uniformly at random on the object. Negative user corrections are sampled according to three strategies: (1) uniformly at random from pixels around the object, (2) uniformly at random on other objects, and (3) uniformly around the object. We use these corrections to train the model until convergence. Then, we continue training by iteratively sampling corrections following <ref type="bibr" target="#b37">[38]</ref>. For each image we keep a set of user corrections c. Given c we predict a segmentation mask, simulate the next user correction (as done at test time), and add it to c. Based on this additional correction, we predict a new segmentation mask and minimize the loss <ref type="figure">(Eq. (1)</ref>). Initially, c corresponds to the corrections simulated in the first stage, and over time more user corrections are added. As we want the model to work well even with few user corrections, we thus periodically reset c to the initial clicks <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We extensively evaluate our single image adaptation and image sequence adaptation methods on several standard datasets as well as on aerial and medical images. These correspond to increasingly challenging adaptation scenarios. Adaptation scenarios. We first consider distribution shift, where the training and test image sets come from the same general domain, consumer photos, but differ in their image and object statistics (Sec. 4.1). This includes differences in image complexity, object size distribution, and when the test set contains object classes absent during training. Then, we consider a class specialization scenario, where a sequence of objects of a single class has to be iteratively segmented (Sec. 4.2). Finally we test how our method handles large domain changes where the imaging modality changes between training and testing. We demonstrate this by going from consumer photos to aerial and medical images (Sec. 4.3). Model Details. We use a strong re-implementation of <ref type="bibr" target="#b37">[38]</ref> as our interactive segmentation model (Sec. 3.1). We pre-train its parameters on PASCAL VOC12 <ref type="bibr" target="#b19">[20]</ref> augmented with SBD <ref type="bibr" target="#b25">[26]</ref> (10582 images with 24125 segmented instances of 20 object classes). As a baseline, we use this model as in <ref type="bibr" target="#b37">[38]</ref>, i.e. without updating its parameters at test time. We call this the frozen model. This baseline already achieves state-of-the-art results on the PASCAL VOC12 validation set, simply by increasing the encoder resolution compared to <ref type="bibr" target="#b37">[38]</ref> (3.44 clicks). This shows that using a fixed set of model parameters works well when the train and test distributions match. We evaluate our proposed method by adapting the parameters of that same model at test time using single image adaptation (IA), image sequence adaptation (SA), and their combination (IA + SA). Evaluation metrics. We use two standard metrics <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b28">29]</ref>: (1) IoU@k, the average intersection-over-union between the ground-truth and predicted segmentation masks, given k corrections per image, and (2) clicks@q%, the average number of corrections needed to reach an IoU of q% on every image (thresholded at 20 clicks). We always report mean performance over 10 runs (standard deviation is negligible at ≈ 0.01 for clicks@q%).</p><p>Hyperparameter selection. We optimize the hyperparameters for both adaptation methods on a subset of the ADE20k dataset <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b59">60]</ref>. Hence, the hyperparameters are optimized for adapting from PASCAL VOC12 to ADE20k, which is distinct from the distribution shifts and domain changes we evaluate on. Implementation Details are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Adapting to distribution shift</head><p>We test how well we can adapt the model which is trained on PASCAL VOC12 to other consumer photos datasets. Datasets. We test on: (1) Berkeley <ref type="bibr" target="#b39">[40]</ref>, 100 images with a single foreground object.</p><p>(2) YouTube -VOS <ref type="bibr" target="#b56">[57]</ref>, a large video object segmentation dataset. We use the test set of the 2019 challenge, where we take the first frame with ground truth (1169 objects, downscaled to 855 × 480 maximal resolution). (3) COCO [36], a large segmentation dataset with 80 object classes. 20 of those overlap with the ones in the PASCAL VOC12 dataset and are thus seen during training. The other 60 are unseen. We sample 10 objects per class from the validation set and separately report results for seen (200 objects) and unseen classes (600 objects) as in <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b38">39]</ref>. We also study how image sequence adaptation behaves on longer sequences of 100 objects for each unseen class (named COCO unseen 6k).</p><p>Results. We report our results in Tab. 1 and <ref type="figure" target="#fig_4">Fig. 4</ref>. Both types of adaptation improve performance on all tested datasets. On the first few user corrections single image adaptation (IA) performs similarly to the frozen model as it is initialized with the same parameters. But as more corrections are provided, it uses these more effectively to adapt its appearance model to a specific image. Thus, it performs particularly well in the high-click regime, which is most useful for objects that are challenging to segment (e.g. due to low illumination, <ref type="figure" target="#fig_1">Fig. 3</ref>  During image sequence adaptation (SA), the model adapts to the test image distribution and thus learns to produce good segmentation masks given just a few clicks <ref type="figure" target="#fig_4">(Fig. 4a)</ref>. As a result, SA outperforms using a frozen model on all datasets with distribution shifts (Tab. 1). By adapting from images to the video frames of YouTube -VOS, SA reduces the clicks needed to reach 85% IoU by 15%. Importantly, we find that our method adapts fast, making a real difference after just a few images, and then keeps on improving even as the test sequence becomes thousands of images long <ref type="figure" target="#fig_4">(Fig. 4b)</ref>. This translates to a large improvement given a fixed budget of 4 clicks per object: on the COCO unseen 6k split it achieves 69% IoU compared to the 57% of the frozen model <ref type="figure" target="#fig_4">(Fig. 4a)</ref>.</p><p>Generally, the curves for image sequence adaptation grow faster in the low click regime than the single image adaptation ones, but then exhibit stronger diminishing returns in the higher click regime <ref type="figure" target="#fig_4">(Fig. 4a</ref>). Hence, combining the two compounds their advantages leading to a method that considerably improves over the frozen model on the full range of number of corrections and sequence lengths <ref type="figure" target="#fig_4">(Fig. 4a</ref>). Compared to the frozen model, our combined method significantly reduces the number of clicks needed to reach the target accuracy on all datasets: from a 9% reduction on Berkeley and COCO seen, to a 30% reduction on COCO unseen 6k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Adapting to a specific class</head><p>When a user segments objects of a single class at test-time, image sequence adaptation naturally specializes its appearance model to that class. We evaluate this phenomenon on 4 COCO classes. We form 4 test image sequences, each focusing on a single class, containing objects of varied appearance. The classes are selected based on how image sequence adaptation performs compared to the frozen model in Sec. 4.1. We selected the following classes, with increasing order of difficulty for image sequence adaptation: (1) donut (2540 objects) (2) bench (3500) (3) umbrella (3979) and (4) bed (1450).      Results. Tab. 2, <ref type="figure" target="#fig_4">Fig. 4c</ref> present results. The class specialization brought by our image sequence adaptation (SA) leads to good masks from very few clicks. For example, on the donut class it reduces clicks@85% by 39% compared to the frozen model and by 44% when combined with single image adaptation (Tab. 2). Given just 2 clicks, SA reaches 66% IoU for that class, compared to 25% IoU for the frozen model <ref type="figure" target="#fig_4">(Fig. 4c)</ref>. The results for the other classes follow a similar pattern, showing that image sequence adaptation learns an effective appearance model for a single class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Adapting to domain changes</head><p>We test our method's ability of adapting to domain changes by training on consumer photos (PASCAL VOC12) and evaluating on aerial and medical imagery. Datasets. We explore two test datasets: (1) Rooftop Aerial <ref type="bibr" target="#b52">[53]</ref>, a dataset of 65 aerial images with segmented rooftops and (2) DRIONS-DB <ref type="bibr" target="#b13">[14]</ref>, a dataset of 110 retinal images with a segmentation of the optic disc of the eye fundus. (we use the masks of the first expert). Importantly, the initial model parameters θ * were optimized for the PASCAL VOC12 dataset, which consists of consumer photos. Hence, we explore truly large domain changes here.</p><p>Results. Both our forms of adaptation significantly improve over the frozen model (Tab. 3, <ref type="figure" target="#fig_7">Fig. 5</ref>). Single image adaptation can only adapt to a limited extent, as it independently adapts to each object instance, always starting from the same initial model parameters θ * . Nonetheless, it offers a significant improvement, reducing the number of clicks needed to reach the desired IoU by 14%-29%. Image sequence adaptation (SA) shows extremely strong performance, as its adaptation effects accumulate over the duration of the test sequence. It reduces the needed user input by 60% for the Rooftop Aerial dataset and by over 70% for DRIONS-DB. When combining the two types of adaptation, the reduction increases to 77% for the DRIONS-DB dataset (Tab. 3). Importantly, our method adapts fast: on DRIONS-DB clicks@90% drops quickly and converges to just 2 corrections, as the length of the test sequence increases <ref type="figure" target="#fig_7">(Fig. 5a)</ref>. In contrast, the frozen model performs poorly on both datasets. On the Rooftop Aerial dataset, it needs even more clicks than there are points in the ground truth polygons (8.9 vs. 5.1). This shows that even a state-of-the-art model like <ref type="bibr" target="#b37">[38]</ref> fails to generalize to truly different domains and highlights the importance of adaptation.</p><p>To summarize: We show that our method can bridge large domain changes spanning varied datasets and sequence lengths. With just a single gradient descent step per image, our image sequence adaptation successfully addresses a major shortcoming of neural networks, for the case of interactive segmentation: Their poor generalization to changing distributions <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b3">4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison to Previous Methods</head><p>While the main focus of our work is tackling challenging adaptation scenarios, we also compare our method against state-of-the-art interactive segmentation methods on standard datasets. These datasets are typically similar to PASCAL VOC12, hence have a small distribution mismatch between training and testing. , 50 high-resolution videos out of which we sample 10% of the frames uniformly at random as in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b28">29]</ref> (We note that the standard evaluation protocol of DAVIS16 favors adaptive methods, as the same objects appear repeatedly in the test sequence.) and (4) PASCAL VOC12 validation, with 1449 images.   Results. Tab. 4 shows results. Our adaptation method achieves strong results: At the time of initially releasing our work <ref type="bibr" target="#b31">[32]</ref>, it outperformed all previous state-of-the-art methods on all datasets (it was later overtaken by <ref type="bibr" target="#b51">[52]</ref>). It brings improvements even when the previous methods (which have frozen model parameters) already offers strong performance and need less than 4 clicks on average (PASCAL VOC12, GrabCut). The improvement on PASCAL VOC12 further shows that our method helps even when the training and testing distributions match exactly (the frozen model needs 3.44 clicks). Importantly, we find that our method outperforms <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b28">29]</ref>, even though we use a standard segmentation backbone <ref type="bibr" target="#b16">[17]</ref> which predicts at 1 4 of the input resolution. Instead <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b28">29]</ref> propose specialized network architectures in order to predict at full image resolution, which is crucial for their good performance <ref type="bibr" target="#b28">[29]</ref>. We note that our adaptation method is orthogonal to these architectural optimizations and can be combined with them easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We ablate the benefit of treating corrections as training examples (on COCO unseen 6k). For this, we selectively remove them from the loss <ref type="figure" target="#fig_4">(Eq. (4)</ref>). For single image adaptation, this leads to a parameter update that makes the model more confident in its current prediction, but this does not improve the segmentation masks. Instead, training on corrections improves clicks@85% from 13.2 to 10.6. For image sequence adaptation, switching off the corrections corresponds to treating the predicted mask as ground-truth and updating the model with it. This approach implicitly contains corrections in the mask and thus improves clicks@85% from 13.2 for the frozen model to 11.9. Explicitly using correction offers an additional gain of almost 2 clicks, down to 10. This shows that treating user corrections as training examples is key to our method: They are necessary for single image adaptation and highly beneficial for image sequence adaptation.  <ref type="figure">6</ref>). We recommend to use 3 update steps, reducing adaptation time to 0.5 s, with a negligible effect on the number of corrections required (average difference of less than 1%, over all datasets). To increase speed further, the following optimizations are possible: (1) Using a faster backbone, e.g. with a ResNet-50 <ref type="bibr" target="#b26">[27]</ref>, the time for an update step reduces to 0.06 s; (2) Using faster accelerators such as Google Cloud TPUs; (3) Employing a fixed feature extractor and only updating a light-weight segmentation head <ref type="bibr" target="#b32">[33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Adaptation speed</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose to treat user corrections as sparse training examples and introduce a novel method that capitalizes on that idea to update the model parameters onthe-fly at test time. Our extensive evaluation on 8 datasets shows the benefits of our method. When distribution shifts between training and testing are small, our methods offers gains of 9%-30%. When specializing to a specific class, our gains are 12%-44%. For large domain changes, where the imaging modality changes between training and testing, it reduces the required number of user corrections by 60% and 77%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Implementation Details</head><p>A.1 Subsampling for image sequence adaptation</p><p>We subsample the corrections in the guidance maps to avoid trivial solutions. If all corrections were used equally in the loss and guidance maps, the model could eventually degrade to predict the corrections given the corrections themselves. Specifically it could degrade to only use the information in the guidance maps as its prediction, without relying on image appearance. We avoid this by subsampling the clicks given to the network as guidance, but using all clicks to compute the adaptation loss (Eq. <ref type="formula">(4)</ref>). This forces the network to rely on appearance for propagating the corrections to the rest of the image, where the loss is sparsely evaluated at the pixel locations which were corrected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Robustness to image order</head><p>Since our image sequence adaptation (SA) and combined adaptation (IA+SA) methods are processing images sequentially, we tested our method's sensitivity to the image order. We repeated all our experiments 10 times by randomising the image order and computed the variance of the results. We found the variance to be minimal (≤ 0.01 standard deviation) verifying that our adaptation methods are not sensitive to the order in which the images are processed. Hence, we only report averages to improve readability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Adaptation parameters</head><p>Our adaptation methods use Adam optimizer <ref type="bibr" target="#b29">[30]</ref> with learning rate of 10 −6 and batch size 1. For single image adaptation we do 10 SGD steps and regularize with λ = 1 and γ = 1. For image sequence adaptation we do 1 SGD step and use λ = 0.5 and γ = 2. For the DRIONS-DB dataset we use a learning rate of 10 −5 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Model details</head><p>We use DeeplabV3+ <ref type="bibr" target="#b16">[17]</ref> with Xception-65 <ref type="bibr" target="#b17">[18]</ref> as our backbone architecture (pretrained on ImageNet <ref type="bibr" target="#b18">[19]</ref> and PASCAL VOC12 <ref type="bibr" target="#b19">[20]</ref>). We extend this model with 2 extra channels for the guidance maps and train it for interactive segmentation model using SGD with momentum and an initial learning rate 0.0002 with polynomial decay. We use a batch size of 2, and atrous rates {12, 24, 36}. We use in input image resolution of 513 × 513 and an output stride of 8 for the encoder and 4 for the decoder, respectively. For generating corrections, we sample at most 5 foreground and 5 background corrections for stage one of training (see Sec. 3.3 in the main paper). Corrections are encoded with disks of radius 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Comparison on the COCO dataset</head><p>We have showed that all our adaptation methods are exhibiting substantial improvement compared to the frozen model in many datasets including the COCO dataset. The improvement is especially large on the unseen classes of COCO (16.8% improvement, <ref type="table" target="#tab_0">Table 1</ref> in the main paper) and on adaptation to a particular unseen class (44% improvement for the donut class, <ref type="table" target="#tab_2">Table 2</ref> in the main paper), two cases where adaptation is particularly useful. While we outperform all previous methods on PASCAL VOC12, GrabCut, Berkeley and DAVIS16, some existing works report better clicks@q% than us on COCO. e.g. <ref type="bibr" target="#b32">[33]</ref> reports 7.86 compared to 9.69 for our method. We however note that these results are not directly comparable. 10 instances are sampled per class to form a test set and the selected instances have not been made available by previous works. But how the selection is done is crucial, as segmenting smaller objects is more challenging. If we ignore objects smaller than 80 × 80 pixels as in <ref type="bibr" target="#b10">[11]</ref>, for example, our IA+SA improves from 9.9 to 5.4 (4.5 clicks less). Optimizing the architecture to better handle small objects is however not the focus of our work, as our adaptation methods work with any network architecture and can hence be combined with architectural improvements easily.</p><p>Benenson et al. <ref type="bibr" target="#b10">[11]</ref> performed a rigorous user study on interactive segmentation. There, they find that a user spends ≈ 3 seconds per click correction (not including computation time). For the network used in our work, a single update step takes 0.16 seconds (Sec. 4.6 in the main paper) and the forward pass takes 0.04 seconds. Thus, single image adaptation requires a computation time of ≈ 0.5s with 3 update steps. Image sequence adaptation is even faster as it only requires a single update step, which is done after an object is segmented. Hence, given these timings, user interaction time is the dominating factor for the total segmentation time. Reducing the number of corrections required, as in our work, is therefore an effective way to save user time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Qualitative Results</head><p>We show additional results for our two adaptation methods compared to the frozen model in <ref type="figure">Fig. 7</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Corrections as training examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Qualitative results of the frozen and our combined adaptation model. Red circles are negative clicks and green ones are positive. Green and red areas respectively show the pixels that turned to FG/BG with the latest clicks. Our method produces accurate masks with fewer clicks k.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>IoU@4 clicks as a function of the number of images processed. SA quickly improves over the model with frozen weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>for varying k when specializing to donuts. SA offers large gains by learning a class specific appearance model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Results for adapting to dist. shift (a,b) or a specific class (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Adaptation (SA) Image Adaptation (IA) (a) DRIONS-DB dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 5 :</head><label>5</label><figDesc>Results for domain change. For each dataset, we show the mean IoU at k corrections (left in 5a, 5b) and the number of clicks to reach the target IoU as a function of the number of images processed (right in 5a, 5b). Single image adaptation provides a consistent improvement over the test sequences. Instead, image sequence adaptation adapts its appearance model to the new domain gradually, improving with every image processed (right in 5a, 5b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Datasets. (1) Berkeley, introduced in Sec. 4.1 (2) GrabCut [49], 49 images with segmentation masks. (3) DAVIS16 [43]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Adapting to distribution shifts. Mean number of clicks required to attain a particular mIoU score on Berkeley, YouTube-VOS and COCO datasets (Lower is better). Both of our adaptive methods, single image adaptation (IA) and image sequence adaptation (SA) improve over the model that keeps the weights frozen at test time.</figDesc><table><row><cell></cell><cell>Berkeley</cell><cell>YouTube -VOS</cell><cell></cell><cell cols="2">COCO [36]</cell></row><row><cell></cell><cell>[40]</cell><cell>[57]</cell><cell>seen</cell><cell cols="2">unseen unseen 6k</cell></row><row><cell>Method</cell><cell>clicks@90%</cell><cell>clicks@85%</cell><cell></cell><cell>clicks@85%</cell><cell></cell></row><row><cell>Frozen model [38]</cell><cell>5.4</cell><cell>7.9</cell><cell>10.0</cell><cell>11.9</cell><cell>13.2</cell></row><row><cell>IA</cell><cell>4.9</cell><cell>7.0</cell><cell>9.1</cell><cell>10.7</cell><cell>10.6</cell></row><row><cell>SA</cell><cell>5.3</cell><cell>6.9</cell><cell>9.7</cell><cell>10.6</cell><cell>10.0</cell></row><row><cell>IA+SA</cell><cell>4.9</cell><cell>6.7</cell><cell>9.1</cell><cell>9.9</cell><cell>9.3</cell></row><row><cell>∆ over frozen model</cell><cell>8.5%</cell><cell>15.2%</cell><cell>9.0%</cell><cell>16.8%</cell><cell>29.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Class specialization. We test segmenting objects of only one specific class. Our adaptive methods outperforms the frozen model on all tested classes. Naturally, gains are larger for image sequence adaptation, as it can adapt to the class over time.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">clicks @ 85% IoU</cell><cell></cell></row><row><cell></cell><cell cols="3">Donut Bench Umbrella</cell><cell>Bed</cell></row><row><cell>Frozen model [38]</cell><cell>11.6</cell><cell>15.1</cell><cell>13.1</cell><cell>6.8</cell></row><row><cell>IA (Ours)</cell><cell>9.2</cell><cell>14.1</cell><cell>11.9</cell><cell>5.5</cell></row><row><cell>SA (Ours)</cell><cell>7.1</cell><cell>14.0</cell><cell>11.1</cell><cell>5.5</cell></row><row><cell>IA+SA (Ours)</cell><cell>6.5</cell><cell>13.3</cell><cell>10.2</cell><cell>5.0</cell></row><row><cell>∆ over frozen model</cell><cell>44.0%</cell><cell>11.9%</cell><cell>22.1%</cell><cell>26.5%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Domain change results. We evaluate our model on 2 datasets that belong to different domains: aerial (Rooftop) and medical (DRIONS-DB). Both types of adaptation (IA and SA) outperform the frozen model.</figDesc><table><row><cell></cell><cell>DRIONS-DB [14]</cell><cell>Rooftop [53]</cell></row><row><cell>Method</cell><cell>clicks@90% IoU</cell><cell>clicks@80% IoU</cell></row><row><cell>Frozen model [38]</cell><cell>13.3</cell><cell>8.9</cell></row><row><cell>IA (Ours)</cell><cell>11.4</cell><cell>6.3</cell></row><row><cell>SA (Ours)</cell><cell>3.6</cell><cell>3.6</cell></row><row><cell>IA+SA (Ours)</cell><cell>3.1</cell><cell>3.6</cell></row><row><cell>∆ over frozen model</cell><cell>76.7%</cell><cell>59.6%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">: The focus of our work is handling distribution shifts and domain changes</cell></row><row><cell cols="5">between training and testing (Tab. 1, 2 &amp; 3). For completeness, we also compare</cell></row><row><cell cols="5">our method against existing methods on standard datasets, where the distribution</cell></row><row><cell cols="5">mismatch between training and testing is small. At the time of initially releasing</cell></row><row><cell cols="5">our work [32], our method outperformed all previous state-of-the-art models on</cell></row><row><cell cols="5">all datasets. Later, F-BRS [52] (CVPR 2020) achieved even better results.</cell></row><row><cell></cell><cell cols="4">VOC12 [20] GrabCut [49] Berkeley [40] DAVIS [43]</cell></row><row><cell></cell><cell>validation</cell><cell></cell><cell></cell><cell>10% of frames</cell></row><row><cell>Method</cell><cell>clicks@85%</cell><cell>clicks@90%</cell><cell>clicks@90%</cell><cell>clicks@85%</cell></row><row><cell>iFCN w/ GraphCut [56]</cell><cell>6.88</cell><cell>6.04</cell><cell>8.65</cell><cell>-</cell></row><row><cell>RIS [35]</cell><cell>5.12</cell><cell>5.00</cell><cell>6.03</cell><cell>-</cell></row><row><cell>TSLFN [28]</cell><cell>4.58</cell><cell>3.76</cell><cell>6.49</cell><cell>-</cell></row><row><cell>VOS-Wild [10]</cell><cell>5.6</cell><cell>3.8</cell><cell>-</cell><cell>-</cell></row><row><cell>ITIS [38]</cell><cell>3.80</cell><cell>5.60</cell><cell>-</cell><cell>-</cell></row><row><cell>CAG [39]</cell><cell>3.62</cell><cell>3.58</cell><cell>5.60</cell><cell>-</cell></row><row><cell>Latent Diversity [33]</cell><cell>-</cell><cell>4.79</cell><cell>-</cell><cell>5.95</cell></row><row><cell>BRS [29]</cell><cell>-</cell><cell>3.60</cell><cell>5.08</cell><cell>5.58</cell></row><row><cell>F-BRS [52] (Concurrent Work)</cell><cell>-</cell><cell>2.72</cell><cell>4.57</cell><cell>5.04</cell></row><row><cell>IA+SA combined (Ours)</cell><cell>3.18</cell><cell>3.07</cell><cell>4.94</cell><cell>5.16</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>Image sequence adaptation only needs a single update step, done after an object is segmented (Sec. 3.2). Thus, the adaptation overhead is negligible here. For single image adaptation we used 10 update steps, for a total time of 1.6 s. We chose this number of steps based on hyperparameter search (see supp. material). In practice, fewer update steps can be used to increase speed, as they quickly show diminishing returns (Fig.</figDesc><table><row><cell>While our method updates the parameters at test time, it remains fast enough for interactive usage. For the model used throughout our paper a parameter update step takes 0.16 s (Nvidia V100 GPU, mixed-precision training, Berkeley dataset).</cell><cell>Relative improvement [%]</cell><cell>0 5 10 15</cell><cell>0</cell><cell>2</cell><cell>4 Number of iterations 6</cell><cell>8</cell><cell>10</cell></row><row><cell></cell><cell cols="7">Fig. 6: Iterations vs. relative</cell></row><row><cell></cell><cell cols="7">improvement over a frozen</cell></row><row><cell></cell><cell cols="7">model (mean over all datasets).</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. We thank Rodrigo Benenson, Jordi Pont-Tuset, Thomas Mensink and Bastian Leibe for their inputs on this work.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Single image adaptation algorithm</head><p>11:</p><p>x ← UpdateGuidance(x, c)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>for step ← 1..k do Update model parameters 13:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Number of corrections as proxy for segmentation time</head><p>As is common practice <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b28">29]</ref>, we rely on simulated user corrections to evaluate our method. The number of corrections required to reach a certain segmentation quality serves as a proxy for the total time a user requires to segment an object. When less corrections are needed, segmenting an object is generally faster. But the time for making a correction might vary, as it comprises user interaction time (the time it takes for a user to make a correction) and computation time. We now contrast these two factors for our method.  <ref type="figure">Fig. 7</ref>: Additional results. For each image we show results for our two adaptation methods (IA and SA) and the frozen model for the same number of user corrections (IoU@5 is given in parenthesis). When the frozen model is applied to classes that are unseen during training, it sometimes produces segmentation masks that span multiple objects (7a &amp; 7b) or do not respect object boundaries (7c). Single image adaptation (IA) handles such cases much better, by adapting the model parameters to that specific object and its background. This allows it to correctly segment objects even when the foreground and background have similar appearance (7b). Image sequence adaptation (SA) optimizes the model parameters for the test sequence. This allows it to produce good masks from very few clicks, and additional clicks are only required close to the object to refine the exact boundary (7a &amp; 7c).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Efficient Interactive Annotation of Segmentation Datasets with Polygon-RNN++</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Adobe: Select a subject with just one click</title>
		<ptr target="https://helpx.adobe.com/photoshop/how-to/select-subject-one-click.html" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Interactive full image segmentation by considering all regions jointly</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Alcorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Memory aware synapses: Learning what (not) to forget</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Babiloni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Task-free continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kelchtermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Geodesic matting: A framework for fast interactive image and video segmentation and matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">What&apos;s the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Il2m: Class incremental learning with dual memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Belouadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Popescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<title level="m">Interactive video object segmentation in the wild. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Large-scale interactive object segmentation with human annotators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Interactive graph cuts for optimal boundary and region segmentation of objects in N-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Jolly</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">One-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identification of the optic nerve head with genetic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Carmona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rincón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>García-Feijoó</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Martínez-De-La Casa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence in Medicine</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Annotating object instances with a Polygon-RNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Tap and shoot segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PAS-CAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Farquhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
		<title level="m">Towards robust evaluations of continual learning. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Geodesic star convexity for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Deep value networks learn to evaluate and iteratively refine structured outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Semantic contours from inverse detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A fully convolutional two-stream fusion network for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Soltoggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Interactive image segmentation via backpropagating refinement scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">D</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ramalho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grabska-Barwinska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. Nat. Acad. Sci</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kontogianni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12709v1</idno>
		<title level="m">Continuous adaptation for interactive object segmentation by learning from corrections</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Interactive image segmentation with latent diversity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Regional interactive image segmentation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Fast interactive object annotation with Curve-GCN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Iteratively trained interactive segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Content-aware multi-level guidance for interactive instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A comparative evaluation of interactive segmentation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Incremental learning techniques for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Michieli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zanuttigh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">We don&apos;t need no bounding-boxes: Training object class detectors using only human verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Geodesic graph cut for interactive image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">L</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Morse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Human-centric indoor scene synthesis using stochastic grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">icarl: Incremental classifier and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rebuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sperl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lampert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shankar</surname></persName>
		</author>
		<title level="m">Do CIFAR-10 classifiers generalize to CIFAR-10? arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">GrabCut -Interactive Foreground Extraction using Iterated Graph Cut</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Incremental learning of object detectors without catastrophic forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shmelkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">F-BRS: Rethinking Backpropagating Refinement for Interactive Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sofiiuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Konushin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Free-shape polygonal object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Christoudias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<title level="m">Test-time training for out-of-distribution generalization. arXiv</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Online adaptation of convolutional neural networks for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Deep interactive object selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark. arXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Continual learning through synaptic intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zenke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganguli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Scene parsing through ADE20K dataset</title>
		<imprint>
			<publisher>CVPR</publisher>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<title level="m">Semantic understanding of scenes through the ADE20K dataset. IJCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Fine-tuning convolutional neural networks for biomedical image analysis: actively and incrementally</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gurudu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gotway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR (2017) Frozen model (53.8%)</title>
		<imprint/>
	</monogr>
	<note>Image adaptation (91.3%) Sequence adaptation (88.3%)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
