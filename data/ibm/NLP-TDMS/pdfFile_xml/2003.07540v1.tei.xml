<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting the Sibling Head in Object Detector</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
							<email>1songguanglu@sensetime.com</email>
							<affiliation key="aff0">
								<orgName type="department">SenseTime X-Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff1">
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
								<address>
									<settlement>Hong Kong</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting the Sibling Head in Object Detector</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T10:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The "shared head for classification and localization" (sibling head), firstly denominated in Fast RCNN [9], has been leading the fashion of the object detection community in the past five years. This paper provides the observation that the spatial misalignment between the two object functions in the sibling head can considerably hurt the training process, but this misalignment can be resolved by a very simple operator called task-aware spatial disentanglement (TSD). Considering the classification and regression, TSD decouples them from the spatial dimension by generating two disentangled proposals for them, which are estimated by the shared proposal. This is inspired by the natural insight that for one instance, the features in some salient area may have rich information for classification while these around the boundary may be good at bounding box regression. Surprisingly, this simple design can boost all backbones and models on both MS COCO and Google OpenImage consistently by ∼3% mAP. Further, we propose a progressive constraint to enlarge the performance margin between the disentangled and the shared proposals, and gain ∼1% more mAP. We show the TSD breaks through the upper bound of nowadays single-model detector by a large margin (mAP 49.4 with ResNet-101, 51.2 with SENet154), and is the core model of our 1st place solution on the Google OpenImage Challenge 2019.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the breakthrough of object detection performance has been achieved by seminal R-CNN families <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b29">30]</ref> and powerful FPN <ref type="bibr" target="#b20">[21]</ref>, the subsequent performance enhancement of this task seems to be hindered by some concealed bottlenecks. Even the advanced algorithms bolstered by AutoML <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref> have been delved, the performance gain is still limited to an easily accessible improvement range. As the most obvious distinction from the Figure 1. Illustration of the task spatial misalignment. The first column is the sensitive location for classification and the second column is the sensitive location for localization. The third column is the 3D visualization of the sensitivity distribution. generic object classification task, the specialized sibling head for both classification and localization comes into focus and is widely used in most of advanced detectors including single stage family <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b11">12]</ref>, two-stage family <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b18">19]</ref> and anchor free family <ref type="bibr" target="#b16">[17]</ref>. Considering the two different tasks share almost the same parameters, a few works become conscious about the conflict between the two object functions in the sibling head and try to find a trade-off way.</p><p>IoU-Net <ref type="bibr" target="#b14">[15]</ref> is the first to reveal this problem. They find the feature which generates a good classification score always predicts a coarse bounding box. To handle this problem, they first introduce an extra head to predict the IoU as the localization confidence, and then aggregate the localization confidence and the classification confidence together to be the final classification score. This approach does reduce the misalignment problem but in a compromise mannerthe essential philosophy behind it is relatively raising the confidence score of a tight bounding box and reduce the score of a bad one. The misalignment still exists in each spatial point. Along with this direction, Double-Head R-CNN <ref type="bibr" target="#b34">[35]</ref> is proposed to disentangle the sibling head into two specific branches for classification and localization, respectively. Despite of elaborate design of each branch, it can be deemed to disentangle the information by adding a new branch, essentially reduce the shared parameters of the two tasks. Although the satisfactory performance can be obtained by this detection head disentanglement, conflict between the two tasks still remain since the features fed into the two branches are produced by ROI Pooling from the same proposal.</p><p>In this paper, we meticulously revisit the sibling head in the anchor-based object detector to seek the essence of the tasks misalignment. We explore the spatial sensitivity of classification and localization on the output feature maps of each layer in the feature pyramid of FPN. Based on the commonly used sibling head (a fully connected head 2-fc), we illustrate the spatial sensitive heatmap in <ref type="figure">Figure.</ref>1. The first column is the spatial sensitive heatmap for classification and the second column is for localization. The warmer the better for the color. We also show their 3D visualizations in the third column. It's obvious that for one instance, the features in some salient areas may have rich information for classification while these around the boundary may be good at bounding box regression. This essential tasks misalignment in spatial dimension greatly limits the performance gain whether evolving the backbone or enhancing the detection head. In other words, if a detector try to infer the classification score and regression result from a same spatial point/anchor, it will always get an imperfect tradeoff result. This significant observation motivates us to rethink the architecture of the sibling head. The optimal solution for the misalignment problem should be explored by the spatial disentanglement. Based on this, we propose a novel operator called task-aware spatial disentanglement (TSD) to resolve this barrier. The goal of TSD is to spatially disentangle the gradient flows of classification and localization. To achieve this, TSD generates two disentangled proposals for these two tasks, based on the original proposal in classical sibling head. It allows two tasks to adaptively seek the optimal location in space without compromising each other. With the simple design, the performance of all backbones and models on both MS COCO and Google OpenImage are boosted by ∼3% mAP. Furthermore, we propose a progressive constraint (PC) to enlarge the performance margin between TSD and the classical sibling head. It introduces the hyper-parameter margin to advocate the more confident classification and precise regression. ∼1% more mAP is gained on the basis of TSD. Whether for variant backbones or different detection frameworks, the integrated algorithms can steadily improve the performance by ∼4% and even ∼6% for lightweight MobileNetV2. Behind the outstanding performance gains, only a slight increased parameter is required, which is negligible for some heavy backbones.</p><p>To summarize, the contributions of this paper are as follows:</p><p>1) We delve into the essential barriers behind the tangled tasks in RoI-based detectors and reveal the bottlenecks that limit the upper bound of detection performance.</p><p>2) We propose a simple operator called task-aware spatial disentanglement (TSD) to deal with the tangled tasks conflict. Through the task-aware proposal estimation and the detection head, it could generate the task-specific feature representation to eliminate the compromises between classification and localization.</p><p>3) We further propose a progressive constraint (PC) to enlarge the performance margin between TSD and the classical sibling head. 4) We validate the effectiveness of our approach on the standard COCO benchmark and large-scale OpenImage dataset with thorough ablation studies. Compared with the state-of-the-art methods, our proposed method achieves the mAP of 49.4 using a single model with ResNet-101 backbone and mAP of 51.2 with heavy SENet154.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Methods</head><p>In this section, we first describe the overall framework of our proposed task-aware spatial disentanglement (TSD), then detail the sub-modules in Sec. 2.2 and 2.3. Finally, we delve into the inherent problem in sibling head and demonstrate the advantage of TSD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">TSD</head><p>As shown in <ref type="figure">Figure.</ref>2 (a), denote a rectangular bounding box proposal as P and the ground-truth bounding box as B with class y, the classical Faster RCNN <ref type="bibr" target="#b29">[30]</ref> aims to minimize the classification loss and localization loss based on the shared P :</p><formula xml:id="formula_0">L = L cls (H 1 (F l , P ), y) + L loc (H 2 (F l , P ), B) (1)</formula><p>where H 1 (·) = {f (·), C(·)} and H 2 (·) = {f (·), R(·)}. f (·) is the feature extractor and C(·) and R(·) are the functions for transforming feature to predict specific category and localize object. Seminal work <ref type="bibr" target="#b34">[35]</ref> thinks the shared f for classification and localization is not optimal, and they disentangle it to f c and f r for classification and regression, respectively. Although the appropriate headdecoupling brings a reasonable improvement, the inherent conflict caused by the tangled tasks in the spatial dimension is still lurking.</p><p>For this potential problem, our goal is to alleviate the inherent conflict in sibling head by disentangling the tasks from the spatial dimension. We propose a novel TSD head for this goal as shown in <ref type="figure">Figure 2</ref>. In TSD, the Eq. 1 can be written as:  <ref type="figure">Figure 2</ref>. Illustration of the proposed TSD cooperated with Faster RCNN <ref type="bibr" target="#b29">[30]</ref>. Input images are first fed into the FPN backbone and then, region proposal P is generated by RPN. TSD adopts the RoI feature of P as input and estimates the derived proposalsPc andPr for classification and localization. Finally, two parallel branches are used to predict specific category and regress precise box, respectively.</p><formula xml:id="formula_1">L = L D cls (H D 1 (F l ,P c ), y) + L D loc (H D 2 (F l ,P r ), B) (2) Input image Backbone RPN Sibling head (•) (•) ℛ(•)</formula><p>where disentangled proposalsP c = τ c (P, ∆C) andP r = τ r (P, ∆R) are estimated from the shared P . ∆C is a pointwise deformation of P and ∆R is a proposal-wise translation. In TSD,</p><formula xml:id="formula_2">H D 1 (·) = {f c (·), C(·)} and H D 2 (·) = {f r (·), R(·)}.</formula><p>In particular, TSD tasks the RoI feature of P as input, and then generates the disentangled proposalsP c andP r for classification and localization, respectively. Different tasks can be disentangled from the spatial dimension via the separated proposals. The classification-specific feature mapsF c and localization-specific feature mapsF r can be generated through parallel branches. In the first branch,F c is fed into a three-layer fully connected networks for classification. In the second branch, the RoI featureF r corresponding to derived proposalP r will be extracted and fed into a similar architecture with the first branch to perform localization task. By disentangling the shared proposal for the classification and localization, TSD can learn the task-aware feature representation adaptively. TSD is applicable to most existing RoI-based detectors. As the training procedure adopts an end-to-end manner cooperated with the well-designed progressive constraint (PC), it is robust to the change of backbones and input distributions (e.g., training with different datasets.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Task-aware spatial disentanglement learning</head><p>Inspired by <ref type="figure">Figure.</ref>1, we introduce the task-aware spatial disentanglement learning to alleviate the misalignment caused by the shared spatial clues. As shown in Figure.2 (b), define the RoI feature of P as F , we embed the deformation-learning manner into TSD to achieve this goal. For localization, a three-layer fully connected network F r is designed to generate a proposal-wise translation on P to produce a new derived proposalP r . This procedure can be formulated as:</p><formula xml:id="formula_3">∆R = γF r (F ; θ r ) · (w, h)<label>(3)</label></formula><p>where ∆R ∈ R 1×1×2 and the output of F r for each layer is {256, 256, 2}. γ is a pre-defined scalar to modulate the magnitude of the ∆R and (w, h) is the width and height of P . The derived function τ r (·) for generatingP r is:</p><formula xml:id="formula_4">P r = P + ∆R<label>(4)</label></formula><p>Eq. 4 indicates the proposal-wise translation where the coordinate of each pixel in P will be translated to a new coordinate with the same ∆R. The derived proposalP r only focuses on the localization task and in the pooling function, we adopt the bilinear interpolation the same as <ref type="bibr" target="#b4">[5]</ref> to make ∆R differentiable. For classification, given the shared P , a pointwise deformation on a regular grid k × k is generated to estimate a derived proposalP c with an irregular shape. For (x,y)-th grid, the translation ∆C(x, y, * ) is performed on the sample points in it to obtain the new sample points forP c . This procedure can be formulated as:</p><formula xml:id="formula_5">∆C = γF c (F ; θ c ) · (w, h)<label>(5)</label></formula><p>where ∆C ∈ R k×k×2 . F c is a three-layer fully connected network with output {256, 256, k × k × 2} for each layer and θ c is the learned parameter. The first layer in F r and F c is shared to reduce the parameter. For generating feature mapF c by irregularP c , we adopt the same operation with deformable RoI pooling <ref type="bibr" target="#b4">[5]</ref>:</p><formula xml:id="formula_6">F c (x, y) = p∈G(x,y) F B (p 0 + ∆C(x, y, 1), p 1 + ∆C(x, y, 2)) |G(x, y)|<label>(6)</label></formula><p>where G(x, y) is the (x,y)-th grid and |G(x, y)| is the number of sample points in it. (p x , p y ) is the coordinate of the sample point in grid G(x, y) and F B (·) is the bilinear interpolation <ref type="bibr" target="#b4">[5]</ref> to make the ∆C differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Progressive constraint</head><p>At the training stage, the TSD and the sibling detection head defined in Eq. 1 can be jointly optimized by L cls and L loc . Beyond this, we further design the progressive constraint (PC) to improve the performance of TSD as shown in <ref type="figure">Figure.</ref>2 (c). For classification branch, PC is formulated as:</p><formula xml:id="formula_7">M cls = |H 1 (y|F l , P )−H D 1 (y|F l , τ c (P, ∆C))+m c | + (7)</formula><p>where H(y|·) indicates the confidence score of the y-th class and m c is the predefined margin. | · | + is same as ReLU function. Similarly, for localization, there are:</p><formula xml:id="formula_8">M loc = |IoU (B, B) − IoU (B D , B) + m r | +<label>(8)</label></formula><p>whereB is the predicted box by sibling head andB D is regressed by H D 2 (F l , τ r (P, ∆R)). If P is a negative proposal, M loc is ignored. According to these designs, the whole loss function of TSD with Faster RCNN can be define as:</p><formula xml:id="formula_9">L = L rpn +L cls +L loc classical loss + L D cls +L D loc +M cls +M loc T SD loss<label>(9)</label></formula><p>We directly set the loss weight to 1 without carefully adjusting it. Under the optimization of L, TSD can adaptively learn the task-specific feature representation for classification and localization, respectively. Extensive experiments in Sec.3 indicates that disentangling the tangled tasks from the spatial dimension can significantly improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Discussion in context of related works</head><p>In this section, we delve into the inherent conflict in tangled tasks. Our work is related to previous works in different aspects. We discuss the relations and differences in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.1">Conflict in sibling head with tangled tasks</head><p>Two core designs in classical Faster RCNN are predicting the category for a given proposal and learning a regression function. Due to the essential differences in optimization, classification task requires translation-agnostic property and to the contrary, localization task desires translationaware property. The specific translation sensitivity property for classification and localization can be formulated as:</p><formula xml:id="formula_10">C(f (F l , P )) = C(f (F l , P + ε)), R(f (F l , P )) = R(f (F l , P + ε))<label>(10)</label></formula><p>where ∀ε, IoU (P + ε, B) ≥ T . C is to predict category probability and R is the regression function whose output is (∆x, ∆ŷ, ∆ŵ, ∆ĥ). f (·) is the shared feature extractor in classical sibling head and T is the threshold to determine whether P is a positive sample. There are entirely different properties in these two tasks. The shared spatial clues in F l and feature extractor for these two tasks will become the obstacles to hinder the learning. Different from <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b42">43]</ref> where the evolved backbone or feature extractor is designed, TSD decouples the classification and regression from spatial dimension by separatedP * and f * (·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.2">Different from other methods</head><p>IoU-Net <ref type="bibr" target="#b14">[15]</ref> first illustrates the misalignment between classification and regression. To alleviate this, it directly predicts the IoU to adjust the classification confidence via an extra branch. Unfortunately, this approach does not solve the inherent conflict between tangled tasks. For this same problem, Double-Head R-CNN <ref type="bibr" target="#b34">[35]</ref> explores the optimal architectures for classification and localization, respectively. To learn more effective feature representation, DCN <ref type="bibr" target="#b4">[5]</ref> with deformable RoI pooling is proposed to extract the semantic information from the irregular region. Whether evolving the backbone or adjusting the detection head, performance can be improved, but the increase is limited.</p><p>In this paper, we observe that the essential problem behind the limited performance is the misaligned sensitivity in the spatial dimension between classification and localization. Neither designing better feature extraction methods nor searching for the best architecture can solve this problem. In this dilemma, TSD is proposed to decouple the classification and localization from both the spatial dimension and feature extractor. TSD first performs spatial disentanglement for classification and localization via separated proposals and feature extractors to break the predicament. With the further well-designed PC, it can learn the optimal sensitive location for classification and localization, respectively. Moreover, TSD is still applicable to DCN <ref type="bibr" target="#b4">[5]</ref> although deformable RoI pooling in DCN is used to assist in estimatingF c . By task-aware spatial disentanglement, the simple TSD can easily achieve excellent performance for different backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>We perform extensive experiments with variant backbones on the 80-category MS-COCO dataset <ref type="bibr" target="#b22">[23]</ref> (object detection and instance segmentation) and 500-category OpenImageV5 challenge dataset <ref type="bibr" target="#b15">[16]</ref>. For COCO dataset, following the standard protocol <ref type="bibr" target="#b26">[27]</ref>, training is performed on the union of 80k train images and 35k subset of val images and testing is evaluated on the remaining 5k val images (minival). We also report results on 20k test-dev. For OpenImage dataset, following the official protocol <ref type="bibr" target="#b15">[16]</ref>, the model is trained on 1,674,979 training images and evaluated  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Implementation details</head><p>We initialize weights from pre-trained models on Ima-geNet <ref type="bibr" target="#b30">[31]</ref> and the configuration of hyper-parameters follows existing Faster RCNN <ref type="bibr" target="#b29">[30]</ref>. Images are resized such that the shorter edge is 800 pixels. The anchor scale and aspect ratio are set to 8 and {0.5, 1, 2}. We train the models on 16 GPUs (effective mini-batch size is 32) for 13 epochs, with a learning rate warmup strategy <ref type="bibr" target="#b10">[11]</ref> from 0.00125 to 0.04 in the first epoch. We decrease the learning rate by 10 at epoch 8 and epoch 11, respectively. RoIAlign <ref type="bibr" target="#b12">[13]</ref> is adopted in all experiments, and the pooling size is 7 in both H * 1 and H * 2 . We use SGD to optimize the training loss with 0.9 momentum and 0.0001 weight decay. No data augmentations except standard horizontal flipping are used. Synchronized BatchNorm mechanism <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b10">11]</ref> is used to make multi-GPU training more stable. At the inference stage, NMS with 0.5 IoU threshold is applied to remove duplicate boxes. For experiments in the OpenImage dataset, classaware sampling is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Ablation studies</head><p>In this section, we conduct detailed ablation studies on COCO minival to evaluate the effectiveness of each module and illustrate the advance and generalization of the proposed TSD. m c and m r are set to 0.2 in these experiments.</p><p>Task-aware disentanglement. When it comes to tangled tasks conflict in sibling detection head, it's natural to think about decoupling different tasks from the backbone or detection head. To evaluate these ideas, we conduct several experiments to illustrate the comparison between them. As shown in <ref type="figure">Figure.</ref>3, we design different decoupling options including backbone disentanglement and head disentanglement. Detailed performance is shown in Joint training with sibling head H * . In TSD, the shared proposal P can also be used to perform classification and localization in an extra sibling head. We empirically observe that the training of sibling head is complementary to the training of TSD, and the results are demonstrated in <ref type="table">Table.</ref>2. This indicates that the derived proposalsP c andP r are not conflict with the original proposal P . At the inference stage, only the TSD head is retained. Effectiveness of PC. In Sec. 2.3, we further propose the PC to enhance the performance of TSD. <ref type="table">Table.</ref>3 reports the detailed ablations on it. We find that PC significantly improves the AP .75 by 1.5 and AP .5 is barely affected. This demonstrates that PC aims to advocate more confidential classification and precise regression for the accurate boxes. Even on the strict testing standards AP (IoU from 0.5:0.95), Derived proposal learning manner for H D * . There are different programmable strategies to generate the derived proposalP r andP c including proposal-wise translation (Prop.w) in Eq. 4, pointwise deformation (Point.w) such as deformable RoI pooling <ref type="bibr" target="#b4">[5]</ref> or the tricky combination of them. To explore the differences of these learning manners, we conduct extensive experiments for COCO minival with ResNet-50. <ref type="table">Table.</ref>4 demonstrates the comparison results. These comparisons illustrate that Point.w is beneficial to the classification task and cooperated with PC, Prop.w performs a slight advantage on localization. For generating the derived proposals, classification requires the optimal local features without regular shape restrictions and regression requires the maintenance of global geometric shape information.  Delving to the effective PC. PC demonstrates its superiority on regressing more precise bounding boxes. The hyper-parameters m c and m r play important roles in the training of TSD and to better understand their effects on performance, we conduct detailed ablation studies on them. <ref type="figure" target="#fig_3">Figure.4</ref> reports the results and note that both of the M los and M cls can further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Applicable to variant backbones</head><p>Since the TSD and PC have demonstrated their outstanding performance on ResNet-50 with FPN, we further delve </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Applicable to Mask R-CNN</head><p>The proposed algorithms largely surpass the classical sibling head in Faster R-CNN. Its inherent properties determine its applicability to other R-CNN families such as Mask R-CNN for instance segmentation. To validate this, we conduct experiments with Mask R-CNN <ref type="bibr" target="#b12">[13]</ref>. Performances are shown in  <ref type="bibr" target="#b6">[7]</ref>. † indicates the result on COCO minival set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Generalization on large-scale OpenImage</head><p>In addition to evaluate on the COCO dataset, we further corroborate the proposed method on the large-scale Open-Image dataset. As the public dataset with large-scale boxes and hierarchy property, it brings a new challenge to the generalization of detection algorithms. To fully delve the effectiveness of the proposed algorithm, we run a number of ablations to analyze TSD. Table.6 illustrates the comparison and note that, even for heavy backbone, TSD can still give satisfactory improvements. Furthermore, TSD is complementary to Cascade R-CNN <ref type="bibr" target="#b1">[2]</ref> and embedding it into this framework can also enhance the performance by a satisfactory margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Comparison with state-of-the-Arts</head><p>In this section, we evaluate our proposed method on COCO test-dev set and compare it with other state-of-the-art methods. m c and m r are set to 0.5 and 0.2, respectively. For a fair comparison, we report the results of our methods under different settings in <ref type="table">Table.</ref>8. For comparison with Grid R-CNN <ref type="bibr" target="#b26">[27]</ref>, we extend the training epochs for ResNet-101 to be consistent with it. For comparing with the best single-model TridentNet * , in TSD * , we apply the same configuration with it including multi-scale training, soft-NMS <ref type="bibr" target="#b0">[1]</ref>, deformable convolutions and the 3× training scheme on ResNet-101. The best single-model ResNet-101-DCN gives an AP of 49.4, already surpassing all of the other methods with the same backbone. To our best knowledge, for a single model with ResNet-101 backbone, our result is the best entry among the state-of-the-arts. TSD demonstrates its advantage on promoting precise localization and confidential classification, especially on higher IoU thresholds (AP .75 ). Furthermore, we explore the upperbound of TSD with a heavy backbone. Surprisingly, it can </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Analysis and discussion</head><p>Performance in different IoU criteria. Since TSD exhibits superior ability on regressing precise localization and predicting confidential category, we conduct several evaluations with more strict IoU criteria on COCO minival. <ref type="figure">Figure</ref>.6 illustrates the comparison between TSD based Faster R-CNN and baseline Faster R-CNN with the same ResNet-50 backbone across IoU thresholds from 0.5 to 0.9. Obviously, with the increasing IoU threshold, the improvement brought by TSD is also increasing.</p><p>Performance in different scale criteria. We have analyzed the effectiveness of TSD under different IoU criteria. To better explore the specific improvement, we further test the mAP under objects with different scales. <ref type="table">Table.9</ref> reports the performance and TSD shows successes in objects with variant scales, especially for medium and large objects.</p><p>What did TSD learn? Thanks to the task-aware spatial disentanglement (TSD) and the progressive constraint (PC), stable improvements can be easily achieved whether for variant backbones or variant datasets. Beyond the quantitative promotion, we wonder what TSD learned compared with the sibling head in Faster R-CNN. To better interpret  <ref type="table">Table 9</ref>. mAP across scale criteria from 0.5 to 0.9 with 0.1 interval.</p><p>this, We showcase the illustrations of our TSD compared with sibling head as shown in <ref type="figure">Figure.</ref> 5. As expected, through TSD, it can depose many false positives and regress the more precise box boundary. ForP r , it tends to translate to the boundary that is not easily regressed. ForP c , it tends to concentrate on the local appearance and object context information as it did in sibling head with deformable RoI pooling <ref type="bibr" target="#b4">[5]</ref>. Note that the tangled tasks in sibling head can be effectively separated from the spatial dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>In this paper, we present a simple operator TSD to alleviate the inherent conflict in sibling head, which learns the task-aware spatial disentanglement to bread through the performance limitation. In particular, TSD derives two disentangled proposals from the shared proposal and learn the specific feature representation for classification and localization, respectively. Further, we propose a progressive constraint to enlarge the performance margin between the disentangled and the shared proposals, which provides additional performance gain. Without bells and whistles, this simple design can easily boost most of the backbones and models on both COCO and large scale OpenImage consistently by 3%∼5%, and is the core model in our 1st solution of OpenImage Challenge 2019.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Ablation studies on variant disentanglement options. (a)-(d) indicate disentangling the detector from stride 8, stride 16, stride 32 and sibling head, respectively. on the 34,917 val images. The AP .5 on public leaderboard is also reported.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Results of TSD with variant m * for PC. These experiments are conducted based on ResNet-50 with FPN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Visualization of the learntPr andPc on examples from the COCO minival set. The first row indicates the proposal P (yellow box) and the derivedPr (red box) andPc (pink point, center point in each grid). The second row is the final detected boxes where the white box is ground-truth. TSD deposes the false positives in the first two columns and in other columns, it regresses more precise boxes. mAP across IoU criteria from 0.5 to 0.9 with 0.1 interval. achieve the AP of 51.2 with the single-model SENet154-DCN on COCO test-dev set. Soft-NMS is not used in this evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2</head><label>2</label><figDesc>Table.1. Decoupling the classification and localization from the backbone largely degrades the performance. It clearly shows that the Disentanglement #param AP AP .5 AP .75</figDesc><table><row><cell>ResNet-50</cell><cell cols="2">41.8M 36.1 58.0</cell><cell>38.8</cell></row><row><cell>ResNet-50+D s8</cell><cell cols="2">81.1M 22.3 46.3</cell><cell>16.7</cell></row><row><cell>ResNet-50+ D s16</cell><cell cols="2">74.0M 22.0 46.2</cell><cell>16.3</cell></row><row><cell>ResNet-50+ D s32</cell><cell>59M</cell><cell>20.3 44.7</cell><cell>13.2</cell></row><row><cell cols="3">ResNet-50+ D head 55.7M 37.3 59.4</cell><cell>40.2</cell></row><row><cell>TSD w/o PC</cell><cell cols="2">58.9M 38.2 60.5</cell><cell>41.1</cell></row><row><cell cols="4">Table 1. Detailed performance and #parameter of different disen-</cell></row><row><cell>tanglement methods.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">semantic information in the backbone should be shared by</cell></row><row><cell cols="4">different tasks. As expected, the task-specific head can sig-</cell></row><row><cell cols="4">nificantly improve the performance. Compared with D head ,</cell></row><row><cell cols="4">TSD w/o PC can further enhance the AP with the slight in-</cell></row><row><cell cols="4">creased parameters, even for the demanding AP .75 . When</cell></row><row><cell cols="4">faced with heavy backbones, a slight increased parameter is</cell></row><row><cell cols="4">trivial but can still significantly improve the performance.</cell></row><row><cell cols="4">This also substantiates the discussion in Sec. 2.4.1 that dis-</cell></row><row><cell cols="4">entangling the tasks from spatial dimension can effectively</cell></row><row><cell cols="4">alleviate the inherent conflict in sibling detection head.</cell></row><row><cell>Method</cell><cell></cell><cell cols="2">AP AP .5 AP .75</cell></row><row><cell>TSD w/o PC</cell><cell></cell><cell>38.2 60.5</cell><cell>41.1</cell></row><row><cell cols="3">+ Joint training with sibling head H  *  39.7 61.7</cell><cell>42.8</cell></row></table><note>. Result of joint training with sibling H * . The ResNet-50 with FPN is used as the basic detector.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Ablation studies on PC. All of the experiments is joint training with sibling head H * . mc and mr are set to 0.2.</figDesc><table><row><cell>Method</cell><cell>TSD</cell><cell>PC M cls M loc</cell><cell cols="2">AP AP .5 AP .75</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell>39.7 61.7</cell><cell>42.8</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell>40.1 61.7</cell><cell>43.2</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell>40.8 61.7</cell><cell>43.8</cell></row><row><cell>ResNet-50</cell><cell></cell><cell></cell><cell>41.0 61.7</cell><cell>44.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>1.3 AP gain can also be obtained. PCP cPr AP AP .5 AP .75 Results of different proposal learning manners for H D</figDesc><table><row><cell>Method TSD</cell><cell>Point.w</cell><cell>-</cell><cell cols="2">38.0 60.3 40.89</cell></row><row><cell>TSD</cell><cell cols="3">Point.w Point.w 38.5 60.7</cell><cell>41.7</cell></row><row><cell>TSD</cell><cell cols="3">Point.w Prop.w 38.2 60.5</cell><cell>41.1</cell></row><row><cell>TSD</cell><cell cols="3">Prop.w Prop.w 39.8 60.1</cell><cell>42.9</cell></row><row><cell>TSD</cell><cell cols="3">Point.w Point.w 40.7 61.8</cell><cell>44.4</cell></row><row><cell>TSD</cell><cell cols="3">Point.w Prop.w 41.0 61.7</cell><cell>44.3</cell></row></table><note>* .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Table.5 summarizes the detailed performance on COCO minival. TSD can steadily improve the performance by 3%∼5% with additional ∼10% time cost. Note that ResNet-50+TSD with 58.9M parameter can even outperform the ResNet-152 with 76.39M parameter. Based on the ResNet family, TSD is a more preferred choice than increasing backbone to improve performance. If not specified, all subsequent TSD indicates TSD+PC.</figDesc><table><row><cell>Method</cell><cell cols="3">Ours AP AP .5 AP .75</cell><cell>runtime</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>36.1 58.0</cell><cell>38.8</cell><cell>159.4 ms</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>41.0 61.7</cell><cell>44.3</cell><cell>174.9 ms</cell></row><row><cell>ResNet-101</cell><cell></cell><cell>38.6 60.6</cell><cell>41.8</cell><cell>172.4ms</cell></row><row><cell>ResNet-101</cell><cell></cell><cell>42.4 63.1</cell><cell>46.0</cell><cell>189.0ms</cell></row><row><cell>ResNet-101-DCN</cell><cell></cell><cell>40.8 63.2</cell><cell>44.6</cell><cell>179.3ms</cell></row><row><cell>ResNet-101-DCN</cell><cell></cell><cell>43.5 64.4</cell><cell>47.0</cell><cell>200.8ms</cell></row><row><cell>ResNet-152</cell><cell></cell><cell>40.7 62.6</cell><cell>44.6</cell><cell>191.3ms</cell></row><row><cell>ResNet-152</cell><cell></cell><cell>43.9 64.5</cell><cell>47.7</cell><cell>213.2ms</cell></row><row><cell>ResNeXt-101 [36]</cell><cell></cell><cell>40.5 62.6</cell><cell>44.2</cell><cell>187.5ms</cell></row><row><cell>ResNeXt-101 [36]</cell><cell></cell><cell>43.5 64.5</cell><cell>46.9</cell><cell>206.6ms</cell></row><row><cell cols="5">Table 5. Results of TSD + PC with variant backbones. DCN means</cell></row><row><cell cols="5">deformable convolution. The runtime includes network forward</cell></row><row><cell cols="5">and post-processing (e.g., NMS for object detection). The runtime</cell></row><row><cell cols="5">is the averaged value on a single Tesla V100 GPU and CPU E5-</cell></row><row><cell>2680 v4.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">into the adaptation on variant backbones. Based on Faster</cell></row><row><cell cols="5">R-CNN, we directly conduct several experiments with dif-</cell></row><row><cell cols="2">ferent backbones and Method</cell><cell cols="3">TSD AP .5 (Val) AP .5 (LB)</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>64.64</cell><cell></cell><cell>49.79</cell></row><row><cell>ResNet-50</cell><cell></cell><cell>68.18</cell><cell></cell><cell>52.55</cell></row><row><cell cols="2">Cascade-DCN-SENet154</cell><cell>69.27</cell><cell></cell><cell>55.979</cell></row><row><cell cols="2">Cascade-DCN-SENet154</cell><cell>71.17</cell><cell></cell><cell>58.34</cell></row><row><cell cols="2">DCN-ResNeXt101  *</cell><cell>68.70</cell><cell></cell><cell>55.05</cell></row><row><cell cols="2">DCN-ResNeXt101  *</cell><cell>71.71</cell><cell></cell><cell>58.59</cell></row><row><cell cols="2">DCN-SENet154  *</cell><cell>70</cell><cell></cell><cell>57.771</cell></row><row><cell cols="2">DCN-SENet154  *</cell><cell>72.19</cell><cell></cell><cell>60.5</cell></row><row><cell cols="5">Table 6. Results of TSD on OpenImage dataset. * indicates we</cell></row><row><cell cols="5">expand the anchor scale to {8, 11, 14} and anchor aspect ratio</cell></row><row><cell cols="5">to {0.1, 0.5, 1, 2, 4, 8}. Furthermore, mult-scale test is used for</cell></row><row><cell cols="3">public leaderboard (LB) except for ResNet-50.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7 .Table 8 .</head><label>78</label><figDesc>Table.7 and the training configuration in Mask R-CNN is the same as the experiments in Faster R-CNN. It's obvious that TSD is still capable of detection branch in Mask R-CNN. The instance segmentation mask AP can also obtain promotion. Results of Mask R-CNN with TSD. The proposed methods are only applied on the detection branch in Mask R-CNN. AP bb means the detection performance and AP mask indicates the segmentation performance. Method backbone b&amp;w AP AP .5 AP .75 AP s AP m AP l Comparisons of single-model results for different algorithms evaluated on the COCO test-dev set. b&amp;w indicates training with bells and whistles such as multi-scale train/test, Cascade R-CNN or DropBlock</figDesc><table><row><cell>Method</cell><cell></cell><cell>Ours AP bb AP bb .5</cell><cell>AP bb .75</cell><cell cols="3">AP mask AP mask .5</cell><cell>AP mask .75</cell></row><row><cell cols="2">ResNet-50 w. FPN</cell><cell>37.2 58.8</cell><cell>40.2</cell><cell>33.6</cell><cell></cell><cell>55.3</cell><cell>35.4</cell></row><row><cell cols="2">ResNet-50 w. FPN</cell><cell>41.5 62.1</cell><cell>44.8</cell><cell>35.8</cell><cell></cell><cell>58.3</cell><cell>37.7</cell></row><row><cell cols="2">ResNet-101 w. FPN</cell><cell>39.5 61.2</cell><cell>43.0</cell><cell>35.7</cell><cell></cell><cell>57.9</cell><cell>38.0</cell></row><row><cell cols="2">ResNet-101 w. FPN</cell><cell>43.0 63.6</cell><cell>46.8</cell><cell>37.2</cell><cell></cell><cell>59.9</cell><cell>39.5</cell></row><row><cell>RefineDet512 [41]</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell cols="2">36.4 57.5</cell><cell>39.5</cell><cell cols="2">16.6 39.9 51.4</cell></row><row><cell>RetinaNet800 [22]</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell cols="2">39.1 59.1</cell><cell>42.3</cell><cell cols="2">21.8 42.7 50.2</cell></row><row><cell>CornerNet [17]</cell><cell></cell><cell>Hourglass-104 [28]</cell><cell></cell><cell cols="2">40.5 56.5</cell><cell>43.1</cell><cell cols="2">19.4 42.7 53.9</cell></row><row><cell>ExtremeNet [42]</cell><cell></cell><cell>Hourglass-104 [28]</cell><cell></cell><cell cols="2">40.1 55.3</cell><cell>43.2</cell><cell cols="2">20.3 43.2 53.1</cell></row><row><cell>FCOS [34]</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell cols="2">41.5 60.7</cell><cell>45.0</cell><cell cols="2">24.4 44.8 51.6</cell></row><row><cell>RPDet [39]</cell><cell></cell><cell>ResNet-101-DCN</cell><cell></cell><cell cols="2">46.5 67.4</cell><cell>50.9</cell><cell cols="2">30.3 49.7 57.1</cell></row><row><cell>CenterNet511 [6]</cell><cell></cell><cell>Hourglass-104</cell><cell></cell><cell cols="2">47.0 64.5</cell><cell>50.7</cell><cell cols="2">28.9 49.9 58.9</cell></row><row><cell>TridentNet [20]</cell><cell></cell><cell>ResNet-101-DCN</cell><cell></cell><cell cols="2">48.4 69.7</cell><cell>53.5</cell><cell cols="2">31.8 51.3 60.3</cell></row><row><cell>NAS-FPN [8]</cell><cell cols="2">AmoebaNet (7 @ 384)</cell><cell></cell><cell>48.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Faster R-CNN w FPN [21]</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell cols="2">36.2 59.1</cell><cell>39.0</cell><cell cols="2">18.2 39.0 48.2</cell></row><row><cell>Auto-FPN  † [38]</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell>42.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Regionlets [37]</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell cols="2">39.3 59.8</cell><cell>-</cell><cell cols="2">21.7 43.7 50.9</cell></row><row><cell>Grid R-CNN [27]</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell cols="2">41.5 60.9</cell><cell>44.5</cell><cell cols="2">23.3 44.9 54.1</cell></row><row><cell>Cascade R-CNN [2]</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell cols="2">42.8 62.1</cell><cell>46.3</cell><cell cols="2">23.7 45.5 55.2</cell></row><row><cell>DCR [4]</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell cols="2">40.7 64.4</cell><cell>44.6</cell><cell cols="2">24.3 43.7 51.9</cell></row><row><cell>IoU-Net  † [15]</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell cols="2">40.6 59.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Double-Head-Ext  † [35]</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell cols="2">41.9 62.4</cell><cell>45.9</cell><cell cols="2">23.9 45.2 55.8</cell></row><row><cell>SNIPER [32]</cell><cell></cell><cell>ResNet-101-DCN</cell><cell></cell><cell cols="2">46.1 67.0</cell><cell>51.6</cell><cell cols="2">29.6 48.9 58.1</cell></row><row><cell>DCNV2 [43]</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell cols="2">46.0 67.9</cell><cell>50.8</cell><cell cols="2">27.8 49.1 59.5</cell></row><row><cell>PANet [24]</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell cols="2">47.4 67.2</cell><cell>51.8</cell><cell cols="2">30.1 51.7 60.0</cell></row><row><cell>GCNet [3]</cell><cell></cell><cell>ResNet-101-DCN</cell><cell></cell><cell cols="2">48.4 67.6</cell><cell>52.7</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TSD  †</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell cols="2">43.1 63.6</cell><cell>46.7</cell><cell cols="2">24.9 46.8 57.5</cell></row><row><cell>TSD</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell cols="2">43.2 64.0</cell><cell>46.9</cell><cell cols="2">24.0 46.3 55.8</cell></row><row><cell>TSD  *</cell><cell></cell><cell>ResNet-101-DCN</cell><cell></cell><cell cols="2">49.4 69.6</cell><cell>54.4</cell><cell cols="2">32.7 52.5 61.0</cell></row><row><cell>TSD  *</cell><cell></cell><cell>SENet154-DCN [14]</cell><cell></cell><cell cols="2">51.2 71.9</cell><cell>56.0</cell><cell cols="2">33.8 54.8 64.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>CriteriaTSD AP .5 AP .6 AP .7 AP .8 AP .9</figDesc><table><row><cell>AP small</cell><cell>38.4 33.7 26.7 16.2</cell><cell>3.6</cell></row><row><cell>AP small</cell><cell>40.0 35.6 28.8 17.7</cell><cell>5.3</cell></row><row><cell>AP medium</cell><cell>62.9 58.4 49.7 33.6</cell><cell>8.7</cell></row><row><cell>AP medium</cell><cell cols="2">67.7 62.4 54.9 40.2 15.4</cell></row><row><cell>AP large</cell><cell cols="2">69.5 65.5 56.8 43.2 14.8</cell></row><row><cell>AP large</cell><cell cols="2">74.8 71.6 65.0 53.2 27.9</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Soft-nms-improving object detection with one line of code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5561" to="5569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gcnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11492</idno>
		<title level="m">Non-local networks meet squeeze-excitation networks and beyond</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Revisiting rcnn: On awakening the classification power of faster rcnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honghui</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10727" to="10737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Nas-fpn: Learning scalable feature pyramid architecture for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Golnaz</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Region-based convolutional networks for accurate object detection and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="142" to="158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scale-aware face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6186" to="6195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Piotr Dollár, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hei</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient harmonized single-stage detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8577" to="8584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Zoom out-and-in network with map attention decision for region proposal and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="225" to="238" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Scale-aware trident networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuntao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01892</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Dollár. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8759" to="8768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Recurrent scale approximation for object detection in cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="571" to="579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Grid r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Megdet: A large mini-batch object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6181" to="6189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sniper: Efficient multi-scale training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharat</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9310" to="9320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Beyond trade-off: Accelerate fcn-based face detector with higher accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanglu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Leng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7756" to="7764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fcos: Fully convolutional one-stage object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV), October 2019</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rethinking classification and localization in r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06493</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep regionlets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xutao</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navaneeth</forename><surname>Zhou Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Bodla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="798" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Auto-fpn: Automatic network architecture adaptation for object detection beyond classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV), October 2019</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaohui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.11490</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Crafting gbd-net for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yucong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2109" to="2123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Single-shot refinement neural network for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longyin</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4203" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bottom-up object detection by grouping extreme and center points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="850" to="859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deformable convnets v2: More deformable, better results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
