<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Luhman</surname></persName>
							<email>ericluhman2@gmail.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Troy</forename><surname>Luhman</surname></persName>
							<email>troyluhman@gmail.com</email>
						</author>
						<title level="a" type="main">Knowledge Distillation in Iterative Generative Models for Improved Sampling Speed</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Iterative generative models, such as noise conditional score networks and denoising diffusion probabilistic models, produce high quality samples by gradually denoising an initial noise vector. However, their denoising process has many steps, making them 2-3 orders of magnitude slower than other generative models such as GANs and VAEs. In this paper, we establish a novel connection between knowledge distillation and image generation with a technique that distills a multi-step denoising process into a single step, resulting in a sampling speed similar to other single-step generative models. Our Denoising Student generates high quality samples comparable to GANs on the CIFAR-10 and CelebA datasets, without adversarial training. We demonstrate that our method scales to higher resolutions through experiments on 256 × 256 LSUN. Code and checkpoints are available at</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image Generation is an important and well studied problem in computer vision. There are a variety of approaches to image generation which yield high quality results, such as Generative Adversarial Networks (GANs, <ref type="bibr" target="#b9">Goodfellow et al., 2014)</ref>, Variational Autoencoders (VAEs, Kingma and Welling, 2014), and energy-based models <ref type="bibr" target="#b20">(Lecun et al., 2006</ref>). GANs generally have been able to generate the highest quality images, especially at higher resolutions <ref type="bibr" target="#b0">Brock et al., 2019)</ref>. However, their adversarial training procedure makes them more difficult to train than other methods of generative modeling.</p><p>Two increasingly popular methods of generative modeling are Denoising score-matching <ref type="bibr" target="#b38">(Vincent, 2011;</ref><ref type="bibr" target="#b30">Song and Ermon, 2019)</ref> and Denoising Diffusion Probabilistic Models (DDPMs, Sohl-Dickstein et al., 2015; <ref type="bibr" target="#b12">Ho et al., 2020)</ref>, both of which model data by gradually reversing a noise-adding process. Denoising score-matching methods use a neural network such as a Noise conditional score network (NCSN) to estimate the score, or gradient of the logarithmic data density. DDPMs are trained to reverse each step of the noise-adding (a.k.a. diffusion) process, and can also be parameterized to implicitly estimate scores <ref type="bibr" target="#b12">(Ho et al., 2020;</ref><ref type="bibr">Song et al., 2020b)</ref>. We refer to both models as score-based generative models. Both approcahes use MCMC methods similar to Langevin dynamics during sampling time. Score based models are capable of producing samples that rival even the best GAN methods, without adversarial training <ref type="bibr" target="#b12">(Ho et al., 2020;</ref><ref type="bibr">Song et al., 2020b)</ref>.</p><p>A major downside to score-based generative models is that they require performing expensive MCMC sampling, often with a thousand steps or more. As a result, they can be up to three orders of magnitude slower than GANs, which only require a single network evaluation. To address this issue, Denoising Diffusion Implicit Models, or DDIMs, have been proposed <ref type="bibr">(Song et al., 2020a)</ref>. DDIMs use a generative Markov chain to reverse a non-Markovian inference process, and are a type of implicit probabilistic model <ref type="bibr" target="#b23">(Mohamed and Lakshminarayanan, 2017)</ref>. Unlike DDPMs and NCSNs, they have a deterministic generative process that only depends on an initial latent variable. Nevertheless, DDIMs are still rather inefficient, requiring 20-100 function evaluations to produce good samples.</p><p>In this work, we propose a method of approximating a multi-step generative process with only a single function evaluation, through the use of knowledge distillation <ref type="bibr" target="#b1">(Bucilua et al., 2006;</ref><ref type="bibr" target="#b11">Hinton et al., 2015)</ref>, a technique of compressing a computationally expensive teacher into a smaller student model that learns to approximate the teacher's output distribution. Our Denoising Student synthesizes data directly from Gaussian noise without any intermediate denoising steps, and is trained to predict the output of a DDIM given the same initial noise vector.</p><p>Our approach has a number of desirable properties. Firstly, it is far more efficient (20× to 1000×) than existing score-based approaches, making it similar to GANs and VAEs in sampling speed. Secondly, it has a simple, stable objective that does not involve any adversarial training or surrogate losses (e.g. the encoder in VAEs). Thirdly, it can be easily applied to any iterative model with a deterministic generative process, as it involves no additional architectural considerations or hyperparameters (e.g. the noise schedule, the stepsize of Langevin dynamics). Lastly, it retains the abilities of other implicit models, such as semantically meaningful interpolations through the latent space.</p><p>Despite these advantages, it produces high fidelity samples comparable to GANs on lower resolution datasets such as CIFAR-10 and CelebA 64 × 64. It also scales to higher resolutions, generating decent quality samples of size 256 × 256. To the best of our knowledge, Denoising Student is the first non-adversarial model that can produce 256 × 256 LSUN images in only a single step.</p><p>2 Knowledge Distillation in Deterministic Generative Models</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge Distillation</head><p>Knowledge distillation <ref type="bibr" target="#b1">(Bucilua et al., 2006;</ref><ref type="bibr" target="#b11">Hinton et al., 2015)</ref> involves compressing an expensive but high-performing teacher model into a smaller student model. The student model is trained to minimize the cross-entropy between its output distribution and the output distribution of the teacher, which results in better performance than if it was trained normally. In supervised learning tasks, the motivation for this approach comes from the information hidden in the teacher's output, which isn't present in a one-hot label. For instance, an image with two objects could be plausibly classified as either one, which is reflected in the teacher's output distribution, but not the original label.</p><p>One condition of knowledge distillation is that the function to be learned must be deterministic, otherwise, it will be difficult, if not impossible to learn. In supervised learning tasks, this is a rather trivial condition, since models produce the same output given the same input. In generative modeling, however, this condition is no longer trivial. Scorebased and energy-based models use a stochastic MCMC procedure such as Langevin dynamics, making their usage as a teacher infeasible. We could apply knowledge distillation to a GAN, which is deterministic given the same latent variable, but we would gain only a minor speedup over an already fast model.</p><p>Recently, Song et al. (2020a) proposed a new class of generative models called denoising diffusion implicit models (DDIMs), which are similar to other iterative generative models in terms of sample quality but have somewhat faster sampling speed. Unlike other iterative models 1 , their generative process is deterministic, which motivates their usage as a teacher model. We will discuss them below. For further explanation and proofs related to DDIMs, the reader is referred to Song et al. (2020a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Denoising Diffusion Implicit Models</head><p>Consider an inference process with latent variables x 1 , . . . , x T of the same dimensionality as the data x 0 ∼ q(x 0 ), parameterized by a decreasing sequence of constants α 1:T ∈ (0, 1] T . The inference process q(x 1:T |x 0 ) is defined as:</p><formula xml:id="formula_0">q(x 1:T |x 0 ) := q(x T |x 0 ) T t=2 q(x t−1 |x t , x 0 ), with (1) q(x T |x 0 ) = N (x T ; √ α T x 0 , (1 − α T )I), and (2) q(x t−1 |x t , x 0 ) = δ(x t−1 − µ(x t , x 0 )) (3) where δ(·) is the Dirac delta function, µ(x t , x 0 ) = √ α t−1 x 0 + 1−α t−1 1−αt (x t − √ α t x 0 )</formula><p>. This inference process is not trainable, unlike in other probabilistic models such as VAEs (Kingma and Welling, 2014). One notable property of this inference process is that for</p><formula xml:id="formula_1">any t, q(x t |x 0 ) = N (x t ; √ α t x 0 , (1 − α t )I)</formula><p>, so we can write:</p><formula xml:id="formula_2">x t = √ α t x 0 + √ 1 − α t , ∼ N (0, I)<label>(4)</label></formula><p>A denoising diffusion implicit model <ref type="bibr">(Song et al., 2020a</ref>) is a latent variable model whose generative process reverses the above inference process. Formally, it is defined as:</p><formula xml:id="formula_3">p θ (x 0 ) := p θ (x 0:T )dx 1:T , with p θ (x 0:T ) := p(x T ) T t=1 p θ (x t−1 |x t ) (5) where p θ (x 0:T ) is the generative process, with prior p(x T ) = N (x T ; 0, I). Each distri- bution p θ (x t−1 |x t ) in the generative process estimates the corresponding inference distri- bution q(x t−1 |x t , x 0 ). However, sampling from q(x t−1 |x t , x 0 ) requires knowledge of x 0 ,</formula><p>which is unavailable during the generative process. As a result, we make a prediction f θ (x t ) of x 0 , and use this in our estimate q(x t−1 |x t , f θ (x t )). Putting it together, we get</p><formula xml:id="formula_4">p θ (x t−1 |x t ) = q(x t−1 |x t , f θ (x t )) if t &gt; 1 δ(x 0 − f θ (x 1 )) if t = 1 (6) where f θ (x t ) is found by rewriting Eq. (4), to get f θ (x t ) = 1 √ αt (x t − √ 1 − α t θ ).</formula><p>To sample from a DDIM, we first sample a noise vector x T from a standard normal, then gradually denoise it over time by iteratively computing x T −1 , x T −2 , . . . , x 1 , x 0 with:</p><formula xml:id="formula_5">1 α t−1 x t−1 = 1 α t x t − 1 − α t α t − 1 − α t−1 α t−1 θ (x t ), or<label>(7)</label></formula><formula xml:id="formula_6">x t−1 = α t−1 α t (x t − √ 1 − α t θ (x t )) + 1 − α t−1 θ (x t )<label>(8)</label></formula><p>The generative model is trained by maximizing the variational lower bound:</p><formula xml:id="formula_7">ELBO = E q [log p θ (x 0:T )−log q(x 1:T |x 0 )].</formula><p>Here, we treat p θ (x t−1 |x t ) and each q(x t−1 |x t , x 0 ) as isotropic Gaussians with very small variance, to ensure that the generative and inference processes are nonzero everywhere. <ref type="bibr" target="#b1">2</ref> This objective can then be simplified to a closed form expression:</p><formula xml:id="formula_8">L = T t=1 γ t E x 0 , t t − θ ( √ α t x 0 + √ 1 − α t t , t) 2 2 + C,<label>(9)</label></formula><p>2 Their original definitions were chosen to emphasize their non-stochasticity. where γ t is a positive constant 3 related to α t and α t−1 , and θ is a trainable function (i.e. neural network) that approximates t given a noisy input x t and timestep t. This is the same training objective used by DDPMs (Ho et al., 2020), so one can use a trained DDPM as a DDIM by changing only the sampling procedure.</p><p>From Eq. <ref type="formula" target="#formula_6">(8)</ref>, we see that for a given θ , x t−1 is known and fixed given x t . In other words, each individual step of the sampling procedure is deterministic. As a result, the entire generative process from x T −1 to x 0 is deterministic as well, dependent only on the initial latent x T . So, one could choose to ignore the intermediate latents x 1:T −1 , since knowledge of them is not necessary to predict x 0 . We can therefore directly model the data x 0 with a conditional distribution p θ (x 0 |x T ).</p><p>From this perspective, we could think of a DDIM as an implicit probabilistic model that transforms a latent variable x T with a deterministic function to model data. Unfortunately, evaluating this function requires T forward passes through a neural network, making it very computationally expensive. This motivates our usage of knowledge distillation, where we distill the knowledge in this "teacher" function into a much faster "student" network that uses only a single network evaluation. The student's output distribution p student (x 0 |x T ) approximates the teacher's output distribution p teacher (x 0 |x T ), using only x T . We illustrate this concept in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>3 Generative Modeling with Denoising Student</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Objective</head><p>As previously mentioned, the student's conditional distribution p student (x 0 |x T ) should be as close as possible close to the teacher's conditional distribution p teacher (x 0 |x T ). We therefore train the student by minimizing the following:</p><formula xml:id="formula_9">L student = E x T [D KL (p teacher (x 0 |x T ) p student (x 0 |x T ))]<label>(10)</label></formula><p>This training objective is very similar to that in a supervised learning task, where models are trained minimize the KL Divergence between two categorical distributions over labels.</p><p>We parameterize the student as a simple Gaussian model with trainable mean F student (x T ) and unit variance. Since the teacher is a deterministic function F teacher of x T , we add some Gaussian noise to its output to ensure that p teacher (x 0 |x T ) is nonzero everywhere. Since the KL term in Eq. (10) is between two Gaussians, the equation simplifies to:</p><formula xml:id="formula_10">L student = 1 2 E x T [ F student (x T ) − F teacher (x T ) 2 2 ] + C<label>(11)</label></formula><p>Training of the student is done by sampling random x T from the prior p(x T ) and computing the corresponding F teacher (x T ) with a DDIM, then minimizing Eq. <ref type="bibr" target="#b10">(11)</ref>. Unlike other generative models, our method does not require the joint training of two networks (e.g. discriminator in GANs, inference model in VAEs). In theory, one could train the student on as many unique examples as desired, since both the prior and the teacher model are known. In our experiments however, we use only 1.024 million synthetic examples for each dataset and iterate over these same ones multiple times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Architecture</head><p>Unlike normalizing flows and autoregressive models, our method does not have any significant architectural restrictions. So, one could use an arbitrary neural network as the student, then randomly initialize weights. But we observe that the entirety of the teacher's knowledge is contained in a much smaller neural network θ that is reused many times. As such, we initialize the student to have the same architecture and weights as θ . This allows the student to inherit knowledge from the teacher, which speeds up training.</p><p>In the remaining sections, we use the term "teacher" to refer to the DDIM's sampling procedure from x T to x 0 , and the term "teacher network" to refer to the neural network θ (x t , t) that parameterizes this sampling procedure.</p><p>There are a couple minor concerns with this approach that we will address here. Firstly, the student is trained to model the data x 0 , so it may not be ideal to initialize it to a teacher network that models the noise t . To resolve this, we have the student predict the noise as well, and subtract the predicted noise from the input noise x T to produce samples and compute loss. Secondly, the teacher network (and therefore the student as well) is conditioned on time, so we condition the student on timestep T, corresponding to the highest noise level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we examine the proposed method through multiple experiments. We consider four datasets: CIFAR-10 32 × 32 <ref type="bibr" target="#b19">[20]</ref>, CelebA 64 × 64 <ref type="bibr" target="#b21">[22]</ref>, LSUN <ref type="bibr" target="#b44">[45]</ref> Bedroom 256 × 256, and LSUN Church 256 × 256. We use the pretrained models from Ho et al.</p><p>(2020) as our teacher network, except for CelebA, where we use the one from <ref type="bibr">Song et al. (2020a)</ref>. For our experiments, we use a 100-step DDIM as our teacher for CIFAR-10 and CelebA, and a 50-step DDIM for LSUN. Further details can be found in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image Generation</head><p>We show random samples from our CIFAR-10 and CelebA model in <ref type="figure" target="#fig_1">Figures 2 and 3</ref> respectively. The samples are both high quality and diverse, demonstrating the efficacy of our method. Note that unlike VAEs, we do not reduce the temperature (standard deviation) of our prior when showing qualitative samples, which hinders diversity.</p><p>For quantitative evaluation, we show FID <ref type="bibr" target="#b10">[11]</ref> and Inception scores <ref type="bibr" target="#b26">[27]</ref> in <ref type="table" target="#tab_0">Table 1</ref> for various methods on CIFAR-10, as well as the number of steps (network evaluations) each needs. On CIFAR-10, our model achieves an FID of 9.36, which is lower than several GANs and far lower than NVAE <ref type="bibr" target="#b37">[38]</ref>, a state of the art VAE. <ref type="bibr" target="#b3">4</ref> Our CIFAR-10 scores are also similar to state-of-the-art EBMs, even though our method uses only a single network evaluation. On CelebA, we obtain a competitive FID of 10.68.</p><p>To see how our method scales to higher resolutions, we evaluated it on the 256 × 256 LSUN Bedroom and Church datasets. We show random samples in <ref type="figure" target="#fig_3">Figure 4</ref> (Bedroom) and 5 (Church). We find that our model learns structure and color quite well, as well as larger details such as lamps and windows in Bedroom, and clouds and trees in Church. We do find, however, that our model produces rather blurry samples, without very defined textures. We believe this bluriness arises from the choice of objective. While GANs are designed to produce images that fool the discriminator, our model has to replicate the teacher's output down to the pixel, which is difficult for higher dimensional data.      To assess the sampling speed of Denoising Student, we measured the time it took to generate 50k CIFAR-10 images, and compared it in <ref type="table" target="#tab_1">Table 2</ref>. Notably, it is about 100× faster than the teacher, and 1000× faster than a DDPM, indicating that for the same architecture, sampling time is proportional to number of steps. When compared with NVAE, we find ours is slightly faster, showing that our model is indeed similar to other single-step models in speed. Timing experiments were done with a batch size of 250 on a single Nvidia V100 GPU. <ref type="bibr">Song et al. (2020a)</ref> observed that in DDIMs, the latent variable x T is an effective representation of the data, where interpolations in the latent space result in meaningful image interpolations. Our model was capable of learning the latent mapping of a DDIM, so we hypothesize it would be able to produce meaningful interpolations as well. To test this hypothesis, we perform spherical interpolation between 2 random x T , and show the result in <ref type="figure">Figure 6</ref>. See <ref type="figure" target="#fig_0">Figure 12</ref> for more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Latent Space Manipulation</head><p>To gain some further insight into the learned latent representation of Denoising Student, we decided to apply it to different sized images than it was trained on, by simply changing the size of the latent variable. Remarkably, it still manages to replicate the teacher quite well, producing coherent images of a different size <ref type="figure" target="#fig_0">(Figure 13</ref> in Appendix). This would indicate that the student retains the teacher's advanced generalization capabilities. <ref type="figure">Figure 6</ref>: Interpolation results on the CelebA dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Knowledge distillation <ref type="bibr" target="#b1">(Bucilua et al., 2006;</ref><ref type="bibr" target="#b11">Hinton et al., 2015)</ref> has been explored extensively in supervised learning tasks such as image classification <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b42">43]</ref>, language modeling <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b33">34]</ref>, and speech recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b8">9]</ref>. Most existing work on knowledge distillation is concerned with finding the best way to transfer knowledge (e.g. best loss function) from the teacher to the student. Additionally, knowledge distillation is usually concerned with reducing the size (parameters) of a network, instead of the number of evaluations.</p><p>Our work is also based on denoising score-based models such as NCSNs <ref type="bibr" target="#b30">(Song and Ermon, 2019)</ref> and DDPMs <ref type="bibr" target="#b12">(Ho et al., 2020)</ref>, which estimate the score of the data density and use a denoising auto-encoder objective. <ref type="bibr">Song et al. (2020b)</ref> observed that an NCSN / DDPM's generative process could be seen as solving a reverse time stochastic differential equation that reverses a noise-adding SDE. A DDIM's sampling procedure, on the other hand, more closely resembles integrating an ODE (see. Eq. 7). From this perspective, our student model could loosely be seen as approximating this integral with a function, instead of a more accurate but expensive numerical method (the teacher).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we presented a simple method to vastly reduce the sampling time of certain iterative generative models, with only a minor degradation in performance. Our knowledge distillation technique can be easily applied to any trained model with a deterministic generative process, since it has a stable training objective and requires no extra architectural considerations. Despite its simplicity and stability, our model is capable of producing significantly better samples than other non-adversarial, single-step methods such as VAEs. It also learns meaningful latent representations, allowing for easy data manipulation through the latent space. Future work includes bridging the gap between the teacher and student through more advanced distillation techniques, and producing sharper images at high resolutions.     </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental Details</head><p>A list of the hyperparameters we used can be seen in <ref type="table" target="#tab_2">Table 3</ref>. We used the Adam (Kingma and Ba, 2017) optimizer. We linearly increased the learning rate for a number of warmup steps, and used no decay. We did not use dropout or any other regularization, and used test loss evaluated on unseen images for early stopping. We made some interesting observations when training, and list them as follows:</p><p>• For CIFAR-10, we found that training on L1 distance yielded better results than L2.</p><p>• For LSUN Bedroom, we initially clipped the values of the gradients to ±1.0, but observed that the gradients were very large (norm consistently &gt; 10 5 ). We found that not clipping by value improved stability.</p><p>• For LSUN Bedroom, we initially chose β 1 = 0.9, lr = 2 × 10 −5 , but found that increasing β 1 to 0.98 and decreasing the learning rate to 5 × 10 −6 helped stabilize training. We applied these to Church without sweeping. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>A graphical representation of our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Uncurated samples from our CIFAR-10 model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Uncurated samples from our CelebA model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Uncurated samples from our LSUN Bedroom model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Uncurated samples from our LSUN Church-Outdoor model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Uncurated Samples from our CIFAR-10 model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Uncurated Samples from our LSUN Bedroom model</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Uncurated Samples from our LSUN Church-Outdoor model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Inception feature space nearest neighbors for CIFAR-10. Images generated from our model are in the leftmost column, and to the right of the black line are the nearest neighbors in the training set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Extended Interpolation results. Rows 1 and 2: CelebA images, Rows 3 and 4: LSUN Bedroom, Rows 5 and 6: LSUN church Uncurated 256×384 images from a 50 step DDIM (teacher, left) and our model (right). Neither model has ever seen a 256 × 384 image in training, yet both produce decent samples, demonstrating that both the teacher and student generalize well.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results for CIFAR-10. "Steps" refers to the number of neural network evaluations needed to generate a sample, and a model labeled with "cond." is class-conditional. Scores marked with an asterisk were computed by us and are not official.</figDesc><table><row><cell>Model</cell><cell>FID ↓</cell><cell>IS ↑</cell><cell>Steps ↓</cell></row><row><cell>Single-Step</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Denoising Student (Ours)</cell><cell>9.36</cell><cell>8.36</cell><cell>1</cell></row><row><cell>NVAE [38]</cell><cell>51.67</cell><cell>5.51</cell><cell>1</cell></row><row><cell>MoLM [25]</cell><cell>18.9</cell><cell>7.90</cell><cell>1</cell></row><row><cell>Single-Step, GAN</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SNGAN [23]</cell><cell>21.7</cell><cell>8.22</cell><cell>1</cell></row><row><cell>BigGAN (cond.) [1]</cell><cell>14.73</cell><cell>9.22</cell><cell>1</cell></row><row><cell>PPOGAN [41]</cell><cell>10.87</cell><cell>8.69</cell><cell>1</cell></row><row><cell>StyleGAN2+ADA [16]</cell><cell>2.92</cell><cell>9.83</cell><cell>1</cell></row><row><cell>StyleGAN2+ADA (cond.) [16]</cell><cell>2.42</cell><cell>10.14</cell><cell>1</cell></row><row><cell>Many-Step</cell><cell></cell><cell></cell><cell></cell></row><row><cell>DDIM (100 step, Teacher)</cell><cell>4.16</cell><cell>8.96*</cell><cell>100</cell></row><row><cell>EBM [5]</cell><cell>38.2</cell><cell>6.78</cell><cell>60</cell></row><row><cell>VAEBM [42]</cell><cell>12.19</cell><cell>8.43</cell><cell>16</cell></row><row><cell>EBM+recovery likelihood [8]</cell><cell>9.60</cell><cell>8.58</cell><cell>180</cell></row><row><cell>NCSNv2 [32]</cell><cell>10.87</cell><cell>8.40</cell><cell>1160</cell></row><row><cell>DDPM [13]</cell><cell>3.17</cell><cell>9.46</cell><cell>1000</cell></row><row><cell>NCSN++ (8 blocks/res) [33]</cell><cell>2.20</cell><cell>9.89</cell><cell>2000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table><row><cell cols="2">Time (s) to gener-</cell></row><row><cell cols="2">ate 50k CIFAR-10 images</cell></row><row><cell>Model</cell><cell>Time</cell></row><row><cell>Denoising Student</cell><cell>51.5</cell></row><row><cell>NVAE</cell><cell>146.5</cell></row><row><cell>DDIM (Teacher)</cell><cell>4.96 × 10 3</cell></row><row><cell>DDPM</cell><cell>5.02 × 10 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Hyperparameters used in our experiments.</figDesc><table><row><cell>Model</cell><cell cols="4">CIFAR-10 CelebA LSUN Bedroom LSUN Church</cell></row><row><cell>Parameters (M)</cell><cell>35.7</cell><cell>78.7</cell><cell>114</cell><cell>114</cell></row><row><cell>Batch size</cell><cell>512</cell><cell>512</cell><cell>32</cell><cell>32</cell></row><row><cell>Learning Rate</cell><cell cols="2">2 × 10 −4 5 × 10 −5</cell><cell>5 × 10 −6</cell><cell>5 × 10 −6</cell></row><row><cell>Adam β 1</cell><cell>0.9</cell><cell>0.9</cell><cell>0.98</cell><cell>0.98</cell></row><row><cell>Adam β 2</cell><cell>0.98</cell><cell>0.98</cell><cell>0.999</cell><cell>0.999</cell></row><row><cell>EMA decay rate</cell><cell>0.995</cell><cell>0.995</cell><cell>0.9995</cell><cell>0.9995</cell></row><row><cell>Warmup steps</cell><cell>5000</cell><cell>5000</cell><cell>1000</cell><cell>1000</cell></row><row><cell>Max training iterations</cell><cell>50k</cell><cell>64k</cell><cell>320k</cell><cell>256k</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">An even more recent work(Song et al., 2020b)  proposed a neural probability flow ODE that also has a deterministic, multi-step generative process. However, pretrained models are unavailable as of writing, and we wanted to avoid retraining the teacher.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In practice, Ho et al. (2020) found it beneficial to set γ t = 1 for all t</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">While Child (2020) has slightly lower negative log-likelihood than NVAE (2.87 vs 2.91), we do not believe that this improvement would fully account for the disparity between our model and NVAE.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Large Scale GAN Training for High Fidelity Natural Image Synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=B1xsqj09Fm" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Model Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Bucilua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<idno type="DOI">10.1145/1150402.1150464</idno>
		<ptr target="https://doi.org/10.1145/1150402.1150464" />
		<imprint>
			<date type="published" when="2006" />
			<publisher>Association for Computing Machinery</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distilling Knowledge from Ensembles of Neural Networks for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yevgen</forename><surname>Chebotar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: Interspeech</title>
		<imprint>
			<biblScope unit="page" from="3439" to="3443" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<idno type="arXiv">arXiv:2011.10650[cs.LG</idno>
		<title level="m">Very Deep VAEs Generalize Autoregressive Models and Can Outperform Them on Images. 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Implicit generation and modeling with energy based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="3608" to="3618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Efficient Knowledge Distillation from an Ensemble of Teachers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fukuda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Born Again Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommaso</forename><surname>Furlanello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04770</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning Energy-Based Models by Diffusion Recovery Likelihood</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.08125</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Distilling Knowledge from Ensembles of Acoustic Models for Joint CTC-Attention End-to-End Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Titouan</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Lane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.09310</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative Adversarial Nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Denoising Diffusion Probabilistic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11239</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Tinybert: Distilling bert for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqi</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10351</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Analyzing and Improving the Image Quality of StyleGAN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.04958</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Training Generative Adversarial Networks with Limited Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06676</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Knowledge distillation using output errors for self-attention end-to-end models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ho-Gyeong</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6181" to="6185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012-05" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
	<note>In: Predicting structured data</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Spectral Normalization for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning in Implicit Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaji</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03483</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Learning Implicit Generative Models with the Method of Learned Moments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Ravuri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.11006</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Hints for Thin Deep Nets</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Improved Techniques for Training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03585</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Denoising Diffusion Implicit Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02502</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11918" to="11930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improved Techniques for Training Score-Based Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09011</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Score-Based Generative Modeling through Stochastic Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.13456[cs.LG</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Contrastive Distillation on Intermediate Representations for Language Model Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14167</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Patient Knowledge Distillation for BERT Model Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09355</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Contrastive Representation Distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Well-Read Students Learn Better: On the Importance of Pretraining Compact Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulia</forename><surname>Turc</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>cs.CL</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">NVAE: A Deep Hierarchical Variational Autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03898</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural computation</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Student-teacher network learning with enhanced features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shinji</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5275" to="5279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Improving GAN Training with Probability Ratio Clipping and Sample Reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhisheng</forename><surname>Xiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.00654</idno>
		<title level="m">A Symbiosis between Variational Autoencoders and Energy-based Models. 2020</title>
		<imprint/>
	</monogr>
	<note>cs.LG</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Knowledge distillation meets self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="588" to="604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a Large-scale Image Dataset using Deep Learning with Humans in the Loop. 2016</title>
		<imprint/>
	</monogr>
	<note>cs.CV</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations. 2017. A Samples We show additional samples in Figure 7 (CIFAR-10), 8 (CelebA)</title>
		<imprint>
			<publisher>Church</publisher>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Additional interpolation results can be seen in 12. and different size images can be seen in Figure 13</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Since our model was not trained on real data, but instead on the teacher&apos;s output, we do not expect it to memorize training examples</title>
		<imprint/>
	</monogr>
	<note>To demonstrate this, we include nearest neighbor visualizations in Figure 11</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
