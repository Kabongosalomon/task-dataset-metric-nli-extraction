<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Connecting Vision and Language with Localized Narratives</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soravit</forename><surname>Changpinyo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Connecting Vision and Language with Localized Narratives</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:38+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose Localized Narratives, a new form of multimodal image annotations connecting vision and language. We ask annotators to describe an image with their voice while simultaneously hovering their mouse over the region they are describing. Since the voice and the mouse pointer are synchronized, we can localize every single word in the description. This dense visual grounding takes the form of a mouse trace segment per word and is unique to our data. We annotated 849k images with Localized Narratives: the whole COCO, Flickr30k, and ADE20K datasets, and 671k images of Open Images, all of which we make publicly available. We provide an extensive analysis of these annotations showing they are diverse, accurate, and efficient to produce. We also demonstrate their utility on the application of controlled image captioning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Much of our language is rooted in the visual world around us. A popular way to study this connection is through Image Captioning, which uses datasets where images are paired with human-authored textual captions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b50">51]</ref>. Yet, many researchers want deeper visual grounding which links specific words in the caption to specific regions in the image <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>. Hence Flickr30k Entities <ref type="bibr" target="#b44">[45]</ref> enhanced Flickr30k <ref type="bibr" target="#b65">[66]</ref> by connecting the nouns from the captions to bounding boxes in the images. But these connections are still sparse and important aspects remain ungrounded, such as words capturing relations between nouns (as "holding" in "a woman holding a balloon"). Visual Genome <ref type="bibr" target="#b30">[31]</ref> provides short descriptions of regions, thus words are not individually grounded either.</p><p>In this paper we propose Localized Narratives, a new form of multimodal image annotations in which we ask annotators to describe an image with their voice while simultaneously hovering their mouse over the region they are describing. <ref type="figure">Figure 1</ref> illustrates the process: the annotator says "woman" while using their mouse to indicate her spatial extent, thus providing visual grounding for this noun. Later they move the mouse from the woman to the balloon following its string, saying "holding". This provides direct visual grounding of this relation. They also describe attributes like "clear blue sky" and "light blue jeans". Since voice is synchronized to the mouse pointer, we can determine the image location of every single word in the description. This provides dense visual grounding in the form of a mouse trace segment for each word, which is unique to our data.</p><p>In order to obtain written-word grounding, we additionally need to transcribe the voice stream. We observe that automatic speech recognition <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b45">46]</ref> typically results in imperfect transcriptions. To get data of the highest quality, we arXiv:1912.03098v4 [cs.CV] 20 Jul 2020 Caption: Image and Trace:</p><p>In the front portion of the picture we can see a dried grass area with dried twigs. There is a woman standing wearing light blue jeans and ash colour long sleeve length shirt. This woman is holding a black jacket in her hand. On the other hand she is holding a balloon which is peach in colour. On the top of the picture we see a clear blue sky with clouds. The hair colour of the woman is brownish.</p><p>Voice:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fig. 1:</head><p>Localized Narrative example: Caption, voice, and mouse trace synchronization represented by a color gradient . The project website <ref type="bibr" target="#b59">[60]</ref> contains a visualizer with many live examples.</p><p>ask annotators to transcribe their own speech, immediately after describing the image. This delivers an accurate transcription, but without temporal synchronization between the mouse trace and the written words. To address this issue, we perform a sequence-to-sequence alignment between automatic and manual transcriptions, which leads to accurate and temporally synchronized captions. Overall, our annotation process tightly connects four modalities: the image, its spoken description, its textual description, and the mouse trace. Together, they provide dense grounding between language and vision.</p><p>Localized Narratives is an efficient annotation protocol. Speaking and pointing to describe things comes naturally to humans <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b41">42]</ref>. Hence this step takes little time <ref type="bibr">(40.4</ref> sec. on average). The manual transcription step takes 104.3 sec., for a total of 144.7 sec. This is lower than the cost of related grounded captioning datasets Flickr30k Entities and Visual Genome <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b44">45]</ref>, which were made by more complicated annotation processes and involved manually drawing bounding boxes (Sec. 4.1 -Annotation Cost). Moreover, if automatic speech recognition improves in the future it might be possible to skip the manual transcription step, making our approach even more efficient.</p><p>We collected Localized Narratives at scale: we annotated the whole COCO <ref type="bibr" target="#b34">[35]</ref> (123k images), ADE20K <ref type="bibr" target="#b68">[69]</ref> (20k) and Flickr30k <ref type="bibr" target="#b65">[66]</ref> (32k) datasets, as well as 671k images of Open Images <ref type="bibr" target="#b32">[33]</ref>. We make the Localized Narratives for these 848,749 images publicly available <ref type="bibr" target="#b59">[60]</ref>. We provide an extensive analysis (Sec. 4) and show that: (i) Our data is rich: we ground all types of words (nouns, verbs, prepositions, etc.), and our captions are substantially longer than in most previous datasets <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b50">51]</ref>. (ii) Our annotations are diverse both in the language modality (e.g. caption length varies widely with the content of the image) and in the visual domain (different pointing styles and ways of grounding relationships). (iii) Our data is of high quality: the mouse traces match well the location of the objects, the words in the captions are semantically correct, and the manual transcription is accurate. (iv) Our annotation protocol is more efficient than for related grounded captioning datasets <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b44">45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Text Speech Grounding Task</head><p>In Out --Image captioning <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b57">58]</ref>, Paragraph generation <ref type="bibr">[30,63,</ref>  Since Localized Narratives provides four synchronized modalities, it enables many applications (Tab. 1). We envision that having each word in the captions grounded, beyond the sparse set of boxes of previous datasets <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b30">31]</ref>, will enable richer results in many of these tasks and open new doors for tasks and research directions that would not be possible with previously existing annotations. As a first example, we show how to use the mouse trace as a fine-grained control signal for a user to request a caption on a particular image (Sec. 5). Mouse traces are a more natural way for humans to provide a sequence of grounding locations, compared to drawing a list of bounding boxes <ref type="bibr" target="#b12">[13]</ref>. We therefore envision its use as assistive technology for people with imperfect vision. In future work, the mouse trace in our Localized Narratives could be used as additional attention supervision at training time, replacing or complementing the self-supervised attention mechanisms typical of modern systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b61">62]</ref>. This might train better systems and improve captioning performance at test time, when only the image is given as input. Alternatively, our mouse traces could be used at test time only, to inspect whether current spatial attention models activate on the same image regions that humans associate with each word.</p><p>Besides image captioning, Localized Narratives are a natural fit for: (i) image generation: the user can describe which image they want to generate by talking and moving their mouse to indicate the position of objects (demonstration in App. A); (ii) image retrieval: the user naturally describes the content of an image they are looking for, in terms of both what and where; (iii) grounded speech recognition: considering the content of an image would allow better speech transcription, e.g. 'plant' and 'planet' are easier to distinguish in the visual than in the voice domain; (iv) voice-driven environment navigation: the user describes where they want to navigate to, using relative spatial language.</p><p>To summarize, our paper makes the following contributions: (i) We introduce Localized Narratives, a new form of multimodal image annotations where every word is localized in the image with a mouse trace segment; (ii) We use Localized Narratives to annotate 848,749 images and provide a thorough analysis of the data. (iii) We demonstrate the utility of our data for controlled image captioning.  The man at bat readies to swing at the pitch while the umpire looks on</p><p>The Eiffel Tower in the background Man jumping for a picture with a skateboard Light brown shoe with red strip Green shirt with logo across front A man with pierced ears is wearing glasses and an orange hat.</p><formula xml:id="formula_0">(d) (c) (b) (a)</formula><p>There is a kid standing on the bed, holding one of the railing, with a hand under his chin. He is wearing a blue jacket. Behind him there is a pillow and bed sheets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Captioning Datasets. Various annotation efforts connect vision and language via captioning (Tab. 2). We focus on how their captions are grounded, as this is the key differentiating factor of Localized Narratives from these works. As a starting point, classical image captioning <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b50">51]</ref> and visual paragraph generation <ref type="bibr" target="#b29">[30]</ref> simply provide a whole caption for the whole image ( <ref type="figure" target="#fig_0">Fig. 2(a)</ref>). This lack of proper grounding was shown to be problematic <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>Flickr30k Entities <ref type="bibr" target="#b44">[45]</ref> annotated the nouns mentioned in the captions of Flickr30k <ref type="bibr" target="#b65">[66]</ref> and drew their bounding box in the image ( <ref type="figure" target="#fig_0">Fig. 2(b)</ref>): the grounding is therefore from nouns to regions (including their attached adjectives, Tab. 2). Visual Genome <ref type="bibr" target="#b30">[31]</ref> and related previous efforts <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b38">39]</ref> provide short phrases describing regions in the images <ref type="figure" target="#fig_0">(Fig. 2(c)</ref>): grounding is therefore at the phrase level (Tab. 2). While Visual Genome uses these regions as a seed to generate a scene graph, where each node is grounded in the image, the connection between the region descriptions and the scene graph is not explicit.</p><p>In Localized Narratives, in contrast, every word is grounded to a specific region in the image represented by its trace segment ( <ref type="figure" target="#fig_0">Fig. 2(d)</ref>). This includes all types of words (nouns, verbs, adjectives, prepositions, etc.), in particular valuable spatial-relation markers ("above", "behind", etc.) and relationship in-dicators ("riding", "holding", etc.). Another disadvantage of Flickr30k Entities and Visual Genome is that their annotation processes require manually drawing many bounding boxes a posteriori, which is unnatural and time-consuming compared to our simpler and more natural protocol (Sec. 4.1 -Annotation Cost).</p><p>SNAG <ref type="bibr" target="#b52">[53]</ref> is a proof of concept where annotators describe images using their voice while their gaze is tracked using specialized hardware. This enables inferring the image location they are looking at. As a consequence of the expensive and complicated setup, only 100 images were annotated. In our proposed Localized Narratives, instead, we collect the data using just a mouse, a keyboard, and a microphone as input devices, which are commonly available. This allows us to annotate a much larger set of images (848,749 to date).</p><p>In the video domain, ActivityNet-Entities <ref type="bibr" target="#b69">[70]</ref> adds visual grounding to the ActivityNet Captions, also in two stages where boxes were drawn a posteriori.</p><p>Annotation using Voice. A few recent papers use voice as an input modality for computer vision tasks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b18">19]</ref>. The closest work to ours <ref type="bibr" target="#b18">[19]</ref> uses voice to simultaneously annotate the class name and the bounding box of an object instance in an image. With Localized Narratives we bring it to the next level by producing richer annotations both in the language and vision domains with long free-form captions associated to synchronized mouse traces.</p><p>In the video domain, EPIC-KITCHENS <ref type="bibr" target="#b14">[15]</ref> contains videos of daily kitchen activities collected with a head-mounted camera. The actions were annotated with voice, manually transcribed, and time-aligned using YouTube's automatic closed caption alignment tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Annotation Process</head><p>The core idea behind the Localized Narratives annotation protocol is to ask the annotators to describe the contents of the image using their voice while hovering their mouse over the region being described. Both voice and mouse location signals are timestamped, so we know where the annotators are pointing while they are speaking every word. <ref type="figure" target="#fig_1">Figure 3</ref> shows voice (a) and mouse trace data (b), where the color gradient represents temporal synchronization. We summarize how to process this data to produce a Localized Narrative example. First, we apply an Automatic Speech Recognition (ASR) algorithm and get a synchronized, but typically imperfect, transcription (c). After finishing a narration, the annotators transcribe their own recording, which gives us an accurate caption, but without synchronization with the mouse trace (d). Finally, we obtain a correct transcription with timestamps by performing sequence-to-sequence alignment between the manual and automatic transcriptions (e). This time-stamped transcription directly reveals which trace segment corresponds to each word in the caption (f), and completes the creation of a Localized Narrative instance. Below we describe each step in detail.</p><p>Annotation Instructions. One of the advantages of Localized Narratives is that it is a natural task for humans to do: speaking and pointing at what we are describing is a common daily-life experience <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b41">42]</ref>. This makes it easy for annotators to understand the task and perform as expected, while increasing the pool of qualified annotators for the task. The instructions we provide are concise:</p><p>Use the mouse to point at the objects in the scene. Simultaneously, use your voice to describe what you are pointing at.</p><p>-Focus on concrete objects (e.g. cow, grass, person, kite, road, sky).</p><p>-Do not comment on things you cannot directly see in the image (e.g. feelings that the image evokes, or what might happen in the future).</p><p>-Indicate an object by moving your mouse over the whole object, roughly specifying its location and size.</p><p>-Say the relationship between two objects while you move the mouse between them, e.g. "a man is flying a kite", "a bottle is on the table".</p><p>-If relevant, also mention attributes of the objects (e.g. old car).</p><p>Automatic and Manual Transcriptions. We apply an ASR algorithm <ref type="bibr" target="#b16">[17]</ref> to obtain an automatic transcription of the spoken caption, which is timestamped but typically contains transcription errors. To fix these errors, we ask the annotators to manually transcribe their own recorded narration. Right after they described an image, the annotation tool plays their own voice recording accompanied by the following instructions:</p><p>Type literally what you just said.</p><p>-Include filler words if you said them (e.g. "I think", "alright") but not filler sounds (e.g. "um", "uh", "er").</p><p>-Feel free to separate the text in multiple sentences and add punctuation.</p><p>The manual transcription is accurate but not timestamped, so we cannot associate it with the mouse trace to recover the grounding of each word.</p><p>Transcription Alignment. We obtain a correct transcription with timestamps by performing a sequence-to-sequence alignment between the manual and automatic transcriptions ( <ref type="figure" target="#fig_1">Fig. 3</ref>).</p><p>Let a = {(a 1 , . . . , a |a| } and m = {m 1 , . . . , m |m| } be the automatic and manual transcriptions of the spoken caption, where a i and m j are individual words. a i is timestamped: let [t 0 i , t 1 i ] be the time segment during which a i was spoken. Our goal is to align a and m to transfer the timestamps from the automatically transcribed words a i to the manually provided m j .</p><p>To do so, we apply Dynamic Time Warping <ref type="bibr" target="#b31">[32]</ref> between a and m. Intuitively, we look for a matching function µ that assigns each word a i to a word m µ(i) , such that if i 2 &gt; i 1 then µ(i 2 ) ≥ µ(i 1 ) (it preserves the order of the words). Note that µ assigns each a i to exactly one m j , but m j can match to zero or multiple words in a. We then look for the optimal matching µ * such that:</p><formula xml:id="formula_1">µ * = arg min µ Dµ(a, m) Dµ(a, m) = |a| i=1 d(ai, m µ(i) ) (1)</formula><p>where d is the edit distance between two words, i.e. the number of character inserts, deletes, and replacements required to get from one word to the other. D µ * (a, m) provides the optimal matching score (used below to assess quality).</p><p>Given µ * , let the set of matches for m j be defined as to the manual one (d) to transfer the timestamps from the former to the latter, resulting in a transcription that is both accurate and timestamped (e). To do so, we perform a sequence-to-sequence alignment (gray box) between ai and mj (black thick lines). The timestamps of matched words mj are defined as the segment (green) containing the original timestamps (red) of the matched words ai. Unmatched words mj get assigned the time segments in between matched neighboring words (blue). These timestamps are transferred to the mouse trace and define the trace segment for each word mj.</p><formula xml:id="formula_2">A j = {i | µ * (i) = j}. The timestamp [t 0 j ,t 1 j ] of word m j in the manual transcription is the interval</formula><p>spanned by its matching words (if any) or to the time between neighboring matching words (if none). Formally:</p><formula xml:id="formula_3">t 0 j =    min t 0 i | i ∈ Aj if Aj = ∅, max t 1 i | i ∈ A k | k &lt; j if ∃ k &lt; j s.t. A k = ∅ T 0 otherwise,<label>(2)</label></formula><formula xml:id="formula_4">t 1 j =    max t 1 i | i ∈ Aj if Aj = ∅, min t 0 i | i ∈ A k | k &gt; j if ∃ k &gt; j s.t. A k = ∅ T 1 otherwise,</formula><p>where T 0 is the first time the mouse pointer enters the image and T 1 is the last time it leaves it. Finally, we define the trace segment associated with a word m j as the segment of the mouse trace spanned by the time interval [t 0 j ,t 1 j ] ( <ref type="figure" target="#fig_1">Fig. 3</ref>). Automatic quality control. To ensure high-quality annotations, we devise an automatic quality control mechanism by leveraging the fact that we have a double source of voice transcriptions: the manual one given by the annotators (m) and the automatic one given by the ASR system (a, <ref type="figure" target="#fig_1">Fig. 3</ref>). We take their distance D µ * (a, m) in the optimal alignment µ * as a quality control metric (Eq. (1)). A high value of D µ * indicates large discrepancy between the two transcriptions, which could be caused by the annotator having wrongly transcribed the text, or due to the ASR failing to recognize the annotators' spoken words. In contrast, a low value of D µ * indicates that the transcription is corroborated by two sources. In practice, we manually analyzed a large number of annotations at different values of D µ * and choose a specific threshold below which essentially all transcriptions were correct. We discarded all annotations above this threshold.</p><p>In addition to this automatic quality control, we also evaluate the quality of the annotations in terms of semantic accuracy, visual grounding accuracy, and quality of manual voice transcription (Sec. 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Dataset Collection, Quality, and Statistics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset collection</head><p>Image Sources and Scale. We annotated a total of 848,749 images with Localized Narratives over 4 datasets: (i) COCO <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b10">11]</ref> (train and validation, 123k images); (ii) Flickr30k <ref type="bibr" target="#b65">[66]</ref> (train, validation, and test, 32k); (iii) ADE20K <ref type="bibr" target="#b68">[69]</ref> (train and validation, 20k); (iv) Open Images (full validation and test, 167k, and part of train, 504k). For Open Images, to enable cross-modal applications, we selected images for which object segmentations <ref type="bibr" target="#b5">[6]</ref>, bounding boxes or visual relationships <ref type="bibr" target="#b32">[33]</ref> are already available. We annotated 5,000 randomly selected COCO images with replication 5 (i.e. 5 different annotators annotated each image). Beyond this, we prioritized having a larger set covered, so the rest of images were annotated with replication 1. All analyses in the remainder of this section are done on the full set of 849k images, unless otherwise specified.</p><p>Annotation Cost. Annotating one image with Localized Narratives takes 144.7 seconds on average. We consider this a relatively low cost given the amount of information harvested, and it allows data collection at scale. Manual transcription takes up the majority of the time (104.3 sec., 72%), while the narration step only takes 40.4 seconds (28%). In the future, when ASR systems improve further, manual transcription could be skipped and Localized Narratives could become even faster thanks to our core idea of using speech.</p><p>To put our timings into perspective, we can roughly compare to Flickr30k Entities <ref type="bibr" target="#b44">[45]</ref>, which is the only work we are aware of that reports annotation times. They first manually identified which words constitute entities, which took 235 seconds per image. In a second stage, annotators drew bounding boxes for these selected entities, taking 408 seconds (8.7 entities per image on average). This yields a total of 643 seconds per image, without counting the time to write the actual captions (not reported). This is 4.4× slower than the total annotation cost of our method, which includes the grounding of 10.8 nouns per image and the writing of the caption. The Visual Genome <ref type="bibr" target="#b30">[31]</ref> dataset was also annotated by a complex multi-stage pipeline, also involving drawing a bounding box for each phrase describing a region in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Dataset Quality</head><p>To ensure high quality, Localized Narratives was made by 156 professional annotators working full time on this project. Annotator managers did frequent manual inspections to keep quality consistently high. In addition, we used an automatic quality control mechanism to ensure that the spoken and written transcriptions match (Sec. 3 -Automatic quality control). In practice, we placed a high quality bar, which resulted in discarding 23.5% of all annotations. Below we analyze the quality of the annotations that remained after this automatic discarding step (all dataset statistics reported in this paper are after this step too).</p><p>Semantic and Transcription Accuracy. In this section we quantify (i) how well the noun phrases and verbs in the caption correctly represent the objects in the image (Semantic accuracy) and (ii) how well the manually transcribed caption matches the voice recording (Transcription accuracy). We manually check every word in 100 randomly selected Localized Narratives on COCO and log each of these two types of errors. This was done carefully by experts (authors of this paper), not by the annotators themselves (hence an independent source).</p><p>In terms of semantic accuracy, we check every noun and verb in the 100 captions and assess whether that object or action is indeed present in the image. We allow generality up to a base class name (e.g. we count either "dog" or "Chihuahua" as correct for a Chihuahua in the image) and we strictly enforce correctness (e.g. we count "skating" as incorrect when the correct term is "snowboarding" or "bottle" in the case of a "jar"). Under these criteria, semantic accuracy is very high: 98.0% of the 1,582 nouns and verbs are accurate.</p><p>In terms of transcription accuracy, we listen to the voice recordings and compare them to the manual transcriptions. We count every instance of (i) a missing word in the transcription, (ii) an extra word in the transcription, and (iii) a word with typographical errors. We normalize these by the total number of words in the 100 captions (4,059). This results in 3.3% for type (i), 2.2% for (ii), and 1.1% for (iii), showing transcription accuracy is high.</p><p>Localization Accuracy. To analyze how well the mouse traces match the location of actual objects in the image, we extract all instances of any of the 80 COCO object classes in our captions (exact string matching, 600 classes in the case of Open Images). We recover 146,723 instances on COCO and 374,357 on Open Images train. We then associate each mouse trace segment to the closest ground-truth box of its corresponding class. <ref type="figure">Figure 4</ref> displays the 2D histogram of the positions of all trace segment points with respect to the closest box ( ), normalized by box size for COCO. We observe that most of the trace points are within the correct bounding box (the figure for Open Images is near-identical, see App. C).</p><p>We attribute the trace points that fall outside the box to two different effects. First, circling around the objects is commonly used by annotators ( <ref type="figure" target="#fig_3">Fig. 1 and  Fig. 6</ref>). This causes the mouse traces to be close to the box, but not inside it. Second, some annotators sometimes start moving the mouse before they describe the object, or vice versa. We see both cases as a research opportunity to better understand the connection between vision and language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Dataset Statistics</head><p>Richness. The mean length of the captions we produced is 36.5 words (Tab. 2), substantially longer than all previous datasets, except Stanford Visual Paragraphs <ref type="bibr" target="#b29">[30]</ref> (e.g. 3.5× longer than the individual COCO Captions <ref type="bibr" target="#b10">[11]</ref>). Both Localized Narratives and Stanford Visual Paragraphs describe an image with a whole paragraph, as opposed to one sentence <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b65">66]</ref>. However, Localized Narratives additionally provide dense visual grounding via a mouse trace segment for each word, and has annotations for 40× more images than Stanford Visual Paragraphs (Tab.2).</p><p>We also compare in terms of the average number of nouns, pronouns, adjectives, verbs, and adpositions (prepositions and postpositions, Tab. 3). We determined this using the spaCy <ref type="bibr" target="#b21">[22]</ref> part-of-speech tagger. Localized Narratives has a higher occurrence per caption for each of these categories compared to most previous datasets, which indicates that our annotations provide richer use of natural language in connection to the images they describe.</p><p>Diversity. To illustrate the diversity of our captions, we plot the distribution of the number of nouns per caption, and compare it to the distributions obtained over previous datasets <ref type="figure">(Fig. 5</ref>). We observe that the range of number of nouns is significantly higher in Localized Narratives than in COCO Captions, Flickr30k, Visual Genome, and comparable to Stanford Visual Paragraphs. This poses an additional challenge for captioning methods: automatically adapting the length of the descriptions to each image, as a function of the richness of its content. Beyond nouns, Localized Narratives provide visual grounding for every word (verbs, prepositions, etc.). This is especially interesting for relationship words, e.g. "woman holding ballon" <ref type="figure">(Fig. 1)</ref> or "with a hand under his chin" <ref type="figure" target="#fig_0">(Fig. 2(d)</ref>). This opens the door to a new venue of research: understanding how humans naturally ground visual relationships.</p><p>Diversity in Localized Narratives is present not only in the language modality, but also in the visual modality, such as the different ways to indicate the spatial location of objects in an image. In contrast to previous works, where the   grounding is in the form of a bounding box, our instructions lets the annotator hover the mouse over the object in any way they feel natural. This leads to diverse styles of creating trace segments ( <ref type="figure" target="#fig_3">Fig. 6</ref>): circling around an object (sometimes without even intersecting it), scribbling over it, underlining in case of text, etc. This diversity also presents another challenge: detect and adapt to different trace styles in order to make full use of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Controlled Image Captioning</head><p>We now showcase how localized narratives can be used for controlled image captioning. Controlled captioning was first proposed in <ref type="bibr" target="#b12">[13]</ref> and enables a user to specify which parts of the image they want to be described, and in which order. In <ref type="bibr" target="#b12">[13]</ref> the user input was in the form of user-provided bounding boxes. In this paper we enable controllability through a mouse trace, which provides a more intuitive and efficient user interface. One especially useful application for controlled captioning is assistive technology for people with imperfect vision <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b67">68]</ref>, who could utilize the mouse to express their preferences in terms of how the image description should be presented.</p><p>Task definition. Given both an image and a mouse trace, the goal is to produce an image caption which matches the mouse trace, i.e. it describes the image regions covered by the trace, and in the order of the trace. This task is illustrated by several qualitative examples of our controlled captioning system in <ref type="figure" target="#fig_10">Fig. 11</ref>. In both the image of the skiers and the factory, the caption correctly matches the given mouse trace: it describes the objects which were indicated by the user, in the order which the user wanted.</p><p>Method. We start from a state-of-the-art, transformer-based encoder-decoder image captioning model <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>. This captioning model consumes Faster-RCNN features <ref type="bibr" target="#b47">[48]</ref> of the top 16 highest scored object proposals in the image. The Faster-RCNN module is pre-trained on Visual Genome <ref type="bibr" target="#b30">[31]</ref> (excluding its intersection with COCO). The model uses these features to predict an image caption based on an attention model, inspired by the Bottom-Up Top-Down approach of <ref type="bibr" target="#b2">[3]</ref>. This model is state of the art for standard image captioning, i.e. it produces captions given images alone as input ( <ref type="figure" target="#fig_6">Fig. 8(a)</ref>). We modify this model to also input the mouse trace, resulting in a model that consumes four types of features both at training time and test time: (i)</p><p>In this image I can see ground full of snow and on it I can see few people are standing. Here I can see a flag and on it I can see something is written. I can also see something is written over here.</p><p>In this picture we can see a person skiing on ski boards, in the bottom there is snow, we can see some people standing and sitting here, at the bottom there is snow, we can see a flag here.</p><p>As we can see in the image there is a white color wall, few people here and there and there are food items.</p><p>In this image there are doughnuts kept on the grill. In the front there is a white color paper attached to the machine. On the right side there is a machine which is kept on the floor. In the background there are group of people standing near the table. On the left side there is a person standing on the floor. In the background there is a wall on which there are different types of doughnuts. At the top there are lights.  Evaluation. Our first metric is the standard ROUGE-L <ref type="bibr" target="#b33">[34]</ref>. This metric determines the longest common subsequence (LCS) of words between the predicted caption and the reference caption, and calculates the F1-score (harmonic mean over precision and recall of words in the LCS). This means ROUGE-L explicitly measures word order. We also measure the F1 score of ROUGE-1, which we term ROUGE-1-F1. This measures the F1-score w.r.t. co-occurring words. Hence ROUGE-1-F1 is the orderless counterpart of ROUGE-L and enables us to separate the effects of caption completeness (the image parts which the user wanted to be described) and word order (the order in which the user wanted the image to be described). For completeness we also report other standard captioning metrics: BLEU-1, BLEU-4 <ref type="bibr" target="#b42">[43]</ref>, CIDEr-D <ref type="bibr" target="#b55">[56]</ref>, and SPICE <ref type="bibr" target="#b1">[2]</ref>. For all measures, a higher number reflects a better agreement between the caption produced by the model and the ground-truth caption written by the annotator.</p><p>We observe that in standard image captioning tasks there typically are multiple reference captions to compare to <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b65">66]</ref>, since that task is ambiguous: it is unclear what image parts should be described and in which order. In contrast, our controlled image captioning task takes away both types of ambiguity, resulting in a much better defined task. As such, in this evaluation we compare</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output 2: Controlled caption</head><p>In this image, we can see a platform, there is a yellow color pot on it. Left side, there is a road, car, few trees we can see on the right side.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output 3: Controlled caption</head><p>This image consists of a car parked on the road. To the top left, there is a car. To the right, there is a footpath on which a fire hydrant is kept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input 2: Image + Trace Input 3: Image + Trace Input 1: Image Output 1: Caption</head><p>There is a fire hydrant and this is road. to a single reference only: given an image plus a human-provided mouse trace, we use its corresponding human-provided caption as reference.</p><p>Results. We perform experiments on the Localized Narratives collected on COCO images, using the standard 2017 training and validation splits. To get a feeling of what a trace can add to image captioning, we first discuss the qualitative examples in <ref type="figure" target="#fig_6">Fig. 11 and 8</ref>. First of all, the trace focuses the model attention on specific parts of the image, leading it to mention objects which would otherwise be missed: In the top-left image of <ref type="figure" target="#fig_10">Fig. 11</ref>, the trace focuses attention on the skiers, which are identified as such (in contrast to the top-right). Similarly, the top-left and right of <ref type="figure" target="#fig_10">Fig. 11</ref>, using the trace results in focusing on specific details which leads to more complete and more fine-grained descriptions (e.g. doughnuts, grill, machine, lights). Finally in <ref type="figure" target="#fig_6">Fig. 8a</ref>, the standard captioning model misses the car since it is not prominent in the image. In <ref type="figure" target="#fig_6">Fig 8b and c</ref> instead, the augmented model sees both traces going over the car and produces a caption including it. In this same figure, we can also see that different traces lead to different captions. These results suggests that conditioning on the trace helps with covering the image more completely and highlighting specific objects within it. At the same time, we can see in all examples that the trace order maps nicely to the word order in the caption, which is the order the user wanted. <ref type="table" target="#tab_7">Table 4</ref> shows quantitative results. Compared to standard captioning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9]</ref>, all metrics improve significantly when doing controlled captioning using the mouse trace. BLEU-4 and CIDEr-D are particularly affected and improve by more than 3×. ROUGE-1-F1 increased from 0.479 for standard captioning to 0.607 for controlled captioning using the full mouse trace. Since ROUGE-1-F1 ignores word order, this increase is due to the completeness of the caption only: it indicates  that using the mouse trace enables the system to better describe those parts of the image which were indicated by the user. Switching from ROUGE-1-F1 to ROUGE-L imposes a word order. The standard captioning model yields a ROUGE-L of 0.317, a drop of 34% compared to ROUGE-1-F1. Since standard captioning does not input any particular order within the image (but does use a linguistically plausible ordering), this drop can be seen as a baseline for not having information on the order in which the image should be described. When using the mouse trace, the controlled captioning model yields a ROUGE-L of 0.483, which is a much smaller drop of 20%. This demonstrates quantitatively that our controlled captioning model successfully exploits the input trace to determine the order in which the user wanted the image to be described. Overall, the controlled captioning model outperforms the standard captioning model by 0.166 ROUGE-L on this task (0.483 vs 0.317).</p><p>Ablations. We perform two ablations to verify whether most improvements indeed come from the mouse trace itself, as opposed to the other features we added. Starting from standard captioning, we add the locations of the object proposals from which the model extracts visual features (Tab. 4, "+ proposal locations", feature (ii)). This has negligible effects on performance, suggesting that this model does not benefit from knowing where in the image its appearance features (i) came from. Next, we add the trace time duration (Tab. 4, "+ mouse trace duration"). This gives an indication of how long the caption the user wants should be. This brings minor improvements only. Hence, most improvements come when using the full mouse trace, demonstrating that most information comes from the location and order of the mouse trace (Tab. 4, controlled captioning).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary.</head><p>To summarize, we demonstrated that using the mouse trace leads to large improvements when compared to a standard captioning model, for the task of controlled captioning. Importantly, we do not claim the resulting captions are better in absolute terms. Instead, they are better fitting what the user wanted, in terms of which parts of the image are described and in which order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>This paper introduces Localized Narratives, an efficient way to collect image captions in which every single word is visually grounded by a mouse trace. We annotated 849k images with Localized Narratives. Our analysis shows that our data is rich and provides accurate grounding. We demonstrate the utility of our data through controlled image captioning using the mouse trace. labeled with these classes, and generates an image. In this section we exploit Localized Narratives as a natural interface for producing these segmentation maps efficiently, as the user can specify both the location and class label of the desired image elements at the same time, and can intuitively specify elements in their order of importance.</p><p>Localized Narrative to Semantic Segmentation Map. For this application we need to convert the labelled traces into an appropriate segmentation map. We found that scene elements should have a realistic shape for SPADE to produce a pleasing image. Furthermore, SPADE deals poorly with maps which consist mostly of unlabelled pixels. To overcome this, we first collect masks for 1000 instances of each class from the COCO-stuff training set (both object and background classes). Given a trace segment with a class label, we first create its convex hull. We then compare it to all training instances of the same class and select the mask with the highest spatial overlap. This mask has a natural shape since it comes from a real instance.</p><p>Equipped with these retrieved masks, we construct a semantic segmentation map. We start from an empty map where all pixels are unlabelled, and iteratively add masks in the same order as the trace segments. An object mask is pasted on top of the current map, overwriting any previously labelled pixels. A background mask only overwrites pixels labeled as another background class. This approach results in using masks that cover more surface compared to the input trace segments, which helps reducing the surface of unlabelled pixels.</p><p>Results. <ref type="figure" target="#fig_8">Figure 9</ref> shows seven examples created based on Localized Narratives.</p><p>In both examples, the images get increasingly complex as the Localized Narrative continues, while also preserving previous details. In the first example, the closed boat becomes open once the user indicated that a person should be visible on the boat. Moreover, the addition of the mountain alters the appearance of the water. In the second example, adding the clouds effectively changes the weather conditions and therefore the illumination. The other examples follow similar patterns.</p><p>Conclusions. We demonstrated that Localized Narratives can be used for image generation. Since we kept the pre-trained SPADE <ref type="bibr" target="#b43">[44]</ref> model unmodified and only used traces to create segmentation maps, we do not believe our framework generates better images. Instead, we demonstrated that we can generate images incrementally with an intuitive interface. More importantly, while we now only generated nouns, Localized Narratives opens up the possibility to also consider adjectives such as red or old and verbs such as holding and riding. We feel this presents exciting and challenging new research opportunities.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Qualitative Examples for Controlled Image Captioning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Traditional Captioning Controlled Captioning</head><p>This is a black and white picture. Here we can see clocks on the pole. In the background there is a building and this is sky.</p><p>In the center of the image there is a black pole to which clocks are placed. At the bottom of the image, we can see a group of people walking on the road. In the background, there is a building.</p><p>In this image, there is snow on the ground which is in white color, in the middle there is a person standing on the ski board and wearing a red color jacket, in the background there are some green color plants.</p><p>Here in this picture we can see a person skiing on snow with ski board on her legs and she is also wearing gloves, goggles and a helmet on her and we can see the ground is covered with snow over there. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Traditional Captioning Controlled Captioning</head><p>In this picture we can see a man wore jacket holding bicycle with his hand and beside to him we can see rocks, water, ship and in the background we can see sky.</p><p>In this image we can see a man standing on the left side. He is holding a bicycle in his hand. Here we can see stones on the right side. Here we can see a ship on the top right side. Here we can see a tower on the left side. This is a sky.</p><p>In this picture we can see a group of persons standing on the ground and in the background we can see a building, trees, sky.</p><p>A person is standing wearing a black dress and holding a umbrella. Behind her there are other people standing. At the left and right there are kites. There are trees at the back. C Localization accuracy on Open Images <ref type="figure" target="#fig_0">Figure 12</ref> shows the histograms of mouse trace segment locations on COCO (left) and Open Images (right) with respect to the closest box of the relevant class (The main paper also shows the histogram for COCO). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Controlled Image Captioning Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Method and Training Details</head><p>Our transformer-based encoder-decoder image captioning model follows the architecture in <ref type="bibr" target="#b8">[9]</ref> with a few minor differences. First, we set the number of Transformers' layers for both the encoder and the decoder to 2 instead of 6. Second, our projection layers also consist of layer normalization <ref type="bibr" target="#b4">[5]</ref> (Sec. D.2). Third, we set the maximum number of iterations to 150k, much smaller than the 2M used in that work. Finally, we allow the maximum number of target captions to be as long as 225 to account for the longer nature of the narration. Besides above, our input features are standard regional Faster R-CNN <ref type="bibr" target="#b47">[48]</ref> features: no ultra-finegrained, global, or entity features are involved. We will describe how we represent these and additional features in Section D.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Representations of visual and trace features</head><p>Recall from the main text that our model consumes up to four types of features: (i) Faster R-CNN features of the automatically-detected top object proposals, representing their semantic information; (ii) the coordinate and size features of these proposals, representing the location of the detected objects. (iii) the total time duration of the mouse trace, capturing information about the expected length of the full image description. (iv) the position of the mouse trace as it moves over the image, representing the visual grounding. To create this representation, we first divide the mouse trace evenly into pseudo-segments based on the prior median word duration (0.4 sec over the whole training set). We then represent each pseudo-segment by its encapsulating bounding box, resulting in a set of features which take the same form as (ii).</p><p>Visual Features. Faster R-CNN features (i) are represented by a sequence of R=16 2,048D vectors: f 1 , f 2 , . . . , f R (output by the Faster R-CNN), which are later projected onto a 512D vector and followed by layer normalization.</p><p>We represent the location of detected objects (ii) with a sequence of 5D vectors: p 1 , p 2 , . . . , p R . Each vector contains numbers between 0 and 1 corresponding to the top-left x and y coordinates, the bottom-right x and y coordinates, and the area with respect to the whole image. We project it to a 512D space as for (i) above.</p><p>To construct a representation of (i + ii), we add the projected and normalized vectors from each source and apply another layer normalization to the resulting vector, leading to a sequence of R 512D vectors.</p><p>For the visual features above, we do not use "time" positional encoding such that the model is permutation-invariant to the sequence vectors.</p><p>Trace Features. As mentioned in the main text, the mouse trace coordinates are uniformly divided into a 0.4-second pseudo-segments of trace coordinates and then converted into a series of corresponding bounding boxes. Thus, we now have a sequence of 5D vectors: q 1 , q 2 , . . . , q T , where q j has the same form as (ii).</p><p>Each box corresponds to the smallest region that covers the trace, which might potentially not cover the whole object. To mitigate this, we extend the box in each direction by the offset δ. To represent (iii), we set δ = 1.0 such as q j 's become all [0, 0, 1, 1, 1] (the region of the whole image). In other words, all the trace position information is dismissed, leaving only the total time duration of the mouse trace. On the other hand, setting δ = 0.1 gives the (iii + iv) signal. After the transformation, we have a sequence of transformed 5D vectors: q 1 , q 2 , . . . , q T , which are later projected onto a 512D vector and followed by layer normalization.</p><p>Different from the visual features, each trace comes with the notion of "time" -the order of the regions that are derived from traces matters. Thus, we construct such a time representation sinusoids(1), sinusoids(2), . . . , sinusoids(T ), where sinusoids(j) is a 512D vector of j based on sine and cosine functions of different frequencies <ref type="bibr" target="#b54">[55]</ref>. Similarly to (i + ii), when combining the the trace features with sinusoids, we add the vectors from each source and apply another layer normalization to the resulting vector. In the end, we have a sequence of T 512D vectors.</p><p>Combining Visual and Trace Features. To construct (i + ii + iii) or (i + ii + iii + iv), we simply concatenate the "visual feature" sequence and the "trace feature" sequence and use the result as the input to the model.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Sample annotations from (a) COCO Captions<ref type="bibr" target="#b10">[11]</ref>, (b) Flickr30k Entities<ref type="bibr" target="#b44">[45]</ref>, (c) Visual Genome<ref type="bibr" target="#b30">[31]</ref>, and (d) Localized Narratives (Ours). For clarity, (b) shows a subset of region descriptions and (d) shows a shorter-than-average Localized Narrative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>Localized Narratives annotation: We align the automatic transcription (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :Fig. 5 :</head><label>45</label><figDesc>Mouse trace segment locations on COCO with respect to the closest box of the relevant class ( Distribution of number of nouns per caption. As inTable 3, these counts are per individual caption.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 :</head><label>6</label><figDesc>Examples of mouse trace segments and their corresponding word(s) in the caption with different pointing styles: circling, scribbling, and underlining.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Qualitative results for controlled image captioning. Gradient indicates time. Captioning controlled by mouse traces (left) and without traces (right). The latter misses important objects: e.g. skiers in the sky, doughnuts -all in bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Faster R-CNN features of the automatically-detected top object proposals, representing their semantic information; (ii) the coordinate and size features of these proposals, representing the location of the detected objects. (iii) the total time duration of the mouse trace, capturing information about the expected length of the full image description. (iv) the position of the mouse trace as it moves over the image, representing the visual grounding. To create this representation, we first divide the mouse trace evenly into pseudo-segments based on the prior median word duration (0.4 sec over the whole training set). We then represent each pseudo-segment by its encapsulating bounding box, resulting in a set of features which take the same form as (ii). This new model takes an image plus a mouse trace as input and produces the caption that the user is interested in. More technical details in App. D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :</head><label>8</label><figDesc>Qualitative results for controlled image captioning. Standard (a) versus controlled captioning (b) and (c). (a) misses important objects such as the car or the footpath. In (b) and (c) the controlled output captions adapt to the order of the objects defined by the trace. Gradient indicates time. More in the App. B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 and</head><label>10</label><figDesc>Figure 11show additional qualitative examples of controlled versus classical image captioning on our data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 :</head><label>9</label><figDesc>Seven examples (one per row) of image generation using mouse traces. New image elements are iteratively added (from left to right) using a noun and its associated trace segment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 :</head><label>10</label><figDesc>Controlled Captioning Qualitative Examples 1: Traditional captioning where the input is only the image (left) versus our captioning controlled by mouse traces where the mouse traces are also an input to the model (right). Gradient indicates time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 11 :</head><label>11</label><figDesc>Controlled Captioning Qualitative Examples 2: Traditional captioning where the input is only the image (left) versus our captioning controlled by mouse traces where the mouse traces are also an input to the model (right). Gradient indicates time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 12 :</head><label>12</label><figDesc>Histograms of mouse trace segment locations on COCO (left) andOpen Images (right) with respect to the closest box of the relevant class ( ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Tasks enabled by Localized Narratives.</figDesc><table><row><cell>Each row represents different</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Datasets connecting vision and language via image captioning, com-</figDesc><table><row><cell>pared with respect to their type of grounding, scale, and caption length. Num. captions</cell></row><row><cell>is typically higher than num. images because of replication (i.e. several annotators writ-</cell></row><row><cell>ing a caption for the same image).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Richness</figDesc><table><row><cell>Ship</cell><cell>Open land with some grass on it</cell><cell>Main stairs</cell></row></table><note>of individual captions of Localized Narratives versus previous works. Please note that since COCO Captions and Flickr30K have replication 5 (and Visual Genome also has a high replication), counts per image would be higher in these datasets. However, many of them would be duplicates. We want to highlight the richness of captions as units and thus we show word counts averaged over individual captions.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Controlled image captioning results on the COCO validation set, versus standard (non-controlled) captioning, and two ablations.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>The appendices are organized as follows:</p><p>Appendix A presents a demonstration of application of Localized Narratives for image generation. The user describes the image they want by means of a Localized Narrative and the method generates an image that matches the description. Appendix B provides additional qualitative examples for the controlled image captioning application. Appendix C provides an additional quantitative plot of the localization accuracy of the mouse trace in Localized Narratives for Open Images. It was suppressed from the main paper due to space limitations. Appendix D provides additional technical details on the framework that we use for controlled image captioning.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Deep speech 2: End-to-end speech recognition in English and Mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">SPICE: semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">VQA: visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Large-scale interactive object segmentation with human annotators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">VizWiz: nearly real-time answers to visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Bigham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jayant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tatarowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23nd annual ACM symposium on User interface software and technology</title>
		<meeting>the 23nd annual ACM symposium on User interface software and technology</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">COCO-Stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decoupled box proposal and featurization with ultrafine-grained semantic labels improve image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Changpinyo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP-IJCNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO captions: Data collection and evaluation server. arXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<title level="m">Visual referring expression recognition: What do systems actually learn? In: NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Towards Cost-Effective and Performance-Aware Vision Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>ETH Zurich</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The EPIC-KITCHENS dataset: Collection, challenges and baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural sequential phrase grounding (seqground)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Google cloud speech-to-text API</title>
		<ptr target="https://cloud.google.com/speech-to-text/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Speech recognition with deep recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>ICASSP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Efficient object annotation via speaking and pointing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Fast Object Class Labelling via Speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Jointly Discovering Visual Objects and Spoken Words from Raw Sensory Input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Surís</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Montani</surname></persName>
		</author>
		<idno>spacy. io</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">GQA: A new dataset for real-world visual reasoning and compositional question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Attention and effort</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kahneman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient neural audio synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Noury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Casagrande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Lockhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Stimberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Dense relational captioning: Triplestream networks for relationship-based captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">A hierarchical approach for generating descriptive image paragraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The symmetric time-warping problem: from continuous to discrete</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Kruskal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Time Warps, String Edits, and Macromolecules -The Theory and Practice of Sequence Comparison</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>chap. 4. CSLI Publications</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.00982</idno>
		<title level="m">The Open Images Dataset V4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common objects in context</title>
		<imprint>
			<publisher>ECCV</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Attention correctness in neural image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Neural baby talk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Samplernn: An unconditional end-to-end neural audio generation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sotelo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Wavenet: A generative model for raw audio. arXiv 1609</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">3499</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Multimodal interfaces. The human-computer interaction handbook: Fundamentals, evolving technologies and emerging applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oviatt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Bleu: A method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Semantic image synthesis with spatially-adaptive normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="93" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">The pytorch-kaldi speech recognition toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parcollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICASSP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS. pp</title>
		<imprint>
			<biblScope unit="page" from="217" to="225" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Object hallucination in image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Burns</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Taking a HINT: Leveraging explanations to make vision and language models more grounded</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Text2scene: Generating compositional scenes from textual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vaidyanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Prud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pelz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">O</forename><surname>Alm</surname></persName>
		</author>
		<title level="m">SNAG : Spoken Narratives and Gaze Dataset</title>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Object Referring in Visual Scene with Spoken Language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">CIDEr: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Show and tell: Lessons learned from the 2015 mscoco image captioning challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on PAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="652" to="663" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Highresolution image synthesis and semantic manipulation with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<ptr target="https://google.github.io/localized-narratives" />
		<title level="m">Website: Localized Narratives Data and Visualization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Automatic alt-text: Computergenerated image descriptions for blind users on a social network service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wieland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Farivar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Supported Cooperative Work and Social Computing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICML</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Robertson</surname></persName>
		</author>
		<title level="m">ParaCNN: Visual paragraph generation via adversarial twin contextual CNNs. arXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Semantics disentangling for text-to-image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Context and attribute grounded dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="67" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Multimodal transformer with multi-view visual representation for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<idno>arXiv 1905.07841</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">The effect of computer-generated descriptions on photo-sharing experiences of people with visual impairments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Azenkot</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>ACM on Human-Computer Interaction</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="321" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Grounded video description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Encoder-agnostic adaptation for conditional language generation. arXiv (2019) wellstudied application</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gehrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<imprint/>
	</monogr>
	<note>10,24,44,59</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">This opens up a new and intuitive way for the user to provide guidance to the image generation process. We start from SPADE [44], which is an existing, state-of-the-art framework for generating images conditioned on a pixel-wise segmentation map</title>
		<imprint/>
	</monogr>
	<note>However, while such segmentation maps give control over the image to be synthesized, they do not provide a natural interface for the user. In this section, we show how we can use labelled mouse traces to generate images. We use their publicly available model that is pre-trained on COCO-stuff [8,35. which features 182 semantic classes, including object and background classes (stuff</note>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">At test time, the model takes as input a segmentation map where pixels are</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
