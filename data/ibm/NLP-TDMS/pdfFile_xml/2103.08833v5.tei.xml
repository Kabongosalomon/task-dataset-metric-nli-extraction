<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Skeleton Aware Multi-modal Sign Language Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyao</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Bin Sun §</roleName><forename type="first">Lichen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Bai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Skeleton Aware Multi-modal Sign Language Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>2021 Looking at People Large Scale Signer Independent Isolated SLR Challenge. Our code is available at https://github.com/jackyjsy/ CVPR21Chal-SLR</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:36+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Sign language is commonly used by deaf or speech impaired people to communicate but requires significant effort to master. Sign Language Recognition (SLR) aims to bridge the gap between sign language users and others by recognizing signs from given videos. It is an essential yet challenging task since sign language is performed with the fast and complex movement of hand gestures, body posture, and even facial expressions. Recently, skeleton-based action recognition attracts increasing attention due to the independence between the subject and background variation. However, skeleton-based SLR is still under exploration due to the lack of annotations on hand keypoints. Some efforts have been made to use hand detectors with pose estimators to extract hand key points and learn to recognize sign language via Neural Networks, but none of them outperforms RGB-based methods. To this end, we propose a novel Skeleton Aware Multi-modal SLR framework (SAM-SLR) to take advantage of multi-modal information towards a higher recognition rate. Specifically, we propose a Sign Language Graph Convolution Network (SL-GCN) to model the embedded dynamics and a novel Separable Spatial-Temporal Convolution Network (SSTCN) to exploit skeleton features. RGB and depth modalities are also incorporated and assembled into our framework to provide global information that is complementary to the skeleton-based methods SL-GCN and SSTCN. As a result, SAM-SLR achieves the highest performance in both RGB (98.42%) and RGB-D (98.53%) tracks in 2021 Looking at People Large Scale Signer Independent Isolated SLR Challenge. Our code is available at https://github.com/jackyjsy/ CVPR21Chal-SLR arXiv:2103.08833v5 [cs.CV] 2 May 2021</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sign language <ref type="bibr" target="#b13">[14]</ref> is a visual language performed with the dynamic movement of hand gestures, body posture, and facial expressions. It is an effective and helpful approach for § Equal contribution This work was supported by the U.S. Army Research Office Award W911NF-17-1-0367. deaf and speech-impaired people to communicate with others. Understanding and utilizing sign language requires a considerable time of learning and training which is not practical and feasible for the public. Moreover, sign language is affected by the language <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b66">67]</ref> (e.g., English and Chinese) and culture <ref type="bibr" target="#b34">[35]</ref> which further limits its popularization potential. As machine learning and computer vision achieved great progress in the past decade, it is important to explore sign language recognition (SLR) which automatically interprets sign language and helps deaf-mute people communicate smoothly with others in their daily lives.</p><p>Compared with conventional action recognition, SLR is a more challenging problem. First, sign language requires both global body motion and delicate arm/hand gestures to distinctly and accurately express its meaning. Facial expression can be utilized to express emotions as well. Similar gestures can even impose various meanings depending on the number of repetitions. Second, different signers may perform sign language differently (e.g., speed, localism, left-hander, right-hander, body shape), making SLR more challenging. Collecting more samples from as many signers as possible is desired yet expensive.</p><p>Traditional SLR methods mainly deploy handcrafted features such as HOG <ref type="bibr" target="#b70">[71]</ref> and SIFT <ref type="bibr" target="#b32">[33]</ref>) associated with conventional classifiers like kNN and SVM <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b33">34]</ref>. As deep learning achieves significant progress, general video and time-series representation learning methods (e.g., RNN, LSTM) and effective action recognition frameworks (e.g., 3D CNNs) are first explored for SLR tasks in <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b57">58]</ref>. To more effectively capture the local motion information, attention modules are combined with other modules for higher accuracy <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref>. Besides, <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44]</ref> use semantic detection/segmentation models to explicitly guide the recognition network in a two-stage pipeline.</p><p>Recently, skeleton-based methods have become popular in action recognition tasks <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b52">53]</ref> and draw increasing attention due to their strong adaptability to the dynamic circumstances and complicated background. As the skeleton-based methods provide complementary information to the RGB modality, their ensemble results further improve the overall performance. However, some deficiencies hinder their extension to the SLR task. Those skeleton-based action recognition methods rely on groundtruth skeleton annotations provided by motion capture systems, restricting themselves to fewer available datasets captured in controlled environments. Besides, most motion capture systems only consider the main body coordinates that do not provide ground truth annotations for hands. Those skeleton data contains insufficient information to recognize sign language, which contains dynamic motions of hand gestures interacted with other body parts. <ref type="bibr" target="#b64">[65]</ref> attempts to obtain body skeleton and hand poses using separate models and proposes using an RNN-based model to recognize the sign language. But their obtained hand poses are unreliable, and the RNN base model cannot properly model the human skeleton dynamics.</p><p>Inspired by the recent development on whole-body pose estimation <ref type="bibr" target="#b22">[23]</ref>, in this paper, we propose a novel skeletonbased SLR method using whole-body keypoints and features provided by pretrained whole-body pose estimators. We design a new spatio-temporal skeleton graph for SLR and propose a Sign Language Graph Convolution Network (SL-GCN) to model the dynamics embedded. To fully exploit the information in whole-body keypoints, we propose a novel Separable Spatial-Temporal Convolution Network (SSTCN) for the whole-body skeleton features. Studies on action recognition have revealed that multi-modal data complement each other and provide extra information in recognition. To further improve the recognition rate, we propose a Skeleton Aware Multi-modal SLR framework (SAM-SLR) to ensemble the proposed skeleton-based method with other modalities in both RGB and RGB-D scenarios. Our main contributions can be summarized as follows:</p><p>• We construct a novel skeleton graph designed for SLR using whole-body keypoints and graph reduction. Our method utilizes pretrained whole-body pose estimator and requires no extra annotation effort.</p><p>• We propose SL-GCN to extract information from the whole-body skeleton graph. To our best knowledge, this is the first successful attempt to tackle the SLR task using whole-body skeleton graphs.</p><p>• We propose a novel SSTCN to further exploit wholebody skeleton features, which can significantly improve the accuracy on whole-body keypoints comparing with the traditional 3D convolution.</p><p>• We propose a SAM-SLR framework for RGB and RGB-D based SLR, which learns from six modalities and achieves the state-of-the-art performance in AUTSL dataset. Our proposed method ranked 1st in both RGB and RGB-D tracks in the CVPR-21 Challenge on Isolated SLR <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Sign Language Recognition (SLR) achieves significant progress and obtained high recognition accuracy in recently years due to the development on practical deep learning architectures and the surge of computational power <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44]</ref>. One remaining challenge of SLR is to capture global body motion information and local arm/hand/facial expression simultaneously. <ref type="bibr" target="#b35">[36]</ref> proposes a multi-scale and multi-modal framework which captures spatial information at particular spatial scales. An autoencoder framework with connectionist-based recognition module is proposed in <ref type="bibr" target="#b38">[39]</ref> for sequence modelling. <ref type="bibr" target="#b25">[26]</ref> introduces an end-to-end embedding of a convolutional module into a Hidden-Markov-Models, while interpreting the prediction results in a Bayesian framework. <ref type="bibr" target="#b19">[20]</ref> proposes a 3D-convolutional neural network associated with attention module which learns the spatio-temporal features from raw video. <ref type="bibr" target="#b37">[38]</ref> incorporates bidirectional recurrence and temporal convolutions together which demonstrates the effectiveness of temporal information in gesture related tasks. <ref type="bibr" target="#b51">[52]</ref> utilizes CNN, Feature Pooling Module, and LSTM Networks associated with adaptive weights to obtain distinctive representations. <ref type="bibr" target="#b20">[21]</ref> designs a Hierarchical Attention Network with Latent Space to eliminate the preprocessing of temporal segmentation. However, these methods mainly consider pure visual feature which is not effective enough to explicitly capture the body movement and hand gesture. <ref type="bibr" target="#b26">[27]</ref> designs a pose-based temporal graph convolution networks that model spatial and temporal dependencies in human pose trajectories. <ref type="bibr" target="#b9">[10]</ref> adopts deep CNNs with stacked temporal fusion layers as the feature extraction module, and bidirectional RNNs as the sequence learning module. <ref type="bibr" target="#b14">[15]</ref> proposes a hierarchical-LSTM (HLSTM) autoencoder model with visual content and word embedding for translation. It tackles different granularities by conveying spatio-temporal transitions among frames, clips and viseme units. These methods are still not effective enough to capture the complete motion information.</p><p>Skeleton Based Action Recognition mainly focuses on exploring distinctive patterns from human joint position and motion. Skeleton data can be utilized individually to perform efficient action recognition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>. On the other hand, it can also be collaborated with other modalities to achieve multi-modal learning aiming for higher recognition performances <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b71">72]</ref>. RNNs are once popular for modeling skeleton data <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b48">49]</ref>. Recently, <ref type="bibr" target="#b65">[66]</ref> is the first attempt to designed a graph-based approach, called ST-GCN, to model the dynamic patterns in skeleton data via a Graph Convolutional Network (GCN). Such approach draws much attention and a few improvements have been developed as well <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b52">53]</ref>. Specifically, <ref type="bibr" target="#b27">[28]</ref> propose a AS-GCN to dig the latent joint connections to boost the recognition performance. A two-stream approach is presented in <ref type="bibr" target="#b45">[46]</ref> and further extended to four streams in <ref type="bibr" target="#b46">[47]</ref>. <ref type="bibr" target="#b6">[7]</ref> develops a decoupling GCN to increase the model capacity with no extra computational cost. ResGCN is proposed in <ref type="bibr" target="#b52">[53]</ref> which adopts a bottleneck structure from ResNet <ref type="bibr" target="#b16">[17]</ref> to reduce parameters while increasing model capacity. However, skeleton based SLR is still under-explored. An attempt to extend ST-GCN to SLR directly <ref type="bibr" target="#b11">[12]</ref> has been unsuccessful that only achieves around 60% recognition rate on 20 sign classes, which is significantly lower than handcrafted features.</p><p>Multi-modal Approach aims to explore action data captured from different resources/devices to improve the final performance. It is based on the assumption that different modalities contain unique motion information which could potentially complement each other and eventually obtain comprehensive and distinctive action representations. View-invariant representation learning framework is proposed in <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b60">61]</ref> to obtain robust representation for downstream tasks. <ref type="bibr" target="#b17">[18]</ref> deploys shared weights network on multi-modal scenario to obtain modality hallucination for image classification task. DA-Net <ref type="bibr" target="#b59">[60]</ref> proposes a viewspecific and a view-independent modules to capture the features and effectively merges the prediction scores together. A feature factorization framework is proposed in <ref type="bibr" target="#b42">[43]</ref> which explores the view shared-specific information for RGB-D action recognition. A cascaded residual autoencoder is designed in <ref type="bibr" target="#b56">[57]</ref> to handle incomplete view classification setting. A super vector is proposed in <ref type="bibr" target="#b3">[4]</ref> to fuse the multiview representations together. <ref type="bibr" target="#b62">[63]</ref> proposes a cross-view generative strategy to explore the latent view distribution connections and a late fusion strategy to effectively learn the prediction correlations. Encouraged by the success of those multi-modal methods, we aim to explore more visual, depth, gesture, and hand modalities jointly to capture information from all aspects and fuse them together via a universal framework to achieve higher performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our approach</head><p>This section will first introduce SL-GCN and SSTCN models based on skeleton keypoints and features, respectively. Then we will present a baseline 3D CNNs model for other modalities. Last, we will introduce our SAM-SLR framework and explain the multi-modal ensemble process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">SL-GCN</head><p>We construct a spatio-temporal graph to model the dynamics of human body skeleton for SLR, and propose a SL-GCN model with attention mechanism to extract motion dynamics from the graph. We also adopt a multi-stream approach to further improve the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Graph Construction and Reduction</head><p>Hand gestures play an important role in performing sign language. For action recognition, researchers tend to use the ground-truth skeleton annotations provided by motion capture system such as Kinect v2 <ref type="bibr" target="#b36">[37]</ref>. Unfortunately, such system does not provide annotations for the hands. We use a pretrained whole-body pose estimation network to provide 133 keypoints estimated from the detected person in videos. A spatio-temporal graph can then be constructed by connecting the adjacent keypoints in the spatial dimension according to the natural connections of human body, and connecting all keypoints to themselves in the temporal dimension. In this graph, the node set V = {v i,t |i = 1, ..., N, t = 1, ..., T } includes all facial landmarks, body skeleton, hands, and feet keypoints. Then an adjacent matrix A in spatial dimension can be constructed as</p><formula xml:id="formula_0">A i,j = 1 if d(v i , v j ) = 1 0 else (1)</formula><p>where d(v i , v j ) calculate the minimum distance (the minimum number of nodes in the shortest path) between skeleton node v i and v j . However, different from the graph used in action recognition which contains a small number of nodes, the large number of nodes and edges in the whole-body skeleton graph introduces a lot of noise to the model. Besides, if two nodes are far away with many nodes in between, it is difficult to learn the interactions between those nodes. Simply using such whole-body skeleton graph containing all the 133 nodes gives a low accuracy in our experiment. Therefore, based on observations on the data and visualizations of GCN activations, we conduct a graph reduction on the whole-body skeleton graph and trim down the 133 nodes to 27 nodes. The remaining graph contains 10 nodes for each hand and 7 nodes for the upper body, which is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(b). Our experiments demonstrate that such graph contain the essential information we need for SLR. Graph reduction results in faster model convergence and significantly higher recognition rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Graph Convolution</head><p>To capture the pattern embedded in the whole-body skeleton graph, we adopt the spatio-temporal GCN in <ref type="bibr" target="#b65">[66]</ref> with spatial partitioning strategy to model the dynamic skeletons. The implementation of spatial GCN can be expressed as</p><formula xml:id="formula_1">x out = D − 1 2 (A + I)D − 1 2 x in W,<label>(2)</label></formula><p>where adjacent matrix A represents intra-body connections and an identity matrix I represents self-connections, D presents the diagonal degree of (A+I), and W is a trainable weight matrix of the convolution. In practice, such GCN is implemented as performing standard 2D convolution and then multiplying the results by D − 1 2 (A+I)D − 1 2 . The temporal GCN can be also implemented as a standard 2D convolution with kernel size k t × 1 that it performs on the temporal dimension with a reception field k t . We adopt a extended variation of the spatial graph convolution called decoupling graph convolution proposed in <ref type="bibr" target="#b6">[7]</ref> to further boost the capacity of GCN. In decoupling graph convolution, the channels of graph features split into G groups and channels in each group share an independent trainable adjacent matrix A. The convolution results of the decoupling groups are concatenated together as the output feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">SL-GCN Block</head><p>Our proposed SL-GCN Block is constructed with decoupled spatial convolutional network, self-attention and graph dropping module inspired by <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b6">7]</ref>. As illustrated in <ref type="figure" target="#fig_1">Figure 2(d)</ref>, a basic GCN block of our proposed SL-GCN network consists of a decoupled spatial convolutional layer (Decouple SCN), a STC (spatial, temporal and channelwise) attention module, a temporal convolutional layer (TCN) and a DropGraph module. The Decouple SCN boosts the GCN modeling capacity without extra cost. The DropGraph module avoid overfitting. The STC attention mechanism consists of a spatial attention module, a temporal attention module and a channel attention module connected in a cascaded configuration, as illustrated in <ref type="figure" target="#fig_1">Figure  2</ref>(e). Our proposed spatio-temporal GCN consists of 10 such GCN blocks. At the end, a global average pooling is applied on both spatial and temporal dimensions before classification using a fully-connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Multi-stream Approach</head><p>Inspired by <ref type="bibr" target="#b46">[47]</ref>, 1st-order representation (joints coordinate), 2nd-order representation (bone vector), and their motion vectors are worth to be investigated for SLR. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>(c), our multi-stream SL-GCN uses joint, bone, joint motion, and bone motion. Bone data are generated by representing joint data in a vector form pointing from source joints to their target joints following the natural connections of human body. The nose node is used as the root joint so that its bone vector is assigned to be zeros. Let the source and target joint be represented as v J i,t = (x i,t , y i,t , s i,t ) and v J j,t = (x j,t , y j,t , s j,t ) where (x, y, s) represents x-y coordinates and confidence score, the bone vectors of the other nodes can be calculated by subtracting their source joint coordinates from their current joint coordinates as v B j,t = (x j,t − x i,t , y j,t − y i,t , s j,t ), for all (i, j) ∈ H where H is the set of naturally connected human body. Motion data are generated by calculating the difference between adjacent frames in both joint and bone streams. Joint motion can be calculated as v JM i,t = (x i,t+1 −x i,t , y i,t+1 −y i,t , s i,t ) and bone motion can be calculated as v BM</p><formula xml:id="formula_2">i,t = v B i,t+1 − v B i,t .</formula><p>We train each stream separately and combine their predicted results by adding them together with weights using the same ensemble strategy described in Section 3.4. We tried to adopt an early fused multi-stream method proposed in Res-GCN <ref type="bibr" target="#b52">[53]</ref> which captures multi-stream features via multiple input blocks capture and concatenates them together afterwards. However, our implementation does not provide better performance, so we stick to the late ensemble method and leave it to be explored in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">SSTCN</head><p>Besides using key point coordinates generated from the whole-body pose network, we also propose a SSTCN model to recognize the sign language from whole-body features. We extract features of 33 keypoints from 60 frames of each video as the input to our model, which contain 1 landmark on the nose, 4 landmarks on mouth, 2 landmarks on shoulders, 2 landmarks on elbows, 2 landmarks on wrists, and 22 landmarks on hands. All the features are down-sampled to 24 × 24 using max pooling. Instead of using 3D convolution, we process the input features with a 2D convolution layer separably, which reduces the parameters and makes it easier to converge. The pipeline is shown in <ref type="figure" target="#fig_4">Figure 4</ref>. There are four stages in total. In the first stage, we reshape the features from 60 × 33 × 24 × 24 to 60 × 792 × 24, and feed them to 1 × 1 convolution layers, which means we only process temporal information in this stage. Then we shuffle the features and divide them into 60 groups, and utilize grouped 3 × 3 convolution to extract temporal and spatial information among the same key point features from different frames. In this stage, temporal information and part of spatial information are processed. In the third stage, the features are shuffled again and divided into 33 groups. We still use grouped 3 × 3 convolution, but only spatial information in each frame is extracted. Finally, a couple of 3 × 3 fully connected layers are implemented to generate prediction features. In the first 3 stages, all the output is added by a residual. Moreover, a dropout layer is deployed in each module to avoid over-fitting <ref type="bibr" target="#b53">[54]</ref>. An ablation study on the effectiveness of SSTCN is shown in Section 4.5. To further improve the performance, we utilize the Swish <ref type="bibr" target="#b40">[41]</ref> activation function, which can be written as:  Since using one-hot labels with cross-entropy loss results in overfitting in some cases <ref type="bibr" target="#b41">[42]</ref>, we adopt the label smoothing technique to alleviate such effect. Mathematically, label smoothing can be represented as</p><formula xml:id="formula_3">f (x) = x · Sigmoid(x).<label>(3)</label></formula><formula xml:id="formula_4">q (k|x) = (1 − )δ k,y + u(k),<label>(4)</label></formula><p>where q (k|x) is a new form of predicted distribution, is a hyper-parameter between 0 and 1, u() is a uniform distribution and k is the number of classes. The cross-entropy loss can then be replaced as</p><formula xml:id="formula_5">H(q , p) = − K k=1 log p(k)q (k) = (1− )H(q, p)+ H(u, p),<label>(5)</label></formula><p>where such representation can be regarded as a combination of penalties to the difference between the predicted distribution with the real distribution and the prior distribution (uniform distribution).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">3D CNNs</head><p>As mentioned in Section 2, studies on action recognition have revealed that multi-modal ensembles can further boost each modality's performance, hence we construct a simple but effective baseline using 3D CNNs for the other modalities of RGB frames, optical flows, depth HHA and depth flow. In our study, we find out that ResNet2+1D <ref type="bibr" target="#b55">[56]</ref>, which decouples spatial and temporal convolution in 3D CNNs and does them one after another, provides the best result among popular 3D CNN architectures. We find that increasing the architecture depth does not improve the performance and makes the network easier to overfit. So in our experiment, we choose ResNet2+1D-18 with weights pretrained on Kinectics dataset <ref type="bibr" target="#b4">[5]</ref> as the backbone network. To further improve the recognition rate, for RGB frames, we pretrain the model on the Chinese Sign Language (CSL) dataset <ref type="bibr" target="#b68">[69]</ref>. We find that pretraining on CSL can improve the model convergence and increase the final accuracy by around 1%. Same as SSTCN, we replace the ReLU activations with Swish activations <ref type="figure" target="#fig_2">(Equation 3</ref>) and use the label smoothing technique with corresponding cross-entropy loss in Equation 4 and 5 to avoid overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Multi-modal Ensemble</head><p>We use a simple ensemble method to ensemble all four modalities above. Specifically, we save the output of the last fully-connected layers of each modality before softmax layer. Those outputs have the size n c where n c is the number of classes. We assign weights to the every modality according to their accuracy on validation set and sum them up with weights as our final predicted score</p><formula xml:id="formula_6">q RGB = α 1 q skel + α 2 q RGB + α 3 q flow + α 4 q feat , (6) q RGB-D =α 1 q skel + α 2 q RGB + α 3 q flow + α 4 q feat + α 5 q HHA + α 6 q depthflow ,<label>(7)</label></formula><p>where q represents the result of each modality, α 1,2,3,4,5,6 are hyper-parameters to be tuned based on ensemble accuracy on validation set. We find the indices of maximum scores as our final predicted classes using an argmax() operator. In our experiments, we use α = [1, 0.9, 0.4, 0.4] for RGB track and α = [1.0, 0.9, 0.4, 0.4, 0.4, 0.1] for RGB-D track. We have tried other ensemble methods such as early fusion or training fully-connected layers to ensemble the final prediction. Despite that, we find that the simplest method we presented above gives us the best accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we present evaluation of our proposed SAM-SLR framework on the AUTSL dataset. We start from a brief introduction about the AUTSL dataset and how we extract the data of all modalities. Then we evaluate our single-modal models and using the validation data compared with the baseline methods. Besides, we demonstrate that the effectiveness of proposed approaches via ablation studies on the model components. After that, we fuse the results from different modalities in both RGB and RGB-D scenarios to show that those modalities complement each other and improve the overall recognition rate. Last, we show our evaluated results on the test set, which ranked the 1st place in the SLR challenge <ref type="bibr" target="#b49">[50]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subsets</head><p>Signers <ref type="table" target="#tab_0">Samples  Training  31  28,142  Validation  5  4,418  Testing  7  3,742  Total  43  36,302   Table 1</ref>. A statistical summary of the balanced AUTSL dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">AUTSL Dataset</head><p>AUTSL <ref type="bibr" target="#b50">[51]</ref> is collected for general SLR tasks in Turkish sign language. Kinect V2 sensor <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b0">1]</ref> is utilized in the collection procedure. Specifically, 43 signers with 20 backgrounds are assigned to perform 226 different sign actions. In general, it contains 38,336 video clips which is split to training, validation, and testing subsets. The statistical summary of the balanced dataset, which is used in the challenge, is listed in <ref type="table">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baseline Methods</head><p>Along with the AUTSL benchmark <ref type="bibr" target="#b50">[51]</ref>, several deep learning based models are proposed. We treat the best model benchmarked in <ref type="bibr" target="#b50">[51]</ref> as well as the SLR challenge leader board as the baseline model here (Baseline RGB and Baseline RGB-D in <ref type="table">Table 6</ref>). Specifically, the model is mainly constructed using CNN + LSTM structure, where 2D-CNN model are used to extract feature for each video frame and bidirectional LSTMs (BLSTM) are adopted on top of the these 2D CNN features to lean their temporal relations. A feature pooling model (FPM) <ref type="bibr" target="#b51">[52]</ref> is plugged in after the 2D CNN model to obtain multi-scale representation of the features. A spacial-temporal attention model <ref type="bibr" target="#b39">[40]</ref> is then built on top of BLSTM features to better focus on important spacial-temporal information for SLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-modal Data Preparation</head><p>Whole-body Pose Keypoints and Features. We use a pretrained HRNet <ref type="bibr" target="#b54">[55]</ref> whole-body pose estimator provided by MMPose <ref type="bibr" target="#b8">[9]</ref> to estimate 133-point whole-body keypoints from the RGB videos and construct the 27-node skeleton graph in Section 3.1.1. As mentioned in Section 3.1.4, we process the graph into four streams (joint, bone, joint motion and bone motion). Randomly sampling, mirroring, rotating, scaling, jittering and shifting are applied as data augmentations. We use a sample length of 150 in our experiment. If a video has lesser frames than 150, we repeat that video until we get 150 frames. Coordinates of keypoints are normalized to <ref type="bibr">[-1,1]</ref>. For skeleton features, as mentioned in 3.2, we choose 33 joint features for each frame. 60 frames will be uniformly sampled from each video. RGB Frames and Optical Flow. All frames of RGB videos are extracted and saved as images for faster parallel loading and processing. We follow the same process  <ref type="table">Table 3</ref>. Ablation studies on SL-GCN using joint stream.</p><p>in <ref type="bibr" target="#b63">[64]</ref> to obtain optical flow features using TVL1 algorithm <ref type="bibr" target="#b67">[68]</ref> implemented with OpenCV and CUDA. The output flow maps of x and y directions are concatenated in channel dimension. RGB frames and optical flow frames are cropped and resized to 256×256 based on the keypoints obtained from whole-body pose estimation. Such cropping and resizing operations are performed on the other imagelike modalities as well. During training, we randomly sample 32 consecutive frames for each video. When testing, we uniformly sample 5 such clips from input videos and average on their predicted score. Depth HHA and Depth Flow. We extract HHA features from depth videos as another modality. HHA features encode depth information into a RGB-like 3-channel output, where HHA stand for horizontal disparity, height above the ground and angle normal. Using HHA instead of using gray-scale depth videos directly enables better understanding of the scene and improves the recognition rate. We observe that the provided depth videos come with a mask. So when generating HHA features, we mask out those regions and fill them with zeros. An example of our extracted HHA with mask can be found in <ref type="figure" target="#fig_2">Figure 3</ref>(c). Black regions are masked out pixels. We treat HHA feature the same way as the RGB frames in data augmentation. Besides, we follow the exactly the same procedure used for RGB to extract optical flow from the depth modality (named depth flow). The depth flow is cleaner and captures different information compared with the RGB flow, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>(e).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance of SL-GCN</head><p>The results of our proposed SL-SLR are reported in Table 2 in terms of Top-1 and Top-5 recognition rate. The joint stream provides the best performance among all four streams, and their ensemble further improve the overall recognition rate, which demonstrates the effectiveness of our proposed multi-stream SL-GCN model using wholebody skeleton graph. Our SL-GCN performs the best among the other single-modality models as shown in Table 6. Another major advantage of the graph based method is that it is much faster to run compared with 3D CNNs using RGB frames, since the data is less complex and requires lower computational operations. Ablation studies on the proposed SL-GCN model is presented in <ref type="table">Table 3</ref>. Our graph reduction technique is the most significant contributor to the performance. Without the graph reduction, the GCN model can hardly learn from the complex dynamics in the skeleton graph with too many nodes and edges. The data augmentation techniques (i.e., random sampling, mirror, rotate, shift, jitter) are also crucial in learning the dynamics embedded, since the GCN model is easy to overfit on the data. The decoupling GCN module, the DropGraph module and the STC attention mechanism all contribute to our final recognition rate as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">SSTCN Results</head><p>In this subsection, the details of training will be presented. Besides, we will show the comparison results with ResNet3D <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> and ResNet2+1D <ref type="bibr" target="#b55">[56]</ref> to show the effectiveness of our model. As shown in <ref type="figure" target="#fig_4">Figure 4</ref>, our model has 4 stages in total. Each stage has two layers. Since the last two layers are fully connected layers that may significantly impact performance due to their redundant parameters, we utilize three ResNet3D and ResNet2+1D modules while implementing ResNet3D ResNet2+1D, respectively. The training loss and the position of dropout layers are mentioned in Section 3.2. The learning rate is 1e − 3 in the beginning with weight decay 1e−4. At epoch 50, the learning rate is set as 1e − 4, and the weight decay is set to 0. At epoch 100, the learning rate is set as 1e − 5. We trained 200 epochs in total. The hyper-parameters remain the same while training ResNet3D and ResNet2+1D as baselines. We also compare the results of different feature sizes. The comparison results are shown in <ref type="table" target="#tab_1">Table 4</ref>. From the table, we can find out that our SSTCN has the highest accuracy comparing with ResNet3D and ResNet2+1D on the same scale of features. With a larger feature size, our SSTCN can achieve even better performance.  <ref type="table">Table 6</ref>. Results of single modalities on AUTSL validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D CNN Variations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Other Modalities and Ensembles</head><p>The results of our baseline 3D CNNs for the the other modalities are reported in <ref type="table">Table 6</ref>. Keypoints method represents our proposed multi-stream SL-GCN, which performs the best among the other single-modality methods. If we consider the feature based method using SSTCN as the same modality as SL-GCN (both skeleton based), their ensemble result achieves even higher recognition rate, see <ref type="table">Table 7</ref>. We observe that the depth flow provides slightly better accuracy compared with RGB flow due to the lesser noise introduced. An ablation study on the 3D CNN architecture is also provided in <ref type="table" target="#tab_2">Table 5</ref> using the RGB frames. From the ablation study, we find that label smoothing and swish activation both improve the recognition rate by 1% and 2%, respectively. Pretraining on CSL dataset <ref type="bibr" target="#b68">[69]</ref> improves the final accuracy by 1.4%.</p><p>The ensemble results in both RGB and RGB-D scenarios using different combinations of modalities are summarized in <ref type="table">Table 7</ref> as two groups. The skeleton based method combined from SL-GCN and SSTCN performs better than RGB + Flow and Depth ensemble models, which shows the effectiveness of our proposed skeleton based approach. The ensemble results of RGB All and RGB-D All demonstrate that the whole-body skeleton based approaches are able to collaborate with the other modalities and further improve the final recognition rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Evaluated on Challenge Test Set</head><p>When training our models on the training set, we adopt an early stopping technique based on the validation accuracy to obtain our best models. Then we test our best mod-  els on the test set and use the hyperparameters tuned on validation set to obtain our ensemble prediction. In the final test phase of the challenge, we are allowed to finetune our model using the validation set. To further improve our performance, we finetune our best models on the union of training and validation set. Since we cannot validate the training models this time, we stop training when the training loss in our finetuning experiment is reduced to the same level as our best models in the training phase. Our predictions with and without finetuning are evaluated on the challenge server and reported in <ref type="table" target="#tab_4">Table 8</ref>. Our proposed SAM-SLR approach surpasses the baseline methods significantly and ranked 1st in both RGB and RGB-D tracks of the SLR challenge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel Skeleton Aware Multimodal SLR framework (SAM-SLR) to take advantage of multi-modal information towards effective SLR. Specifically, we construct a skeleton graph for SLR using pretrained whole-body pose estimators and propose SL-GCN to model the embedded spatial and temporal dynamics. Our approach requires no extra effort on skeleton annotation. In addition to modeling keypoints dynamics, we propose SSTCN to exploit information in skeleton features. Furthermore, we implement effective baselines for the other RGB and depth modalities and assemble all modalities together in the proposed SAM-SLR framework, which achieves the state-of-the-art performance and won the challenge on SLR in both RGB and RGB-D tracks. We hope our work could encourage and facilitate future research on SLR.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Concept of our Skeleton Aware Multi-modal Sign Language Recognition Framework (SAM-SLR). All local and global motion information is extracted and utilized for final prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the SL-GCN pipeline: (a) Input sign language videos; (b) SLR graph constructed from whole-body keypoints after graph reduction; (c) Workflow of the multi-stream SL-GCN (Joint, Bone, JM=Joint Motion, BM=Bone Motion); (d) SL-GCN block architecture; (e) STC attention module used in the SL-GCN block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Visualization of modalities: (a) RGB with whole-body keypoints overlay; (b) Depth; (c) Masked HHA; (d) Optical flow; (e) Depth flow. (better viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Illustration of the proposed SSTCN for skeleton features. Abbrevs: J=Joints; F=Frames; NF=New Features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>Performance of multi-stream SL-GCN on validation set.</figDesc><table><row><cell>Streams</cell><cell cols="2">Top-1 Top-5</cell></row><row><cell>Joint</cell><cell cols="2">95.02 99.21</cell></row><row><cell>Bone</cell><cell cols="2">94.70 99.14</cell></row><row><cell cols="3">Joint Motion 93.01 98.85</cell></row><row><cell cols="3">Bone Motion 92.49 98.78</cell></row><row><cell cols="3">Multi-stream 95.45 99.25</cell></row><row><cell>Variations</cell><cell></cell><cell>Top-1</cell></row><row><cell>SL-GCN (Joint)</cell><cell></cell><cell>95.02</cell></row><row><cell cols="2">w/o Graph Reduction</cell><cell>63.69</cell></row><row><cell>w/o Decouple GCN</cell><cell></cell><cell>94.66</cell></row><row><cell>w/o Drop Graph</cell><cell></cell><cell>94.81</cell></row><row><cell cols="3">w/o Keypoints Augmentation 90.16</cell></row><row><cell>w/o STC Attention</cell><cell></cell><cell>93.53</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 4 .</head><label>4</label><figDesc>Comparing our SSTCN with ResNet3D and ResNet2+1D on 12 × 12 feature size shows the effectiveness of our SSTCN. Using larger feature size will further improve the performance.</figDesc><table><row><cell>Methods</cell><cell cols="2">Feature size Top-1</cell></row><row><cell>ResNet3D</cell><cell>12 × 12</cell><cell>92.82</cell></row><row><cell>ResNet2+1D</cell><cell>12 × 12</cell><cell>93.03</cell></row><row><cell>SSTCN</cell><cell>12 × 12</cell><cell>93.60</cell></row><row><cell>SSTCN</cell><cell>24 × 24</cell><cell>94.32</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5 .</head><label>5</label><figDesc>Ablation studies on 3D CNN using RGB frames.</figDesc><table><row><cell></cell><cell></cell><cell>Top-1</cell></row><row><cell>Ours (RGB Frame)</cell><cell></cell><cell>94.77</cell></row><row><cell cols="2">w/o Label Smoothing</cell><cell>93.75</cell></row><row><cell cols="2">w/o Swish Activation</cell><cell>92.88</cell></row><row><cell cols="2">w/o Pretraining on CSL</cell><cell>93.41</cell></row><row><cell cols="3">w/ ResNet3D-18 Backbone 93.10</cell></row><row><cell>Modality</cell><cell cols="2">Top-1 Top-5</cell></row><row><cell>Baseline RGB</cell><cell>42.58</cell><cell>-</cell></row><row><cell cols="2">Baseline RGB-D 63.22</cell><cell>-</cell></row><row><cell>Keypoints</cell><cell cols="2">95.45 99.25</cell></row><row><cell>Features</cell><cell cols="2">94.32 98.84</cell></row><row><cell>RGB Frames</cell><cell cols="2">94.77 99.48</cell></row><row><cell>RGB Flow</cell><cell cols="2">91.65 98.76</cell></row><row><cell>Depth HHA</cell><cell cols="2">95.13 99.25</cell></row><row><cell>Depth Flow</cell><cell cols="2">92.69 98.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 8 .</head><label>8</label><figDesc>Performance our ensemble results (with and without finetuning) evaluated on AUTSL test set.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Evaluation of the spatial resolution accuracy of the face tracking system for Kinect for windows V1 and V2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Amon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferdinand</forename><surname>Fuhrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franz</forename><surname>Graf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial</title>
		<meeting>AAAI Conference on Artificial</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="16" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Human action recognition: Pose-based attention draws focus to hands</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Jolo-gcn: Mining joint-centered light-weight information for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinmiao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianjuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2735" to="2744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-view super vector for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="596" to="603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A short note about kinetics-600</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Decoupling GCN with DropGraph module for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congqi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jérôme</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7024" to="7033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">OpenMMLab pose estimation toolbox and benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mmpose</forename><surname>Contributors</surname></persName>
		</author>
		<ptr target="https://github.com/open-mmlab/mmpose,2020.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A deep neural framework for continuous sign language recognition by iterative training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runpeng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changshui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1880" to="1891" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Real-time hand gesture detection and recognition using bag-of-features and support vector machine techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nasser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><forename type="middle">D</forename><surname>Dardas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Georganas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Instrumentation and measurement</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="3592" to="3607" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spatial-temporal graph convolutional networks for sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cleison</forename><surname>Correia De Amorim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Macêdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cleber</forename><surname>Zanchettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="646" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Language, cognition, and the brain: Insights from sign language research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Emmorey</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Hierarchical LSTM for sign language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Can spatiotemporal 3D CNNs retrace the history of 2D CNNs and ImageNet?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6546" to="6555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning with side information through modality hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="826" to="834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep bilinear learning for rgb-d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Fang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianhuang</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="335" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Attention-based 3D-CNNs for large-vocabulary sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2822" to="2832" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video-based sign language recognition without temporal segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiping</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI Conference on Artificial Intelligence</title>
		<meeting>AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning on lie groups for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengde</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6099" to="6108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Whole-body human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lumin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Australian Sign Language (Auslan): An introduction to sign language linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Johnston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Schembri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Would mega-scale datasets further enhance spatiotemporal 3D CNNs?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tenga</forename><surname>Wakamiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.04968</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep sign: Enabling robust statistical continuous sign language recognition via hybrid CNN-HMMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepehr</forename><surname>Zargaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1311" to="1325" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Winter Conference on Applications of Computer Vision</title>
		<meeting>IEEE Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1459" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Independently recurrent neural network (indrnn): Building a longer and deeper rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbo</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5457" to="5466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep attention network for joint hand gesture localization and recognition using static RGB-D images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">441</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="66" to="78" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Isolated sign language recognition using convolutional neural network hand modelling and hand energy image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Kian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">Wee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiat</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chin</forename><forename type="middle">Poo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shing Chiang</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">14</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spatio-temporal lstm with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object recognition from local scale-invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A Kinect based sign language recognition system using spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Memiş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songül</forename><surname>Albayrak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Vision</title>
		<meeting>International Conference on Machine Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">9067</biblScope>
			<biblScope unit="page">90670</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Reading between the signs: Intercultural communication for sign language interpreters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Mindess</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Nicholas Brealey</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ModDrop: Adaptive multi-modal gesture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Nebout</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1692" to="1706" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Calibration of kinect for Xbox One and comparison between the two generations of microsoft sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Pagliari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Livio</forename><surname>Pinto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Beyond temporal pooling: Recurrence and temporal convolutions for gesture recognition in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Pigou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mieke</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joni</forename><surname>Van Herreweghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dambre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="430" to="439" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Iterative alignment network for continuous sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfu</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4165" to="4174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Feed-forward networks with attention can solve some long-term memory problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ellis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.08756</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faster R-Cnn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<title level="m">Towards real-time object detection with region proposal networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deep multimodal feature analysis for action recognition in RGB+D videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihong</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1045" to="1058" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">American sign language fingerspelling recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurora Martinez Del</forename><surname>Rio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Michaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Brentari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Spoken Language Technology Workshop</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with directed graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7912" to="7921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Twostream adaptive graph convolutional networks for skeletonbased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with multi-stream adaptive graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">An attention enhanced graph convolutional lstm network for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1227" to="1236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Chalearn LAP large scale signer independent isolated sign language recognition challenge: Design, results and future research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julio</forename><forename type="middle">C S</forename><surname>Ozge Mercanoglu Sincan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Jacques Junior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hacer Yalim</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2021</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, 2021</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">AUTSL: A large scale multi-modal turkish sign language dataset and baseline methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozge</forename><surname>Mercanoglu Sincan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hacer Yalim</forename><surname>Keles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="181340" to="181355" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Isolated sign language recognition with multi-scale features using LSTM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Ozge Mercanoglu Sincan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hacer Yalim</forename><surname>Osman Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Signal Processing and Communications Applications Conference (SIU)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Stronger, faster and more explainable: A graph convolutional baseline for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Fan</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caifeng</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM International Conference on Multimedia</title>
		<meeting>ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Missing modalities imputation via cascaded residual autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1405" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Isolated sign recognition with a siamese neural network of RGB and depth streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anil</forename><surname>Osman Tur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hacer Yalim</forename><surname>Keles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Smart Technologies</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Linguistics of American sign language: An introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clayton</forename><surname>Valli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceil</forename><surname>Lucas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>Gallaudet University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Dividing and aggregating network for multi-view action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Dense trajectories and motion boundary descriptors for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="79" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Modeling temporal dynamics and spatial configurations of actions using twostream recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Generative multi-view human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision</title>
		<meeting>IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6212" to="6221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Skeleton-based chinese sign language recognition and generation for bidirectional communication between deaf and hearing people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinkun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minying</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="41" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI conference on artificial intelligence</title>
		<meeting>AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Chinese sign language recognition based on video sequence appearance modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Industrial Electronics and Applications</title>
		<meeting>IEEE Conference on Industrial Electronics and Applications</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime TV-L1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Joint Pattern Recognition Symposium</title>
		<meeting>Joint Pattern Recognition Symposium</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Chinese sign language recognition with adaptive HMM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wengang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junfu</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Multimedia and Expo</title>
		<meeting>IEEE International Conference on Multimedia and Expo</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Crossview action recognition via transferable dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuolin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2542" to="2556" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Fast human detection using a cascade of histograms of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei-Chen</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwang-Ting</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Avidan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1491" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Chained multi-stream networks exploiting pose, motion, and appearance for action classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2904" to="2913" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
