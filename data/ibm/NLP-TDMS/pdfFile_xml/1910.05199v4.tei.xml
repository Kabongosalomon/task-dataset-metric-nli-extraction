<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Patacchiola</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Turner</surname></persName>
							<email>jack.turner@ed.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliot</forename><forename type="middle">J</forename><surname>Crowley</surname></persName>
							<email>elliot.j.crowley@ed.ac.uk</email>
							<affiliation key="aff2">
								<orgName type="department">School of Engineering</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>O&amp;apos;boyle</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
							<email>a.storkey@ed.ac.uk</email>
							<affiliation key="aff4">
								<orgName type="department">School of Informatics</orgName>
								<orgName type="institution">University of Edinburgh</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian Meta-Learning for the Few-Shot Setting via Deep Kernels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, different machine learning methods have been introduced to tackle the challenging few-shot learning scenario that is, learning from a small labeled dataset related to a specific task. Common approaches have taken the form of meta-learning: learning to learn on the new problem given the old. Following the recognition that meta-learning is implementing learning in a multi-level model, we present a Bayesian treatment for the meta-learning inner loop through the use of deep kernels. As a result we can learn a kernel that transfers to new tasks; we call this Deep Kernel Transfer (DKT). This approach has many advantages: is straightforward to implement as a single optimizer, provides uncertainty quantification, and does not require estimation of task-specific parameters. We empirically demonstrate that DKT outperforms several state-of-the-art algorithms in few-shot classification, and is the state of the art for cross-domain adaptation and regression. We conclude that complex meta-learning routines can be replaced by a simpler Bayesian model without loss of accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>One of the key differences between state-of-the-art machine learning methods, such as deep learning <ref type="bibr" target="#b24">(LeCun et al., 2015;</ref><ref type="bibr" target="#b35">Schmidhuber, 2015)</ref>, and human learning is that the former needs a large amount of data in order to find relevant patterns across samples, whereas the latter acquires rich structural information from a handful of examples. Moreover, deep learning methods struggle in providing a measure of uncertainty, which is a crucial requirement when dealing with scarce data, whereas humans can effectively weigh up different alternatives given limited evidence. In this regard, some authors have suggested that the human ability for few-shot inductive reasoning could derive from a Bayesian inference mechanism <ref type="bibr" target="#b37">(Steyvers et al., 2006;</ref><ref type="bibr" target="#b39">Tenenbaum et al., 2011)</ref>. Accordingly, we argue that the natural interpretation of meta-learning as implementing learning in a hierarchical model, leads to a Bayesian equivalent through the use of deep kernel methods.</p><p>Deep kernels combine neural networks with kernels to provide scalable and expressive closed-form covariance functions <ref type="bibr" target="#b18">(Hinton and Salakhutdinov, 2008;</ref><ref type="bibr" target="#b45">Wilson et al., 2016)</ref>. If one has a large number of small but related tasks, as in few-shot learning, it is possible to define a common prior that induces knowledge transfer. This prior can be a deep kernel with parameters shared across tasks, so that given a new unseen task it is possible to effectively estimate the posterior distribution over a query set conditioned on a small support set. In a meta-learning framework <ref type="bibr" target="#b19">(Hospedales et al., 2020)</ref>  LG] 13 Oct 2020 corresponds to a Bayesian treatment for the inner loop cycle. This is our proposed approach, which we refer to as deep kernel learning with transfer, or Deep Kernel Transfer (DKT) for short.</p><p>We derive two versions of DKT for both the regression and the classification setting, comparing it against recent methods on a standardized benchmark environment; the code is released with an open-source license 1 . DKT has several advantages over other few-shot methods, which can be summarized as follows:</p><p>1. Simplicity and efficiency: it does not require any complex meta-learning optimization routines, it is straightforward to implement as a single optimizer as the inner loop is replaced by an analytic marginal likelihood computation, and it is efficient in the low-data regime. 2. Flexibility: it can be used in a variety of settings such as regression, cross-domain and within-domain classification, with state-of-the-art performance. 3. Robustness: it provides a measure of uncertainty with respect to new instances, that is crucial for a decision maker in the few-shot setting.</p><p>Main contributions: (i) a novel approach to deal with the few-shot learning problem through the use of deep kernels, (ii) an effective Bayesian treatment for the meta-learning inner-loop, and (iii) empirical evidence that complex meta-learning routines for few-shot learning can be replaced by a simpler hierarchical Bayesian model without loss of accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Motivation</head><p>The Bayesian meta-learning approach to the few-shot setting has predominantly followed the route of hierarchical modeling and multi-task learning <ref type="bibr" target="#b12">Gordon et al., 2019;</ref><ref type="bibr" target="#b48">Yoon et al., 2018)</ref>.</p><p>The underlying directed graphical model distinguishes between a set of shared parameters θ, common to all tasks, and a set of N task-specific parameters ρ t . Given a train dataset of tasks D = {T t } N t=1 , each one containing input-output pairs T = {(x l , y l )} L l=1 , and given a test point x * from a new task T * , learning consists of finding an estimate of θ, forming the posterior distribution over the task-specific parameters p(ρ t |x * , D, θ), and then computing the posterior predictive distribution p(y * |x * , θ). This approach is principled from a probabilistic perspective, but is problematic, as it requires managing two levels of inference via amortized distributions or sampling, often requiring cumbersome architectures.</p><p>In recent differentiable meta-learning methods, the two sets of parameters are learned by maximum likelihood estimation, by iteratively updating θ in an outer loop, and ρ t in a inner loop <ref type="bibr" target="#b8">(Finn et al., 2017)</ref>. This case has various issues, since learning is destabilized by the joint optimization of two sets of parameters, and by the need to estimate higher-order derivatives (gradient of the gradient) for updating the weights .</p><p>To avoid these drawbacks we propose a simpler solution, that is marginalizing ρ t over the data of a specific task. This marginalization is analytic and leads to a closed form marginal likelihood, which measures the expectedness of the data under the given set of parameters. By finding the parameters of a deep kernel we can maximize the marginal likelihood. Following our approach there is no need to estimate the posterior distribution over the task-specific parameters, meaning that it is possible to directly compute the posterior predictive distribution, skipping an intermediate inference step. We argue that this approach can be very effective in the few-shot setting, significantly reducing the complexity of the model with respect to meta-learning approaches, while retaining the advantages of Bayesian methods (e.g. uncertainty estimation) with state-of-the-art performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Few-shot Learning</head><p>The terminology describing the few-shot learning setup is dispersive due to the colliding definitions used in the literature; the reader is invited to see <ref type="bibr" target="#b5">Chen et al. (2019)</ref> for a comparison. Here, we use the nomenclature derived from the meta-learning literature which is the most prevalent at time of writing. Let S = {(x l , y l )} L l=1 be a support-set containing input-output pairs, with L equal to one (1-shot) or five (5-shot), and Q = {(x m , y m )} M m=1 be a query-set (sometimes referred to in the literature as a target-set), with M typically one order of magnitude greater than L. For ease of notation the support and query sets are grouped in a task T = {S, Q}, with the dataset D = {T t } N t=1 defined as a collection of such tasks. Models are trained on random tasks sampled from D, then given a new task T * = {S * , Q * } sampled from a test set, the objective is to condition the model on the samples of the support S * to estimate the membership of the samples in the query set Q * . In the most common scenario, training, validation and test datasets each consist of distinct tasks sampled from the same overall distribution over tasks. Note that the target value y can be a continuous value (regression) or a discrete one (classification), though most previous work has focused on classification. We also consider the cross-domain scenario, where the test tasks are sampled from a different distribution over tasks than the training tasks; this is potentially more representative of many real-world scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Kernels</head><p>Given two input instances x and x and a function f (·), the kernel k(x, x ) is a covariance function that expresses how the correlation of the outputs at two points depends on the relationship between their two locations in input space</p><formula xml:id="formula_0">k(x, x ) = cov(f (x), f (x )).</formula><p>(1)</p><p>The simplest kernel has a linear expression k LIN (x, x ) = v x, x , where · denotes an inner product, and v is a variance hyperparameter. The use of a linear kernel is computationally convenient and it induces a form of Bayesian linear regression, however this is often too simplistic. For this reason, a variety of other kernels has been proposed in the literature: Radial Basis Function kernel (RBF), Matérn kernel, Cosine Similarity kernel (CosSim), and the spectral mixture kernel <ref type="bibr" target="#b44">(Wilson and Adams, 2013)</ref>. Details about the kernels used in this work are given in Appendix A.</p><p>In deep kernel learning <ref type="bibr" target="#b18">(Hinton and Salakhutdinov, 2008;</ref><ref type="bibr" target="#b45">Wilson et al., 2016)</ref> an input vector x is mapped to a latent vector h through a non-linear function F φ (x) → h (e.g. a neural network) parameterized by a set of weights φ. The embedding is defined such that the dimensionality of the input is significantly reduced, meaning that if x ∈ R J and h ∈ R K then J K. Once the input has been encoded in h the latent vector is passed to a kernel. When the inputs are images a common choice for F φ is a Convolutional Neural Network (CNN). Specifically we construct a kernel</p><formula xml:id="formula_1">k(x, x |θ, φ) = k (F φ (x), F φ (x )|θ)<label>(2)</label></formula><p>from some latent space kernel k with hyperparameters θ by passing the inputs through the non-linear function F φ . Then the hyperparameters θ and the parameters of the model φ are jointly learned by maximizing the log marginal likelihood, backpropagating the error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Description of the method</head><p>Let us start from the interpretation of meta-learning as a hierarchical model <ref type="bibr" target="#b13">Grant et al., 2018)</ref>, considering a set of task-common parameters in the upper hierarchy (optimized in an outer loop), along with a process for determining task-specific parameters in the lower hierarchy (optimized in an inner loop). For example, in MAML <ref type="bibr" target="#b8">(Finn et al., 2017)</ref>, the outer-parameters are the common neural network initialization weights and the inner-parameters are the final network weights, with prior implicitly defined by the probability that a particular parameterization can be reached in a few gradient steps from the initial parameters. Both outer and inner loops are obtained end-to-end, by differentiating through the inner loop to obtain derivatives for the outer loop parameters. This causes well known instability problems .</p><p>Algorithm 1 Deep Kernel Transfer (DKT) in the few-shot setting, train and test functions.</p><p>Require: D = {Tn} N n=1 train dataset and T * = {S * , Q * } test task. Require:θ kernel hyperparameters andφ neural network weights.</p><p>Randomly initialized Require: α, β: step size hyperparameters for the optimizers.</p><p>1: function TRAIN(D, α, β,θ,φ) 2:</p><p>while not done do 3:</p><p>Sample task T = {S, Q} ∼ D 4:</p><p>Assign T x ← ∀x ∈ S ∪ Q and T y ← ∀y ∈ S ∪ Q 5:</p><formula xml:id="formula_2">L = − log p(T y |T x ,θ,φ) see Eq. (3) 6:</formula><p>Updateθ ←θ − α∇θL andφ ←φ − β∇φL 7: end while 8:</p><p>returnθ,φ 9: end function 10: function TEST(T * ,θ,φ) 11:</p><p>Assign T x * ← ∀x ∈ S * , T y * ← ∀y ∈ S * , and x * ← x ∈ Q * 12:</p><p>return p(y * |x * , T x * , T y * ,θ,φ) see Eq. (5) 13: end function Our proposal is to replace the inner loop with a Bayesian integral, while still optimizing for the parameters. This is commonly called a maximum likelihood type II (ML-II) approach. Specifically, we learn a set of parameters and hyperparameters of a deep kernel (outer-loop) that maximize a marginal likelihood across all tasks. The marginalization of this likelihood integrates out over each of the task-specific parameters for each task using a Gaussian process approach, replacing the inner loop model with a kernel.</p><p>Let all the input data (support and query) for task t be denoted by T x t and the target data be T y t . Let D x and D y denote the respective collections of these datasets over all tasks; this data is hierarchically grouped by task. The marginal-likelihood of a Bayesian hierarchical model, conditioned on taskcommon hyperparametersθ and other task-common parametersφ (e.g. neural network weights) would take the form</p><formula xml:id="formula_3">P (D y |D x ,θ,φ) = t P (T y t |T x t ,θ,φ),<label>(3)</label></formula><p>where P (T y t |T x t ,θ,φ) is a marginalization over each set of task-specific parameters. Let these task-specific parameters for task t be denoted by ρ t , then</p><formula xml:id="formula_4">P (T y t |T x t , θ, φ) = k P (y k |x k , θ, φ, ρ t )dρ t ,<label>(4)</label></formula><p>where k enumerates elements of x k ∈ T x t , and corresponding elements y k ∈ T y t . In typical meta-learning, the task-specific integral (4) would be replaced by an inner-loop optimizer for the task-specific objective (and the parameters of that optimizer); any additional cross-task parameters θ, φ would be optimized in the outer loop. Instead, we do a full integral of the task specific parameters, and optimize only for the cross-task parameters θ, φ. We do that implicitly rather than explicitly by using a Gaussian process model for P (T y t |T x t , θ), which is the outcome of an analytic integral of Equation 4 for many model classes <ref type="bibr" target="#b30">(Rasmussen and Williams, 2006)</ref>. Predictions of the value y * for a new point x * given a small set of exemplars T x t * , T y t * for a new task t * can be made using the predictive distribution p(y * |x * , T x t * , T y t * ) ≈ p(y * |x * , T x t * , T y t * ,θ,φ).</p><p>Our claim is that, though the number of data points for each task is potentially small, the total number of points over all tasks contributing to the marginal likelihood (3) is sufficiently large to make ML-II appropriate for finding a set of shared weights and parameters without underfitting or overfitting.</p><p>Those parameters provide a model with good generalization capability to new unseen tasks, removing the need for inferring the task-specific parameters ρ t . Results in Section 5 indicate that our proposal is competitive with much more complicated meta-learning methods. Note that this approach differs from direct deep kernel learning, where the marginalization is over all data; this would ignore the task distinctions which is vital given the hierarchical mode (see experimental comparison in Section 5.1). The problem also differs from multitask learning where the tasks share the same input values.</p><p>For stochastic gradient training, at each iteration, a task T = {S, Q} is sampled from D, then the log marginal likelihood, that is the logarithm of (3), is estimated over S ∪ Q (assuming y ∈ Q to be observed) and the parameters of the kernel are updated via a gradient step on the marginal likelihood objective for that task. This procedure allows us to find a kernel that can represent the task in its entirety over both support and query sets. At test time, given a new task T * = {S * , Q * } the prediction on the query set Q * is made via conditioning on the support set S * , using the parameters that have been learned at training time. Pseudocode is given in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Regression</head><p>We want to find a closed form expression of (3) for the regression case. Assume we are interested in a continuous output y * generated by a clean signal f * (x * ) corrupted by homoscedastic Gaussian noise with variance σ 2 . We are interested in the joint distribution of the observed outputs and the function values at test location. For ease of notation, let us define k * = k(x * , x) to denote the N -dimensional vector of covariances between x * and the N training points in D. Similarly, let us write k * * = k(x * , x * ) for the variance of x * , and K to identify the covariance matrix on the training inputs in D. The predictive distribution p(y * |x * , D) is obtained by Bayes' rule, and given the conjugacy of the prior, this is a Gaussian with mean and covariance specified as</p><formula xml:id="formula_6">E[f * ] = k * (K + σ 2 I) −1 y,<label>(6a)</label></formula><formula xml:id="formula_7">cov(f * ) = k * * − k * (K + σ 2 I) −1 k * .<label>(6b)</label></formula><p>Note that (6) defines a distribution over functions, which assumes that the collected values at any finite set of points have a joint Gaussian distribution <ref type="bibr" target="#b30">(Rasmussen and Williams, 2006)</ref>. Hereon, we absorb the noise σ 2 I into the covariance matrix K and treat it as part of a vector of learnable parameters θ, that also include the hyperparameters of the kernel (e.g. variance of a linear kernel).</p><p>Let us collect all the target data items for task t into vector y t , and denote the kernel between all task inputs by K t . It follows that the marginal likelihood of Equation <ref type="formula" target="#formula_3">(3)</ref> can be rewritten as</p><formula xml:id="formula_8">log P (D y |D x ,θ,φ) = t − 1 2 y t [Kt(θ,φ)] −1 yt data-fit − 1 2 log |Kt(θ,φ)| penalty +c,<label>(7)</label></formula><p>where c is a constant. The parameters are estimated via ML-II maximizing (7) via gradient ascent. In practice we use a stochastic gradient ascent with each batch containing the data for a single task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Classification</head><p>A Bayesian treatment for the classification case does not come without problems, since a non-Gaussian likelihood breaks the conjugacy. For instance, in the case of binary classification the Bernoulli likelihood induces an intractable marginalization of the evidence and therefore it is not possible to estimate the posterior in a closed form. Common approaches to deal with this issue (e.g. MCMC or variational methods), incur a significant computational cost for few-shot learning: for each new task, the posterior is estimated by approximation or sampling, introducing an inner loop that increases the time complexity from constant O(1) to linear O(K), with K being the number of inner cycles. An alternative solution would be to treat the classification problem as if it were a regression one, therefore reverting to analytical expressions for both the evidence and the posterior.</p><p>In the literature this has been called label regression (LR) <ref type="bibr" target="#b22">(Kuss, 2006)</ref> or least-squares classification (LSC) <ref type="bibr" target="#b32">(Rifkin and Klautau, 2004;</ref><ref type="bibr" target="#b30">Rasmussen and Williams, 2006)</ref>. Experimentally, LR and LSC tend to be more effective than other approaches in both binary <ref type="bibr" target="#b22">(Kuss, 2006)</ref> and multi-class <ref type="bibr" target="#b32">(Rifkin and Klautau, 2004)</ref> settings. Here, we derive a classifier based on LR which is computationally cheap and straightforward to implement.</p><p>Let us define a binary classification setting, with the class being a Bernoulli random variable c ∈ {0, 1}. The model is trained as a regressor with a target y + = 1 to denote the case c = 1, and y − = −1 to denote the case c = 0. Even though y ∈ {−1, 1} there is no guarantee that f (x) ∈ [y − , y + ]. Predictions are made by computing the predictive mean and passing it through a sigmoid function, inducing a probabilistic interpretation. Note that it is still possible to use ML-II to make point estimates of θ and φ. When generalizing to a multi-label task we apply the one-versus-rest scheme where C binary classifiers are used to classify each class against all the rest. The log marginal likelihood, that is the logarithm of Equation <ref type="formula" target="#formula_3">(3)</ref>, is replaced by the sum of the marginals for each one of the C individual class outputs y c , as</p><formula xml:id="formula_9">log p(y|x,θ,φ) = C c=1 log p(y c |x,θ,φ).<label>(8)</label></formula><p>Given a new input x * and the C outputs of all the binary classifiers, a decision is made by selecting the output with the highest probability c * = argmax c σ(m c (x * )) , where m(x) is the predictive mean, σ(·) the sigmoid function, and c * ∈ {1, ..., C}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>There is a wealth of literature on feature transfer <ref type="bibr" target="#b27">(Pan and Yang, 2009</ref>). As a baseline for few-shot learning, the standard procedure consists of two phases: pre-training and fine-tuning. During pretraining, a network and classifier are trained on examples for the base classes. When fine-tuning, the network parameters are fixed and a new classifier is trained on the novel classes. This approach has its limitations; part of the model has to be trained from scratch for each new task, and often overfits. <ref type="bibr" target="#b5">Chen et al. (2019)</ref> extend this by proposing the use of cosine distance between examples (called Baseline++). However, this still relies on the assumption that a fixed fine-tuning protocol will balance the bias-variance tradeoff correctly for every task.</p><p>Alternatively, one can compare new examples in a learned metric space. Matching Networks (MatchingNets, <ref type="bibr" target="#b41">Vinyals et al., 2016)</ref> use a softmax over cosine distances as an attention mechanism, and an LSTM to encode the input in the context of the support set, considered as a sequence. Prototypical Networks (ProtoNets, <ref type="bibr" target="#b36">Snell et al., 2017)</ref> are based on learning a metric space in which classification is performed by computing distances to prototypes, where each prototype is the mean vector of the embedded support points belonging to its class. Relation Networks (RelationNets, <ref type="bibr" target="#b38">Sung et al., 2018)</ref> use an embedding module to generate representations of the query images that are compared by a relation module with the support set, to identify matching categories.</p><p>Meta-learning <ref type="bibr" target="#b3">(Bengio et al., 1992;</ref><ref type="bibr" target="#b34">Schmidhuber, 1992;</ref><ref type="bibr" target="#b19">Hospedales et al., 2020)</ref> methods have become very popular for few-shot learning tasks. MAML <ref type="bibr" target="#b8">(Finn et al., 2017)</ref> has been proposed as a way to meta-learn the parameters of a model over many tasks, so that the initial parameters are a good starting point from which to adapt to a new task. MAML has provided inspiration for numerous meta-learning approaches <ref type="bibr" target="#b29">Rajeswaran et al., 2019)</ref>.</p><p>In several works, MAML has been interpreted as a Bayesian hierarchical model <ref type="bibr" target="#b13">Grant et al., 2018;</ref><ref type="bibr" target="#b21">Jerfel et al., 2019)</ref>. Bayesian MAML <ref type="bibr" target="#b48">(Yoon et al., 2018)</ref> combines efficient gradientbased meta-learning with nonparametric variational inference, while keeping an application-agnostic approach. <ref type="bibr" target="#b12">Gordon et al. (2019)</ref> have recently proposed an amortization network-VERSA-that takes few-shot learning datasets as inputs, and outputs a distribution over task-specific parameters which can be used to meta-learn probabilistic inference for prediction. <ref type="bibr" target="#b46">Xu et al. (2019)</ref> have used conditional neural processes with an encoder-decoder architecture to project labeled data into an infinite-dimensional functional representation.</p><p>For the regression case <ref type="bibr" target="#b15">Harrison et al. (2018)</ref> have proposed a method named ALPaCA, which uses a dataset of sample functions to learn a domain-specific encoding and a prior over weights. <ref type="bibr" target="#b40">Tossou et al. (2019)</ref> have presented a variant of kernel learning for Gaussian Processes called Adaptive Deep Kernel Learning (ADKL), which finds a kernel for each task using a task encoder network. The difference between our method and ADKL is that we do not need an additional module for task encoding as we can rely on a single set of shared general-purpose hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In the few-shot setting a fair comparison between methods is often obfuscated by substantial differences in the implementation details of each algorithm. <ref type="bibr" target="#b5">Chen et al. (2019)</ref> have recently released an open-source benchmark to allow for a fair comparison between methods. We integrated our algorithm into this framework using PyTorch and GPyTorch <ref type="bibr" target="#b10">(Gardner et al., 2018)</ref>. In all experiments the proposed method is marked as DKT. Training details are reported in Appendix B.  We consider two tasks: amplitude prediction for unknown periodic functions, and head pose trajectory estimation from images. The former was treated as a few-shot regression problem by <ref type="bibr" target="#b8">Finn et al. (2017)</ref> to motivate MAML: support and query scalars are uniformly sampled from a periodic wave with amplitude ∈ [0.1, 5.0], phase ∈ [0, π], and range ∈ [−5.0, 5.0], and Gaussian noise (µ = 0, σ = 0.1). The training set is composed of 5 support and 5 query points, and the test set of 5 support and 200 query points. We first test in-range: the same domain as the training set as in <ref type="bibr" target="#b8">Finn et al. (2017)</ref>. We also consider a more challenging out-of-range regression, with test points drawn from an extended domain [−5.0, 10.0] where portions from the range [5.0, 10.0] have not been seen at training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Regression</head><p>For head pose regression, we used the Queen Mary University of London multiview face dataset (QMUL, <ref type="bibr" target="#b11">Gong et al., 1996)</ref>, it comprises of grayscale face images of 37 people (32 train, 5 test). There are 133 facial images per person, covering a viewsphere of ±90 • in yaw and ±30 • in tilt at 10 • increment. Each task consists of randomly sampled trajectories taken from this discrete manifold, where in-range includes the full manifold and out-of-range allows training only on the leftmost 10 angles, and testing on the full manifold; the goal is to predict tilt. For the periodic function prediction experiment, we compare our approach against feature transfer and MAML <ref type="bibr" target="#b8">(Finn et al., 2017)</ref>. Moreover we report the results of ADKL <ref type="bibr" target="#b40">(Tossou et al., 2019)</ref>, R2-D2 <ref type="bibr" target="#b4">(Bertinetto et al., 2019)</ref>, and ALPaCA <ref type="bibr" target="#b15">(Harrison et al., 2018</ref>) obtained on a similar task (as defined in <ref type="bibr" target="#b48">Yoon et al., 2018)</ref>. To highlight the importance of kernel transfer, we add a baseline where a deep kernel is trained from scratch on the support points of every incoming task without transfer (DKBaseline), this correspond to standard deep kernel learning <ref type="bibr" target="#b45">(Wilson et al., 2016)</ref>. Few methods have tackled few-shot regression from images, so in the head pose trajectory estimation we compare against feature transfer and MAML. As metric we use the average Mean-Squared Error (MSE) between predictions and true values. Additional details are reported in Appendix B.</p><p>Results for the regression experiments are summarized in <ref type="table" target="#tab_1">Table 1</ref> and a qualitative comparison is provided in <ref type="figure" target="#fig_0">Figure 1a</ref> and in appendix. DKT obtains a lower MSE than feature transfer and MAML on both experiments. For unknown periodic function estimation, using a spectral kernel gives a large advantage over RBF, being more precise in both in-range and out-of-range (1.38 vs 0.08, and 2.61 vs 0.10 MSE). Uncertainty is correctly estimated in regions with low point density, and increases overall in the out-of-range region. Conversely, feature transfer severely underfits (1 step, 2.94 MSE) or overfits (100 step, 2.67), and was unable to model out-of-range points (6.13 and 6.94). MAML is effective in-range (2.76), but significantly worse out-of-range (8.45). ADKL, R2-D2, and ALPaCA (0.14, 0.46, 0.14) are better than DKT with an RBF kernel (1.38), but worse than DKT with a Spectral kernel (0.08). This indicates that the combination of an appropriate kernel with our method is more effective than an adaptive approach. The DKBaseline performs significantly worse than DKT in all conditions, confirming the necessity of using kernel transfer for few-shot problems. Qualitative comparison in <ref type="figure" target="#fig_0">Figure 1a</ref> shows that both feature transfer and MAML are unable to fit the true function, especially out-of-range; additional samples are reported in Appendix C. We observe similar results for head pose estimation, with DKT reporting lower MSE in all cases <ref type="table" target="#tab_1">(Table 1)</ref>. In Appendix C we also examine the latent spaces generated by the RBF and spectral kernel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty quantification (regression)</head><p>In the low-data regime it is fundamental to account for uncertainty in the prediction; DKT is one of the few methods able to do it. To highlight the benefits of our method versus other approaches, we perform an experiment on quantifying uncertainty, sampling head pose trajectories and corrupting one input with Cutout (DeVries and Taylor, 2017), randomly covering 95% of the image. Qualitative results are shown in <ref type="figure" target="#fig_0">Figure 1b</ref>. For the corrupted input, DKT predicts a value close to the true one, while giving a high level of uncertainty (red shadow). Feature transfer performs poorly, predicting an unrealistic pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Classification</head><p>We consider two challenging datasets: the Caltech-UCSD Birds (CUB-200, <ref type="bibr" target="#b42">Wah et al., 2011)</ref>, and mini-ImageNet <ref type="bibr" target="#b31">(Ravi and Larochelle, 2017)</ref>. All the experiments are 5-way (5 random classes) with 1 or 5-shot (1 or 5 samples per class in the support set). A total of 16 samples per class are provided for the query set. Additional details in Appendix B. We compare the following kernels: linear, RBF, Matérn, Polynomial, CosSim, and BNCosSim. Where BNCosSim is a variant of CosSim with features centered through BatchNorm (BN) statistics <ref type="bibr" target="#b20">(Ioffe and Szegedy, 2015)</ref>, this has shown to improve performance . We compare our approach to several state-of-the-art methods, such as MAML <ref type="bibr" target="#b8">(Finn et al., 2017)</ref>, ProtoNets <ref type="bibr" target="#b36">(Snell et al., 2017)</ref>, MatchingNet <ref type="bibr" target="#b41">(Vinyals et al., 2016)</ref>, and RelationNet <ref type="bibr" target="#b38">(Sung et al., 2018)</ref>. We further compare against feature transfer, and Baseline++ from <ref type="bibr" target="#b5">Chen et al. (2019)</ref>. All these methods have been trained from scratch with the same backbone and learning schedule. We additionally report the results for approaches with comparable training procedures and convolutional architectures <ref type="bibr" target="#b25">(Mishra et al., 2018;</ref><ref type="bibr" target="#b31">Ravi and Larochelle, 2017;</ref><ref type="bibr" target="#b43">Wang et al., 2019)</ref> including recent hierarchical Bayesian methods <ref type="bibr" target="#b12">(Gordon et al., 2019;</ref><ref type="bibr" target="#b13">Grant et al., 2018;</ref><ref type="bibr" target="#b21">Jerfel et al., 2019)</ref>. We have excluded approaches that use deeper backbones or more sophisticated learning schedules <ref type="bibr" target="#b26">Oreshkin et al., 2018;</ref><ref type="bibr" target="#b28">Qiao et al., 2018;</ref><ref type="bibr" target="#b47">Ye et al., 2018)</ref> so that the quality of the algorithms can be assessed separately from the power of the underlying discriminative model.</p><p>We report the results for the more challenging 1-shot case in <ref type="table" target="#tab_3">Table 2</ref> and 3, and the results for the 5shot case in appendix. DKT achieves the highest accuracy in both CUB (63.37%) and mini-ImageNet (49.73%), performing better than any other approach including hierarchical Bayesian methods such as LLAMA (49.40%) and VERSA (48.53%). The best performance of first-order kernels ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Uncertainty quantification (classification)</head><p>We provide results on model calibration on the CUB dataset. We followed the protocol of <ref type="bibr" target="#b14">Guo et al. (2017)</ref> estimating the Expected Calibration Error (ECE), a scalar summary statistic (the lower the better). We first scaled each model output, calibrating the temperature by minimizing the NLL on logits/labels via LBFGS on 3000 tasks; then we estimated  Method mini-ImageNet ML-LSTM <ref type="bibr" target="#b31">(Ravi and Larochelle, 2017)</ref> 43.44 ± 0.77 SNAIL <ref type="bibr" target="#b25">(Mishra et al., 2018)</ref> 45.10 iMAML-HF <ref type="bibr" target="#b29">(Rajeswaran et al., 2019)</ref> 49.30 ± 1.88</p><p>LLAMA <ref type="bibr" target="#b13">(Grant et al., 2018)</ref> 49.40 ± 1.83</p><p>VERSA <ref type="bibr" target="#b12">(Gordon et al., 2019)</ref> 48.53 ± 1.84</p><p>Amortized VI <ref type="bibr" target="#b12">(Gordon et al., 2019)</ref> 44.13 ± 1.78</p><p>Meta-Mixture <ref type="bibr" target="#b21">(Jerfel et al., 2019)</ref> 49.60 ± 1.50</p><p>SimpleShot  49.69 ± 0.19</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feature Transfer</head><p>39.51 ± 0.23</p><p>Baseline++ <ref type="bibr" target="#b5">(Chen et al., 2019)</ref> 47.15 ± 0.49</p><p>MatchingNet <ref type="bibr" target="#b41">(Vinyals et al., 2016)</ref> 48.25 ± 0.65</p><p>ProtoNet <ref type="bibr" target="#b36">(Snell et al., 2017)</ref> 44.19 ± 1.30</p><p>MAML <ref type="bibr" target="#b8">(Finn et al., 2017)</ref> 45.39 ± 0.49</p><p>RelationNet <ref type="bibr" target="#b38">(Sung et al., 2018)</ref> 48.76 ± 0.17 <ref type="bibr">DKT + CosSim (ours)</ref> 48.64 ± 0.45</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DKT + BNCosSim (ours)</head><p>49.73 ± 0.07 the ECE on the test set. The complete results for CUB 1-shot and 5-shot (percentage, average of three runs) are reported in Appendix D, <ref type="table">Table 7</ref>. In 1-shot DKT achieves one of the lowest ECE 2.6% beating most of the competitors (only ProtoNet and MAML do better). In 5-shot our model achieves the second lowest ECE 1.1% (ProtoNet does marginally better).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Cross-domain classification</head><p>The objective of cross-domain classification is to train a model on tasks sampled from one distribution, that then generalizes to tasks sampled from a different distribution. Specifically, we combine datasets so that the training split is drawn from one, and the validation and test split are taken from another. We experiment on mini-ImageNet→CUB (train split from mini-ImageNet and val/test split from CUB) and Omniglot→EMNIST. We compare our method to the previously-considered approaches, using identical settings for number of epochs and model selection strategy (see Appendix B). Results for the 1-shot case are given in <ref type="table" target="#tab_3">Table 2</ref>. DKT achieves the highest accuracy in most conditions. In Omniglot→EMNIST, the best performance is achieved with a linear kernel (75.97%). In mini-ImageNet→CUB, DKT surpasses all the other methods obtaining the highest accuracy with CosSim (40.22%) and BNCosSim (40.14%). Note that most competing methods experience difficulties in this setting, as shown by their low accuracies and large standard deviations. A comparison of kernels shows that first order ones are more effective (see Appendix E, <ref type="table">Table 9</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have introduced DKT, a highly flexible Bayesian model based on deep kernel learning. Compared with some other approaches in the literature, DKT performs better in regression and cross-domain classification while providing a measure of uncertainty. Based on the results, we argue that many complex meta-learning routines for few-shot learning can be replaced by a simple hierarchical Bayesian model without loss of accuracy. Future work could focus on exploiting the flexibility of the model in related settings, especially those merging continual and few-shot learning , where DKT has the potential to thrive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>The main motivation of this work has been to design a simple yet effective Bayesian method for dealing with the few-shot learning setting. The ability to learn from a reduced amount of data is crucial if we want to have systems that are able to deal with concrete real-world problems. Applications include (but are not limited to): classification and regression under constrained computational resources, medical diagnosis from small datasets, biometric identification from a handful of images, etc. Our method is one of the few which is able to provide a measure of uncertainty as a feedback for the decision maker. However, it is important to wisely choose the data on which the system is trained, since the low-data regime may be prone to bias more than the standard counterpart. If data is biased our method is not guaranteed to provide a correct estimation; this could harm the final users and should be carefully taken into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Kernels</head><p>Polynomial. This computes a covariance matrix based on the Polynomial kernel between inputs</p><formula xml:id="formula_10">k (x, x ) = (x x + c) p ,<label>(9)</label></formula><p>where p is the degree of the polynomial and c is an offset parameter. We used p = 1 and p = 2 in our experiments.</p><p>Radial Basis Function kernel (RBF). The RBF is a stationary kernel given by the squared Euclidean distance between the two inputs</p><formula xml:id="formula_11">k (x, x ) = exp − ||x − x || 2 2l 2 ,<label>(10)</label></formula><p>where l is a lengthscale parameters learned at training time.</p><p>Matérn kernel. This is a stationary kernel which is a generalization of the RBF and the absolute exponential kernel. It is parameterized by a value ν &gt; 0, commonly chosen as ν = 1.5 (giving once-differentiable functions) or ν = 2.5 (giving twice differentiable functions). The kernel is defined as follows:</p><formula xml:id="formula_12">k (x, x ) = |x − x | ν Kν (|x − x |).<label>(11)</label></formula><p>We used a value of ν = 2.5 in our experiments.</p><p>Spectral mixture kernel. The spectral mixture kernel was introduced by Wilson and Adams (2013) as a powerful stationary kernel for estimating periodic functions. The kernel models a spectral density with a Gaussian mixture</p><formula xml:id="formula_13">k (τ ) = Q q=1 wq P p=1 exp −2π 2 τ 2 p v (p) q cos 2πτpµ (p) q ,<label>(12)</label></formula><p>where τ = x−x , wq are weights that specify the contribution of each mixture component, µq are the component periods, and vq are lengthscales determining how quickly a component varies with the inputs x. We used 4 mixtures in our experiments.</p><p>Cosine similarity kernel (CosSim). The cosine similarity kernel consists in taking the product between the unit-normalized input vectors</p><formula xml:id="formula_14">k (x, x ) = xx ||x|| ||x || .<label>(13)</label></formula><p>The cosine similarity ranges from -1 (opposite) to 1 (same), with 0 indicating decorrelation (orthogonal). Following the suggestions in <ref type="bibr" target="#b43">Wang et al. (2019)</ref> we experimented with another variant, meaning centering the input vectors through BatchNorm (BN) statistics <ref type="bibr" target="#b20">Ioffe and Szegedy (2015)</ref> before the normalization (BNCosSim).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training Details</head><p>Datasets. The CUB dataset <ref type="bibr" target="#b42">(Wah et al., 2011)</ref> consists of 11788 images across 200 classes. We divide the dataset in 100 classes for train, 50 for validation, and 50 for test <ref type="bibr" target="#b17">(Hilliard et al., 2018;</ref><ref type="bibr" target="#b5">Chen et al., 2019)</ref>. The mini-ImageNet dataset <ref type="bibr" target="#b31">(Ravi and Larochelle, 2017)</ref> consists of a subset of 100 classes (600 images for each class) taken from the ImageNet dataset <ref type="bibr" target="#b33">(Russakovsky et al., 2015)</ref>. We use 64 classes for train, 16 for validation and 20 for test, as is common practice <ref type="bibr" target="#b31">(Ravi and Larochelle, 2017;</ref><ref type="bibr" target="#b5">Chen et al., 2019)</ref>. The Omniglot dataset <ref type="bibr" target="#b23">(Lake et al., 2011)</ref> contains 1623 black and white characters taken from 50 different languages. Following standard practice, the number of classes is increased to 6492 by adding examples rotated by 90 • , and we use 4114 for training. The EMNIST dataset <ref type="bibr" target="#b6">(Cohen et al., 2017)</ref> contains single digits and characters from the English alphabet. We split the 62 classes into 31 for validation and 31 for test.</p><p>Regression. In the function prediction experiment, we use the same backbone network described in <ref type="bibr" target="#b8">Finn et al. (2017)</ref>: a two-layer MLP, where each layer has 40 units and ReLU activations. We use the Adam optimizer with learning rate 10 −3 over 5 × 10 5 training iterations. For regression with feature transfer, a network is trained to predict the output of a function over all tasks, before being fine-tuned on a new task (with 1 or 100 steps of size 10 −3 ). For the head pose estimation backbone, we use a three-layer convolutional neural network, each with 36 output channels, stride 2, and dilation 2 to downsample the 100 × 100 input images. We train for 100 steps using the Adam optimizer with learning rate 10 −3 .</p><p>Classification. At training time we apply standard data augmentation (random crop, horizontal flip, and color jitter). The 1-shot training consists of 600 epochs, and 5-shot of 400, for MAML it corresponds to 60000 and 40000 episodes, and for Feature Transfer and Baseline++ to 400 and 600 supervised epochs with a mini-batch size of 16. In DKT, the hyperparameters of the kernel are optimized with a learning rate one order of magnitude lower than that used for training the CNN. This helped with convergence. In all experiments we used first-order MAML for memory efficiency. This does not significantly affect results (see <ref type="bibr" target="#b5">Chen et al., 2019)</ref>. In all cases the validation set has been used to select the training epoch/episode with the best accuracy. In classification and cross-domain experiments, each method uses the same backbone (a four layer CNN), optimizer (Adam), and learning rate (10 −3 ). We use shallow backbones because they have been shown to highlight differences between methods <ref type="bibr" target="#b5">(Chen et al., 2019)</ref>. The CNN used for classification is given in <ref type="figure">Figure 2</ref>.  <ref type="figure">Figure 2</ref>: The CNN used as a backbone for classification. It consists of 4 convolutional layers, each consisting of a 2D convolution, a batch-norm layer, and a ReLU non-linearity. The first convolution changes the the number of channels of the input to 64, and the remaining convolutions retain this channel dimension. Each convolutional layer is followed by a max-pooling operation that decreases the spatial resolution of its input by a half. Finally, the output is flattened into a vector when is used as a feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Results: Regression Experiments</head><p>Here, we provide additional samples of the few-shot regression experiments for a qualitative comparison ( <ref type="figure">Figure 3</ref>). Additionally we compare the latent spaces in the head trajectory estimation experiment. We reduced the number of hidden units to h = {h1, h2} and used a hyperbolic tangent activation function (tanh) to project the values to a Cartesian plane with hi ∈ [−1, 1]. We then sampled 100 trajectories from the test set and recorded the value of h for the targets. The resulting plot is shown in <ref type="figure" target="#fig_1">Figure 4</ref>. The spectral kernel enforces a more compact manifold, clustering the head poses on a linear gradient based on the value of the target, leading to more accurate predictions. <ref type="figure">Figure 3</ref>: Additional samples for the unknown periodic function prediction experiment. We compare methods for in-range (top row) and out-of-range (bottom row) conditions. The true function is plotted in solid blue, the out-of-range portion in dotted blue, the approximation in red, and the uncertainty is given by a red shadow. The 5 support points (blue stars) are uniformly sampled in the available range.  Kernel comparison. In <ref type="table" target="#tab_2">Table 5</ref> we show a comparison between different kernels (linear, RBF, Matérn, Polynomial p = 1 and p = 2, CosSim, BNCosSim) trained on CUB and mini-ImageNet. In this setting using a BNCosSim kernel gives a large advantage in almost all conditions. This result is in line with the findings of <ref type="bibr" target="#b43">Wang et al. (2019)</ref>, who showed how centering and unit normalizing the features considerably improve the performance in classification tasks. The overall performance of CosSim and BNCosSim is also in accordance with the findings of <ref type="bibr" target="#b5">Chen et al. (2019)</ref> and their implementation of Baseline++, an effective feature transfer method based on the cosine distance. Further investigations are necessary in this direction to understand the reason why cosine metrics and normalization are so important in few-shot learning.  <ref type="table">Table 6</ref>: Average accuracy and standard deviation (percentage) over three runs on 1-shot and 5-shot classification (5-ways), for different backbones in the CUB dataset. We use the same setup as in the classification setting. The results for the ResNet are the ones reported in <ref type="bibr" target="#b5">Chen et al. (2019)</ref>. DKT has the best score in 1-shot Conv-4, and 5-shot ResNet, while being competitive in the other conditions. Best results highlighted in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Results: Classification Experiments</head><p>Conv-4 ResNet-10 Method 1-shot 5-shot 1-shot 5-shot Feature Transfer 46.19 ± 0.64 68.40 ± 0.79 63.64 ± 0.91 81.27 ± 0.57</p><p>Baseline++ <ref type="bibr" target="#b5">(Chen et al., 2019)</ref> 61.75 ± 0.95 78.51 ± 0.59 69.55 ± 0.89 85.17 ± 0.50 MatchingNet <ref type="bibr" target="#b41">(Vinyals et al., 2016)</ref> 60.19 ± 1.02 75.11 ± 0.35 71.29 ± 0.87 83.47 ± 0.58</p><p>ProtoNet <ref type="bibr" target="#b36">(Snell et al., 2017)</ref> 52.52 ± 1.90 75.93 ± 0.46 73.22 ± 0.92 85.01 ± 0.52 MAML <ref type="bibr" target="#b8">(Finn et al., 2017)</ref> 56.11 ± 0.69 74.84 ± 0.62 70.32 ± 0.99 80.93 ± 0.71</p><p>RelationNet <ref type="bibr" target="#b38">(Sung et al., 2018)</ref> 62.52 ± 0.34 78.22 ± 0.07 70.47 ± 0.99 83.70 ± 0.55</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DKT + CosSim (ours)</head><p>63.37 ± 0.19 77.73 ± 0.26 70.81 ± 0.52 83.26 ± 0.50 <ref type="bibr">DKT + BNCosSim (ours)</ref> 62.96 ± 0.62 77.76 ± 0.62 72.27 ± 0.30 85.64 ± 0.29 <ref type="table">Table 7</ref>: Average Expected Calibration Error (ECE, <ref type="bibr" target="#b14">Guo et al. 2017)</ref> with standard deviation (percentage) over three runs on 1-shot and 5-shot classification (5-ways) in the CUB dataset. The lower the better. For the training phase we used the same setup as in the classification experiments. In the evaluation phase, the temperature of all models has been calibrated on 3000 randomly generated tasks, then each method has been evaluated on a separate set of 3000 randomly generated test tasks. DKT has the third lowest error in 1-shot, and the second lowest error in 5-shot. Best results highlighted in bold. 12.57 ± 0.23 18.43 ± 0.16</p><p>Baseline++ <ref type="bibr" target="#b5">(Chen et al., 2019)</ref> 4.91 ± 0.81 2.04 ± 0.67</p><p>MatchingNet <ref type="bibr" target="#b41">(Vinyals et al., 2016)</ref> 3.11 ± 0.39 2.23 ± 0.25</p><p>ProtoNet <ref type="bibr" target="#b36">(Snell et al., 2017)</ref> 1.07 ± 0.15 0.93 ± 0.16</p><p>MAML <ref type="bibr" target="#b8">(Finn et al., 2017)</ref> 1.14 ± 0.22 2.47 ± 0.07</p><p>RelationNet <ref type="bibr" target="#b38">(Sung et al., 2018)</ref> 4.13 ± 1.72 2.80 ± 0.63</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DKT + BNCosSim (ours)</head><p>2.62 ± 0.19 1.15 ± 0.21 E Additional Results: Cross-Domain Experiments <ref type="table">Table 8</ref>: Average accuracy and standard deviation (percentage) over three runs on the cross-domain setting (5-ways). We use the same setup as in the classification setting. The proposed method (DKT) has the best score on most conditions. Best results highlighted in bold.</p><p>Omniglot→EMNIST mini-ImageNet→CUB Method 1-shot 5-shot 1-shot 5-shot Feature Transfer 64.22 ± 1.24 86.10 ± 0.84 32.77 ± 0.35 50.34 ± 0.27</p><p>Baseline++ <ref type="bibr" target="#b5">(Chen et al., 2019)</ref> 56.84 ± 0.91 80.01 ± 0.92 39.19 ± 0.12 57.31 ± 0.11 MatchingNet <ref type="bibr" target="#b41">(Vinyals et al., 2016)</ref> 75.01 ± 2.09 87.41 ± 1.79 36.98 ± 0.06 50.72 ± 0.36</p><p>ProtoNet <ref type="bibr" target="#b36">(Snell et al., 2017)</ref> 72.04 ± 0.82 87.22 ± 1.01 33.27 ± 1.09 52.16 ± 0.17 MAML <ref type="bibr" target="#b8">(Finn et al., 2017)</ref> 72.68 ± 1.85 83.54 ± 1.79 34.01 ± 1.25 48.83 ± 0.62</p><p>RelationNet <ref type="bibr" target="#b38">(Sung et al., 2018)</ref> 75.62 ± 1.00 87.84 ± 0.27 37.13 ± 0.20 51.76 ± 1.48 Kernel comparison. In <ref type="table">Table 9</ref> we show a comparison between different kernels (linear, RBF, Matérn, Polynomial p = 1 and p = 2, CosSim, BNCosSim) trained on Omniglot→EMNIST and mini-ImageNet→CUB. Overall using a BNCosSim kernel still gives an advantage in almost all conditions, showing stable results. The best accuracy is achieved using more specialized kernels, however they often reach peak performance in specific conditions while underperforming in others. <ref type="table">Table 9</ref>: Average accuracy and standard deviation (percentage) over three runs on the cross-domain setting (5-ways) for different kernels. We use the same setup as in the classification setting.</p><p>Omniglot→EMNIST mini-ImageNet→CUB Kernel </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Comparison between different methods for unknown function approximation (out-ofrange, 5 support points). DKT better fits (red line) the true function (solid blue) and the out-of-bound portion never seen at training time (dashed blue). Uncertainty (red shadow) increases in lowconfidence regions. (b) Uncertainty estimation for an outlier (Cutout noise, red frame) in the head trajectory estimation from images. DKT is able to estimate a mean value (red line) close to the true value (blue circle) showing large variance. Feature transfer performs poorly at the same location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Latent space representation enforced by an RBF (left) and Spectral (right) kernel on the head trajectory experiments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>in-range</cell><cell>out-of-range</cell></row><row><cell cols="2">Periodic functions</cell><cell></cell></row><row><cell>ADKL (Tossou et al., 2019)  *</cell><cell>0.14</cell><cell>-</cell></row><row><cell>R2-D2 (Bertinetto et al., 2019)  *</cell><cell>0.46</cell><cell>-</cell></row><row><cell cols="2">ALPaCA (Harrison et al., 2018) 0.14 ± 0.09</cell><cell>5.92 ± 0.11</cell></row><row><cell>Feature Transfer/1</cell><cell>2.94 ± 0.16</cell><cell>6.13 ± 0.76</cell></row><row><cell>Feature Transfer/100</cell><cell>2.67 ± 0.15</cell><cell>6.94 ± 0.97</cell></row><row><cell>MAML (1 step)</cell><cell>2.76 ± 0.06</cell><cell>8.45 ± 0.25</cell></row><row><cell>DKBaseline + RBF</cell><cell>2.85 ± 1.14</cell><cell>3.65 ± 1.63</cell></row><row><cell>DKBaseline + Spectral</cell><cell>2.08 ± 2.31</cell><cell>4.11 ± 1.92</cell></row><row><cell>DKT + RBF (ours)</cell><cell>1.38 ± 0.03</cell><cell>2.61 ± 0.16</cell></row><row><cell>DKT + Spectral (ours)</cell><cell>0.08 ± 0.06</cell><cell>0.10 ± 0.06</cell></row><row><cell cols="2">Head pose trajectory</cell><cell></cell></row><row><cell>Feature Transfer/1</cell><cell>0.25 ± 0.04</cell><cell>0.20 ± 0.01</cell></row><row><cell>Feature Transfer/100</cell><cell>0.22 ± 0.03</cell><cell>0.18 ± 0.01</cell></row><row><cell>MAML (1 step)</cell><cell>0.21 ± 0.01</cell><cell>0.18 ± 0.02</cell></row><row><cell>DKT + RBF (ours)</cell><cell>0.12 ± 0.04</cell><cell>0.14 ± 0.03</cell></row><row><cell>DKT + Spectral (ours)</cell><cell>0.10 ± 0.01</cell><cell>0.11 ± 0.02</cell></row></table><note>Average Mean-Squared Error (MSE) and standard deviation (three runs) on few- shot regression for periodic functions (top) and head pose trajectory (bottom), using 10 samples for train, and 5 for test. Same do- main is marked as in-range, extended unseen domain as out-of-range. Lowest error in bold.* Results reported by Tossou et al. (2019).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 5</head><label>5</label><figDesc>, in appendix) is likely due to a low-curvature manifold induced by the neural network in the latent space, increasing the linear separability of data. Overall our results confirm the findings of<ref type="bibr" target="#b5">Chen et al. (2019)</ref> regarding the effectiveness of cosine metrics, and those of<ref type="bibr" target="#b43">Wang et al. (2019)</ref> on the importance of feature normalization (Appendix D and E). In Table 6 (in appendix) we report results with a deeper backbone (ResNet-10,<ref type="bibr" target="#b16">He et al. 2016)</ref>, showing that DKT outperforms all other methods in 5-shot (85.64%) with the second best result in 1-shot (72.27%). The difference in performance between CosSim and BNCosSim is larger for the deeper backbone, indicating that centering the features is important when additional layers are added to the network.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Average accuracy and standard deviation (percentage) over three runs (1-shot, 5-ways, Conv-4) on CUB and cross-domain classification (Omniglot to EMNIST and mini-Imagenet to CUB). Best results highlighted in bold.</figDesc><table><row><cell>Method</cell><cell>CUB</cell><cell cols="2">Omni→EMNIST ImgNet→CUB</cell></row><row><cell>Feature Transfer</cell><cell>46.19 ± 0.64</cell><cell>64.22 ± 1.24</cell><cell>32.77 ± 0.35</cell></row><row><cell>Baseline++ (Chen et al., 2019)</cell><cell>61.75 ± 0.95</cell><cell>56.84 ± 0.91</cell><cell>39.19 ± 0.12</cell></row><row><cell cols="2">MatchingNet (Vinyals et al., 2016) 60.19 ± 1.02</cell><cell>75.01 ± 2.09</cell><cell>36.98 ± 0.06</cell></row><row><cell>ProtoNet (Snell et al., 2017)</cell><cell>52.52 ± 1.90</cell><cell>72.04 ± 0.82</cell><cell>33.27 ± 1.09</cell></row><row><cell>MAML (Finn et al., 2017)</cell><cell>56.11 ± 0.69</cell><cell>72.68 ± 1.85</cell><cell>34.01 ± 1.25</cell></row><row><cell>RelationNet (Sung et al., 2018)</cell><cell>62.52 ± 0.34</cell><cell>75.62 ± 1.00</cell><cell>37.13 ± 0.20</cell></row><row><cell>DKT + Linear (ours)</cell><cell>60.23 ± 0.76</cell><cell>75.97 ± 0.70</cell><cell>38.72 ± 0.42</cell></row><row><cell>DKT + CosSim (ours)</cell><cell>63.37 ± 0.19</cell><cell>73.06 ± 2.36</cell><cell>40.22 ± 0.54</cell></row><row><cell>DKT + BNCosSim (ours)</cell><cell>62.96 ± 0.62</cell><cell>75.40 ± 1.10</cell><cell>40.14 ± 0.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Classification: 1-shot, 5ways, Conv-4. Best results in bold.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Average accuracy and standard deviation (percentage) on the few-shot classification setting(5-ways).[top]  Results reported in recent literature. For a fair comparison we selected only those methods that have been trained with a similar backbone and training schedule.[center-bottom]   Methods trained from scratch (three runs) with the same backbone (a four layer CNN), optimizer (Adam), and learning rate (10 −3 ). Test performed on novel classes with 3000 randomly generated tasks. DKT is competitive across various datasets and conditions. Best results highlighted in bold. Reported by<ref type="bibr" target="#b21">Jerfel et al. (2019)</ref> using a comparable backbone. ± 0.64 68.40 ± 0.79 39.51 ± 0.23 60.51 ± 0.55Baseline++<ref type="bibr" target="#b5">(Chen et al., 2019)</ref> 61.75 ± 0.95 78.51 ± 0.59 47.15 ± 0.49 66.18 ± 0.18MatchingNet<ref type="bibr" target="#b41">(Vinyals et al., 2016)</ref> 60.19 ± 1.02 75.11 ± 0.35 48.25 ± 0.65 62.71 ± 0.44ProtoNet<ref type="bibr" target="#b36">(Snell et al., 2017)</ref> 52.52 ± 1.90 75.93 ± 0.46 44.19 ± 1.30 64.07 ± 0.65 MAML<ref type="bibr" target="#b8">(Finn et al., 2017)</ref> 56.11 ± 0.69 74.84 ± 0.62 45.39 ± 0.49 61.58 ± 0.53RelationNet<ref type="bibr" target="#b38">(Sung et al., 2018)</ref> 62.52 ± 0.34 78.22 ± 0.07 48.76 ± 0.17 64.20 ± 0.28DKT + CosSim (ours)63.37 ± 0.19 77.73 ± 0.26 48.64 ± 0.45 62.85 ± 0.37 DKT + BNCosSim (ours) 62.96 ± 0.62 77.76 ± 0.62 49.73 ± 0.07 64.00 ± 0.09</figDesc><table><row><cell>CUB</cell><cell>mini-ImageNet</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Average accuracy and standard deviation (percentage) on the few-shot classification setting (5-ways) for different kernels. Methods trained from scratch (three runs) with the same backbone (a four layer CNN), optimizer (Adam), and learning rate (10 −3 ). Test performed on novel classes with 3000 randomly generated tasks.60.23 ± 0.76 74.74 ± 0.22 48.44 ± 0.36 62.88 ± 0.46 RBF 55.34 ± 2.56 73.20 ± 1.41 45.92 ± 1.08 61.42 ± 0.74 Matérn 58.20 ± 0.63 73.21 ± 1.30 47.65 ± 0.85 62.59 ± 0.12 Polynomial (p = 1) 59.54 ± 1.10 74.51 ± 0.98 47.78 ± 0.60 62.54 ± 0.96 Polynomial (p = 2) 5718 ± 0.40 71.14 ± 0.58 46.36 ± 0.34 60.26 ± 0.40 CosSim 63.37 ± 0.19 77.73 ± 0.26 48.64 ± 0.45 62.85 ± 0.37 BNCosSim 62.96 ± 0.62 77.76 ± 0.62 49.73 ± 0.07 64.00 ± 0.09</figDesc><table><row><cell></cell><cell>CUB</cell><cell></cell><cell cols="2">mini-ImageNet</cell></row><row><cell>Kernel</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row></table><note>Linear</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>DKT + Linear (ours) 75.97 ± 0.70 89.51 ± 0.44 38.72 ± 0.42 54.20 ± 0.37 DKT + CosSim (ours) 73.06 ± 2.36 88.10 ± 0.78 40.22 ± 0.54 55.65 ± 0.05 DKT + BNCosSim (ours) 75.40 ± 1.10 90.30 ± 0.49 40.14 ± 0.18 56.40 ± 1.34</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>75.97 ± 0.70 89.51 ± 0.44 38.72 ± 0.42 54.20 ± 0.37 RBF 74.46 ± 0.41 88.38 ± 0.53 36.22 ± 0.40 51.30 ± 0.52 Matérn 75.46 ± 0.20 88.04 ± 1.81 36.98 ± 0.41 51.35 ± 0.16 Polynomial (p = 1) 74.33 ± 0.67 90.72 ± 0.47 38.24 ± 0.30 54.11 ± 0.40 Polynomial (p = 2) 75.58 ± 1.18 88.06 ± 0.70 36.83 ± 0.46 51.92 ± 0.87 CosSim 73.06 ± 2.36 88.10 ± 0.78 40.22 ± 0.54 55.65 ± 0.05 BNCosSim 75.40 ± 1.10 90.30 ± 0.49 40.14 ± 0.18 56.40 ± 1.34</figDesc><table><row><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row></table><note>Linear</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/BayesWatch/deep-kernel-transfer</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">How to train your MAML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Defining benchmarks for continual few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patacchiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ochal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11967</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to learn by self-critique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the optimization of a synaptic learning rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cloutier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gecsei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Preprints Conf. Optimality in Artificial and Biological Neural Networks</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable closed-form solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">EMNIST: an extension of MNIST to handwritten letters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Afshar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tapson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Schaik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05373</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Improved regularization of convolutional neural networks with cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Probabilistic model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">GPyTorch: Blackbox matrixmatrix gaussian process inference with gpu acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bindel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An investigation into face pose distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mckenna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Automatic Face and Gesture Recognition</title>
		<meeting>the International Conference on Automatic Face and Gesture Recognition</meeting>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Meta-learning probabilistic inference for prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Recasting gradient-based meta-learning as hierarchical bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Meta-learning priors for efficient online bayesian regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.08912</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Few-shot learning with metric-agnostic conditional embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hilliard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Howland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yankov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Corley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">O</forename><surname>Hodas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04376</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Using deep belief nets to learn covariance kernels for gaussian processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05439</idno>
		<title level="m">Meta-learning in neural networks: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reconciling meta-learning and continual learning with online mixtures of tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jerfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Heller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gaussian process models for robust regression, classification, and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kuss</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
		<respStmt>
			<orgName>Technische Universität</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Cognitive Science Society</title>
		<meeting>the Annual Meeting of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page">436</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple neural attentive meta-learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Tadam: Task dependent adaptive metric for improved few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Oreshkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuille</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Meta-learning with implicit gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rajeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Gaussian Processes for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>MIT press</publisher>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">In defense of one-vs-all classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rifkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klautau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="141" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to control fast-weight memories: An alternative to dynamic recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="131" to="139" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Probabilistic inference in human semantic memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steyvers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in Cognitive Sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="327" to="334" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">How to grow a mind: Statistics, structure, and abstraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="issue">6022</biblScope>
			<biblScope unit="page" from="1279" to="1285" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tossou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lacoste</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12131</idno>
		<title level="m">Adaptive deep kernel learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<title level="m">The Caltech-UCSD birds-200-2011 dataset</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Simpleshot: Revisiting nearestneighbor classification for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.04623</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gaussian process kernels for pattern discovery and extrapolation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Ton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Kosiorek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.02738</idno>
		<title level="m">Metafun: Meta-learning with iterative functional updates</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning embedding adaptation for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-C</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03664</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Bayesian model-agnostic meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Dia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahn</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
