<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Graph Network for Multi-hop Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
							<email>siqi.sun@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
							<email>zhe.gan@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Pillai</surname></persName>
							<email>rohit.pillai@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
							<email>shuohang.wang@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Microsoft Dynamics 365 AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Graph Network for Multi-hop Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we present Hierarchical Graph Network (HGN) for multi-hop question answering. To aggregate clues from scattered texts across multiple paragraphs, a hierarchical graph is created by constructing nodes on different levels of granularity (questions, paragraphs, sentences, entities), the representations of which are initialized with pre-trained contextual encoders. Given this hierarchical graph, the initial node representations are updated through graph propagation, and multihop reasoning is performed via traversing through the graph edges for each subsequent sub-task (e.g., paragraph selection, supporting facts extraction, answer prediction). By weaving heterogeneous nodes into an integral unified graph, this hierarchical differentiation of node granularity enables HGN to support different question answering sub-tasks simultaneously. Experiments on the HotpotQA benchmark demonstrate that the proposed model achieves new state of the art, outperforming existing multi-hop QA approaches. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In contrast to one-hop question answering <ref type="bibr" target="#b17">(Rajpurkar et al., 2016;</ref><ref type="bibr" target="#b23">Trischler et al., 2016;</ref><ref type="bibr" target="#b8">Lai et al., 2017)</ref> where answers can be derived from a single paragraph <ref type="bibr" target="#b27">(Wang and Jiang, 2017;</ref><ref type="bibr" target="#b18">Seo et al., 2017;</ref><ref type="bibr" target="#b10">Liu et al., 2018;</ref><ref type="bibr">Devlin et al., 2019)</ref>, many recent studies on question answering focus on multi-hop reasoning across multiple documents or paragraphs. Popular tasks include WikiHop <ref type="bibr" target="#b29">(Welbl et al., 2018)</ref>, ComplexWebQuestions <ref type="bibr" target="#b22">(Talmor and Berant, 2018)</ref>, and HotpotQA <ref type="bibr" target="#b33">(Yang et al., 2018)</ref>.</p><p>An example from HotpotQA is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. In order to correctly answer the question ("The director of the romantic comedy 'Big Stone Gap' is based in what New York city"), the model is 1 Code will be released at https://github.com/yuwfan/HGN. required to first identify P1 as a relevant paragraph, whose title contains the keywords that appear in the question ("Big Stone Gap"). S1, the first sentence of P1, is then chosen by the model as a supporting fact that leads to the next-hop paragraph P2. Lastly, from P2, the span "Greenwich Village, New York City" is selected as the predicted answer.</p><p>Most existing studies use a retriever to find paragraphs that contain the right answer to the question (P1 and P2 in this case). A Machine Reading Comprehension (MRC) model is then applied to the selected paragraphs for answer prediction <ref type="bibr" target="#b15">(Nishida et al., 2019;</ref><ref type="bibr" target="#b14">Min et al., 2019b)</ref>. However, even after successfully identifying a reasoning chain through multiple paragraphs, it still remains a critical challenge how to aggregate evidence from scattered arXiv:1911.03631v4 [cs.CL] 6 Oct 2020 sources on different granularity levels (e.g., paragraphs, sentences, entities) for joint answer and supporting facts prediction.</p><p>To better leverage fine-grained evidences, some studies apply entity graphs through query-guided multi-hop reasoning. Depending on the characteristics of the dataset, answers can be selected either from entities in the constructed entity graph <ref type="bibr" target="#b20">(Song et al., 2018;</ref><ref type="bibr">Dhingra et al., 2018;</ref><ref type="bibr">De Cao et al., 2019;</ref><ref type="bibr" target="#b25">Tu et al., 2019;</ref><ref type="bibr">Ding et al., 2019)</ref>, or from spans in documents by fusing entity representations back into token-level document representation <ref type="bibr" target="#b31">(Xiao et al., 2019)</ref>. However, the constructed graph is mostly used for answer prediction only, while insufficient for finding supporting facts. Also, reasoning through a simple entity graph <ref type="bibr">(Ding et al., 2019)</ref> or paragraph-entity hybrid graph <ref type="bibr" target="#b25">(Tu et al., 2019)</ref> lacks the ability to support complicated questions that require multi-hop reasoning.</p><p>Intuitively, given a question that requires multiple hops through a set of documents to reach the right answer, a model needs to: (i) identify paragraphs relevant to the question; (ii) determine strong supporting evidence in those paragraphs; and (iii) pinpoint the right answer following the garnered evidence. To this end, Graph Neural Network with its inherent message passing mechanism that can pass on multi-hop information through graph propagation, has great potential of effectively predicting both supporting facts and answer simultaneously for complex multi-hop questions.</p><p>Motivated by this, we propose a Hierarchical Graph Network (HGN) for multi-hop question answering, which empowers joint answer/evidence prediction via multi-level fine-grained graphs in a hierarchical framework. Instead of only using entities as nodes, for each question we construct a hierarchical graph to capture clues from sources with different levels of granularity. Specifically, four types of graph node are introduced: questions, paragraphs, sentences and entities (see <ref type="figure" target="#fig_1">Figure 2</ref>). To obtain contextualized representations for these hierarchical nodes, large-scale pre-trained language models such as <ref type="bibr">BERT (Devlin et al., 2019)</ref> and RoBERTa <ref type="bibr" target="#b11">(Liu et al., 2019)</ref> are used for contextual encoding. These initial representations are then passed through a Graph Neural Network for graph propagation. The updated node representations are then exploited for different sub-tasks (e.g., paragraph selection, supporting facts prediction, entity prediction). Since answers may not be entities in the graph, a span prediction module is also introduced for final answer prediction.</p><p>The main contributions of this paper are threefold: (i) We propose a Hierarchical Graph Network (HGN) for multi-hop question answering, where heterogeneous nodes are woven into an integral hierarchical graph. (ii) Nodes from different granularity levels mutually enhance each other for different sub-tasks, providing effective supervision signals for both supporting facts extraction and answer prediction. (iii) On the HotpotQA benchmark, the proposed model achieves new state of the art in both Distractor and Fullwiki settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Multi-Hop QA Multi-hop question answering requires a model to aggregate scattered pieces of evidence across multiple documents to predict the right answer. WikiHop <ref type="bibr" target="#b29">(Welbl et al., 2018)</ref> and Hot-potQA <ref type="bibr" target="#b33">(Yang et al., 2018)</ref> are two recent datasets designed for this purpose. Existing work on Hot-potQA Distractor setting focuses on converting the multi-hop reasoning task into single-hop subproblems. Specifically, QFE <ref type="bibr" target="#b15">(Nishida et al., 2019)</ref> regards evidence extraction as a query-focused summarization task, and reformulates the query in each hop. DecompRC <ref type="bibr" target="#b14">(Min et al., 2019b)</ref> decomposes a compositional question into simpler sub-questions and leverages single-hop MRC models to answer the sub-questions. A neural modular network is also proposed in <ref type="bibr" target="#b6">Jiang and Bansal (2019b)</ref>, where neural modules are dynamically assembled for more interpretable multi-hop reasoning. Recent studies <ref type="bibr">(Chen and Durrett, 2019;</ref><ref type="bibr" target="#b12">Min et al., 2019a;</ref><ref type="bibr" target="#b5">Jiang and Bansal, 2019a)</ref> have also studied the multi-hop reasoning behaviors that models have learned in the task.</p><p>Graph Neural Network Recent studies on multi-hop QA also build graphs based on entities and reasoning over the constructed graph using graph neural networks <ref type="bibr" target="#b7">(Kipf and Welling, 2017;</ref><ref type="bibr" target="#b26">Veličković et al., 2018)</ref>. MHQA-GRN <ref type="bibr" target="#b20">(Song et al., 2018)</ref> and Coref-GRN (Dhingra et al., 2018) construct an entity graph based on co-reference resolution or sliding windows. Entity-GCN <ref type="bibr">(De Cao et al., 2019)</ref> considers three different types of edges that connect different entities in the entity graph. HDE-Graph <ref type="bibr" target="#b25">(Tu et al., 2019)</ref> enriches information in the entity graph by adding document nodes and creating interactions among documents, entities and answer candidates. Cognitive Graph QA (Ding  <ref type="bibr" target="#b31">(Xiao et al., 2019)</ref> constructs a dynamic entity graph, where in each reasoning step irrelevant entities are softly masked out and a fusion module is designed to improve the interaction between the entity graph and documents.</p><p>More recently, SAE <ref type="bibr" target="#b24">(Tu et al., 2020)</ref> defines three types of edge in the sentence graph based on the named entities and noun phrases appearing in the question and sentences. C2F Reader <ref type="bibr" target="#b19">(Shao et al., 2020)</ref> uses graph attention or self-attention on entity graph, and argues that this graph may not be necessary for multi-hop reasoning. <ref type="bibr" target="#b0">Asai et al. (2020)</ref> proposes a new graph-based recurrent method to find evidence documents as reasoning paths, which is more focused on information retrieval. Different from the above methods, our proposed model constructs a hierarchical graph, effectively exploring relations on different granularities and employing different nodes to perform different tasks.</p><p>Hierarchical Coarse-to-Fine Modeling Previous work on hierarchical modeling for question answering is mainly based on a coarse-to-fine framework. <ref type="bibr">Choi et al. (2017)</ref> proposes to use reinforcement learning to first select relevant sentences and then produce answers from those sentences. <ref type="bibr" target="#b13">Min et al. (2018)</ref> investigates the minimal context required to answer a question, and observes that most questions can be answered with a small set of sentences. <ref type="bibr" target="#b21">Swayamdipta et al. (2018)</ref> constructs lightweight models and combines them into a cas-cade structure to extract the answer. <ref type="bibr">Zhong et al. (2019)</ref> proposes to use hierarchies of co-attention and self-attention to combine information from evidence across multiple documents. Different from the above methods, our proposed model organizes different granularities in a hierarchical manner and leverages graph neural network to obtain the representations for different downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hierarchical Graph Network</head><p>As illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>, the proposed Hierarchical Graph Network (HGN) consists of four main components: (i) Graph Construction Module (Sec. 3.1), through which a hierarchical graph is constructed to connect clues from different sources; (ii) Context Encoding Module (Sec. 3.2), where initial representations of graph nodes are obtained via a RoBERTa-based encoder; (iii) Graph Reasoning Module (Sec. 3.3), where graph-attention-based message passing algorithm is applied to jointly update node representations; and (iv) Multi-task Prediction Module (Sec. 3.4), where multiple subtasks, including paragraph selection, supporting facts prediction, entity prediction, and answer span extraction, are performed simultaneously.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph Construction</head><p>The hierarchical graph is constructed in two steps: (i) identifying relevant multi-hop paragraphs; and (ii) adding edges representing connections between sentences/entities within the selected paragraphs.</p><p>Paragraph Selection We first retrieve paragraphs whose titles match any phrases in the question (title matching). In addition, we train a para-graph ranker based on a pre-trained RoBERTa encoder, followed by a binary classification layer, to rank the probabilities of whether the input paragraphs contain the ground-truth supporting facts. If multiple paragraphs are found by title matching, only two paragraphs with the highest ranking scores are selected. If title matching returns no results, we further search for paragraphs that contain entities appearing in the question. If this also fails, the paragraph ranker will select the paragraph with the highest ranking score. The number of selected paragraphs in the first-hop is at most 2.</p><p>Once the first-hop paragraphs are identified, the next step is to find facts and entities within the paragraphs that can lead to other relevant paragraphs (i.e,, the second hop). Instead of relying on entity linking, which could be noisy, we use hyperlinks (provided by Wikipedia) in the first-hop paragraphs to discover second-hop paragraphs. Once the links are selected, we add edges between the sentences containing these links (source) and the paragraphs that the hyperlinks refer to (target), as illustrated by the dashed orange line in <ref type="figure" target="#fig_1">Figure 2</ref>. In order to allow information flow from both directions, the edges are considered as bidirectional.</p><p>Through this two-hop selection process, we are able to obtain several candidate paragraphs. In order to reduce introduced noise during inference, we use the paragraph ranker to select paragraphs with top-N ranking scores in each step.</p><p>Nodes and Edges Paragraphs are comprised of sentences, and each sentence contains multiple entities. This graph is naturally encoded in a hierarchical structure, and also motivates how we construct the hierarchical graph. For each paragraph node, we add edges between the node and all the sentences in the paragraph. For each sentence node, we extract all the entities in the sentence and add edges between the sentence node and these entity nodes. Optionally, edges between paragraphs and edges between sentences can also be included in the final graph.</p><p>Each type of these nodes captures semantics from different information sources. Thus, the hierarchical graph effectively exploits the structural information across all different granularity levels to learn fine-grained representations, which can locate supporting facts and answers more accurately than simpler graphs with homogeneous nodes.</p><p>An example hierarchical graph is illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. We define different types of edges as follows: (i) edges between question node and paragraph nodes; (ii) edges between question node and its corresponding entity nodes (entities appearing in the question, not shown for simplicity); (iii) edges between paragraph nodes and their corresponding sentence nodes (sentences within the paragraph); (iv) edges between sentence nodes and their linked paragraph nodes (linked through hyperlinks); (v) edges between sentence nodes and their corresponding entity nodes (entities appearing in the sentences); (vi) edges between paragraph nodes; and (vii) edges between sentence nodes that appear in the same paragraph. Note that a sentence is only connected to its previous and next neighboring sentence. The final graph consists of these seven types of edges as well as four types of nodes, which link the question to paragraphs, sentences, and entities in a hierarchical way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context Encoding</head><p>Given the constructed hierarchical graph, the next step is to obtain the initial representations of all the graph nodes. To this end, we first combine all the selected paragraphs into context C, which is concatenated with the question Q and fed into pre-trained Transformer RoBERTa, followed by a bi-attention layer <ref type="bibr" target="#b18">(Seo et al., 2017)</ref>. We denote the encoded question representation as Q = {q 0 , q 1 , . . . , q m−1 } ∈ R m×d , and the encoded context representation as C = {c 0 , c 1 , ..., c n−1 } ∈ R n×d , where m, n are the length of the question and the context, respectively. Each q i and c j ∈ R d .</p><p>A shared BiLSTM is applied on top of the context representation C, and the representations of different nodes are extracted from the output of the BiLSTM, denoted as M ∈ R n×2d . For entity/sentence/paragraph nodes, which are spans of the context, the representation is calculated from: (i) the hidden state of the backward LSTM at the start position, and (ii) the hidden state of the forward LSTM at the end position. For the question node, a max-pooling layer is used to obtain its representation. Specifically,</p><formula xml:id="formula_0">p i = MLP 1 M[P (i) start ][d:]; M[P (i) end ][:d] s i = MLP 2 M[S (i) start ][d:]; M[S (i) end ][:d] e i = MLP 3 M[E (i) start ][d:]; M[E (i) end ][:d] q = max-pooling(Q) ,<label>(1)</label></formula><p>where P end denote the corresponding end positions. MLP(·) denotes an MLP layer, and [; ] denotes the concatenation of two vectors. As a summary, after context encoding, each p i , s i , and e i ∈ R d , serves as the representation of the i-th paragraph/sentence/entity node. The question node is represented as q ∈ R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Reasoning</head><p>After context encoding, HGN performs reasoning over the hierarchical graph, where the contextualized representations of all the graph nodes are transformed into higher-level features via a graph neural network. Specifically, let</p><formula xml:id="formula_2">P = {p i } np i=1 , S = {s i } ns i=1 , and E = {e i } ne i=1</formula><p>, where n p , n s and n e denote the number of paragraph/sentence/entity nodes in a graph. In experiments, we set n p = 4, n s = 40 and n e = 60 (padded where necessary), and denote H = {q, P, S, E} ∈ R g×d , where g = n p + n s + n e + 1, and d is the feature dimension of each node.</p><p>For graph propagation, we use Graph Attention Network (GAT) <ref type="bibr" target="#b26">(Veličković et al., 2018)</ref> to perform message passing over the hierarchical graph. Specifically, GAT takes all the nodes as input, and updates node feature h i through its neighbors N i in the graph. Formally,</p><formula xml:id="formula_3">h i = LeakyRelu j∈N i α ij h j W ,<label>(2)</label></formula><p>where h j is the j th vector from H, W ∈ R d×d is a weight matrix 2 to be learned, and α ij is the attention coefficients, which can be calculated by:</p><formula xml:id="formula_4">α ij = exp(f ([h i ; h j ]w e ij )) k∈N i exp(f ([h i ; h k ]w e ik )) ,<label>(3)</label></formula><p>where w e ij ∈ R 2d is the weight vector corresponding to the edge type e ij between the i-th and jth nodes, and f (·) denotes the LeakyRelu activation function. In a summary, after graph reasoning, we obtain H = {h 0 , h 1 , . . . , h g } ∈ R g×d , from which the updated representations for each type of node can be obtained, i.e., P ∈ R np×d , S ∈ R ns×d , E ∈ R ne×d , and q ∈ R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gated Attention</head><p>The graph information will further contribute to the context information for answer span extraction. We merge the context representation M and the graph representation H via a 2 Note that we omit the bias term for all the weight matrices in the paper to save space. gated attention mechanism:</p><formula xml:id="formula_5">C = Relu(MW m ) · Relu(H W m ) T H = Softmax(C) · H G = σ([M;H]W s ) · Tanh([M;H]W t ), (4) where W m ∈ R 2d×2d , W m ∈ R 2d×2d , W s ∈ R 4d×4d , W t ∈ R 4d×4d</formula><p>are weight matrices to learn. G ∈ R n×4d is the gated representation which will be used for answer span extraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-task Prediction</head><p>After graph reasoning, the updated node representations are used for different sub-tasks: (i) paragraph selection based on paragraph nodes; (ii) supporting facts prediction based on sentence nodes; and (iii) answer prediction based on entity nodes and context representation G. Since the answers may not reside in entity nodes, the loss for entity node only serves as a regularization term.</p><p>In our HGN model, all three tasks are jointly performed through multi-task learning. The final objective is defined as:</p><formula xml:id="formula_6">L joint = L start + L end + λ 1 L para + λ 2 L sent + λ 3 L entity + λ 4 L type ,<label>(5)</label></formula><p>where λ 1 , λ 2 , λ 3 , and λ 4 are hyper-parameters, and each loss function is a cross-entropy loss, calculated over the logits (described below). For both paragraph selection (L para ) and supporting facts prediction (L sent ), we use a two-layer MLP as the binary classifier:</p><formula xml:id="formula_7">o sent = MLP 4 (S ), o para = MLP 5 (P ) ,<label>(6)</label></formula><p>where o sent ∈ R ns represents whether a sentence is selected as supporting facts, and o para ∈ R np represents whether a paragraph contains the ground-truth supporting facts.</p><p>We treat entity prediction (L entity ) as a multiclass classification problem. Candidate entities include all entities in the question and those that match the titles in the context. If the ground-truth answer does not exist among the entity nodes, the entity loss is zero. Specifically, o entity = MLP 6 (E ) .</p><p>The entity loss will only serve as a regularization term, and the final answer prediction will only rely on the answer span extraction module as follows. The logits of every position being the start and end of the ground-truth span are computed by a two-layer MLP on top of G in Eqn. <ref type="formula" target="#formula_1">(4)</ref>:</p><formula xml:id="formula_9">o start = MLP 7 (G), o end = MLP 8 (G) . (8)</formula><p>Following previous work <ref type="bibr" target="#b31">(Xiao et al., 2019)</ref>, we also need to identify the answer type, which includes the types of span, entity, yes and no. We use a 3-way two-layer MLP for answer-type classification based on the first hidden representation of G:</p><formula xml:id="formula_10">o type = MLP 9 (G[0]) .<label>(9)</label></formula><p>During decoding, we first use this to determine the answer type. If it is "yes" or "no", we directly return it as the answer. Overall, the final cross-entropy loss (L joint ) used for training is defined over all the aforementioned logits:</p><formula xml:id="formula_11">o sent , o para , o entity , o start , o end , o type .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we describe experiments comparing HGN with state-of-the-art approaches and provide detailed analysis on the model and results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We use HotpotQA dataset <ref type="bibr" target="#b33">(Yang et al., 2018)</ref> for evaluation, a popular benchmark for multi-hop QA. Specifically, two sub-tasks are included in this dataset: (i) Answer prediction; and (ii) Supporting facts prediction. For each sub-task, exact match (EM) and partial match (F1) are used to evaluate model performance, and a joint EM and F1 score is used to measure the final performance, which encourages the model to take both answer and evidence prediction into consideration. There are two settings in HotpotQA: Distractor and Fullwiki setting. In the Distractor setting, for each question, two gold paragraphs with groundtruth answers and supporting facts are provided, along with 8 'distractor' paragraphs that were collected via a bi-gram TF-IDF retriever <ref type="bibr">(Chen et al., 2017)</ref>. The Fullwiki setting is more challenging, which contains the same training questions as in the Distractor setting, but does not provide relevant paragraphs for test set. To obtain the right answer and supporting facts, the entire Wikipedia can be used to find relevant documents. Implementation details can be found in Appendix B.  Joint EM/F1 with 2.57/1.08 improvement, despite using an inferior retriever; when using the same retriever as in SemanticRetrievalMRS (Yixin Nie, 2019), our method outperforms by a significant margin, demonstrating the effectiveness of our multi-hop reasoning approach. In the following sub-sections, we provide a detailed analysis on the sources of performance gain on the dev set. Additional ablation study on paragraph selection is provided in Appendix D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on Test Set</head><p>Effectiveness of Hierarchical Graph As described in Section 3.1, we construct our graph with four types of nodes and seven types of edges. For ablation study, we build the graph step by step. First, we only consider edges from question to paragraphs, and from paragraphs to sentences, i.e., only edge type (i), (iii) and (iv) are considered. We call this the PS Graph. Based on this, entity nodes and edges related to each entity node (corresponding to edge type (ii) and (v)) are added. We call this the PSE Graph. Lastly, edge types (vi) and (vii) are added, resulting in the final hierarchical graph. As shown in <ref type="table" target="#tab_6">Table 4</ref>, the use of PS Graph improves the joint F1 score over the plain RoBERTa model by 2.81 points. By further adding entity nodes, the Joint F1 increases by 0.30 points. This indicates that the addition of entity nodes is helpful, but may also bring in noise, thus only leading to limited performance improvement. By including edges among sentences and paragraphs, our final hierarchical graph provides an additional improvement of 0.24 points. We hypothesize that this is due to the explicit connection between sentences that leads to better representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effectiveness of Pre-trained Language Model</head><p>To verify the effects of pre-trained language models, we compare HGN with prior state-of-the-art methods using the same pre-trained language models. Results in <ref type="table" target="#tab_7">Table 5</ref> show that our HGN variants outperform DFGN, EPS and SAE, indicating the performance gain comes from better model design.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis</head><p>In this section, we provide an in-depth error analysis on the proposed model. HotpotQA provides two reasoning types: "bridge" and "comparison". "Bridge" questions require the identification of a bridge entity that leads to the answer, while "comparison" questions compare two entities to infer the answer, which could be yes, no or a span of text. For analysis, we further split "comparison" questions into "comp-yn" and "comp-span". Table 6 indicates that "comp-yn" questions are the easiest, on which our model achieves 88.5 joint F1 score. HGN performs similarly on "bridge" and "comp-span" with 74 joint F1 score, indicating that there is still room for further improvement.</p><p>To provide a more in-depth understanding of our     rors can be roughly grouped into six categories: (i) Annotation: the annotation provided in the dataset is not correct; (ii) Multiple Answers: questions may have multiple correct answers, but only one answer is provided in the dataset; (iii) Discrete Reasoning: this type of error often appears in "comparison" questions, where discrete reasoning is required to answer the question correctly; (iv) Commonsense &amp; External Knowledge: to answer this type of question, commonsense or external knowledge is required; (v) Multi-hop: the model fails to perform multi-hop reasoning, and finds the final answer from wrong paragraphs; (vi) MRC: model correctly finds the supporting paragraphs and sentences, but predicts the wrong answer span.</p><p>Note that these error types are not mutually exclusive, but we aim to classify each example into only one type, in the order presented above. For example, if an error is classified as 'Commonsense &amp; External Knowledge' type, it cannot be classified as 'Multi-hop' or 'MRC' error. <ref type="table" target="#tab_5">Table 3</ref> shows examples from each category (the corresponding paragraphs are omitted due to space limit).</p><p>We observed that a lot of errors are due to the fact that some questions have multiple answers with the same meaning, such as "a body of water vs. creek", "EPA vs. Environmental Protection Agency", and "American-born vs. U.S. born". In these examples, the former is the ground-truth answer, and the latter is our model's prediction. Secondly, for questions that require commonsense or discrete reasoning (e.g., "second" means "Code#02" 3 , "which band has more members", or "who was born earlier"), our model just randomly picks an entity as answer, as it is incapable of performing this type of reasoning. The majority of the errors are from either multi-hop reasoning or MRC model's span selection, which indicates that there is still room for further improvement. Additional examples are provided in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Generalizability Discussion</head><p>The hierarchical graph can be applied to different multi-hop QA datasets, though in this paper mainly tailored for HotpotQA. Here we use Wikipedia hyperlinks to connect sentences and paragraphs. An alternative way is to use an entity linking system to make it more generalizable. For each sentence node, if its entities exist in a paragraph, an edge can be added to connect the sentence and paragraph nodes. In our experiments, we restrict the number of multi-hops to two for the HotpotQA task, which can be increased to accommodate other datasets. The maximum number of paragraphs is set to four for HotpotQA, as we observe that using more documents within a maximum sequence length does not help much (see <ref type="table" target="#tab_13">Table 9</ref> in the Appendix). To generalize to other datasets that need to consume longer documents, we can either: (i) use sliding-windowbased method to chunk a long sequence into short ones; or (ii) replace the BERT-based backbone with other transformer-based models that are capable of dealing with long sequences <ref type="bibr" target="#b1">(Beltagy et al., 2020;</ref><ref type="bibr" target="#b35">Zaheer et al., 2020;</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a new approach, Hierarchical Graph Network (HGN), for multi-hop 3 Please refer to Row 4 in <ref type="table" target="#tab_5">Table 3</ref> for more context. question answering. To capture clues from different granularity levels, our HGN model weaves heterogeneous nodes into a single unified graph. Experiments with detailed analysis demonstrate the effectiveness of our proposed model, which achieves state-of-the-art performances on the Hot-potQA benchmark. Currently, in the Fullwiki setting, an off-the-shelf paragraph retriever is adopted for selecting relevant context from large corpus of text. Future work includes investigating the interaction and joint training between HGN and paragraph retriever for performance improvement. <ref type="bibr">Victor Zhong, Caiming Xiong, Nitish Keskar, and Richard Socher. 2019</ref>. Coarse-grain fine-grain coattention network for multi-evidence question answering. In ICLR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets</head><p>There are two benchmark settings in HotpotQA: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>Our implementation is based on the Transformer library <ref type="bibr">(Wolf et al., 2019)</ref>. To construct the proposed hierarchical graph, we use spacy 4 to extract entities from both questions and sentences. The numbers of entities, sentences and paragraphs in one graph are limited to 60, 40 and 4, respectively. Since HotpotQA only requires two-hop reasoning, up to two paragraphs are connected to each question. Our paragraph ranking model is a binary classifier based on the RoBERTa-large model. For the Fullwiki setting, we leverage the retrieved paragraphs and the paragraph ranker provided by Yixin Nie (2019). We finetune on the training set for 8 epochs, with batch size as 8, learning rate as 1e-5, λ 1 as 1, λ 2 as 5, λ 3 as 1, λ 4 as 1, LSTM dropout rate as 0.3 and GNN dropout rate as 0.3. We search hyperparameters for learning rate from {1e-5, 2e-5, 3e-5} , λ 2 from {1, 3, 5} and dropout rate from {0.1, 0.3, 0.5}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Computing Resources</head><p>We conduct experiments on 4 Quadro RTX 8000 GPUs. The parameters of each component in HGN are summarized in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Effectiveness of Paragraph Selection</head><p>The proposed HGN relies on effective paragraph selection to find relevant multi-hop paragraphs. graph selection process is more accurate, achieving 94.53% precision and 94.53% recall. Besides these two top-ranked paragraphs, we also include two other paragraphs with the next highest ranking scores, to obtain a higher coverage on potential answers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Case Study</head><p>We provide two example questions for case study.</p><p>To answer the question in <ref type="figure" target="#fig_2">Figure 3</ref> (left), Q needs to be linked with P 1. Subsequently, the sentence S4 within P 1 is connected to P 2 through the hyperlink ("John Surtees") in S4. A plain BERT model without using the constructed graph missed S7 as additional supporting facts, while our HGN discovers and utilizes both pieces of evidence as the connections among S4, P 2 and S7 are explicitly encoded in our hierarchical graph.</p><p>For the question in <ref type="figure" target="#fig_2">Figure 3 (right)</ref>, the inference chain is Q → P 1 → S1 → S2 → P 2 → S3. The plain BERT model infers the evidence sentences S2 and S3 correctly. However, it fails to predict S1 as the supporting facts, while HGN succeeds, potentially due to the explicit connections between sentences in the constructed graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Examples for Error Analysis</head><p>Below, we provide additional examples for error analysis, where "Q" denotes question, "A" denotes answer provided with dataset and "P" denotes the prediction of proposed model. A full list of all the 100 examples is provided in  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Category</head><p>Sample IDs Annotation <ref type="bibr">6,</ref><ref type="bibr">23,</ref><ref type="bibr">33,</ref><ref type="bibr">38,</ref><ref type="bibr">47,</ref><ref type="bibr">59,</ref><ref type="bibr">75,</ref><ref type="bibr">81,</ref><ref type="bibr">93 Multiple Answers 1,</ref><ref type="bibr">4,</ref><ref type="bibr">8,</ref><ref type="bibr">10,</ref><ref type="bibr">11,</ref><ref type="bibr">16,</ref><ref type="bibr">19,</ref><ref type="bibr">24,</ref><ref type="bibr">26,</ref><ref type="bibr">28,</ref><ref type="bibr">29,</ref><ref type="bibr">32,</ref><ref type="bibr">39,</ref><ref type="bibr">40,</ref><ref type="bibr">42,</ref><ref type="bibr">50,</ref><ref type="bibr">53,</ref><ref type="bibr">56,</ref><ref type="bibr">60,</ref><ref type="bibr">63,</ref><ref type="bibr">67,</ref><ref type="bibr">68,</ref><ref type="bibr">71,</ref><ref type="bibr">72 Discrete Reasoning 0,</ref><ref type="bibr">2,</ref><ref type="bibr">9,</ref><ref type="bibr">21,</ref><ref type="bibr">22,</ref><ref type="bibr">35,</ref><ref type="bibr">37,</ref><ref type="bibr">45,</ref><ref type="bibr">58,</ref><ref type="bibr">64,</ref><ref type="bibr">77,</ref><ref type="bibr">82,</ref><ref type="bibr">86,</ref><ref type="bibr">88,</ref><ref type="bibr">95 Commonsense &amp; External Knowledge 7,</ref><ref type="bibr">15,</ref><ref type="bibr">20,</ref><ref type="bibr">36,</ref><ref type="bibr">69,</ref><ref type="bibr">70,</ref><ref type="bibr">73,</ref><ref type="bibr">76,</ref><ref type="bibr">78,</ref><ref type="bibr">83,</ref><ref type="bibr">84,</ref><ref type="bibr">85,</ref><ref type="bibr">87,</ref><ref type="bibr">91,</ref><ref type="bibr">92,</ref><ref type="bibr">17,</ref><ref type="bibr">25,</ref><ref type="bibr">27,</ref><ref type="bibr">30,</ref><ref type="bibr">41,</ref><ref type="bibr">43,</ref><ref type="bibr">46,</ref><ref type="bibr">54,</ref><ref type="bibr">57,</ref><ref type="bibr">62,</ref><ref type="bibr">74,</ref><ref type="bibr">79,</ref><ref type="bibr">90,</ref><ref type="bibr">97,</ref><ref type="bibr">99 MRC 5,</ref><ref type="bibr">12,</ref><ref type="bibr">13,</ref><ref type="bibr">14,</ref><ref type="bibr">18,</ref><ref type="bibr">31,</ref><ref type="bibr">34,</ref><ref type="bibr">44,</ref><ref type="bibr">48,</ref><ref type="bibr">49,</ref><ref type="bibr">51,</ref><ref type="bibr">52,</ref><ref type="bibr">55,</ref><ref type="bibr">61,</ref><ref type="bibr">65,</ref><ref type="bibr">66,</ref><ref type="bibr">80,</ref><ref type="bibr">89,</ref><ref type="bibr">94,</ref><ref type="bibr">98</ref>   <ref type="table" target="#tab_2">Table 11</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An example of multi-hop question answering from HotpotQA. The model needs to identify relevant paragraphs, determine supporting facts, and then predict the answer correctly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Model architecture of Hierarchical Graph Network. The constructed graph corresponds to the example in Figure 1. Green, blue, orange, and brown colors represent paragraph (P), sentence (S), entity (E), and question (Q) nodes, respectively. Some entities and hyperlinks are omitted for simplicity. et al., 2019) employs an MRC model to predict answer spans and possible next-hop spans, and then organizes them into a cognitive graph. DFGN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Examples of supporting facts prediction in the HotpotQA Distractor setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Yang et al., 2018) 45.60 59.02 20.32 64.49 10.83 40.16   QFE<ref type="bibr" target="#b15">(Nishida et al., 2019)</ref> 53.86 68.06 57.75 84.49 34.63 59.61 DFGN<ref type="bibr" target="#b31">(Xiao et al., 2019)</ref> 56.31 69.69 51.50 81.62 33.62 59.82 LQR-Net<ref type="bibr" target="#b4">(Grail et al., 2020)</ref> 60.20 73.78 56.21 84.09 36.56 63.68 P-BERT † 61.18 74.16 51.38 82.76 35.42 63.79 TAP2 (Glass et al., 2019) 64.99 78.59 55.47 85.57 39.77 69.12 EPS+BERT † 65.79 79.05 58.50 86.26 42.47 70.48 SAE-large (Tu et al., 2020) 66.92 79.62 61.53 86.86 45.36 71.45 C2F Reader(Shao et al., 2020) 67.98 81.24 60.81 87.63 44.67 72.73 Longformer (Beltagy et al., 2020) 68.00 81.25 63.09 88.34 45.91 73.16 ETC-large (Zaheer et al., 2020) 68.12 81.18 63.25 89.09 46.40 73.62 HGN (ours) 69.22 82.19 62.76 88.47 47.11 74.21</figDesc><table><row><cell>Model</cell><cell>EM</cell><cell>Ans</cell><cell>F1</cell><cell>EM</cell><cell>Sup</cell><cell>F1</cell><cell>Joint EM</cell><cell>F1</cell></row><row><cell>DecompRC (Min et al., 2019b)</cell><cell cols="3">55.20 69.63</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ChainEx (Chen et al., 2019)</cell><cell cols="3">61.20 74.11</cell><cell>-</cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Baseline Model (</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>Table 1: Results on the test set of HotpotQA in the Distractor setting. HGN achieves state-of-the-art results at the time of submission (Dec. 1, 2019). ( †) and ( ) indicates unpublished and concurrent work. RoBERTa-large (Liu et al., 2019) is used for context encoding.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1</head><label>1</label><figDesc><ref type="bibr" target="#b33">Yang et al., 2018)</ref> 23.95 32.89 3.86 37.71 1.85 16.15 QFE<ref type="bibr" target="#b15">(Nishida et al., 2019)</ref> 28.66 38.06 14.20 44.35 8.69 23.10 MUPPET (Feldman and El-Yaniv, 2019) 30.61 40.26 16.65 47.33 10.85 27.01</figDesc><table><row><cell>and 2 summarize</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Results on the test set of HotpotQA in the Fullwiki setting. HGN achieves state-of-the-art results at the time of submission (Feb. 11, 2020). ( †) indicates unpublished work. RoBERTa-large (Liu et al., 2019) and ALBERT-xxlarge-v2 (Lan et al., 2020) are used for context encoding, and SemanticRetrievalMRS is used for retrieval. Leaderboard: https://hotpotqa.github.io/.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3</head><label>3</label><figDesc>MRC' example is "Childe Byron is a 1977 play by Romulus Linney about the strained relationship between the poet, Lord Byron, and his daughter, Ada Lovelace".</figDesc><table><row><cell cols="4">: Error analysis of HGN model. For 'Multi-hop' errors, the model jumps to the wrong film ("Tommy</cell></row><row><cell cols="4">(1975 film)") instead of the correct one ("Quadrophenia (film)") from the starting entity "rock opera 5:15". The</cell></row><row><cell cols="4">supporting fact for the 'Model Ans F1 Sup F1 Joint F1</cell></row><row><cell>w/o Graph</cell><cell>80.58</cell><cell>85.83</cell><cell>71.02</cell></row><row><cell>PS Graph</cell><cell>81.68</cell><cell>88.44</cell><cell>73.83</cell></row><row><cell>PSE Graph</cell><cell>82.10</cell><cell>88.40</cell><cell>74.13</cell></row><row><cell cols="2">Hier. Graph 82.22</cell><cell>88.58</cell><cell>74.37</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Ablation study on the effectiveness of the hierarchical graph on the dev set in the Distractor setting. RoBERTa-large is used for context encoding.</figDesc><table><row><cell>Model</cell><cell cols="3">Ans F1 Sup F1 Joint F1</cell></row><row><cell>DFGN (BERT-base)</cell><cell>69.38</cell><cell>82.23</cell><cell>59.89</cell></row><row><cell>EPS (BERT-wwm)  †</cell><cell>79.05</cell><cell>86.26</cell><cell>70.48</cell></row><row><cell>SAE (RoBERTa)</cell><cell>80.75</cell><cell>87.38</cell><cell>72.75</cell></row><row><cell>HGN (BERT-base)</cell><cell>74.76</cell><cell>86.61</cell><cell>66.90</cell></row><row><cell>HGN (BERT-wwm)</cell><cell>80.51</cell><cell>88.14</cell><cell>72.77</cell></row><row><cell>HGN (RoBERTa)</cell><cell>82.22</cell><cell>88.58</cell><cell>74.37</cell></row><row><cell>HGN (ALBERT-xxlarge-v2)</cell><cell>83.46</cell><cell>89.2</cell><cell>75.79</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>: Results with different pre-trained language</cell></row><row><cell>models on the dev set in the Distractor setting. ( †) is un-</cell></row><row><cell>published work with results on the test set, using BERT</cell></row><row><cell>whole word masking (wwm).</cell></row><row><cell>model's weaknesses (and provide insights for fu-</cell></row><row><cell>ture work), we randomly sample 100 examples in</cell></row><row><cell>the dev set with the answer F1 as 0. After carefully</cell></row><row><cell>analyzing each example, we observe that these er-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Results of HGN for different reasoning types.</figDesc><table /><note>'Pct' is short for 'Percentage'.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Distractor and Fullwiki setting. They both have 90k training samples and 7.4k development samples. In the Distractor setting, there are 2 gold paragraphs and 8 distractors. However, 2 gold paragraphs may not be available in the Fullwiki Setting. Therefore, the Fullwiki setting is more challenge which requires to search the entire Wikipedia to find relevant documents. For both settings, there are 90K hidden test samples. More details about the dataset can be found in<ref type="bibr" target="#b33">Yang et al. (2018)</ref>.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 .</head><label>7</label><figDesc>The computation bottleneck is mainly from RoBERTa. The best model of HGN took around 12 hours for training, which is almost the same as the RoBERTa-large baseline.</figDesc><table><row><cell>Components</cell><cell>#Parameters</cell></row><row><cell>RoBERTa</cell><cell>355M</cell></row><row><cell>Bi-Attention</cell><cell>0.62M</cell></row><row><cell>BiLSTM</cell><cell>1.44M</cell></row><row><cell>GNN</cell><cell>29M</cell></row><row><cell>Multi-task Layer</cell><cell>0.55M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Number of parameters for each component in HGN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell>shows the performance of paragraph selection on</cell></row><row><cell>the dev set of HotpotQA. In DFGN, paragraphs are</cell></row><row><cell>selected based on a threshold to maintain high re-</cell></row><row><cell>call (98.27%), leading to a low precision (60.28%).</cell></row><row><cell>Compared to both threshold-based and pure Top-</cell></row><row><cell>N -based paragraph selection, our two-step para-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell cols="4">summarizes the results on the dev</cell></row><row><cell cols="4">set in the Distractor setting, using our paragraph</cell></row><row><cell cols="4">selection approach for both DFGN and the plain</cell></row><row><cell cols="4">BERT-base model. Note that the original DFGN</cell></row><row><cell cols="4">does not finetune BERT, leading to much worse</cell></row><row><cell cols="4">performance. In order to provide a fair comparison,</cell></row><row><cell cols="4">we modify their released code to allow finetuning</cell></row><row><cell cols="4">of BERT. Results show that our paragraph selec-</cell></row><row><cell cols="4">tion method outperforms the threshold-based one</cell></row><row><cell>in both models.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="3">Ans F1 Sup F1 Joint F1</cell></row><row><cell>DFGN (paper)</cell><cell>69.38</cell><cell>82.23</cell><cell>59.89</cell></row><row><cell>DFGN</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+ threshold-based 71.90</cell><cell>83.57</cell><cell>63.04</cell></row><row><cell>+ 2 para. (ours)</cell><cell>72.53</cell><cell>83.57</cell><cell>63.87</cell></row><row><cell>+ 4 para. (ours)</cell><cell>72.67</cell><cell>83.34</cell><cell>63.63</cell></row><row><cell>BERT-base</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+ threshold-based 71.95</cell><cell>82.79</cell><cell>62.43</cell></row><row><cell>+ 2 para. (ours)</cell><cell>72.42</cell><cell>83.64</cell><cell>63.94</cell></row><row><cell>+ 4 para. (ours)</cell><cell>72.67</cell><cell>84.86</cell><cell>64.24</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Results with selected paragraphs on the dev set in the Distractor setting.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10</head><label>10</label><figDesc>and 11.</figDesc><table><row><cell>Category: Annotation</cell></row><row><cell>ID: 5ae2e0fd55429928c4239524</cell></row><row><cell>Q: What actor was also a president that Richard</cell></row><row><cell>Darman worked with when they were in office?</cell></row><row><cell>A: George H. W. Bush</cell></row><row><cell>P: Ronald Reagan</cell></row><row><cell>ID: 5ab43b755542991779162c21</cell></row><row><cell>Q: What sports club based in Hamburg Germany</cell></row><row><cell>had a Persian born football player who played for</cell></row><row><cell>eight seasons?</cell></row><row><cell>A: Mehdi Mahdavikia</cell></row><row><cell>P: Hamburger SV</cell></row><row><cell>ID: 5a72e28f5542992359bc31ba</cell></row><row><cell>Q: Which technique did the director at Pzena</cell></row><row><cell>Investment Management outline?</cell></row><row><cell>A: outlined by Joel Greenblatt</cell></row><row><cell>P: Magic formula investing</cell></row><row><cell>ID: 5a7e71ab55429949594199bc</cell></row><row><cell>Q: Perfect Imperfection is a 2016 Chinese romantic</cell></row><row><cell>drama film starring a south Korean actor best</cell></row><row><cell>known for his roles in what 2016 television drama?</cell></row><row><cell>A: Reunited Worlds</cell></row><row><cell>P: Cinderella and Four Knights</cell></row><row><cell>ID: 5a7a18b05542990783324e53</cell></row><row><cell>Q: What year was the independent regional brew-</cell></row><row><cell>ery founded that currently operates in Hasting's</cell></row><row><cell>oldest pub?</cell></row><row><cell>A: since 1864</cell></row><row><cell>P: 1698</cell></row><row><cell>Category: Multiple Answers</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 10 :</head><label>10</label><figDesc>The categories and sample IDs for the 100 examples selected for error analysis. The sample IDs are mapped to the ground-truth IDs in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://spacy.io</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to retrieve reasoning paths over wikipedia graph for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuma</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Span selection pretraining for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfio</forename><surname>Gliozzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishav</forename><surname>Chakravarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Ferritto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Bhargav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avirup</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.04120</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameya</forename><surname>Godbole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Kavarthapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyu</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Singhal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07598</idno>
		<title level="m">Multi-step entity-centric information retrieval for multi-hop question answering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Latent question reformulation and information accumulation for multi-hop machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Grail</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Gaussier</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=S1x63TEYvr" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop qa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Selfassembling modular networks for interpretable multi-hop reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Race: Large-scale reading comprehension dataset from examinations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qizhe</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stochastic answer networks for machine reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Compositional questions do not necessitate multi-hop reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Efficient and robust question answering from minimal context over documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-hop reading comprehension through question decomposition and rescoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Answering while summarizing: Multi-task learning for multi-hop qa with evidence extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyosuke</forename><surname>Nishida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masaaki</forename><surname>Nagata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atsushi</forename><surname>Otsuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itsumi</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisako</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junji</forename><surname>Tomita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Answering complex open-domain questions through iterative query generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Mehr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjoon</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddha</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoping Hu</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03096</idno>
		<title level="m">Is graph structure necessary for multi-hop reasoningt</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploring graph-structured passage representation for multihop reading comprehension with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiguo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gildea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.02040</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multi-mention learning for reading comprehension with neural cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swabha</forename><surname>Swayamdipta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><forename type="middle">P</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The web as a knowledge-base for answering complex questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alon</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingdi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaheer</forename><surname>Suleman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09830</idno>
		<title level="m">Newsqa: A machine comprehension dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Select, answer and explain: Interpretable multi-hop reading comprehension over multiple documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-hop reading comprehension across multiple documents by reasoning over heterogeneous graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Machine comprehension using match-lstm and answer pointer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Cluster-former: Clustering-based sparse transformer for long-range dependency encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwei</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siqi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06097</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Constructing datasets for multi-hop reading comprehension across documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Louf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.03771</idno>
		<title level="m">Morgan Funtowicz, and Jamie Brew. 2019. Transformers: State-ofthe-art natural language processing</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Dynamically fused graph network for multi-hop reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunxuan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.07597</idno>
		<title level="m">Simple yet effective bridge reasoning for open-domain multi-hop question answering</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hotpotqa: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Revealing the importance of semantic retrieval for machine reading at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal Yixin Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<title level="m">Big bird: Transformers for longer sequences</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Transformer-xh: Multi-evidence reasoning with extra hop attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corby</forename><surname>Rosset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
