<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Faster, Simpler and More Accurate Hybrid ASR Systems Using Wordpieces</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Zhang</surname></persName>
							<email>frankz@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongqiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Zhang</surname></persName>
							<email>xiaohuizhang@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxi</forename><surname>Liu</surname></persName>
							<email>chunxiliu@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yatharth</forename><surname>Saraf</surname></persName>
							<email>ysaraf@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
							<email>gzweig@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Faster, Simpler and More Accurate Hybrid ASR Systems Using Wordpieces</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms: hybrid speech recognition</term>
					<term>CTC</term>
					<term>acoustic mod- eling</term>
					<term>wordpiece</term>
					<term>transformer</term>
					<term>recurrent neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work, we first show that on the widely used LibriSpeech benchmark, our transformer-based context-dependent connectionist temporal classification (CTC) system produces state-ofthe-art results. We then show that using wordpieces as modeling units combined with CTC training, we can greatly simplify the engineering pipeline compared to conventional frame-based cross-entropy training by excluding all the GMM bootstrapping, decision tree building and force alignment steps, while still achieving very competitive word-error-rate. Additionally, using wordpieces as modeling units can significantly improve runtime efficiency since we can use larger stride without losing accuracy. We further confirm these findings on two internal VideoASR datasets: German, which is similar to English as a fusional language, and Turkish, which is an agglutinative language.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep neural networks have been the de facto architecture for automatic speech recognition (ASR) tasks since they were first introduced <ref type="bibr" target="#b0">[1]</ref>. These network architectures have evolved in recent years and can be broadly classified into two categories: 1) Those that support streaming during inference, such as time delay neural network (TDNN) <ref type="bibr" target="#b1">[2]</ref>, feed-forward sequential memory networks (FSMN) <ref type="bibr" target="#b2">[3]</ref>, long short-term memory (LSTM) <ref type="bibr" target="#b3">[4]</ref>, latency-controlled bi-directional LSTM (LC-BLSTM) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> and time-depth separable convolutions (TDS) <ref type="bibr" target="#b6">[7]</ref> etc. and, 2) Full sequence architectures when latency is not a concern, e.g. BLSTM <ref type="bibr" target="#b7">[8]</ref>, which can be used to provide better accuracy since the neural network can take full advantage of future information. Recently, Transformer <ref type="bibr" target="#b8">[9]</ref> architectures have shown superior results in ASR tasks compared to BLSTMs <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>. In this work, we use LC-BLSTM as a representative for streaming and Transformer for the full sequence use case.</p><p>Traditional hybrid DNN/Hidden Markov Model (HMM) approach utilizes a neural network to produce a posterior distribution over tied HMM states <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> for each acoustic frame, usually followed by sequence discriminative training to boost performance <ref type="bibr" target="#b15">[16]</ref>. CTC <ref type="bibr" target="#b16">[17]</ref> has became an alternative criterion to frame-level cross-entropy (CE) training or sequencelevel lattice-free MMI (LF-MMI) training in recent years and has shown promising results <ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref>. Inspired by the rise of end-to-end training in machine translation, encoder-decoder architecture was also introduced for ASR, e.g. Listen, Attend and Spell (LAS) <ref type="bibr" target="#b22">[23]</ref>. Most recently, neural transducers <ref type="bibr" target="#b23">[24]</ref> have shown great potential for both on-device <ref type="bibr" target="#b24">[25]</ref> and server <ref type="bibr" target="#b25">[26]</ref> use cases. Here, we consider both CE and CTC trained systems</p><p>The authors would like to thank Duc Le for helpful discussion about chenone, Jun Liu about CTC decoding and Abdelrahman Mohamed about transformer models as hybrid since the neural network is solely modeling posteriors distribution over modeling units and a WFST-based decoder was used to produce hypothesis. While the LAS and Transducer based systems are considered end-to-end since there are decoder neural network components that directly produce hypotheses. In this work, our focus will be on CTC-based systems.</p><p>The most extensively studied modeling unit of hybrid ASR systems is tied context-dependent (CD) states/phones, i.e. senone <ref type="bibr" target="#b26">[27]</ref>. As an alternative, chenone <ref type="bibr" target="#b5">[6]</ref> was proposed that has not only shown improvement in accuracy, but also eliminates the need of a phonemic lexicon. Since CTC training does not require alignment labels per frame, graphemes <ref type="bibr" target="#b17">[18]</ref>, wordpieces (WP) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref> or even whole words <ref type="bibr" target="#b31">[32]</ref> can be directly modeled.</p><p>In the rest of this paper, we first compare performance between chenone and wordpiece as modeling unit. We further study the best striding scheme for both modeling units and the impact of wordpiece vocab sizes. Then we perform neural language model rescoring on top of our best system to produce final WERs on LibriSpeech. With a transformer network trained with chenone-CTC, we achieve state-of-the-art result on Lib-riSpeech among hybrid systems. On our internal VideoASR tasks, a system trained with wordpiece-CTC only slightly lags behind in terms of WER, but is up-to 3x faster during inference compared to systems using chenone due to the larger stride in the neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Hybrid Architecture</head><p>In hybrid ASR, an acoustic encoder is used to encode a sequence of acoustic frames x1, · · · , xT to a corresponding sequence of high level embedding vectors z1, · · · , zT . A softmax layer is then applied on these embedding vectors to produce a posterior distribution over the chosen modeling unit, e.g. senone or chenone, for each frame. These posterior distributions are then fed into a weighted finite-state transducer (WFST) <ref type="bibr" target="#b32">[33]</ref> based decoder with a decoding graph also composed with a lexicon and language model (LM) to find the best hypothesis.</p><p>In contrast to CE training that requires pre-computed per frame labels usually through a force alignment process, CTC training can implicitly learn the alignment between the input sequence and target sequence by introducing an additional blank label. The blank label is used to estimate the probability of outputting no label at a given frame. The encoder will be trained to produce probability distribution over all labels including blank. The log-likelihood of a given target sequence y can then be found by summing the probabilities of all allowed alignments. Specifically,</p><formula xml:id="formula_0">log p(y|x1, · · · , xT ) = π∈B −1 (y) t=T t=1 p(πt|xt)<label>(1)</label></formula><p>where B is the mapping operation that removes all blank and arXiv:2005.09150v2 [eess.AS] 16 Aug 2020</p><p>repeating labels in a given sequence. Note that the underlying assumption is probabilities between timestamps are conditional independent, which is ensured since the encoder is nonautoregressive. The network is then trained to maximize the log-likelihood for each training example and p can be computed efficiently using forward-backward algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Acoustic Model Architecture</head><p>In this section, we briefly review the neural network architectures we are going to study in this work: Transformer (in Section 3.1) and LC-BLSTM (in Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Architecture of Transformer</head><p>Unlike when the Transformer <ref type="bibr" target="#b8">[9]</ref> was first proposed as an encoder-decoder architecture for machine translation task, we only use the encoder part for acoustic modeling. Specifically we follow the setup in <ref type="bibr" target="#b9">[10]</ref> and use VGG layers <ref type="bibr" target="#b33">[34]</ref> in lieu of the original sinusoid positional encoding, since we have seen that for ASR tasks, convolutional positional encoding performs the best. Iterated loss <ref type="bibr" target="#b34">[35]</ref> was also applied to the intermediate embedding of transformer layers to help convergence and improve accuracy. Also different from the original transformer, we apply layer normalization <ref type="bibr" target="#b35">[36]</ref> before multi-head attention (MHA) and feed-forward network (FFN) and we have an extra layer normalization operator after the residual connection. This is necessary to prevent bypassing the transformer layer entirely and helps with model convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Architecture of LC-BLSTM</head><p>Unidirectional recurrent neural networks such as LSTMs base their predictions solely on the history they have already seen, hence the prediction accuracy is worse than bi-directional LSTMs that have access to the full context of the input, including future frames. However for streaming applications such as live captioning, we cannot wait for the full context to arrive because the ASR output needs to be made available within a certain latency budget of the audio stream being fed into the ASR system.</p><p>To strike a balance between recognition latency and accuracy, LC-BLSTM was first introduced in <ref type="bibr" target="#b4">[5]</ref>. Unlike BLSTM that cannot produce any hypothesis until the whole audio input is processed, LC-BLSTM only utilizes a limited number of right context (RC) frames to make predictions, which controls the latency. Similar to BLSTM, each LC-BLSTM layer also has two LSTMs, one left-to-right LSTM and one right-to-left LSTM. The difference is that the the input sequence is first divided into overlapping chunks of chunk size (CS) frames. The amount of overlap between chunks is equal to RC frames. When forwarding left-to-right LSTM, hidden states and cell states are carried over between chunks, so that we have unlimited left context as in BLSTM. We forward right-to-left LSTM on each chunk and only keep CS − RC frames of output activations, so that each input frame has at least RC frames of right context to produce its activation. This mechanism enables LC-BLSTM to generate better acoustic embeddings than LSTM, without delaying the generation until the encoder has seen the whole audio.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Modeling Units</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Chenone</head><p>Chenone was first introduced in <ref type="bibr" target="#b5">[6]</ref> as an alternative to traditional senone <ref type="bibr" target="#b26">[27]</ref> unit. Chenone not only eliminated the need for a phonetic lexicon, usually generated by linguists, but also showed WER improvement relative to phonetic modeling units in some cases. In order to use chenone as modeling unit in CTC training, we shift the labels after force alignment by one (all labels +1), and use label #0 as the blank label in CTC. Essentially the neural network is now modeling distribution over all chenones plus one blank symbol. We then squeeze same adjacent labels in the alignment sequence into one label, let the encoder trained with CTC criterion to learn the implicit alignments.</p><p>After a CTC model is trained, the next step is to construct a decoding graph. We first build an H • C • L • G 1 graph following the standard procedure for CD-HMM (here, we always assume every HMM only has 1 state); then we transform it to a new graph which can consume an extra blank symbol. For each FST state s in the decoding graph (shown in <ref type="figure" target="#fig_0">Figure 1</ref>), we split that state into two FST states, s and s ; we moved the outgoing edges (e2 and e4) to start from s ; A self loop edge with blank label as input and as output symbol with weight one (in the semi-ring's sense) is added; s and s are then connected by a : edge. Self-loop edges (e3 in this example) and incoming edge (e1) in this case are not changed. The transition model is on-the-fly converted to a new mapping function that shifts the output units by one. Once we change the transition model and decoding graph, the standard Kaldi decoder can be used to decode chenone-CTC model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Wordpiece</head><p>Subword unit representations such as byte pair encoding (BPE) <ref type="bibr" target="#b36">[37]</ref> and wordpiece model <ref type="bibr" target="#b37">[38]</ref> have been proposed with improved performance in many natural language processing (NLP) tasks. This approach chooses to divide words into a limited set of subword units, e.g. the word "hello" may be encoded as " he ll o", where the underscore indicates word start. In this work, we train wordpiece model using unigram language model word segmentation algorithm <ref type="bibr" target="#b38">[39]</ref>, and use the generated wordpiece vocab plus blank symbol as modeling units.</p><p>Since wordpiece is context independent, we only need to build an H • L • G graph for decoding, where: H transduces n+1 symbols to n wordpieces, i.e. absorbing the blank symbol; L maps the sequence of wordpieces to the sequences of words, e.g. " he ll o" to "hello", and is done by the trained wordpiece model; G is the standard n-gram word level LM. Once these FSTs are constructed, a standard procedure is used to compose H, L and G together.</p><p>Compared to alignment-based training which first need to train a bootstrap model and build decision tree; then use the bootstrap model to perform force alignment and finally CE training, we only need to train a wordpiece model using textonly data and followed by one-stage CTC training. The whole training pipeline is greatly simplified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>To evaluate the performance of different modeling units and training criterion, we first conduct experiments on the Lib-riSpeech corpus <ref type="bibr" target="#b39">[40]</ref>, followed by experiments on two larger and more challenging internal datasets of German and Turkish social media videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data</head><p>The LibriSpeech corpus contains about 960 hours of read speech data for training, and 4 development and test sets ({dev, test}-{clean,other}), where other sets are more acoustic challenging. We use the official 4-gram language model (LM) with 200K vocabulary for all first-pass decoding and n-best generation for neural LM rescoring.</p><p>The German and Turkish datasets are our in-house video datasets, which are sampled from public social media videos. The datasets are completely de-identified before transcription; both transcribers and researchers do not have access to any useridentifiable information (UII). The training, validation and test set sizes are shown in <ref type="table" target="#tab_0">Table 1</ref>. All hyper-parameter tuning is done on validation set. The video datasets contain a diverse array of speakers, accents, video categories, and acoustic conditions, and are more challenging than the LibriSpeech dataset explored in this work.</p><p>We use the same bootstrap model obtained from decision tree building stage to segment the training split of both Lib-riSpeech and video dataset to up to 10 seconds 2 . We also segment the dev and test split of video dataset to up to 10s, while no segmentation is performed on LibriSpeech dev and test sets in order to be comparable with other published results. The benefit of segmenting training data was shown in <ref type="bibr" target="#b9">[10]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiment Setup</head><p>We follow <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref> to use context-and position-dependent graphemes (i.e., chenones) for CE baselines and CTC experiments. We bootstrap our HMM-GMM system using the standard Kaldi <ref type="bibr" target="#b40">[41]</ref> LibriSpeech recipe. We use 1-state HMM topology with fixed self-loop and forward transition probability (both 0.5). 80-dimensional log Mel-filter bank features are extracted with a 10ms frame shift and 25ms FFT windows. Speed perturbation and SpecAugment <ref type="bibr" target="#b41">[42]</ref> (LD policy without time warping) are applied to all experiments unless specially noted.</p><p>Since the focus of this work is not on network architecture searches, we use our previously found best setup across all following experiments. For acoustic transformers, we use a 24-layer transformer encoder architecture with embedding dimension 512 and 8 attention heads; the FFN dimension is 2048. Iterative loss was applied to intermediate output embeddings of layer 6, 12 and 18. We use three VGG blocks <ref type="bibr" target="#b42">[43]</ref> to encode acoustic features before feeding into transformer layers: each VGG block contains 2 consecutive convolution layers with a 3-by-3 kernel followed by a ReLu non-linearity and a pooling layer; 64 channels are used in the convolution layer of the first VGG block and increase to 128 for the second block and 256 for the third block. Max-pooling is performed at a 2-by-2 grid, with optional stride choice from 1 to 3 in each block. This model has about 81M parameters and we note the model as vggTrf. For LC-BLSTMs, we follow <ref type="bibr" target="#b5">[6]</ref> and use 5 layers with 800 hidden units per layer per direction. Optional subsampling by a factor of 2 or 3 can be applied after the first hidden layer. This model has about 83M parameters, similar to the transformer model. Dropout is applied in all experiments: 0.1 for transformers and 0.2 for LC-BLSTM.</p><p>All neural network training is performed using an in-house developed PySpeech framework that is built on top of the opensourced PyTorch-based fairseq <ref type="bibr" target="#b43">[44]</ref> toolkit. Adam optimizer <ref type="bibr" target="#b44">[45]</ref> with (0.9, 0.999) betas and 1e −8 epsilon is used in all experiments. We apply the tri-stage <ref type="bibr" target="#b41">[42]</ref> learning rate (LR) scheduler. The hold stage LR is 1e −3 . For experiments on transformer models, we follow a schedule of (48K, 100K, 200K) steps with a batch contains up to 20,000 frames. For LC-BLSTM models we use schedule of (16K, 32K, 64K) steps with a batch contains up to 50,000 frames. All models are trained on 32 Nvidia V100 GPUs for 200 epochs in total. Training is usually finished between 2 to 4 days.</p><p>Test set WERs are obtained using the best model based on evaluated WER on the development set. The best checkpoints for both LibriSpeech test-clean and test-other are selected using the dev-other development set. For video, the best checkpoint was selected using the valid set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Effect of Stride</head><p>In the first set of experiments, we investigate what is the best stride for different modeling units. For vggTrf model, striding is achieved by setting stride of max-pooling layers, e.g. a total stride of 8 can be achieved by setting stride equals 2 for all three VGG blocks. For LC-BLSTM, we apply subsampling factor of 2 for activation of the three consecutive hidden layers right after the first layer. We follow the setup in <ref type="bibr" target="#b5">[6]</ref> and use stride 2 for the chenone-CE baseline. We use 2K wordpiece size in this study. Results on LibriSpeech test-other dataset shown in <ref type="table" target="#tab_1">Table 2</ref>.</p><p>The results show that for chenone-CTC, the best stride is 4 and it out performs CE baseline. While for WP-CTC system, we find that we can use stride as high as 8 without losing much accuracy. Because wordpiece model is trained based on word frequency, some common whole words, e.g. 'welcome' and 'information', made into the vocab. The acoustic model can use larger stride since the corresponding input frames will span longer. On the other hand, although context dependent, chenones are essentially still characters and have a shorter corresponding time span in input sequence. So we observed that WER degrades quickly when larger stride is used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Effect of Wordpiece Dictionary Size</head><p>In the second set of experiments, we explore the effect of different wordpiece sizes. Here, all models use a total stride of 8.</p><p>We report results on LibriSpeech test-other dataset in <ref type="table" target="#tab_2">Table 3</ref>. The results show that on LibriSpeech, smaller wordpiece vocab size tends to work better. It's worth exploring vocab sizes below 1K in future study. We also explored subword regularization during training following <ref type="bibr" target="#b38">[39]</ref>. We tried the setup of (l = 64, α = 0.1/0.5) but results turn out slightly worse. During decoding we also tried building decoding graph with multiple pronunciations in the lexicon with corresponding pronunciation probability for the lexicon entries. The results turn out on-par. We also explored sMBR and mWER training after CTC stage. It provides very marginal gain so we didn't include the results in this study.</p><p>To achieve best WERs, we trained a 36-layer stride-4 vg-gTrf model with chenone-CTC(about 124M parameters), and changed time masking of SpecAugment LD policy to (T = 30, mT = 10). Its performance and those of some other published LibriSpeech systems can be found in <ref type="table" target="#tab_3">Table 4</ref>. The new system outperforms our previous best hybrid system <ref type="bibr" target="#b9">[10]</ref> by 11% and 14% respectively on test-clean and test-other. We also trained a 42-layer transformer LM following the setup in <ref type="bibr" target="#b45">[46]</ref> with the LibriSpeech transcriptions and 800M-word text-only data. The transformer LM achieved perplexity 52.35 on the dev set (a combination of dev-clean and dev-other). We then perform n-best rescoring on up to 100-best hypotheses generated by first pass decoding. The oracle error rate of the n-best hypotheses are 1.0% and 2.2% on test-clean and test-other respectively. Our final WERs (2.10%/4.20%) are the best results among all hybrid systems on this widely used benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Experiments on Video dataset</head><p>Finally, we tested vggTrf with CTC criterion on the more challenging and larger scale internal VideoASR tasks, as described in Section 5.1. Stride 2 is used for CE, 4 for chenone-CTC and 8 for WP-CTC training. Results are shown in <ref type="table" target="#tab_4">Table 5</ref>. We find that both CTC systems outperform the baseline CE system. It's also consistent that chenone-CTC outperforms WP-CTC in terms of WER. On the other hand, WP-CTC system is significantly better in terms of real-time factor (RTF) due to the larger stride used. Blank frame skipping during decoding also contributes to the speedup. We skip frames if blank label posterior is greater than 99%. Empirically, we found that over 20% of the frames in chenone-CTC and over 50% of frames in WP-CTC were skipped. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussions and Conclusions</head><p>In this work, we pushed the performance boundary of hybrid ASR using transformer-based acoustic models with contextdependent CTC training. Modeling choices are discussed and compared in detail, and as <ref type="table" target="#tab_3">Table 4</ref> shows, our system yields state-of-the-art results on the LibriSpeech benchmark. For real world production system however, we must take into account not only recognition accuracy, but also engineering complexity, inference efficiency, flexibility etc. We have shown that hybrid systems can be built with fewer steps leveraging wordpiece and CTC training. Yet, the system's accuracy is very competitive with a much faster runtime inference speed. Results on a more challenging internal dataset show similar results, confirming that such a wordpiece-CTC system indeed has great potential.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Convert a standard CD-HMM FST to be CTC compatible. A standard FST was shown on the left and a CTC compatible FST after conversion on the right</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Dataset sizes for internal Video ASR tasks. Number of utterances in parentheses.</figDesc><table><row><cell>Language</cell><cell>German</cell><cell>Turkish</cell></row><row><cell>train</cell><cell cols="2">3K hrs (˜135K) 3.1K hrs (˜137K)</cell></row><row><cell>valid</cell><cell>14.5 hrs (˜600)</cell><cell>14.4 hrs (˜600)</cell></row><row><cell>test</cell><cell>24.2 hrs (˜1K)</cell><cell>24.4 hrs (˜1K)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Effect of stride for wordpiece and chenone</figDesc><table><row><cell>Unit</cell><cell cols="4">Criterion Stride LC-BLSTM vggTrf</cell></row><row><cell>chenone</cell><cell>CE</cell><cell>2</cell><cell>8.81</cell><cell>5.59</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>9.75</cell><cell>5.96</cell></row><row><cell></cell><cell></cell><cell>2*2</cell><cell>8.92</cell><cell>5.74</cell></row><row><cell>WP</cell><cell>CTC</cell><cell>3*2</cell><cell>9.16</cell><cell>5.84</cell></row><row><cell></cell><cell></cell><cell>2*2*2</cell><cell>8.94</cell><cell>5.90</cell></row><row><cell></cell><cell></cell><cell>3*2*2</cell><cell>9.34</cell><cell>6.51</cell></row><row><cell></cell><cell></cell><cell>2</cell><cell>9.43</cell><cell>5.28</cell></row><row><cell>chenone</cell><cell>CTC</cell><cell>2*2 3*2</cell><cell>8.51 9.93</cell><cell>5.16 6.35</cell></row><row><cell></cell><cell></cell><cell>2*2*2</cell><cell>17.94</cell><cell>17.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>WER using different wordpiece sizes</figDesc><table><row><cell cols="3">Wordpiece size LC-BLSTM vggTrf</cell></row><row><cell>1K</cell><cell>8.71</cell><cell>5.86</cell></row><row><cell>2K</cell><cell>8.94</cell><cell>5.90</cell></row><row><cell>5K</cell><cell>9.13</cell><cell>6.05</cell></row><row><cell>10K</cell><cell>9.23</cell><cell>6.35</cell></row><row><cell>16K</cell><cell>9.44</cell><cell>6.51</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of our chenone-CTC with previous best results on LibriSpeech. "4g" means the official 4-gram LM was used; "NNLM" means a neural LM was used.</figDesc><table><row><cell>Arch.</cell><cell>System</cell><cell>LM</cell><cell>test-clean</cell><cell>test-other</cell></row><row><cell></cell><cell>Karita et al. [11]</cell><cell>NNLM</cell><cell>2.6</cell><cell>5.7</cell></row><row><cell>LAS</cell><cell>Park et al. [47]</cell><cell>NNLM</cell><cell>2.2</cell><cell>5.2</cell></row><row><cell></cell><cell>Synnaeve et al. [13]</cell><cell>NNLM</cell><cell>2.33</cell><cell>5.17</cell></row><row><cell>Transducer</cell><cell>Zhang et al. [26]</cell><cell>No LM NNLM</cell><cell>2.4 2.0</cell><cell>5.6 4.6</cell></row><row><cell></cell><cell>RWTH [8]</cell><cell>4g +NNLM</cell><cell>3.8 2.3</cell><cell>8.8 5.0</cell></row><row><cell>Hybrid</cell><cell>Han et al. [48] Wang et al. [10]</cell><cell>4g +NNLM 4g +NNLM</cell><cell>2.9 2.2 2.60 2.26</cell><cell>8.3 5.8 5.59 4.85</cell></row><row><cell></cell><cell>Ours</cell><cell>4g +NNLM</cell><cell>2.31 2.10</cell><cell>4.79 4.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Experiment results on internal VideoASR tasks 0.07 19.04 0.06 chenone 13.74 0.10 18.17 0.10</figDesc><table><row><cell>Unit</cell><cell>Criterion</cell><cell>German WER RTF WER RTF Turkish</cell></row><row><cell>chenone</cell><cell>CE</cell><cell>15.54 0.26 21.92 0.25</cell></row><row><cell>WP</cell><cell>CTC</cell><cell>14.32</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that L here is simply a mapping between word and its graphemes. Examples could be found in<ref type="bibr" target="#b5">[6]</ref> </note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This is achieved by force aligning the whole audio against the reference using the LC-BLSTM acoustic model.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A time delay neural network architecture for efficient modeling of long temporal contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Feedforward sequential memory neural networks without recurrent feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02693</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Highway long short-term memory RNNs for distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From Senones to Chenones: Tied Context-Dependent Graphemes for Hybrid Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fügen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sequence-to-Sequence Speech Recognition with Time-Depth Separable Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">RWTH ASR Systems for LibriSpeech: Hybrid vs Attention-w/o Data Augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lüscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NueralIPS</title>
		<meeting>NueralIPS</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Transformer-based Acoustic Modeling for Hybrid Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A Comparative Study on Transformer vs RNN in Speech Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hayashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Self-Attentional Acoustic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sperber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niehues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Endto-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Likhomanenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08460</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large Vocabulary Continuous Speech Recognition With Context-Dependent DBN-HMMS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Acoustic Modeling using Deep Belief Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TASLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Purely sequence-trained neural networks for ASR based on lattice-free MMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Peddinti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Galvez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ghahremani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Acoustic modelling with CD-CTC-SMBR LSTM RNNS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Chaumont Quitry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning acoustic frame labeling for speech recognition with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rsoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Beaufays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lower Frame Rate Neural Network Acoustic Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pundak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequence Transduction with Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A Streaming On-Device End-to-End Model Surpassing Server-Side Conventional Model Quality and Latency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tripathi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TASLP</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Japanese and Korean voice search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Nakajima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">hybrid ctc-attention based end-to-end speech recognition using subword units</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploring Model Units and Training Strategies for End-to-End Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Advancing Acoustic-to-Word CTC Model with Attention and Mixed-Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TASLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Neural Speech Recognizer: Acoustic-to-Word LSTM Model for Large Vocabulary Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Soltau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Weighted Finite-State Transducers in Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TASLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deja-vu: Double Feature Presentation and Iterated loss in Deep Transformer Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tjandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Layer Normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Neural Machine Translation of Rare Words with Subword Units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Google&apos;s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kudo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Librispeech: an ASR corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The Kaldi Speech Recognition Toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghoshal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Boulianne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">fairseq: A Fast, Extensible Toolkit for Sequence Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Myle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sergey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Angela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT 2019: Demonstrations</title>
		<meeting>NAACL-HLT 2019: Demonstrations</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Language Modeling with Deep Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schlter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">SpecAugment on Large Scale Datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICASSP</title>
		<meeting>ICASSP</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">State-of-the-Art Speech Recognition Using Multi-Stream Self-Attention With Dilated 1D Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Prieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ASRU</title>
		<meeting>ASRU</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
