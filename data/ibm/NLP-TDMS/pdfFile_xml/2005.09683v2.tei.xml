<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Collaborative Filtering vs. Matrix Factorization Revisited</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Krichene</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Anderson</surname></persName>
						</author>
						<title level="a" type="main">Neural Collaborative Filtering vs. Matrix Factorization Revisited</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T15:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Embedding based models have been the state of the art in collaborative filtering for over a decade. Traditionally, the dot product or higher order equivalents have been used to combine two or more embeddings, e.g., most notably in matrix factorization. In recent years, it was suggested to replace the dot product with a learned similarity e.g. using a multilayer perceptron (MLP). This approach is often referred to as neural collaborative filtering (NCF). In this work, we revisit the experiments of the NCF paper that popularized learned similarities using MLPs. First, we show that with a proper hyperparameter selection, a simple dot product substantially outperforms the proposed learned similarities. Second, while a MLP can in theory approximate any function, we show that it is non-trivial to learn a dot product with an MLP. Finally, we discuss practical issues that arise when applying MLP based similarities and show that MLPs are too costly to use for item recommendation in production environments while dot products allow to apply very efficient retrieval algorithms. We conclude that MLPs should be used with care as embedding combiner and that dot products might be a better default choice.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Embedding based models have been the state of the art in collaborative filtering for over a decade. A core operation of most of these embedding based models is to combine two or more embeddings. For example, combining a user embedding with an item embedding to obtain a single score that indicates the preference of the user for the item. This can be viewed as a similarity function in the embedding space. Traditionally, a dot product or higher order products have been used for the similarity. Recently, it has become popular to learn the similarity function with a neural network. Most commonly, a multilayer perceptron (MLP) is used for the network architecture (e.g. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27]</ref>). This approach is often referred to as neural collaborative filtering (NCF) <ref type="bibr" target="#b15">[16]</ref>. The rationale is that MLPs are general function approximators so that they should be strictly better than a fixed similarity function such as the dot product. This has made NCF the model of choice for comparison in many recommender studies (e.g. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref>).</p><p>In this work, we study MLP versus dot product similarities in more detail. We start with revisiting the experiments of the NCF paper <ref type="bibr" target="#b15">[16]</ref> that popularized the use of MLPs in recommender systems. We show that a carefully configured dot product baseline largely outperforms the MLP. At first glance, it looks surprising that the MLP, which is a universal function approximator, does not perform at least as well as the dot product. We investigate this issue in a second experiment and show empirically that learning a dot product with high accuracy for a decently large embedding dimension requires a large model capacity as well as many training data. Besides prediction quality, we also discuss the inference cost of dot product versus MLPs, where dot products have a large advantage due to the existence of efficient maximum inner product search algorithms. Finally, we discuss that dot product vs MLP is not a question of whether a deep neural network (DNN) is useful. In fact, many of the most competitive DNN models, such as transformers in natural language processing <ref type="bibr" target="#b8">[9]</ref> or resnets for image classification <ref type="bibr" target="#b13">[14]</ref>, use a dot product similarity in their output layer.</p><p>To summarize, this paper argues that MLP-based similarities for combining embeddings should be used with care. While MLPs can approximate any continuous function, their inductive bias might not be well suited for a similarity measure. Unless the dataset is large or the embedding dimension is very small, a dot product is likely a better choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Definitions</head><p>In this section, we formalize the problem and review dot product (esp., matrix factorization) and learned similarity functions (esp., MLP and NeuMF). We denote matrices by upper case letters X, vectors by lowercase bold letters x, scalars by lowercase letters x. A concatenation of two vectors x, z is denoted by <ref type="bibr">[x, z]</ref>.</p><p>Our paper studies functions φ : R d ×R d → R that combine two d-dimensional embedding vectors p ∈ R d and q ∈ R d into a single score. For example p could be the embedding of a user, q the embedding of an item, and φ(p, q) is the affinity of this user to the item.</p><p>The embeddings p and q can be model parameters such as in matrix factorization, but they can also be functions of other features, for example the user embedding p could be the output of a deep neural network taking user features as input. From here on, we focus mainly on the similarity function φ but in Section 6.1 we will discuss the embeddings in more detail. </p><formula xml:id="formula_0">p ∈ R d q ∈ R d φ dot (p, q) = p, q · p ∈ R d q ∈ R d φ MLP (p, q) = f W l ,b l (. . . f W1,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dot Product</head><p>The most common combination of two embeddings is the dot product.</p><formula xml:id="formula_1">φ dot (p, q) := p, q = p T q = d f =1 p f q f .<label>(1)</label></formula><p>If p and q are free model parameters, then this is equivalent to matrix factorization. A common trick is to add explicit biases:</p><formula xml:id="formula_2">φ dot (p, q) := b + p 1 + q 1 + p [2,...,d] , q [2,...,d] .<label>(2)</label></formula><p>This modification does not add expressiveness but has been found to be useful in many studies, likely because its inductive bias is better suited to the problem <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b21">22]</ref>.</p><p>Learned Similarity Multi layer perceptrons (MLPs) are known to be universal approximators that can approximate any continuous function on a compact set as long as the MLP has enough hidden states <ref type="bibr" target="#b6">[7]</ref>. One layer of a multi layer perceptron can be defined as a function f : R din → R dout :</p><formula xml:id="formula_3">f W,b (x) = σ(W x + b), σ(z) = [σ(z 1 ), . . . , σ(z out )],<label>(3)</label></formula><p>which is parameterized by W ∈ R in×out , b ∈ R out and an activation function σ : R → R. In a multilayer perceptron (MLP), several layers of f are stacked, e.g., for a three layer MLP, f W3,b3 (f W2,b2 (f W1,b1 (x))). He et al. <ref type="bibr" target="#b15">[16]</ref> propose to replace the dot product with learned similarity functions for collaborative filtering. They suggest to concatenate the two embeddings, p and q, and apply an MLP:</p><formula xml:id="formula_4">φ MLP (p, q) := f W l ,b l (. . . f W1,b1 ([p, q]) . . .).<label>(4)</label></formula><p>They further suggest a variation that combines the MLP with a weighted dot product model and name it neural matrix factorization (NeuMF):</p><formula xml:id="formula_5">φ NeuMF (p, q) := φ MLP (p [1,...j] , q [1...j ]) + φ GMF (p [j+1...d] , q [j+1...d] ),<label>(5)</label></formula><p>where GMF is a 'generalized' matrix factorization model:</p><formula xml:id="formula_6">φ GMF (p, q) := σ(w T (p q)) = σ( w p, q ) = σ   d f =1 w f p f q f   . (6)</formula><p>with learned weights w ∈ R d . For NeuMF, they recommend to use one part of the embedding (here the first j entries) in the MLP and the remaining d − j entries with the GMF. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates two models with dot product and MLP-based similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Revisiting NCF Experiments</head><p>In this section, we revisit the experiments of the NCF paper <ref type="bibr" target="#b15">[16]</ref> that popularized the use of MLPs as embedding combiners in recommender systems. We show that a simple dot product yields better results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental setup</head><p>The NCF paper <ref type="bibr" target="#b15">[16]</ref> evaluates on an item retrieval task on two datasets: a binarized version of Movielens 1M <ref type="bibr" target="#b12">[13]</ref> and a dataset from Pinterest <ref type="bibr" target="#b11">[12]</ref>. Both are implicit feedback datasets, i.e. they contain only binary positive tuples between a user and an item. For each user, the last item is held out and used as the test set, the remaining items of the user are placed into the training set. For evaluation, each recommender ranks, for each user, a set of 101 items consisting of the withheld test item together with 100 random items. For each user, the position at which the withheld item is ranked by the recommender is recorded, then two metrics are measured: (1) Hit Ratio (i.e. Recall) among the top 10 ranked items -which in this case is 1 if the withheld item is in the top 10 or 0 otherwise. (2) NDCG among the top 10 ranked items -which in this case is 1/log(r + 1) where r is the rank of the withheld item. The average metric over all users is reported. The authors have published the dataset splits and the evaluation code. This allows us to evaluate on exactly the same setting and to compare our results directly with the ones reported in <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Models, loss and training algorithm</head><p>We compare three models: MLP-learned similarity models introduced in <ref type="bibr" target="#b15">[16]</ref>, which use φ MLP and φ NeuMF respectively, and a simple matrix factorization baseline which uses φ dot from Eq. (2). The only difference between these models is the similarity function. In particular, the embeddings p, q are free parameters in all models. We train the matrix factorization baseline by minimizing a logistic loss with L2 regularization, using stochastic gradient descent (with no batching, no momentum or other variations) with negative sampling, as in the original <ref type="bibr" target="#b15">16</ref>   The results for MLP and NeuMF are from <ref type="bibr" target="#b15">[16]</ref>. The dot product substantially outperforms the learned similarity measures. Only the pretrained NeuMF is competitive, on one dataset, and for large embedding dimension.</p><p>paper <ref type="bibr" target="#b15">[16]</ref> 1 . More precisely, for each training example (consisting of a user and a positive item), we sample m negative items, uniformly at random. Finally, we vary the embedding dimension d ∈ {16, <ref type="bibr">32, 64, 96, 128, 192}</ref>. Additional details about the setup can be found in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Results</head><p>The results are reported in <ref type="figure" target="#fig_1">Fig. 2</ref>. Contrary to the findings of the NCF paper, the simple matrix factorization model exhibits the best quality over all evaluation metrics, and all embedding dimensions but one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Matrix Factorization vs MLP</head><p>Our main interest is to investigate if the MLP-learned similarity is superior to a simple dot product. As can be seen in <ref type="figure" target="#fig_1">Fig. 2</ref>, the dot product substantially outperforms MLP on all datasets, evaluation metrics and embedding dimensions. With a properly set up matrix factorization model, the experiments do not show any evidence that a MLP is superior. In addition to a lower prediction quality, MLP-learned similarity suffers from other disadvantages compared to dot-product: the model has more model parameters (see Section 4.1), and is more expensive to serve (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Matrix Factorization vs NeuMF</head><p>The NCF paper <ref type="bibr" target="#b15">[16]</ref> also proposes a combined model where the similarity function is a sum of dot-product and MLP, as in Eq. (5) -this is called NeuMF 2 . The green curve in <ref type="figure" target="#fig_1">Fig. 2</ref> shows the performance of this combined model. One can observe only a minor improvement over MLP and overall a much worse quality than MF. The experiments do not support the claim in <ref type="bibr" target="#b15">[16]</ref> that a dot product model can be enhanced by feeding some part of its embeddings through an MLP.</p><p>A second variant of NeuMF was proposed in <ref type="bibr" target="#b15">[16]</ref>, that first trains MLP and MF models separately, then fine tunes the combined model. This can be viewed as a form of ensembling. The red curve shows this variant, which performs better than training the combined model directly (in green), but performs worse than the MF baseline overall, except on one datapoint (HR on Movielens with embedding dimension d = 192). Once again, the results do not support the claim that a learned similarity using a MLP is superior to a dot product. The experiment only indicates that ensembling two models can be helpful, a fact that has been observed for a variety of applications, and it is possible that ensembling with different models may yield a similar improvement. The fact remains that using a simple dot product outperforms this ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">On the performance of GMF</head><p>Other variants of matrix factorization were considered in <ref type="bibr" target="#b15">[16]</ref>. In particular, the GMF model uses a weighted dot product φ GMF as described in Eq. <ref type="bibr" target="#b5">(6)</ref>. Except for the weights in the dot product, this model is very similar to the MF baseline we trained, in particular, both models use the same loss and negative sampling method. Nevertheless, the GMF results reported in <ref type="bibr" target="#b15">[16]</ref> are much worse than our MF results. This discrepancy may seem surprising at first glance. We can see two reasons for this difference. First, properly setting up and tuning baseline methods can be difficult in general, as argued in <ref type="bibr" target="#b32">[33]</ref>, and the reported results may be improved by a more careful setup.</p><p>Second, φ GMF introduces new model parameters -the vector w in Eq. <ref type="bibr" target="#b5">(6)</ref>. While this appears to be an innocuous generalization of the dot product similarity, it can have negative effects. For example, L2 regularization of the embeddings (p and q) is meaningless unless w is regularized as well. More precisely, suppose the loss function is of the form</p><formula xml:id="formula_7">L(P, Q, w, λ) = ({φ GMF w (p, q) : p ∈ Rows(P ), q ∈ Rows(Q)})+λ( P 2 F + Q 2 F )</formula><p>where P, Q are embedding matrices, the first term of the loss depends on the pairwise similarities (i.e. the model output), and the second term is a regularization term, where P, Q are regularized but w is not. Observe that if we scale the model parameters as P/a, Q/a, a 2 w for some positive scalar a, then the model output is unchanged (given the expression of φ GMF ), and we have</p><formula xml:id="formula_8">L(P, Q, w, λ) = L 1 a P, 1 a Q, a 2 w, a 2 λ .<label>(7)</label></formula><p>It follows that minimizing L with a given λ is equivalent to minimizing L with any otherλ up to the change of variable (P/a, Q/a, a 2 w) with a = λ λ , a change of variable which leaves the model output unchanged. The solution is therefore unaffected by regularization. A second consequence is that unless λ = 0, minimizing the loss L will likely result in embedding matrices P, Q of vanishing norm and a vector of weights w of diverging norm, leading to numerical instability.</p><p>The GMF results in <ref type="bibr" target="#b15">[16]</ref> support that the model is indeed not properly regularized because its results do not improve with a higher embedding dimension -unlike in our experiments.</p><p>Finally, we observe that GMF does not improve model expressivity compared to a simple dot product, since the weights w can simply be absorbed into the embedding matrices P and Q. This is another indicator that adding parameters to a simple model is not always a good idea and has to be done carefully.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Further comparison</head><p>As reported in the meta study of <ref type="bibr" target="#b7">[8]</ref>, the results for NeuMF and MLP in <ref type="bibr" target="#b15">[16]</ref> were cherry-picked in the following sense: the metrics are reported for the best iteration selected on the test set. The NeuMF and MLP numbers we report in <ref type="figure" target="#fig_1">Fig. 2</ref> are from the original paper and likely over-estimate the actual test performance of those methods. On the other hand, our MF results in <ref type="figure" target="#fig_1">Fig. 2</ref> are not cherry picked, because we select all hyperparameters including the stopping iteration on a validation set -see Appendix A for details. The fact that our baseline MF outperforms the MLP-learned similarity despite the cherry-picking in the latter strengthens our conclusions.</p><p>In this section, we give an additional comparison using non cherry-picked results produced by <ref type="bibr" target="#b7">[8]</ref>. <ref type="table" target="#tab_1">Table 1</ref> includes their results together with our matrix factorization (same as in <ref type="figure" target="#fig_1">Fig. 2)</ref>, with embedding dimension d = 192. The results confirm that the simple matrix factorization model substantially outperforms NeuMF on all metrics and datasets. Our results provide further evidence to the conclusion of <ref type="bibr" target="#b7">[8]</ref> that simple, well-known baselines outperform NCF. Note that matrix factorization was also one of the baselines in <ref type="bibr" target="#b7">[8]</ref> (the iALS method in <ref type="table" target="#tab_1">Table 1</ref>), but our experiment shows a much larger margin than was obtained in <ref type="bibr" target="#b7">[8]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussion</head><p>Following the arguments in <ref type="bibr" target="#b32">[33]</ref>, it is possible that the studies in <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b7">[8]</ref> did not properly set up MLP and NeuMF, and that these results could be further improved. It is also possible that the performance of these models is different on other datasets. Nevertheless, at this point, the revised experiments from <ref type="bibr" target="#b15">[16]</ref> provide no evidence supporting the claim that a MLP-learned similarity is superior to a dot product. This negative result also holds for NeuMF where a GMF is added to the MLP. And it also holds for the pretrained version of NeuMF. Our study treats MLP and NeuMF favorably: (1) we report the results for MLP and NeuMF that were obtained by the original authors, avoiding any bias in improperly running their methods. (2) These cited numbers for MLP and NeuMF are likely too optimistic as they were obtained through cherry picking as identified by <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning a Dot Product with MLP is Hard</head><p>An MLP is a universal function approximator: any continuous function on a compact set can be approximated with a large enough MLP <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b2">3]</ref>. It is tempting to argue that this makes the MLP a more powerful embedding combiner and it should thus perform at least as well or better than a dot product. However, such an argument neglects the difficulty of learning the target function using MLPs: the larger class of functions also implies more parameters needed for representing the function. Hence it would require more data to learn the function and may encounter difficulty in actually learning the desired target function. Indeed, specialized structures, e.g. convolutional, recurrent, and attention structures, are common in neural networks. There is probably no hope to replace them using an MLP though they should all be representable. However, is this also true for the simple "structure" of the dot product? Similar problems turn out to be actively studied subject in machine learning theory <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b0">1]</ref>. To our knowledge, the best theoretical bound for learning the dot product, a degree two polynomial, requires O(d 4 / 2 ) steps for an error bound of <ref type="bibr" target="#b1">[2]</ref>. While the theory gives only a sufficient condition, it does hint that the difficulty scales polynomially with dimension d and 1/ . This motivates us to investigate the question empirically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental setup</head><p>We set up a synthetic learning task 3 where given two embeddings p, q ∈ R d and a label y(p, q), we want to learn a functionŷ : R 2 d → R that approximates y withŷ(p, q). We draw the embeddings p, q from N (0, σ 2 emb I) and set the true label as y(p, q) = p, q + where ∼ N (0, σ 2 label ) models the label noise. From this process we create three datasets each consisting of tuples (p, q, y). One of the datasets is used for training and the remaining two for testing. For the training and first test dataset, we first sample M different user embeddings and N different item embeddings, i.e., there are two fixed embedding matrices P ∈ R M ×d and Q ∈ R N ×d . Then we uniformly sample (without replacement) 100 M user-item combinations and put 90% into the training set and 10% into the test set. We create a second test set that consists of fresh embeddings that did not appear in the training or test set, i.e., we sample the embeddings for every case from N (0, σ 2 emb I) instead of picking them from P and Q. The motivation for this setup is to investigate if the learned similarity function generalizes to embeddings that were not seen during training.</p><p>We train the MLP on the training dataset and evaluate it on both test datasets. For the architecture of the MLP, we follow the suggestions in the NCF paper: we use an input layer of size 2d consisting of the concatenation of the two embeddings, and 3 hidden layers with sizes [4 h, 2 h, h] where h is a parameter, and use the ReLU as the activation function. The NCF paper suggests to use h = d/2, we also experiment with h = d and h = 2d. For h = d, the number of model parameters are about 18 d 2 , so for example for d=8: 1,152 or d=64: 73,728 or for d=256: 1,179,648. For optimization, we also follow the NCF paper and choose the Adam optimizer.</p><p>As evaluation metric, we compute the RMSE between the predicted similarity of the MLP and the true similarity y. We also measure the RMSE of a trivial model that predicts always 0 (=average rating in our dataset). In our setup, this RMSE is equal in expectation to Var(y) = σ 2 label + d σ 4 emb . Secondly, we measure the RMSE of the dot product model, i.e.,ŷ(p, q) = p, q . This RMSE is equal in expectation to σ label . We report the approximation error, i.e., the difference between the RMSE of the dot product model and the MLP. Each experiment is repeated 5 times and we report the mean.</p><p>We want to choose the experimental parameters σ label and σ emb such that the approximation error gives some indication what values are acceptable. To do this we choose values that are related to well-studied rating prediction tasks. In the Netflix prize, the best models have RMSEs of about 0.85 <ref type="bibr" target="#b20">[21]</ref> -for Movielens 10M, the best models have about 0.75 <ref type="bibr" target="#b32">[33]</ref>. For these datasets, it is likely that the label noise is close to these values, thus we choose the label  . With this setup, the trivial model in our experiment has the same RMSE as the trivial model on Netflix. By aligning both the trivial model and the noise to the Netflix prize, absolute differences in our experiment give some indication of the scale of acceptable errors. In both Netflix and ML 10M, a difference in RMSE of 0.01 is considered very large. For example, for the Netflix prize it took the community about a year 4 to lower the RMSE from 0.8712 to 0.8616. Similarly, for Movielens 10M, it took about 4 years to lower the RMSE from 0.7815 to 0.7634. Much smaller differences have been published. For example many published increments on Movielens 10M are about 0.001 <ref type="bibr" target="#b32">[33]</ref>. We will use these thresholds of 0.01 and 0.001 as an indication whether the approximation errors are acceptable in our experiments. While this is not a perfect comparison, we hope that it can serve as a reasonable indicator. <ref type="figure" target="#fig_2">Figure 3</ref> shows the approximation error of the MLP for different choices of embedding dimensions and as a function of training data. The figure suggests that with enough training data and wide enough hidden layers, an MLP can approximate a dot product. This holds for embeddings that have been seen in the training data as well as for fresh embeddings. However, consistent with the theory, the number of samples needed scales polynomially with the increasing dimensions and reduced error. Anecdotally, we observe the number of samples needed is about O(d/ ) α for 1 ≤ α ≤ 2. The experiments clearly indicate that it becomes increasingly difficult for an MLP to fit the dot product function with increasing dimensions. In all cases, the approximation error is well above what is considered a large difference for problems with comparable scale. For example, for the moderate d = 128, with 128000 users, the error is still above 0.02, much higher than the very significant difference of 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Results</head><p>This experiment shows the difficulty of using an MLP to approximate the dot product, even when explicitly trained to do so. Hence, if the dot product performs well on a given task, there could be a significant price to pay for an MLP to approximate it. We hope this can explain, at least partially, why the dot product model outperforms the MLP model in the experiments of Section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Applicability of Dot Product Models</head><p>Most academic studies focus on training runtime when discussing applicability. However, in industrial applications, the serving runtime is often more important, in particular when the recommendations cannot be precomputed offline but need to be computed at the time of the user's request. This is the case for most context-aware recommenders in which the recommendation depends on contextual features that are only available at query time. For instance, consider a sequential recommender that recommends items to a user based on the previously selected L items. Here the top scoring items cannot be precomputed for all possible combinations of L items. Instead the recommender would need to retrieve the highest scoring items from the whole item catalogue with a latency of a few milliseconds after the user's request. Such real time retrieval is a common application in real world recommender systems <ref type="bibr" target="#b5">[6]</ref>.</p><p>Computing a dot product similarity takes O(d) time while computing an MLP-learned similarity takes O(d 2 ) time. If there are n items to score, then the total costs are O(dn) (for dot) vs O(d 2 n) (for MLP). For large scale applications, n is typically in the range of millions and d is in the hundreds, and while dot has a lower complexity, both are impractical for retrieval applications that require latencies of a few milliseconds. However, for a dot product, the problem of finding the top scoring items can be approximated efficiently. Indeed, given the user embedding p, the problem is to find items i that maximize p, q i . This is a well-studied problem, known as approximate nearest neighbor search <ref type="bibr" target="#b25">[26]</ref> or maximum inner product search <ref type="bibr" target="#b33">[34]</ref>. Efficient sublinear time algorithms exist that makes dot product retrieval feasible in typically a few milliseconds, even with millions of items n <ref type="bibr" target="#b5">[6]</ref>. To the best of our knowlegde, no such sublinear techniques exist for nearest neighbor retrieval with MLPs.</p><p>To summarize, MLP similarity is not applicable for real time top-N recommenders, while the dot product allows fast retrieval using well established nearest neighbor search algorithms.</p><p>6 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Dot products at the Output Layer of DNNs</head><p>At first glance it might appear that our work questions the use of neural networks in recommender systems. This is not the case, and as we will discuss now, many of the most competitive neural networks use a dot product for the output but not an MLP. Consider the general multiclass classification task where (x, y) is a labeled training example with input x and label y ∈ {1, . . . , n}. A common approach is to define a DNN f that maps the input x to a representation (or embedding) f(x) ∈ R d . At the final stage, this representation is combined with the class labels to produce a vector of scores. Commonly, this is done by multiplying the input representation f(x) ∈ R d with a class matrix Q ∈ R n×d to obtain a scalar score for each of the n classes. This vector is then used in the loss function, for example as logits in a softmax cross entropy with the label y. This falls exactly under the family of models discussed in this paper, where p = f(x) ∈ R d and the classes are the items. In fact, the model as described above is a dot product model because at the output Q f(x) = Q p = [ p, q i ] n i=1 which means each input-label or user-item combination is a dot product between an input (or user) embedding and label (or item) embedding. This dot product combination of input and class representation is commonly used in sophisticated DNNs for image classification <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b13">14]</ref> and for natural language processing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b8">9]</ref>. This makes our findings that a dot product is a powerful embedding combiner well aligned with the broader DNN community where it is common to apply a dot product at the output for multiclass classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">MLPs at the Output Layer of DNNs</head><p>NeuMF is very closely related to the previously proposed neural network matrix factorization <ref type="bibr" target="#b10">[11]</ref>. Neural network matrix factorization also uses a combination of an MLP plus extra embeddings with an explicit dot product like structure as in GMF. A follow up paper <ref type="bibr" target="#b14">[15]</ref> proposes to replace the MLP in NCF by an outerproduct and pass this matrix through a convolutional neural network. Finding the dot product with this technique is trivial because the sum of the diagonal in the outerproduct is the dot product. Unfortunately, while written by the same authors as the NCF paper, it evaluates on different data, so our results in Section 3.3 cannot be compared to their numbers and it remains unclear if their work improves over a well tuned baseline with a dot product. Besides prediction quality, this proposal suffers from the same applicability issues as the MLP (see Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Specialized Structures inside a DNN</head><p>In DNN modeling it is very common to replace an MLP by a more specialized structure that has an inductive bias that represents the problem better. For example, in image classification structures such as convolutional neural networks are very popular because they represent the spatial structure of the input data. In recurrent neural networks, such parameter sharing is very important too. Another example are attention models, e.g. in Neural Machine Translation <ref type="bibr" target="#b35">[36]</ref> and in the Transformer model <ref type="bibr" target="#b34">[35]</ref>, that contain a matrix product inside the neural network for combining multiple inputs -they can be regarded as the dot product model for combining "internal" embeddings too. All these specialized structures are crucial for advancing the state of the art of deep learning, although in theory they can all be approximated by MLPs.</p><p>The inefficiency of MLPs to capture dot and tensor products has been studied by <ref type="bibr" target="#b4">[5]</ref> in the context of recommender systems. Here the authors examine how to add context to recurrent neural networks. Similar to our work and Section 4.1, <ref type="bibr" target="#b4">[5]</ref> points out that MLPs do not model multiplications and it investigates approximating dot products and tensor products with MLPs empirically. Their study focuses on the model size required to learn a tensor product for embeddings of dimension d = 1 and d = 2, where the number of distinct embeddings is 100 per mode and the training error is measured.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Experimental Issues in Recommender Systems</head><p>In their meta study, <ref type="bibr" target="#b7">[8]</ref> point out issues with evaluation in recommender system research. Their experiments also cover the NCF paper. They show that well studied baselines can get comparable results to (a reproducible value of) NeuMF (see <ref type="bibr">Section 3.4)</ref>. The goal of our study and <ref type="bibr" target="#b7">[8]</ref> is different. While <ref type="bibr" target="#b7">[8]</ref> covers a broad set of methods and publications, we are investigating the specific issue of learned similarity functions in more detail. Our work provides apples to apples comparisons of dot product vs MLP, stronger results (outperforming the original NCF results), and a thorough investigation of the reasons and consequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>Our findings indicate that a dot product might be a better default choice for combining embeddings than learned similarities using MLP or NeuMF. Shifting the focus in the recommender system research community from learned similarities to dot products might have several positive effects: (1) The research becomes more relevant for the industry because models are applicable (see Section 5).</p><p>(2) Dot product similarity simplifies modeling and learning (no pretraining, no need for large datasets) which facilitates both experimentation and understanding. (3) Better alignment with other research areas such as natural language processing or image models where the dot product is commonly used.</p><p>Finally, our experiments give further evidence that running machine learning methods properly is difficult <ref type="bibr" target="#b32">[33]</ref> and one-off studies are prone to drawing wrong conclusions. Introducing shared benchmarks might help to better identify improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiments from NCF Paper</head><p>This section provides details about our setup for establishing a dot product baseline for the experiments of the NCF paper (Section 3.3). The code and datasets of NCF were provided by its authors 5 . We provide code for our implementation of matrix factorization and the script to generate the tuning split 6 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Model and Optimization</head><p>We implemented a matrix factorization with bias (see Eq. 2). The parameters of this model are the embeddings P ∈ R M ×d for M users and Q ∈ R N ×d for N items. Following the NCF paper, for training we cast the implicit data, which contains only positive observations, into a binary two class classification problem and sample m negative items for each tuple in the implicit data. In each epoch a new set of negatives is drawn -the sampling distribution is uniform.</p><p>We minimize the binary logistic loss with L2 regularization. For each training example (u, i, y) where y ∈ {0, 1} is the binary label, the regularized loss is</p><formula xml:id="formula_9">l(u, i, y) = −y ln σ(φ(p u , q i )) − (1 − y) ln(1 − σ(φ(p u , q i ))) + λ p u + λ q i<label>(8)</label></formula><p>with the regularization constant λ ∈ R + . The loss is optimized with stochastic gradient descent with learning rate η, with the update rules:</p><formula xml:id="formula_10">p u,1 ← p u,1 − η[(σ(φ(p u , q i )) − y) + λp u,1 ]<label>(9)</label></formula><formula xml:id="formula_11">q i,1 ← q i,1 − η[(σ(φ(p u , q i )) − y) + λq i,1 ] (10) p u,[2,...,d] ← p u,[2,...,d] − η[(σ(φ(p u , q i )) − y) q i,[2,...,d] + λp u,[2,...,d] ] (11) q i,[2,...,d] ← q i,[2,...,d] − η[(σ(φ(p u , q i )) − y) p u,[2,...,d] + λq i,[2,...,d] ]<label>(12)</label></formula><p>The embeddings are initialized from a normal distribution. This configuration shares the same loss, regularization, negative sampling approach, and initialization procedure with MLP and NeuMF as proposed in <ref type="bibr" target="#b15">[16]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Hyperparameter Tuning</head><p>We create a tuning dataset that follows the same splitting protocol as the final training/test split. In particular, we remove the last feedback from each user from the training set and place it in a test set for tuning and keep the remaining training cases in a training set for tuning. We then train models on the training set for tuning and evaluate the model on the test set for tuning. We choose all hyperparameters including the number of training epochs on this tuning set. Note that both the training set for tuning and test set for tuning contain no information about the final test set.</p><p>From our past experience with matrix factorization models, if the other hyperparameters are chosen properly, then the larger the embedding dimension the better the quality -our experiments <ref type="figure" target="#fig_1">Figure 2</ref> confirm this. For the other hyperparameters: learning rate and number of training epochs influence the convergence curves. Usually, the lower the learning rate, the better the quality but also the more epochs are needed. We set a computational budget of up to 256 epochs and search for the learning rate within this setting. In the first hyperparameter pass, we search a coarse grid of learning rates η ∈ {0.001, 0.003, 0.01} and number of negatives m = {4, 8, 16} while fixing the regularization to λ = 0. Then we did a search for regularization in {0.001, 0.003, 0.01} around the promising candidates. To speed up the search, these first coarse passes were done with 128 epochs and a fixed dimension of d = 64 (Movielens) and d = 128 (Pinterest). We did further refinements around the most promising values of learning rate, number of negatives and regularization using d = 128 and 256 epochs.</p><p>Throughout the experiments we initialize embeddings from a Gaussian distribution with standard deviation of 0.1; we tested some variation of the standard deviation but did not see much effect.</p><p>The final hyperparameters for Movielens are: learning rate η = 0.002, number of negatives m = 8, regularization λ = 0.005, number of epochs 256. For Pinterest: learning rate η = 0.007, number of negative samples m = 10, regularization λ = 0.01, number of epochs 256.</p><p>After hyperparameter selection, we trained on the full dataset with these hyperparameters and evaluated according to the protocol in <ref type="bibr" target="#b15">[16]</ref>. We repeated the final training and evaluation 8 times and report the mean of the metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 MLP and NeuMF Results</head><p>We report the results for MLP and NeuMF from the original NCF paper <ref type="bibr" target="#b15">[16]</ref>. As we share the same evaluation protocol and splits, the numbers are comparable. We report the results for NeuMF from <ref type="table">Table 2</ref> in <ref type="bibr" target="#b15">[16]</ref> and the results for MLP from Tables 3,4 in <ref type="bibr" target="#b15">[16]</ref> using the 'MLP-3' setting.</p><p>It should be noted that in <ref type="bibr" target="#b15">[16]</ref>, the tables and plots use "predictive factor" instead of embedding dimension. The predictive factor is defined as the size of the last hidden layer of the MLP, and as described in <ref type="bibr" target="#b15">[16]</ref>, for the 3-layer MLP a predictive factor of k operates on two input embeddings, each of dimension d = 2 k. For the NeuMF model, a predictive factor of k operates on embeddings of dimension d = 3 k because it consists of an independent MLP with predictive factor of k (embedding size d = 2 k) and a GMF with embedding size d = k. This definition of predictive factor is arbitrary, in fact it can be made arbitrarily small by adding layers to the MLP without changing anything else in the model. We think it is more meaningful to compare models with a fixed embedding dimension. In particular, we want to investigate the prediction quality of an MLP or a dot product over two embeddings of the same size d. We recast all results from the NCF paper in terms of embedding dimension, by multiplying the predictive factor by 3 for NeuMF results and by 2 for MLP results. This allows us to do an apples to apples comparison of different similarity functions over an embedding space of dimension d.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>MLPFigure 1 :</head><label>1</label><figDesc>b1 ([p, q]) . . .) A model with dot product similarity (left) and MLP-based learned similarity (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of learned similarities (MLP, NeuMF) to a dot product:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>How well a MLP can learn a dot product over embeddings of dimension d. The ground truth is generated from a dot product of Gaussian embeddings plus Gaussian label noise. The graphs show the difference between the RMSE of the dot product and the RMSE of the learned similarity measure; the solid line measures the difference on the fresh set, the dotted on the test set. Noise and scale have been chosen such that 0.01 could indicate a very significant difference and 0.001 a significant difference. noise σ label = 0.85. For the Netflix prize, the trivial model that predicts always the average rating has an RMSE of 1.13. Thus we set σ 2 emb = 1.13 2 −0.85 2 d</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison from<ref type="bibr" target="#b7">[8]</ref> of MLP+GMF (NeuMF) with various baselines and our results. The best results are highlighted in bold, the second best result is underlined.</figDesc><table><row><cell>Method</cell><cell cols="2">Movielens</cell><cell cols="2">Pinterest</cell><cell>Result</cell></row><row><cell></cell><cell cols="4">HR@10 NDCG@10 HR@10 NDCG@10</cell><cell>from</cell></row><row><cell>Popularity</cell><cell>0.4535</cell><cell>0.2543</cell><cell>0.2740</cell><cell>0.1409</cell><cell>[8]</cell></row><row><cell>SLIM [29, 24]</cell><cell>0.7162</cell><cell>0.4468</cell><cell>0.8679</cell><cell>0.5601</cell><cell>[8]</cell></row><row><cell>iALS [19]</cell><cell>0.7111</cell><cell>0.4383</cell><cell>0.8762</cell><cell>0.5590</cell><cell>[8]</cell></row><row><cell>MLP+GMF [16]</cell><cell>0.7093</cell><cell>0.4349</cell><cell>0.8777</cell><cell>0.5576</cell><cell>[8]</cell></row><row><cell cols="2">Matrix Factorization 0.7294</cell><cell cols="2">0.4523 0.8895</cell><cell cols="2">0.5794 Fig. 2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The hyperparameters of the dot product model are: embedding dimension d, regularization λ, learning rate η, number of negative samples m, number of training epochs, standard deviation for initialization. Analogously to the NCF paper, we report results for d ∈ {16, 32, 64, 96, 128, 192}.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It is possible that a different loss or a different sampling strategy could lead to an even better performance of our method. However, we wanted to use the same loss and sampling strategy for all competing methods to ensure that this is a meaningful comparison, which will allow us to attribute differences in quality to the choice of similarity functions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Following<ref type="bibr" target="#b15">[16]</ref>, the NeuMF uses 2/3rds of the embeddings for the MLP and 1/3rd for the MF. See the discussion about "predictive factors" in Section A.3 for details.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The code is available at https://github.com/google-research/google-research/tree/ master/dot_vs_learned_similarity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.netflixprize.com/leaderboard_quiz.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/hexiangnan/neural_collaborative_filtering 6 https://github.com/google-research/google-research/tree/master/dot_vs_ learned_similarity</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A convergence theory for deep learning via over-parameterization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="242" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning polynomials with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Panigrahy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Valiant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
	<note>ICML&apos;14, JMLR.org, p. II-1908-II-1916</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Universal approximation bounds for superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="930" to="945" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent cross: Making use of context in recurrent recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
	<note>WSDM &apos;18</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep neural networks for youtube recommendations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Covington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sargin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="191" to="198" />
		</imprint>
	</monogr>
	<note>RecSys &apos;16, Association for Computing Machinery</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Approximation by superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cybenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of control, signals and systems</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="303" to="314" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A troubling analysis of reproducibility and progress in recommender systems research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Dacrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boglio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cremonesi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jannach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bert</surname></persName>
		</author>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient descent finds global minima of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1675" to="1685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Neural network matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning image and user features for recommendation in social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4274" to="4282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The movielens datasets: History and context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Harper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Konstan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Interact. Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Outer product-based neural collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Seventh International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2227" to="2233" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web (Republic and Canton of</title>
		<meeting>the 26th International Conference on World Wide Web (Republic and Canton of<address><addrLine>Geneva, Switzerland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
	<note>WWW &apos;17, International World Wide Web Conferences Steering Committee</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>White</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leveraging meta-path based context for top-n recommendation with a neural co-attention model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1531" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Collaborative filtering for implicit feedback datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Volinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 Eighth IEEE International Conference on Data Mining</title>
		<meeting>the 2008 Eighth IEEE International Conference on Data Mining</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="263" to="272" />
		</imprint>
	</monogr>
	<note>ICDM &apos;08</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A pre-filtering approach for incorporating contextual information into deep learning based recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M A</forename><surname>Jawarneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bellavista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Corradi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Foschini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Montanari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berrocal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Murillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="40485" to="40498" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The bellkor solution to the netflix grand prize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Advances in Collaborative Filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Springer US</publisher>
			<biblScope unit="page" from="145" to="186" />
			<pubPlace>Boston, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Efficient top-n recommendation by linear regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys Large Scale Recommender Systems Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mixture-rank matrix approximation for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="477" to="485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An investigation of practical approximate nearest neighbor algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Neural Information Processing Systems</title>
		<meeting>the 17th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="825" to="832" />
		</imprint>
	</monogr>
	<note>NIPS&apos;04</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mattson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Coleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bailis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bittorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hazelwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kanter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Oguntebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pekhimenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pentecost</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tabaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zaharia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Mlperf training benchmark</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Slim: Sparse linear methods for top-n recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE 11th International Conference on Data Mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Neural personalized ranking for image recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="423" to="431" />
		</imprint>
	</monogr>
	<note>WSDM &apos;18</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Improving regularized singular value decomposition for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paterek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD cup and workshop</title>
		<meeting>KDD cup and workshop</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="5" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Sequential recommendation with dual side neighbor-based collaborative relation modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="465" to="473" />
		</imprint>
	</monogr>
	<note>WSDM &apos;20</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On the difficulty of evaluating baselines: A study on recommender systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koren</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno>abs/1905.01395</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Asymmetric lsh (alsh) for sublinear time maximum inner product search (mips)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="2321" to="2329" />
		</imprint>
	</monogr>
	<note>NIPS&apos;14</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning a joint search and recommendation model from user-item interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zamani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="717" to="725" />
		</imprint>
	</monogr>
	<note>WSDM &apos;20</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improving the estimation of tail ratings in recommender system with multi-latent representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caverlee</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="762" to="770" />
		</imprint>
	</monogr>
	<note>WSDM &apos;20</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
