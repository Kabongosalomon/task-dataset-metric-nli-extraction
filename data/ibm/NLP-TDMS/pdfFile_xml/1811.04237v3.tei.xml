<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SKELETON-BASED ACTION RECOGNITION WITH SYNCHRONOUS LOCAL AND NON-LOCAL SPATIO-TEMPORAL LEARNING AND FREQUENCY ATTENTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guyue</forename><surname>Hu</surname></persName>
							<email>guyue.hu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">Brainnetome Center &amp; National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Cui</surname></persName>
							<email>bo.cui@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">Brainnetome Center &amp; National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Yu</surname></persName>
							<email>shan.yu@nlpr.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Automation</orgName>
								<orgName type="laboratory">Brainnetome Center &amp; National Laboratory of Pattern Recognition</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">CAS Center for Excellence in Brain Science and Intelligence Technology</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SKELETON-BASED ACTION RECOGNITION WITH SYNCHRONOUS LOCAL AND NON-LOCAL SPATIO-TEMPORAL LEARNING AND FREQUENCY ATTENTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T07:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Action recognition</term>
					<term>frequency attention</term>
					<term>syn- chronous local and non-local learning</term>
					<term>soft-margin focal loss</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Benefiting from its succinctness and robustness, skeleton-based action recognition has recently attracted much attention. Most existing methods utilize local networks (e.g. recurrent, convolutional, and graph convolutional networks) to extract spatio-temporal dynamics hierarchically. As a consequence, the local and non-local dependencies, which contain more details and semantics respectively, are asynchronously captured in different level of layers. Moreover, existing methods are limited to the spatio-temporal domain and ignore information in the frequency domain. To better extract synchronous detailed and semantic information from multi-domains, we propose a residual frequency attention (rFA) block to focus on discriminative patterns in the frequency domain, and a synchronous local and non-local (SLnL) block to simultaneously capture the details and semantics in the spatio-temporal domain. Besides, a soft-margin focal loss (SMFL) is proposed to optimize the learning whole process, which automatically conducts data selection and encourages intrinsic margins in classifiers. Our approach significantly outperforms other state-of-the-art methods on several large-scale datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>The skeleton-based human action recognition has recently attracted much attention due to its succinctness of representation and robustness to variations of viewpoints, appearances and surrounding distractions <ref type="bibr" target="#b0">[1]</ref>. Most previous works treat skeletal actions as sequences and pseudo-images, then apply Recurrent Neural Networks (RNN) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> and Convolutional Neural Networks (CNN) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref> to model the temporal evolutions and the spatio-temporal dynamics, respectively. Yan et al. <ref type="bibr" target="#b5">[6]</ref> also feeds skeleton graphs into graph convolutional networks (GCN) to exploit the structure information of human body. However, all the aforementioned methods apply stacked local networks to hierarchically extract spatio-temporal features, which exist two serious problems. 1) The recurrent and convolutional operations are neighborhood-based local operations <ref type="bibr" target="#b6">[7]</ref>, so the local-range detailed information and non-local semantic information mainly be captured asynchronously in the lower and higher layers respectively, which hinders the fusion of details and semantics in action dynamics. 2) Human actions such as shaking hands, brushing teeth, and clapping have characteristic frequency patterns, but previous works are always limited to the spatio-temporal dynamics and ignore periodic patterns in the frequency domain.</p><p>In this paper, we propose a novel model SLnL-rFA to better extract synchronous detailed and semantic information from multidomains. SLnL-rFA is equipped with synchronous local and nonlocal (SLnL) blocks for spatio-temporal learning, and a residual frequency attention (rFA) block for frequency-patterns mining. To optimize whole learning process, a novel soft-margin focal loss (SMFL) is also proposed, which adaptively conducts data selection during training and encourages intrinsic margin in classifiers. <ref type="figure">Fig.1</ref> shows the pipeline of our method. Firstly, an adaptive transform network augments and transforms the skeletal actions. Secondly, the residual frequency attention block selects discriminative frequency patterns. Then, following with M1 synchronous local and non-local (SLnL) blocks and M2 local blocks in the spatio-temporal domain, where SLnL is designed to simultaneously extract local details and nonlocal semantics. Finally, three classifiers with inputs from position, velocity and concatenated features are optimized as a pseudo multitask learning problem according to our soft-margin focal loss.</p><p>Our main contributions are summarized as follows: 1) Moving beyond the spatio-temporal domain, we propose a residual frequency attention block to exploit frequency information for skeleton-based action recognition; 2) We propose a synchronous local and nonlocal block to simultaneously capture details and semantics in the early-stage layers; 3) We propose a soft-margin focal loss, which adaptively conducts data selection during training process and encourages intrinsic soft-margins in the classifiers; 4) Our approach outperforms the state-of-the-art methods with significant margins on two large-scale datasets for skeleton-based action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORKS</head><p>Frequency domain analysis. Generalized frequency domain analysis contains several large classes of methods such as discret Fourier transform (DFT), short-time Fourier transform (SFT) and wavelet tranform, which are classical tools in the fields of signal analysis and image processing. Due to the booming of deep learning techniques <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, methods based on the spatio-temporal domain dominate the field of computer vision, with only a few works paying attention to the frequency domain. For example, frequency domain analysis of critical points trajectories <ref type="bibr" target="#b9">[10]</ref> and frequency divergence image <ref type="bibr" target="#b10">[11]</ref> are applied for RGB-based action recognition. Our work will revisit the frequency domain, and exploit frequency patterns to improve the skeleton-based action recognition.</p><p>Non-local operations. Non-local means is a classical filtering algorithm that allows distant pixels to contribute to the target pixel  <ref type="figure">Fig. 1</ref>. The overall pipeline of the proposed method. The position and velocity information of human joints are fed into a tranform network, a residual attention network, M1 synchronous local and non-local blocks, and M2 local blocks sequentially. Treated as a pseudo multi-task learning task, the proposed model is optimized according to our soft-margin focal loss. <ref type="bibr" target="#b11">[12]</ref>. Block-matching <ref type="bibr" target="#b12">[13]</ref> explores groups of non-local similarity between patches. Block-matching is widely used in computer vision tasks like super-resolution <ref type="bibr" target="#b13">[14]</ref>, image inpainting <ref type="bibr" target="#b14">[15]</ref>, etc. The popular self-attention <ref type="bibr" target="#b15">[16]</ref> in machine translation can also be viewed as a non-local operation. Recently, different non-local blocks are inserted into CNNs for video classification <ref type="bibr" target="#b6">[7]</ref> and RNNs for image restoration <ref type="bibr" target="#b16">[17]</ref>. However, their local and non-local operations apply to objects in different level of layers but our SLnL simultaneously operate on the same objects, thus only the proposed SLnL can extract local and non-local information synchronously.</p><p>Reformed softmax loss. The softmax loss <ref type="bibr" target="#b17">[18]</ref>, consisted of the last fully connected layer, the softmax function, and the crossentropy loss, is widely applied in supervised learning due to its simplicity and clear probabilistic interpretation. However, recent works <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> have exposed its limitations on feature discriminability and have stimulated two types of methods for improvements. One type directly refines or combines the cross-entropy loss with other losses like contrastive loss, triplet loss, etc <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. The other type reformulates the softmax function with geometrical or algebraic margin <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> to encourage intra-class compactness and inter-class separability of feature learning, which completely destroys the probabilistic meaning of the original softmax function. Our SMFL not only conducts data selection but also encourages intrinsic soft-margins in classifiers with a clear probabilistic interpretation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">METHODS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Preliminary</head><p>A skeletal action X ∈ R d×T ×N is represented by d dimensional locations of N body joints in a T frame video. Following Li et al. <ref type="bibr" target="#b20">[21]</ref>, we introduce a skeleton transformer to augment the number of joints and rearrange the order of joints. Similarly, a coordinate transformer is also applied to transform the original representations in single rectangular coordinate system to rich representations in K oblique coordinate systems. The whole transform network in <ref type="figure">Fig.1</ref> is implemented with two fully connected layers and corresponding transpose, flatten, and concatenate operations. As a result, a new adaptive expression X ∈ R Kd×T ×N is formed for each action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Residual Frequency Attention</head><p>Previous works always concentrate on the spatio-temporal domain, but many actions contain inherent frequency-sensitive patterns, such  as shaking hands, and brushing teeth, which motivates us to revisit the frequency domain. The classical operations in the frequency domain, such as high-pass, low-pass, and band-pass filters, only have a few parameters that are far from enough, thus we propose a more general frequency attention block <ref type="figure" target="#fig_1">(Fig. 2</ref>) equipped with abundant learnable parameters to adaptively select frequency components.</p><p>Given a transformed action after the transform network X ∈ R C ×T ×N (C =Kd, T =T ), the 2D discret Fourier transform (DFT) transforms the pseudo spatio-temporal image X in each channel to Y ∈ R C ×T ×N in the frequency domain via  where u, v and c are frequencies and channel of spatio-temporal image respectively, and Fcos/F sin denotes the cosine/sinusoidal component. The frequency spectrum F A = (F 2 cos + F 2 sin ) 1/2 and the phase spectrum F φ = arctan − F sin Fcos . In practice, the DFT and its inverse (IDFT) are computed through the fast Fourier transform (FFT) algorithm and its inverse (IFFT).</p><formula xml:id="formula_0">Y [c, u, v] = T −1 t=0 N −1 n=0 X [c, t, n]cos −2π ut T + vn N + j T −1 t=0 N −1 n=0 X [c, t, n]sin −2π ut T + vn N = Fcos[c, t, n] + jF sin [c, t, n],</formula><p>For each action, the attention weights Mcos and M sin are complex functions of its cosine and sinusoidal components, i.e. ,</p><formula xml:id="formula_1">M i = dup(σ(W i1 (W i2 (Avg(F i )) + b i1 ) + b i2 )),<label>(1)</label></formula><p>where i ∈ {cos, sin}. Specifically, after a channel averaging operation, each component is fed into two fully connected layers (FC) to learn adaptive weights for each frequency, followed by a sigmoid transfom function. The first FC layers serve as a bottleneck layer <ref type="bibr" target="#b8">[9]</ref> for dimensionality reduction with a ratio factor λ. Then, the learned attention weights are duplicated to every channel to pay attention to the input frequency image via</p><formula xml:id="formula_2">F sin = F sin M sin ,<label>(2)</label></formula><formula xml:id="formula_3">F cos = Fcos Mcos,<label>(3)</label></formula><p>where denotes the element-wise multiplication. Finally, a spatiotemporal residual component is applied to obtain the output X ∈ R C ×T ×N after attention, i.e.</p><formula xml:id="formula_4">X = X + if f t2 (F sin , F cos ),<label>(4)</label></formula><p>where if f t2 denotes the efficient 2-dimensional IFFT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Synchronous Local and Non-local Learning in the Spatiotemporal Domain</head><p>Non-local Module. A general non-local operation takes a multichannel signal X ∈ R M ×P as its input and generates a multi-channel output Y ∈ R M ×Q . Here P and Q are channels, and M is the number of Ω, where Ω is the set that enumerates all positions of the signal (image, video, feature map, etc.). Let x i and y i denote the i-th row vector of X and Y , the non-local operation is formulated as follows:</p><formula xml:id="formula_5">y i = 1 Z i (X) j∈Ω φ(x i , x j )g(x j ), ∀i ∈ Ω<label>(5)</label></formula><p>where the multi-channel unary transform g(x j ) computes the embedding of xj, the multi-channel binary transform φ(x i , x j ) computes the affinity between the positions i and j, and Z(X) is a normalization factor. With different choices of φ and g, such as Guassian, embeddded Gaussian and dot product, various of non-local operations could be constructed. For simplicity, we only consider φ and g in the form of linear embedding and embeddded Gaussian respectively, and set</p><formula xml:id="formula_6">Z i (X) = j∈Ω φ(x i , x j ), i.e. g(x j ) = (Wgx T j ) T , ∀j<label>(6)</label></formula><p>where Wg ∈ R Q×P are learnable transform parameters.</p><formula xml:id="formula_7">φ(x i , x j ) = e ϕ(x i ) T ψ(x j ) , ∀i, j<label>(7)</label></formula><p>ϕ(</p><formula xml:id="formula_8">x i ) = (Wϕx T i ) T , ∀i (8) ψ(x j ) = (W ψ x T j ) T , ∀j<label>(9)</label></formula><p>where Wϕ, W ψ ∈ R L×P , and L denotes the embedding channel. To weigh how important the non-local information is when compared to local information, a weighting function is appended, i.e.</p><formula xml:id="formula_9">w(y i ) = (Ww(y i ) T ) T ,<label>(10)</label></formula><p>where Ww ∈ R Q×Q . A non-local module can be completed with some transpose operations, some convolutional layers with the kernels of 1, and a softmax layer, <ref type="figure" target="#fig_3">Fig.3</ref>(a) shows a 2D example. Baseline local block. The local operation is defined as</p><formula xml:id="formula_10">y i = 1 Z i (X) j∈δ i φ(x i , x j )g(x j ), ∀i ∈ Ω<label>(11)</label></formula><p>where δ i is the local neighbor set of target position i, δ i Ω.</p><p>The convolution is a typical local operation with identity affinity φ(x i , x j ) = 1, liner transform g(x j ) = w j x j , identity normalization factor Z i (X) = 1, and δ i is the neighbors around target center i with a same shape of kernel. Our baseline local block is constructed from convolution operation. As shown in <ref type="figure" target="#fig_3">Fig.3(b)</ref>, two convolutional layers with kernel k × 1 and 1 × k are applied to learn temporal local (tLocal) features and spatial local (sLocal) features respectively, and a k × k convolutional layer for spatial-temporal local (stLocal) features. The block also contains a residual path, a rectified linear unit (ReLU) and a batch normalization (BN) layer. Synchronous local and non-local block. In order to synchronously exploit local details and non-local semantics in human actions, three non-local modules are parallel merged into the above baseline local block. As shown in <ref type="figure" target="#fig_3">Fig.3(c)</ref>, two 1D non-local modules to explore temporal non-local (tNon-Local) and spatial nonlocal (sNon-Local) information respectively, followed by a 2D nonlocal module for spatio-temporal non-local (stNon-Local) patterns. We define the affinity field as the representation of the range of pixel indices that could contribute to the target position in the next layer of the local or non-local modules, which is a more general concept than the receptive field of CNNs. The affinity field in <ref type="figure" target="#fig_3">Fig.3(d)</ref> clearly shows our SLnL can mine local details and non-local semantics synchronously in every layer. Note that our SLnL is significantly different from the methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref> which only inserted a few non-local modules after stacked local networks, thus the local and non-local operations are still separately conducted in different layers having different resolutions. Contrastively, our SLnL simultaneously captures local and non-local patterns in every layer ( <ref type="figure" target="#fig_3">Fig.3(d)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Soft-margin focal loss</head><p>A common challenge for classification tasks is that the discrimination difficulties are different among samples and classes, but most previous works for skeleton-based action recognition use the softmax loss that haven't taken it into consideration. There are two possible measures to alleviate it, i.e. data selection and margin encouraging.</p><p>Intuitively, the larger predicted probability a sample has, the farther away from the decision boundary it might be, and vice versa. Motivated by this intuition, we construct a soft-margin (SM) loss term as follows:</p><formula xml:id="formula_11">L SM (pt) = log (e m + (1 − e m )pt) ,<label>(12)</label></formula><p>where pt is the estimated posterior probability of ground truth class, and m is a margin parameter. L SM ∈ [0, m] because that pt ∈ [0, 1]. As <ref type="figure" target="#fig_4">Fig.4</ref> shows when the posterior probability pt is small, the sample is more likely be close to the boundary, thus we penalize it with a large margin loss. Otherwise, a small margin loss is imposed. To further illustrate the idea, we introduce the L SM into cross entropy loss leading to a soft-margin cross entropy (SMCE) loss,</p><formula xml:id="formula_12">L SM CE (pt) = L SM + L CE<label>(13)</label></formula><p>= log (e m + (1 − e m )pt) − log(pt).</p><p>Assuming that x ∈ R d is the features before the last FC layer, the FC layer transforms it into score z = [z 1 , z 2 , . . . , z C ] T ∈ R C of C classes by multiplying W = [w 1 , w 2 , · · · , w C ] ∈ R d×C , where wc is the parameter of the linear classifier corresponding to the class c, i.e. zc = w T c x. Followed with a softmax layer, pt = e w t x C c=1 e wc x and (1 − pt) = C c =t e wc x C c=1 e wc x , then the SMCE can be rewritten as</p><formula xml:id="formula_13">L SM CE = log (pt + e m · (1 − pt)) − log(pt) = log e w t x + e m · C c =t e wcx C c=1 e wcx − log e w t x C c=1 e wcx = −log e w t x e w t x + e m · C c =t e wcx = −log e w t x−m e w t x−m + C c =t e wcx .<label>(14)</label></formula><p>Comparing the standard softmax loss with Eq.14, only the score of the ground truth class wtx is replaced by wtx − m. Optimizing model with SMCE, we will obtain classifiers that meet the constraint wtx − m ≥ w c =t x. As a result, an intrinsic margin m between the positive (belonging to a specific class) samples and the negative (not belonging to the specific class) samples of each class will be formed in classifiers by adding the SM loss term into the loss function.</p><p>In addition, the focal loss <ref type="bibr" target="#b21">[22]</ref> defined as where γ is a focusing parameter, can encourage adaptive data selection without any damage to the original model structure and training processes. As <ref type="figure" target="#fig_4">Fig.4</ref> shows the relative loss for well-classified easy samples is reduced by FL when compared to CE. Although FL pays more attention to hard samples, it has no margin around the decision boundary. Similar to SMCE, we introduce the L SM term into FL to obtain the soft-margin focal loss (SMFL) as follows: Finally, our SMFL can encourage intrinsic margins in classifiers and maintain FL's advantage of data selection as well. Our two stream model ( <ref type="figure">Fig.1</ref>) predicts three probability vectors p p , p v , p c from three modes including position, velocity, and their concatenation. We optimize it as a pseudo multi-task learning problem with our SMFL, i.e. each classifier produces a loss via <ref type="bibr" target="#b16">(17)</ref> where k ∈ {p, v, c} is mode type, and y = (y 1 , y 2 , · · · , y C ) is the one-hot class label. Thus the final loss is as follows:</p><formula xml:id="formula_14">L F L (pt) = −(1 − pt) γ log(pt),<label>(15)</label></formula><formula xml:id="formula_15">L</formula><formula xml:id="formula_16">L k = C i=1 y i log(e m + (1 − e m )p k i ) − (1 − p k i ) γ log(p k i ) ,</formula><formula xml:id="formula_17">L = Lp + Lv + Lc.<label>(18)</label></formula><p>During inference, only p c is used to predict the final class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Experimental details</head><p>NTU RGB+D (NTU) dataset <ref type="bibr" target="#b1">[2]</ref> is currently the largest in-door action recognition dataset. It contains 56,000 clips in 60 actions performed by 40 subjects. Each clip consists of 25 joint locations with one or two persons. There are two evaluation protocols for this dataset, i.e., cross-subject (CS) and cross-view (CV). For the cross-subject evaluation, 40320 samples from 20 subjects were used for training and 16540 samples from the rest subjects were used for testing. For the cross-view evaluation, samples are split by camera views, with two views for training and the rest one for testing. Kinetics dataset is by far the largest unconstrained action recognition dataset, which contains 300,000 video clips in 400 classes retrieved from YouTube <ref type="bibr" target="#b5">[6]</ref>. The skeleton is estimated by Yan et al. from the raw RGB videos by OpenPose toolbox <ref type="bibr" target="#b5">[6]</ref>. Each joint consists of 2D coordinates (X, Y ) in the pixel coordinate system and a confidence score C, thus finally represented by a tuple of (X, Y, C). Each skeleton frame is recorded as an array of 18 tuples.</p><p>Implementation Details: During the data preparation, we randomly crop sequences with a ratio uniformly drawn from [0.5,1] for training, and centrally crop sequences with a fixed ratio of 0.95 for inference. We resize the sequences to 64/128 (NTU/Kinetics) frame with bilinear interpolation. Finally, the obtained data are fed into a batch normalization layer to normalize the scale. During training, we apply Adam optimizer with weight decay of 0.0005. Learning rate is initialized as 0.001, followed by an exponential decay with a rate of 0.98/0.95 (NTU/Kinetics) per epoch. A dropout with ratio of 0.2 is applied to each block to alleviate overfitting. The model is trained for 300/100 epoches with a batch size of 32/128 (NTU/Kinetics). Each stream of model for NTU is composed of totally 6 blocks in <ref type="figure" target="#fig_3">Fig.3</ref> with local kernels of 3 and channels of 64, 64, 128, 128, 256, 256 respectively, also max-pooling is applied every two blocks. For Kinetics, two additional blocks with channels of 512 are appended, also the local kernels of the first two blocks are changed into 5. The numbers of new coordinate systems K and new joints N in the transform network are set as 10 and 64 respectively for both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Results</head><p>On NTU RGB+D, we compare with three LSTM-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, two CNN-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>, one graph convolutional method <ref type="bibr" target="#b5">[6]</ref>, and one graph and LSTM hybridized method <ref type="bibr" target="#b22">[23]</ref>. As the local components of our SLnL are CNN-based while the nonlocal components learn the affinity degree between each target position (node) to every position (node) in the figure (graph), our SLnL-rFA can be treated as a variant of CNN and graph hybridized method. As shown in <ref type="table" target="#tab_1">Table 1</ref>, the CNN-based methods are generally better than LSTM-based methods, and graph-based or graphhybridized methods also perform well. Our method consistently outperforms the state-of-the-art approaches by a large margin for both cross-subject (CS) and cross-view (CV) evaluation. Specifically, our SLnL-rFA outperforms the best CNN-based method (HCN) by 2.6% (CS) and 3.8% (CV), also outperforms the recent LSTM and graph hybridized method (SR-TSL) by 4.3% (CS) and 2.5% (CV).</p><p>On Kinetics, we compare with four characteristic methods, including hand-crafted features <ref type="bibr" target="#b23">[24]</ref>, deep LSTM network <ref type="bibr" target="#b1">[2]</ref>, temporal convolutional network <ref type="bibr" target="#b24">[25]</ref>, and graph convolutional network <ref type="bibr" target="#b5">[6]</ref>. <ref type="table">Table 2</ref> shows the deep models outperform the hand-crafted features, and the CNN-based methods work better than the LSTMbased methods. Our method outperforms the state-of-the-art approach (ST-GCN) by large margins of 5.9% (top1) and 6.3% (top5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Study</head><p>To analyze the effectiveness of every proposed component, extensive ablation studies are conducted on NTU RGB+D. Comparisons on loss function. The baseline model (Baseline1) of this section only contains local blocks in <ref type="figure" target="#fig_3">Fig.3(b)</ref> and the transform network. The model is optimized with the cross entropy loss (CE), focal loss (FL), soft-margin cross entropy loss (SMCE), and softmargin focal loss (SMFL), respectively. To save space, at most two best parameters for each loss are listed in <ref type="table" target="#tab_2">Table 3</ref>. Due to the adaptive data selection, FL performs better than CE. Benefiting from the encouraged margins between the positive and negative samples, the SMCE and SMFL perform better than their original versions CE and FL, respectively. Finally, our SMFL achieves the best for its advantages from adaptive data selection and intrinsic margin encouraging. How to select discriminative frequency patterns? We firstly reform the Baseline1 into Baseline2 (No FA) for this section by adding the SMFL. To validate the effectiveness of proposed rFA, we compare it with several variants. The Amplitude frequency attention (aFA) is built on frequency spectrum instead of sinusoidal and cosine components. Shared FA (sFA) learns shared parameters for sinusoidal and cosine components, while dependent FA (dFA) learns two set of parameters independently. The rfA is formed by applying the residual learning trick to dFA in the spatio-temporal domain <ref type="figure" target="#fig_1">(Fig.2)</ref>. In <ref type="table">Table 4</ref>, we observe that aFA is harmful because the phase angle information is missing when only using the frequency spectrum. The dFA outperforms the sFA because that it has more parameters to model the frequency patterns. The rFA finally achieves the best that outperforms Baseline2 with a large margin, indicating that the frequency information is effective for action recognition. Comparisons of methods with different affinity fields. We further reform the Baseline2 into Baseline3 with a rFA block for this section. Although non-local dependencies can be captured in higher layers of hierarchical local networks, we argue that synchronously explore and fuse non-local information in early stages is preferable. We merge one temporal non-local block (tSLnL), spatial non-local block (sSLnL), or spatial-temporal block (SLnL) into Baseline3 to examine their effectiveness. As shown in <ref type="table">Table 5</ref>, both the non-local information from the temporal and spatial dimensions during early stages are helpful. In addition, benefiting from the synchronous fusion of local details and non-local semantics, our SLnL boosts up the recognition performance by 1.4% (CS) and 1.1% (CV). To further investigate the properties of deeper SLnL, we replace M1 local blocks in Baseline3 with SLnL. <ref type="table">Table 5</ref> shows more SLnL blocks in lower layers generally lead to better results, but the improvements of higher layers is relatively small because the affinity field of local operations is increasing with layers. The results clearly show that synchronously extracting local details and non-local semantics is vital for modeling the spatio-temporal dynamics of human actions. <ref type="table">Table 5</ref>. Comparisons of methods with various affinity fields. M1 and M2 denotes the number of SLnL and local blocks in <ref type="figure">Fig.1</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this work, we propose a novel model SLnL-rFA to extract synchronous detailed and semantic information from multi-domains for skeleton-based action recognition. The SLnL synchronously extracts local details and non-local semantics in the spatio-temporal domain. The rFA adaptively selects discriminative frequency patterns, which sheds a new light to exploit information in the frequency domain for skeleton-based action recognition. In addition, we also propose a novel soft-margin focal loss, which can encourage intrinsic margins in classifiers and conducts adaptive data selection. Our approach significantly outperforms other state-of-the-art methods both on the largest in-door dataset NTU RGB+D and on the largest unconstrained dataset Kinetics for skeleton-based action recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The residual frequency attention. The spatio-temparal domain and frequency domain are switched conveniently through 2D-FFT and 2D-IFFT. The attention for the sinusoidal and cosine components (Fsin, Fcos) are conducted in the frequency domain, and the residual component is applied in the spatio-temporal domain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) 2D Non-local module (b) Baseline local block (c) SLnL block (d) The affinity field of SLnL</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>(a) A 2D example of non-local module. (b) The structure of the baseline local block. (c) The structure of the proposed synchronous local and non-local (SLnL) block. (d) The affinity field of SLnL. Note that the affinity field is a more general concept than the receptive field of CNNs. The red and blue represent local and non-local modules repectively in (d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Comparisons among soft-margin focal loss (SMFL), the softmargin cross entropy (SMCE) loss, the cross-entropy (CE) loss, the focal loss (FL), and the soft-margin loss (SM). The focusing parameter γ and the margin parameter m of losses are expressed as (γ, m).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>SM F L (pt) = L SM + L F L (16) = log (e m + (1 − e m )pt) − (1 − pt) γ log(pt).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>arXiv:1811.04237v3 [cs.CV] 12 Jun 2019</figDesc><table><row><cell></cell><cell></cell><cell>Position network</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Multi-task learning network</cell></row><row><cell>difference</cell><cell>Coordinate transformer Transform network Skeleton transformer Coordinate transformer Skeleton transformer Transform network</cell><cell>Residual frequency attention (rFA) Residual frequency attention (rFA) Velocity Network</cell><cell>Synchronous local and non-local blocks (SLnL) Synchronous local and non-local blocks (SLnL)</cell><cell>Local blocks # Local blocks #</cell><cell>global pooling global pooling</cell><cell>concatenate</cell><cell>FC layer FC layer FC layer</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Comparisons of recognition accuracy (%) on NTU.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Table 2. Comparing with the</cell></row><row><cell></cell><cell></cell><cell cols="2">state-of-the-art approaches in ac-</cell></row><row><cell>Methods</cell><cell>CS CV</cell><cell cols="2">tion recognition accuracy (%) on</cell></row><row><cell>PA-LSTM [2]</cell><cell>70.3 62.9</cell><cell cols="2">Kinetics dataset. Both of the top1</cell></row><row><cell cols="2">ST-LSTM+TG [3] 69.2 77.7</cell><cell cols="2">and top5 accuracies are reported.</cell></row><row><cell>VA-LSTM [1]</cell><cell>79.4 87.6</cell><cell>Methods</cell><cell>top1 top5</cell></row><row><cell>ST-GCN [6]</cell><cell>81.5 88.3</cell><cell cols="2">Feature Enc. [24] 14.9 25.8</cell></row><row><cell>TS-CNN [21]</cell><cell>83.2 89.3</cell><cell>Deep LSTM [2]</cell><cell>16.4 35.3</cell></row><row><cell>HCN [5]</cell><cell>86.5 91.1</cell><cell>Tem. Conv. [25]</cell><cell>20.3 40.0</cell></row><row><cell>SR-TSL [23]</cell><cell>84.8 92.4</cell><cell>ST-GCN [6]</cell><cell>30.7 52.8</cell></row><row><cell>SLnL-rFA (ours)</cell><cell>89.1 94.9</cell><cell>SLnL-rFA (ours)</cell><cell>36.6 59.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Results of different loss functions in accuracy (%).</figDesc><table><row><cell></cell><cell></cell><cell>Table 4.</cell><cell cols="2">Performance compar-</cell></row><row><cell></cell><cell></cell><cell cols="3">isons of different frequency atten-</cell></row><row><cell>Loss types</cell><cell>CS CV</cell><cell cols="3">tion methods in human action recog-</cell></row><row><cell cols="2">CE (Baseline1) 85.5 91.3</cell><cell cols="2">nition accuracy (%).</cell></row><row><cell>FL(2,)</cell><cell>85.8 91.9</cell><cell cols="2">Attention methods</cell><cell>CS CV</cell></row><row><cell>FL(3,)</cell><cell>85.6 91.8</cell><cell cols="3">No FA (Baseline2) 86.9 92.6</cell></row><row><cell>SMCE(,0.4)</cell><cell>86.4 92.0</cell><cell cols="2">Amplitude FA</cell><cell>84.7 89.8</cell></row><row><cell>SMCE(,0.6)</cell><cell>86.2 92.3</cell><cell>Shared FA</cell><cell></cell><cell>87.3 92.9</cell></row><row><cell>SMFL(2,0.4)</cell><cell>86.9 92.5</cell><cell cols="2">Dependent FA</cell><cell>87.5 93.2</cell></row><row><cell>SMFL(2,0.6)</cell><cell>86.5 92.6</cell><cell cols="3">Residual FA (rFA) 87.7 93.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, respectively.</figDesc><table><row><cell>Affinity Field</cell><cell>CS (%)</cell><cell>CV (%)</cell></row><row><cell>Local (Baseline3)</cell><cell>87.7</cell><cell>93.6</cell></row><row><cell>tSLnL (M1 = 1, M2 = 5)</cell><cell>88.1</cell><cell>93.9</cell></row><row><cell>sSLnL (M1 = 1, M2 = 5)</cell><cell>88.0</cell><cell>94.1</cell></row><row><cell>SLnL (M1 = 1, M2 = 5)</cell><cell>88.3</cell><cell>94.3</cell></row><row><cell>SLnL (M1 = 2, M2 = 4)</cell><cell>88.6</cell><cell>94.6</cell></row><row><cell>SLnL (M1 = 3, M2 = 3)</cell><cell>88.8</cell><cell>94.9</cell></row><row><cell>SLnL (M1 = 4, M2 = 2)</cell><cell>88.9</cell><cell>94.8</cell></row><row><cell>SLnL (M1 = 5, M2 = 1)</cell><cell>89.1</cell><cell>94.7</cell></row><row><cell>SLnL (M1 = 6, M2 = 0)</cell><cell>88.8</cell><cell>94.7</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">View adaptive recurrent neural networks for high performance human action recognition from skeleton data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuiling</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junliang</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianru</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2136" to="2145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NTU RGB+D: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatiotemporal LSTM with trust gates for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3d action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuhong</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4570" to="4579" />
		</imprint>
	</monogr>
	<note>Senjian An, Ferdous Ahmed Sohel, and Farid Boussaïd</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cooccurrence feature learning from skeleton data for action recognition and detection with hierarchical aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="786" to="792" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Action recognition in videos using frequency analysis of critical point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrille</forename><surname>Beaudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Péteri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Mascarilla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICIP</title>
		<imprint>
			<biblScope unit="page" from="1445" to="1449" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Frequency divergence image: A novel method for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><forename type="middle">C</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Street</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Symposium on Biomedical Imaging</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-d transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><forename type="middle">O</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Superresolution from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Glasner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Bagon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="349" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Patchmatch: a randomized correspondence algorithm for structural image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connelly</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Attention is all you need,&quot; in NIPS</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Non-local recurrent network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bihan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02919</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yandong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="507" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ensemble soft-margin softmax loss for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shifeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in IJCAI</title>
		<imprint>
			<biblScope unit="page" from="992" to="998" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Skeletonbased action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaoyong</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Pu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="597" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with spatial reasoning and temporal stack learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Amir Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Interpretable 3d human action analysis with temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Tae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1623" to="1631" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
