<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DFNets: Spectral CNNs for Graphs with Feedback-Looped Filters</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asiri</forename><surname>Wijesinghe</surname></persName>
							<email>asiri.wijesinghe@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Wang</surname></persName>
							<email>qing.wang@anu.edu.au</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DFNets: Spectral CNNs for Graphs with Feedback-Looped Filters</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:04+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel spectral convolutional neural network (CNN) model on graph structured data, namely Distributed Feedback-Looped Networks (DFNets). This model is incorporated with a robust class of spectral graph filters, called feedbacklooped filters, to provide better localization on vertices, while still attaining fast convergence and linear memory requirements. Theoretically, feedback-looped filters can guarantee convergence w.r.t. a specified error bound, and be applied universally to any graph without knowing its structure. Furthermore, the propagation rule of this model can diversify features from the preceding layers to produce strong gradient flows. We have evaluated our model using two benchmark tasks: semi-supervised document classification on citation networks and semi-supervised entity classification on a knowledge graph. The experimental results show that our model considerably outperforms the state-of-the-art methods in both benchmark tasks over all datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Convolutional neural networks (CNNs) <ref type="bibr" target="#b19">[20]</ref> are a powerful deep learning approach which has been widely applied in various fields, e.g., object recognition <ref type="bibr" target="#b28">[29]</ref>, image classification <ref type="bibr" target="#b13">[14]</ref>, and semantic segmentation <ref type="bibr" target="#b21">[22]</ref>. Traditionally, CNNs only deal with data that has a regular Euclidean structure, such as images, videos and text. In recent years, due to the rising trends in network analysis and prediction, generalizing CNNs to graphs has attracted considerable interest <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26]</ref>. However, since graphs are in irregular non-Euclidean domains, this brings up the challenge of how to enhance CNNs for effectively extracting useful features (e.g. topological structure) from arbitrary graphs.</p><p>To address this challenge, a number of studies have been devoted to enhancing CNNs by developing filters over graphs. In general, there are two categories of graph filters: (a) spatial graph filters, and (b) spectral graph filters. Spatial graph filters are defined as convolutions directly on graphs, which consider neighbors that are spatially close to a current vertex <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref>. In contrast, spectral graph filters are convolutions indirectly defined on graphs, through their spectral representations <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7]</ref>. In this paper, we follow the line of previous studies in developing spectral graph filters and tackle the problem of designing an effective, yet efficient CNNs with spectral graph filters.</p><p>Previously, Bruna et al. <ref type="bibr" target="#b2">[3]</ref> proposed convolution operations on graphs via a spectral decomposition of the graph Laplacian. To reduce learning complexity in the setting where the graph structure is not known a priori, Henaff et al. <ref type="bibr" target="#b12">[13]</ref> developed a spectral filter with smooth coefficients. Then, Defferrard et al. <ref type="bibr" target="#b6">[7]</ref> introduced Chebyshev filters to stabilize convolution operations under coefficient perturbation and these filters can be exactly localized in k-hop neighborhood. Later, Kipf et al. <ref type="bibr" target="#b18">[19]</ref> proposed a simple layer-wise propagation model using Chebyshev filters on 1-hop neighborhood. Very recently, some works attempted to develop rational polynomial filters, such as Cayley filters <ref type="bibr" target="#b20">[21]</ref> 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. arXiv:1910.10866v5 <ref type="bibr">[cs.</ref>LG] 17 Jan 2020 <ref type="figure">Figure 1</ref>: A simplified example of illustrating feedback-looped filters, where v 1 is the current vertex and the similarity of the colours indicates the correlation between vertices, e.g., v 1 and v 5 are highly correlated, but v 2 and v 6 are less correlated with v 1 : (a) an input graph, where λ i is the original frequency to vertex v i ; (b) the feedforward filtering, which attenuates some low order frequencies, e.g. λ 2 , and amplify other frequencies, e.g. λ 5 and λ 6 ; (c) the feedback filtering, which reduces the error in the frequencies generated by (b), e.g. λ 6 . and ARMA 1 <ref type="bibr" target="#b1">[2]</ref>. From a different perspective, Petar et al. <ref type="bibr" target="#b30">[31]</ref> proposed a self-attention based CNN architecture for graph filters, which extracts features by considering the importance of neighbors.</p><formula xml:id="formula_0">λ 2 (b) Feedforward (c) Feedback v 1 v 3 v 2 v 6 v 4 v 5 λ 5 λ 6 0 1 1 0 (a) Input v 1 v 3 v 2 v 6 v 4 v 5 v 1 v 3 v 2 v 6 v 4 v 5 (q=1) (p=2) 0 1 λ 2 λ 5 λ 6 λ 2 λ 5 λ 6</formula><p>One key idea behind existing works on designing spectral graph filters is to approximate the frequency responses of graph filters using a polynomial function (e.g. Chebyshev filters <ref type="bibr" target="#b6">[7]</ref>) or a rational polynomial function (e.g. Cayley filters <ref type="bibr" target="#b20">[21]</ref> and ARMA 1 <ref type="bibr" target="#b1">[2]</ref>). Polynomial filters are sensitive to changes in the underlying graph structure. They are also very smooth and can hardly model sharp changes, as illustrated in <ref type="figure">Figure 1</ref>. Rational polynomial filters are more powerful to model localization, but they often have to trade off computational efficiency, resulting in higher learning and computational complexities, as well as instability.</p><p>Contributions. In this work, we aim to develop a new class of spectral graph filters that can overcome the above limitations. We also propose a spectral CNN architecture (i.e. DFNet) to incorporate these graph filters. In summary, our contributions are as follows:</p><p>• Improved localization. A new class of spectral graph filters, called feedback-looped filters, is proposed to enable better localization, due to its rational polynomial form. Basically, feedback-looped filters consist of two parts: feedforward and feedback. The feedforward filtering is k-localized as polynomial filters, while the feedback filtering is unique which refines k-localized features captured by the feedforward filtering to improve approximation accuracy. We also propose two techniques: scaled-normalization and cut-off frequency to avoid the issues of gradient vanishing/exploding and instabilities. • Efficient computation. For feedback-looped filters, we avoid the matrix inversion implied by the denominator through approximating the matrix inversion with a recursion. Thus, benefited from this approximation, feedback-looped filters attain linear convergence time and linear memory requirements w.r.t. the number of edges in a graph. • Theoretical properties. Feedback-looped filters enjoy several nice theoretical properties.</p><p>Unlike other rational polynomial filters for graphs, they have theoretically guaranteed convergence w.r.t. a specified error bound. On the other hand, they still have the universal property as other spectral graph filters <ref type="bibr" target="#b16">[17]</ref>, i.e., can be applied without knowing the underlying structure of a graph. The optimal coefficients of feedback-looped filters are learnable via an optimization condition for any given graph. • Dense architecture. We propose a layer-wise propagation rule for our spectral CNN model with feedback-looped filters, which densely connects layers as in DenseNet <ref type="bibr" target="#b14">[15]</ref>. This design enables our model to diversify features from all preceding layers, leading to a strong gradient flow. We also introduce a layer-wise regularization term to alleviate the overfitting issue. In doing so, we can prevent the generation of spurious features and thus improve accuracy of the prediction.</p><p>To empirically verify the effectiveness of our work, we have evaluated feedback-looped filters within three different CNN architectures over four benchmark datasets to compare against the state-of-the-art methods. The experimental results show that our models significantly outperform the state-of-the-art methods. We further demonstrate the effectiveness of our model DFNet through the node embeddings in a 2-D space of vertices from two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Spectral Convolution on Graphs</head><p>Let G = (V, E, A) be an undirected and weighted graph, where V is a set of vertices, E ⊆ V × V is a set of edges, and A ∈ R n×n is an adjacency matrix which encodes the weights of edges. We let n = |V | and m = |E|. A graph signal is a function x : V → R and can be represented as a vector x ∈ R n whose i th component x i is the value of x at the i th vertex in V . The graph Laplacian is defined as L = I − D −1/2 AD −1/2 , where D ∈ R n×n is a diagonal matrix with D ii = j A ij and I is an identity matrix. L has a set of orthogonal eigenvectors {u i } n−1 i=0 ∈ R n , known as the graph Fourier basis, and non-negative eigenvalues {λ i } n−1 i=0 , known as the graph frequencies <ref type="bibr" target="#b4">[5]</ref>. L is diagonalizable by the eigendecomposition such that L = U ΛU H , where Λ = diag ([λ 0 , . . . , λ n−1 ]) ∈ R n×n and U H is a hermitian transpose of U . We use λ min and λ max to denote the smallest and largest eigenvalues of L, respectively.</p><p>Given a graph signal x, the graph Fourier transform of x isx = U H x ∈ R n and its inverse is x = Ux <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30]</ref>. The graph Fourier transform enables us to apply graph filters in the vertex domain. A graph filter h can filter x by altering (amplifying or attenuating) the graph frequencies as</p><formula xml:id="formula_1">h(L)x = h(U ΛU H )x = U h(Λ)U H x = U h(Λ)x.</formula><p>(1)</p><formula xml:id="formula_2">Here, h(Λ) = diag([h(λ 0 ), . . . , h(λ n−1 )])</formula><p>, which controls how the frequency of each component in a graph signal x is modified. However, applying graph filtering as in Eq. 1 requires the eigendecomposition of L, which is computationally expensive. To address this issue, several works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref> have studied the approximation of h(Λ) by a polynomial or rational polynomial function.</p><p>Chebyshev filters. Hammond et al. <ref type="bibr" target="#b11">[12]</ref> first proposed to approximate h(λ) by a polynomial function with k th -order polynomials and Chebyshev coefficients. Later, Defferrard et al. <ref type="bibr" target="#b6">[7]</ref> developed Chebyshev filters for spectral CNNs on graphs. A Chebyshev filter is defined as</p><formula xml:id="formula_3">h θ (λ) = k−1 j=0 θ j T j (λ),<label>(2)</label></formula><p>where θ ∈ R k is a vector of learnable Chebyshev coefficients,λ ∈ [−1, 1] is rescaled from λ, the Chebyshev polynomials T j (λ) = 2λT j−1 (λ) − T j−2 (λ) are recursively defined with T 0 (λ) = 1 and T 1 (λ) = λ, and k controls the size of filters, i.e., localized in k-hop neighborhood of a vertex <ref type="bibr" target="#b11">[12]</ref>. Kipf and Welling <ref type="bibr" target="#b18">[19]</ref> simplified Chebyshev filters by restricting to 1-hop neighborhood.</p><p>Lanczos filters. Recently, Liao et al. <ref type="bibr" target="#b22">[23]</ref> used the Lanczos algorithm to generate a low-rank matrix approximation T for the graph Laplacian. They used the affinity matrix S = D −1/2 AD −1/2 . Since L = I − S holds, L and S share the same eigenvectors but have different eigenvalues. As a result, L and S correspond to the samex. To approximate the eigenvectors and eigenvalues of S, they diagonalize the tri-diagonal matrix T ∈ R m×m to compute Ritz-vectors V ∈ R n×m and Ritz-values R ∈ R m×m , and thus S ≈ V RV T . Accordingly, a k-hop Lanczos filter operation is,</p><formula xml:id="formula_4">h θ (R) = k−1 j=0 θ j R j ,<label>(3)</label></formula><p>where θ ∈ R k is a vector of learnable Lanczos filter coefficients. Thus, spectral convolutional operation is defined as</p><formula xml:id="formula_5">h θ (S)x ≈ V h θ (R)V T x.</formula><p>Such Lanczos filter operations can significantly reduce computation overhead when approximating large powers of S, i.e. S k ≈ V R k V T . Thus, they can efficiently compute the spectral graph convolution with a very large localization range to easily capture the multi-scale information of the graph.</p><p>Cayley filters. Observing that Chebyshev filters have difficulty in detecting narrow frequency bands due toλ ∈ [−1, 1], Levie et al. <ref type="bibr" target="#b20">[21]</ref> proposed Cayley filters, based on Cayley polynomials:</p><formula xml:id="formula_6">h θ,s (λ) = θ 0 + 2Re( k−1 j=1 θ j (sλ − i) j (sλ + i) −j ),<label>(4)</label></formula><p>where θ 0 ∈ R is a real coefficient and (θ 1 , . . . , θ k−1 ) ∈ C k−1 is a vector of complex coefficients.</p><p>Re(x) denotes the real part of a complex number x, and s &gt; 0 is a parameter called spectral zoom, which controls the degree of "zooming" into eigenvalues in Λ. Both θ and s are learnable during training. To improve efficiency, the Jacobi method is used to approximately compute Cayley polynomials.</p><p>ARMA 1 filters. Bianchi et al. <ref type="bibr" target="#b1">[2]</ref> sought to address similar issues as identified in <ref type="bibr" target="#b20">[21]</ref>. However, different from Cayley filters, they developed a first-order ARMA filter, which is approximated by a first-order recursion:</p><formula xml:id="formula_7">x (t+1) = aLx (t) + bx,<label>(5)</label></formula><p>where a and b are the filter coefficients,x (0) = x, andL = (λ max − λ min )/2I − L. Accordingly, the frequency response is defined as:</p><formula xml:id="formula_8">h(λ) = r λ − p ,<label>(6)</label></formula><p>whereλ = (λ max − λ min )/2λ, r = −b/a, and p = 1/a <ref type="bibr" target="#b16">[17]</ref>. Multiple ARMA 1 filters can be applied in parallel to obtain a ARMA k filter. However, the memory complexity of k parallel ARMA 1 filters is k times higher than ARMA 1 graph filters.</p><p>We make some remarks on how these existing spectral filters are related to each other. (i) As discussed in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23]</ref>, polynomial filters (e.g. Chebyshev and Lanczos filters) can be approximately treated as a special kind of rational polynomial filters. (ii) Further, Chebyshev filters can be regarded as a special case of Lanczos filters. (iii) Although both Cayley and ARMA k filters are rational polynomial filters, they differ in how they approximate the matrix inverse implied by the denominator of a rational function. Cayley filters use a fixed number of Jacobi iterations, while ARMA k filters use a first-order recursion plus a parallel bank of k ARMA 1 . (iv) ARMA 1 by Bianchi et al. <ref type="bibr" target="#b1">[2]</ref> is similar to GCN by Kipf et al. <ref type="bibr" target="#b18">[19]</ref> because they both consider localization within 1-hop neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>We introduce a new class of spectral graph filters, called feedback-looped filters, and propose a spectral CNN for graphs with feedback-looped filters, namely Distributed Feedback-Looped Networks (DFNets). We also discuss optimization techniques and analyze theoretical properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feedback-Looped Filters</head><p>Feedback-looped filters belong to a class of Auto Regressive Moving Average (ARMA) filters <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Formally, an ARMA filter is defined as:</p><formula xml:id="formula_9">h ψ,φ (L)x = I + p j=1 ψ j L j −1 q j=0 φ j L j x.<label>(7)</label></formula><p>The parameters p and q refer to the feedback and feedforward degrees, respectively. ψ ∈ C p and φ ∈ C q+1 are two vectors of complex coefficients. Computing the denominator of Eq. 7 however requires a matrix inversion, which is computationally inefficient for large graphs. To circumvent this issue, feedback-looped filters use the following approximation:</p><formula xml:id="formula_10">x (0) = x andx (t) = − p j=1 ψ jL jx(t−1) + q j=0 φ jL j x,<label>(8)</label></formula><p>whereL =L − (λ max 2 )I,L = I −D −1/2ÂD−1/2 ,Â = A + I,D ii = jÂ ij andλ max is the largest eigenvalue ofL. Accordingly, the frequency response of feedback-looped filters is defined as:</p><formula xml:id="formula_11">h(λ i ) = q j=0 φ j λ j i 1 + p j=1 ψ j λ j i .<label>(9)</label></formula><p>To alleviate the issues of gradient vanishing/exploding and numerical instabilities, we further introduce two techniques in the design of feedback-looped filters: scaled-normalization and cut-off frequency.</p><p>Scaled-normalization technique. To assure the stability of feedback-looped filters, we apply the scaled-normalization technique to increasing the stability region, i.e., using the scaled-normalized LaplacianL =L − (λ max 2 )I, rather than justL. This accordingly helps centralize the eigenvalues of the LaplacianL and reduce its spectral radius bound. The scaled-normalized LaplacianL consists of graph frequencies within [0, 2], in which eigenvalues are ordered in an increasing order.</p><p>Cut-off frequency technique. To map graph frequencies within [0, 2] to a uniform discrete distribution, we define a cut-off frequency λ cut = ( λmax such thatλ i = 1 if λ i ≥ λ cut andλ i = 0 otherwise. This trick allows the generation of ideal high-pass filters so as to sharpen a signal by amplifying its graph Fourier coefficients. This technique also solves the issue of narrow frequency bands existing in previous spectral filters, including both polynomial and rational polynomial filters <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref>. This is because these previous spectral filters only accept a small band of frequencies. In contrast, our proposed feedback-looped filters resolve this issue using a cut-off frequency technique, i.e., amplifying frequencies higher than a certain low cut-off value while attenuating frequencies lower than that cut-off value. Thus, our proposed filters can accept a wider range of frequencies and capture better characteristic properties of a graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Coefficient Optimisation</head><p>Given a feedback-looped filter with a desired frequency response:ĥ : {λ i } n−1 i=0 → R, we aim to find the optimal coefficients ψ and φ that make the frequency response as close as possible to the desired frequency response, i.e. to minimize the following error:</p><formula xml:id="formula_12">e(λ i ) =ĥ(λ i ) − q j=0 φ jλ j i 1 + p j=1 ψ jλ j i<label>(10)</label></formula><p>However, the above equation is not linear w.r.t. the coefficients ψ and φ. Thus, we redefine the error as follows:</p><formula xml:id="formula_13">e(λ i ) =ĥ(λ i ) +ĥ(λ i ) p j=1 ψ jλ j i − q j=0 φ jλ j i .<label>(11)</label></formula><p>Let e = [e(λ 0 ), . . . , e(λ n−1 )] T ,ĥ = [ĥ(λ 0 ), . . . ,ĥ(λ n−1 )] T , α ∈ R n×p with α ij =λ j i and β ∈ R n×(q+1) with β ij =λ j−1 i are two Vandermonde-like matrices. Then, we have e =ĥ + diag(ĥ)αψ − βφ. Thus, the stable coefficients ψ and φ can be learned by minimizing e as a convex constrained least-squares optimization problem:</p><formula xml:id="formula_14">minimize ψ,φ ||ĥ + diag(ĥ)αψ − βφ|| 2<label>(12)</label></formula><p>subject to ||αψ|| ∞ ≤ γ and γ &lt; 1</p><p>Here, the parameter γ controls the tradeoff between convergence efficiency and approximation accuracy. A higher value of γ can lead to slower convergence but better accuracy. It is not recommended to have very low γ values due to potentially unacceptable accuracy. ||αψ|| ∞ ≤ γ &lt; 1 is the stability condition, which will be further discussed in detail in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Spectral Convolutional Layer</head><p>We propose a CNN-based architecture, called DFNets, which can stack multiple spectral convolutional layers with feedback-looped filters to extract features of increasing abstraction. Let P = − p j=1 ψ jL j and Q = q j=0 φ jL j . The propagation rule of a spectral convolutional layer is defined as:X</p><formula xml:id="formula_15">(t+1) = σ(PX (t) θ (t) 1 + QXθ (t) 2 + µ(θ (t) 1 ; θ (t) 2 ) + b),<label>(13)</label></formula><p>where σ refers to a non-linear activation function such as ReLU .X (0) = X ∈ R n×f is a graph signal matrix where f refers to the number of features.X (t) is a matrix of activations in the t th layer. θ (t) 1 ∈ R c×h and θ (t)</p><p>2 ∈ R f ×h are two trainable weight matrices in the t th layer. To computeX (t+1) , a vertex needs access to its p-hop neighbors with the output signal of the previous layerX (t) , and its q-hop neighbors with the input signal from X. To attenuate the overfitting issue, we add µ(θ (t) 1 ; θ (t) 2 ), namely kernel regularization <ref type="bibr" target="#b5">[6]</ref>, and a bias term b. We use the xavier normal initialization method <ref type="bibr" target="#b9">[10]</ref> to initialise the kernel and bias weights, the unit-norm constraint technique <ref type="bibr" target="#b7">[8]</ref> to normalise the kernel and bias weights by restricting parameters of all layers in a small range, and the kernel regularization technique to penalize the parameters in each layer during the training. In doing so, we can prevent the generation of spurious features and thus improve the accuracy of prediction 1 .</p><p>In this model, each layer is directly connected to all subsequent layers in a feed-forward manner, as in DenseNet <ref type="bibr" target="#b14">[15]</ref>. Consequently, the t th layer receives all preceding feature maps F 0 , . . . , F t−1 as input. We concatenate multiple preceding feature maps column-wise into a single tensor to obtain more diversified features for boosting the accuracy. This densely connected CNN architecture has several compelling benefits: (a) reduce the vanishing-gradient issue, (b) increase feature propagation and reuse, and (c) refine information flow between layers <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Theoretical Analysis</head><p>Feedback-looped filters have several nice properties, e.g., guaranteed convergence, linear convergence time, and universal design. We discuss these properties and analyze computational complexities.</p><p>Convergence. Theoretically, a feedback-looped filter can achieve a desired frequency response only when t → ∞ <ref type="bibr" target="#b16">[17]</ref>. However, due to the property of linear convergence preserved by feedback-looped filters, stability can be guaranteed after a number of iterations w.r.t. a specified small error <ref type="bibr" target="#b15">[16]</ref>. More specifically, since the pole of rational polynomial filters should be in the unit circle of the z-plane to guarantee the stability, we can derive the stability condition || − p j=1 ψ j L j || &lt; 1 by Eq. 7 in the vertex domain and correspondingly obtain the stability condition ||αψ|| ∞ ≤ γ ∈ (0, 1) in the frequency domain as stipulated in Eq. 12 <ref type="bibr" target="#b15">[16]</ref>.</p><p>Universal design. The universal design is beneficial when the underlying structure of a graph is unknown or the topology of a graph changes over time. The corresponding filter coefficients can be learned independently of the underlying graph and are universally applicable. When designing feedback-looped filters, we define the desired frequency response functionĥ over graph frequencies λ i in a binary format in the uniform discrete distribution as discussed in Section 3.1. Then, we solve Eq. 12 in the least-squares sense for this finite set of graph frequencies to find optimal filter coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spectral Graph Filter Type Learning Time Memory Complexity</head><p>Complexity Complexity Chebyshev filters <ref type="bibr" target="#b6">[7]</ref> Polynomial O(k) O(km) O(m) Lanczos filters <ref type="bibr" target="#b22">[23]</ref> O(k) O(km 2 ) O(m 2 ) Cayley filters <ref type="bibr" target="#b20">[21]</ref> Rational polynomial Complexity. When computingx (t) as in Eq. 8, we need to calculateL jx(t−1) for j = 1, . . . , p and L j x for j = 1, . . . , q. Nevertheless,L j x is computed only once becauseL j x =L(L j−1 x). Thus, we need p multiplications for each t in the first term in Eq. 8, and q multiplications for the second term in Eq. 8. <ref type="table" target="#tab_0">Table 1</ref> summarizes the complexity results of existing spectral graph filters and ours, where r refers to the number of Jacobi iterations in <ref type="bibr" target="#b20">[21]</ref>. Note that, when t = 1 (i.e., one spectral convolutional layer), feedback-looped filters have the same learning, time and memory complexities as Chebyshev filters, where p + q = k.</p><formula xml:id="formula_16">O((r + 1)k) O((r + 1)km) O(m) ARMA 1 filters [2] O(t) O(tm) O(m) d parallel ARMA 1 filters [2] O(t) O(tm) O(dm) Feedback-looped filters (ours) O(tp + q) O((tp + q)m) O(m)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Numerical Experiments</head><p>We evaluate our models on two benchmark tasks: (1) semi-supervised document classification in citation networks, and (2) semi-supervised entity classification in a knowledge graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Set-Up</head><p>Datasets. We use three citation network datasets Cora, Citeseer, and Pubmed <ref type="bibr" target="#b27">[28]</ref> for semi-supervised document classification, and one dataset NELL <ref type="bibr" target="#b3">[4]</ref> for semi-supervised entity classification. NELL is a bipartite graph extracted from a knowledge graph <ref type="bibr" target="#b3">[4]</ref>. <ref type="table">Table 2</ref>  Baseline methods. We compare against twelve baseline methods, including five methods using spatial graph filters, i.e., Semi-supervised Embedding (SemiEmb) <ref type="bibr" target="#b31">[32]</ref>, Label Propagation (LP) <ref type="bibr" target="#b33">[34]</ref>, skip-gram graph embedding model (DeepWalk) <ref type="bibr" target="#b25">[26]</ref>, Iterative Classification Algorithm (ICA) <ref type="bibr" target="#b23">[24]</ref>, and semi-supervised learning with graph embedding (Planetoid*) <ref type="bibr" target="#b32">[33]</ref>, and seven methods using spectral graph filters: Chebyshev <ref type="bibr" target="#b6">[7]</ref>, Graph Convolutional Networks (GCN) <ref type="bibr" target="#b18">[19]</ref>, Lanczos Networks (LNet) and Adaptive Lanczos Networks (AdaLNet) <ref type="bibr" target="#b22">[23]</ref>, CayleyNet <ref type="bibr" target="#b20">[21]</ref>, Graph Attention Networks (GAT) <ref type="bibr" target="#b30">[31]</ref>, and ARMA Convolutional Networks (ARMA 1 ) <ref type="bibr" target="#b1">[2]</ref>.</p><p>We evaluate our feedback-looped filters using three different spectral CNN models: (i) DFNet: a densely connected spectral CNN with feedback-looped filters, (ii) DFNet-ATT: a self-attention based densely connected spectral CNN with feedback-looped filters, and (iii) DF-ATT: a self-attention based spectral CNN model with feedback-looped filters.  <ref type="table">Table 3</ref>: Hyperparameter settings for citation network datasets.</p><p>Hyperparameter settings. We use the same data splitting for each dataset as in Yang et al. <ref type="bibr" target="#b32">[33]</ref>. The hyperparameters of our models are initially selected by applying the orthogonalization technique (a randomized search strategy). We also use a layerwise regularization (L2 regularization) and bias terms to attenuate the overfitting issue. All models are trained 200 epochs using the Adam optimizer <ref type="bibr" target="#b17">[18]</ref> with a learning rate of 0.002. <ref type="table">Table 3</ref> summarizes the hyperparameter settings for citation network datasets. The same hyperparameters are applied to the NELL dataset except for L2 regularization (i.e., 9e-2 for DFNet and DFnet-ATT, and 9e-4 for DF-ATT). For γ, we choose the best setting for each model. For self-attention, we use 8 multi-attention heads and 0.5 attention dropout for DFNet-ATT, and 6 multi-attention heads and 0.3 attention dropout for DF-ATT. The parameters p = 5, q = 3 and λ cut = 0.5 are applied to all three models over all datasets. <ref type="table">Table 4</ref> summarizes the results of classification in terms of accuracy. The results of the baseline methods are taken from the previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b32">33]</ref>. Our models DFNet and DFNet-ATT outperform all the baseline methods over four datasets. Particularly, we can see that: (1) Compared with polynomial filters, DFNet improves upon GCN (which performs best among the models using polynomial filters) by a margin of 3.7%, 3.9%, 5.3% and 2.3% on the datasets Cora, Citeseer, Pubmed and NELL, respectively. (2) Compared with rational polynomial filters, DFNet improves upon CayleyNet and ARMA 1 by 3.3% and 1.8% on the Cora dataset, respectively. For the other datasets, CayleyNet does not have results available in <ref type="bibr" target="#b20">[21]</ref>. (3) DFNet-ATT further improves the results of DFNet due to the addition of a self-attention layer. (4) Compared with GAT (Chebyshev filters with self-attention), DF-ATT also improves the results and achieves 0.4%, 0.6% and 3.3% higher accuracy on the datasets Cora, Citeseer and Pubmed, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparison with Baseline Methods</head><p>Additionally, we compare DFNet (our feedback-looped filters + DenseBlock) with GCN + Dense-Block and GAT + DenseBlock. The results are also presented in <ref type="table">Table 4</ref>. We can see that our feedback-looped filters perform best, no matter whether or not the dense architecture is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Cora Citeseer Pubmed NELL SemiEmb <ref type="bibr" target="#b31">[32]</ref> 59.0 59.  <ref type="table">Table 4</ref>: Accuracy (%) averaged over 10 runs (* was obtained using a different data splitting in <ref type="bibr" target="#b20">[21]</ref>) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison under Different Polynomial Orders</head><p>In order to test how the polynomial orders p and q influence the performance of our model DFNet, we conduct experiments to evaluate DFNet on three citation network datasets using different polynomial orders p = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref> and q = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>. <ref type="figure" target="#fig_0">Figure 2</ref> presents the experimental results. In our experiments, p = 5 and q = 3 turn out to be the best parameters for DFNet over these datasets. In other words, this means that feedback-looped filters are more stable on p = 5 and q = 3 than other values of p and q. This is because, when p = 5 and q = 3, Eq. 12 can obtain better convergence for finding optimal coefficients than in the other cases. Furthermore, we observe that: (1) Setting p to be too low or too high can both lead to poor performance, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>.(a), and (2) when q is larger than p, the accuracy decreases rapidly as shown in <ref type="figure" target="#fig_0">Figure 2</ref>.(b). Thus, when choosing p and q, we require that p &gt; q holds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation of Scaled-Normalization and Cut-off Frequency</head><p>To understand how effectively the scaled-normalisation and cut-off frequency techniques can help learn graph representations, we compare our methods that implement these techniques with the variants of our methods that only implement one of these techniques. The results are presented in <ref type="figure" target="#fig_2">Figure 3</ref>. We can see that, the models using these two techniques outperform the models that only use one of these techniques over all citation network datasets. Particularly, the improvement is significant on the Cora and Citeseer datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Node Embeddings</head><p>We analyze the node embeddings by DFNets over two datasets: Cora and Pubmed in a 2-D space. <ref type="figure" target="#fig_3">Figures 4 and 5</ref> display the visualization of the learned 2-D embeddings of GCN, GAT, and DFNet (ours) on Pubmed and Cora citation networks by applying t-SNE <ref type="bibr" target="#b24">[25]</ref> respectively. Colors denote different classes in these datasets. It reveals the clustering quality of theses models. These figures clearly show that our model DFNet has better separated 3 and 7 clusters respectively in the embedding spaces of Pubmed and Cora datasets. This is because features extracted by DFNet yield better node representations than GCN and GAT models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we have introduced a spectral CNN architecture (DFNets) with feedback-looped filters on graphs. To improve approximation accuracy, we have developed two techniques: scaled normalization and cut-off frequency. In addition to these, we have discussed some nice properties of feedback-looped filters, such as guaranteed convergence, linear convergence time, and universal design. Our proposed model outperforms the state-of-the-art approaches significantly in two benchmark tasks. In future, we plan to extend the current work to time-varying graph structures. As discussed in <ref type="bibr" target="#b16">[17]</ref>, feedback-looped graph filters are practically appealing for time-varying settings, and similar to static graphs, some nice properties would likely hold for graphs that are a function of time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendices</head><p>In the following, we provide further experiments on comparing our work with the others.</p><p>Comparison with different spectral graph filters. We have conducted an ablation study of our proposed graph filters. Specifically, we compare our feedback-looped filters, i.e., the newly proposed spectral filters in this paper, against other spectral filters such as Chebyshev filters and Cayley filters. To conduct this ablation study, we remove the dense connections from our model DFNet.</p><p>The experimental results are presented in table 5. It shows that feedback-looped filters improve localization upon Chebyshev filters by a margin of 1.4%, 1.7% and 7.3% on the datasets Cora, Citeseer and Pubmed, respectively. It also improves upon Cayley filters by a margin of 0.7% on the Cora dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Cora Citeseer Pubmed Chebyshev filters <ref type="bibr" target="#b6">[7]</ref> 81.2 69.8 74.4 Cayley filters <ref type="bibr" target="#b20">[21]</ref> 81.9 --Feedback-looped filters (ours) 82.6 ± 0.3 71.5 ± 0.4 81.7 ± 0.6 <ref type="table">Table 5</ref>: Accuracy (%) averaged over 10 runs.</p><p>Comparison with LNet and AdaLNet using different data splittings. We have benchmarked the performance of our DFNet model against the models LNet and AdaLNet proposed in <ref type="bibr" target="#b22">[23]</ref>, as well as Chebyshev, GCN and GAT, over three citation network datasets Cora, Citeseer and Pubmed. We use the same data splittings as used in <ref type="bibr" target="#b22">[23]</ref>. All the experiments are repeated 10 times. For our model DFNet, we use the same hyperparameter settings as discussed in Section 4.  <ref type="table">Table 6</ref> shows that DFNet performs significantly better than all the other models over the Cora dataset, including LNet and AdaLNet proposed in <ref type="bibr" target="#b22">[23]</ref>. Similarly, <ref type="table">Table 7</ref> shows that DFNet performs significantly better than all the other models over the Citeseer dataset. For the Pubmed dataset, as shown in <ref type="table">Table 8</ref>, DFNet performs significantly better than almost all the other models, except for only one case in which DFNet performs slightly worse than AdaLNet using the splitting 0.03%. These results demonstrate the robustness of our model DFNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2 −</head><label>2</label><figDesc>η), where η ∈ [0, 1] and λ max refers to the largest eigenvalue ofL. The cut-off frequency is used as a threshold to control the amount of attenuation on graph frequencies. The eigenvalues {λ i } n−1 i=0 are converted to binary values {λ i } n−1 i=0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Accuracy (%) of DFNet under different polynomial orders p and q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Accuracy (%) of our models in three cases: (1) using both scaled-normalization and cut-off frequency, (2) using only cut-off frequency, and (3) using only scaled-normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The t-SNE visualization of the 2-D node embedding space for the Pubmed dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The t-SNE visualization of the 2-D node embedding space for the Cora dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Learning, time and space complexities of spectral graph filters.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>contains dataset statistics<ref type="bibr" target="#b32">[33]</ref>.</figDesc><table><row><cell cols="2">Dataset Type</cell><cell>#Nodes</cell><cell cols="4">#Edges #Classes #Features %Labeled Nodes</cell></row><row><cell>Cora</cell><cell>Citation network</cell><cell>2,708</cell><cell>5,429</cell><cell>7</cell><cell>1,433</cell><cell>0.052</cell></row><row><cell cols="2">Citeseer Citation network</cell><cell>3,327</cell><cell>4,732</cell><cell>6</cell><cell>3,703</cell><cell>0.036</cell></row><row><cell cols="2">Pubmed Citation network</cell><cell>19,717</cell><cell>44,338</cell><cell>3</cell><cell>500</cell><cell>0.003</cell></row><row><cell>NELL</cell><cell>Knowledge graph</cell><cell cols="2">65,755 266,144</cell><cell>210</cell><cell>5,414</cell><cell>0.001</cell></row><row><cell></cell><cell></cell><cell cols="3">Table 2: Dataset statistics.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 :Table 7 :Table 8 :</head><label>678</label><figDesc>2% (standard) 78.0 ± 1.2 80.5 ± 0.8 82.6 ± 0.7 79.5 ± 1.8 80.4 ± 1.1 85.2 ± 0.5 3% 62.1 ± 6.7 74.0 ± 2.8 56.8 ± 7.9 76.3 ± 2.3 77.7 ± 2.4 80.5 ± 0.4 1% 44.2 ± 5.6 61.0 ± 7.2 48.6 ± 8.0 66.1 ± 8.2 67.5 ± 8.7 69.5 ± 2.3 0.5% 33.9 ± 5.0 52.9 ± 7.4 41.4 ± 6.9 58.1 ± 8.2 60.8 ± 9.0 61.3 ± 4.3 Accuracy (%) averaged over 10 runs on the Cora dataset. 6% (standard) 70.1 ± 0.8 68.1 ± 1.3 72.2 ± 0.9 66.2 ± 1.9 68.7 ± 1.0 74.2 ± 0.3 1% 59.4 ± 5.4 58.3 ± 4.0 46.5 ± 9.3 61.3 ± 3.9 63.3 ± 1.8 67.4 ± 2.3 0.5% 45.3 ± 6.6 47.7 ± 4.4 38.2 ± 7.1 53.2 ± 4.0 53.8 ± 4.7 55.1 ± 3.2 0.3% 39.3 ± 4.9 39.2 ± 6.3 30.9 ± 6.9 44.4 ± 4.5 46.7 ± 5.6 48.3 ± 3.5 Accuracy (%) averaged over 10 runs on the Citeseer dataset. 3% (standard) 69.8 ± 1.1 77.8 ± 0.7 76.7 ± 0.5 78.3 ± 0.3 78.1 ± 0.4 84.3 ± 0.4 0.1% 55.2 ± 6.8 73.0 ± 5.5 59.6 ± 9.5 73.4 ± 5.1 72.8 ± 4.6 75.2 ± 3.6 0.05% 48.2 ± 7.4 64.6 ± 7.5 50.4 ± 9.7 68.8 ± 5.6 66.0 ± 4.5 67.2 ± 7.3 0.03% 45.3 ± 4.5 57.9 ± 8.1 50.9 ± 8.8 60.4 ± 8.6 61.0 ± 8.7 59.3 ± 6.6 Accuracy (%) averaged over 10 runs on the Pubmed dataset. Tables 6-8 present the experimental results.</figDesc><table><row><cell>2.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">DFNets implementation can be found at: https://github.com/wokas36/DFNets</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diffusion-convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Atwood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Towsley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grattarola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Livi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alippi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.01343</idno>
		<title level="m">Graph neural networks with convolutional ARMA filters</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Toward an architecture for never-ending language learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carlson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Betteridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kisiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Settles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hruschka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fourth AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">C</forename><surname>Graham</surname></persName>
		</author>
		<title level="m">Spectral graph theory. Number 92</title>
		<imprint>
			<publisher>American Mathematical Soc</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">L2 regularization for learning kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On gradient adaptation with unit-norm constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Amari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal processing</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1843" to="1847" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics (AIStats)</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics (AIStats)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Wavelets on graphs via spectral graph theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gribonval</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="129" to="150" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks for hyperspectral image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Sensors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition (CVPR)</title>
		<meeting>the IEEE conference on computer vision and pattern recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Autoregressive moving average graph filters: a stable distributed implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Isufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Leus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4119" to="4123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Autoregressive moving average graph filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Isufi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Loukas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Simonetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Leus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="274" to="288" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cayleynets: Graph convolutional neural networks with complex rational spectral filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Levie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2359" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LanczosNet: Multi-scale deep graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventh International Conference on Learning Representation (ICLR)</title>
		<meeting>the seventh International Conference on Learning Representation (ICLR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Link-based classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Machine Learning (ICML)</title>
		<meeting>the 20th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="496" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (SIGKDD)</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining (SIGKDD)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs: Graph fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="6167" to="6170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph attention networks. International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep learning via semi-supervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>The 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="40" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML)</title>
		<meeting>the 20th International conference on Machine learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
