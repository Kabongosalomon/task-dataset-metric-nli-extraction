<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DVI: Depth Guided Video Inpainting for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Liao</surname></persName>
							<email>miao.liao@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Baidu Research</orgName>
								<orgName type="institution" key="instit2">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feixiang</forename><surname>Lu</surname></persName>
							<email>lufeixiang@baidu.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Baidu Research</orgName>
								<orgName type="institution" key="instit2">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingfu</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Baidu Research</orgName>
								<orgName type="institution" key="instit2">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sibo</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Baidu Research</orgName>
								<orgName type="institution" key="instit2">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Baidu Research</orgName>
								<orgName type="institution" key="instit2">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruigang</forename><surname>Yang</surname></persName>
							<email>ryang2@uky.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Baidu Research</orgName>
								<orgName type="institution" key="instit2">Baidu Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DVI: Depth Guided Video Inpainting for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Video Inpainting</term>
					<term>Autonomous Driving</term>
					<term>Depth</term>
					<term>Image Syn- thesis</term>
					<term>Simulation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To get clear street-view and photo-realistic simulation in autonomous driving, we present an automatic video inpainting algorithm that can remove traffic agents from videos and synthesize missing regions with the guidance of depth/point cloud. By building a dense 3D map from stitched point clouds, frames within a video are geometrically correlated via this common 3D map. In order to fill a target inpainting area in a frame, it is straightforward to transform pixels from other frames into the current one with correct occlusion. Furthermore, we are able to fuse multiple videos through 3D point cloud registration, making it possible to inpaint a target video with multiple source videos. The motivation is to solve the long-time occlusion problem where an occluded area has never been visible in the entire video. To our knowledge, we are the first to fuse multiple videos for video inpainting. To verify the effectiveness of our approach, we build a large inpainting dataset in the real urban road environment with synchronized images and Lidar data including many challenge scenes, e.g., long time occlusion. The experimental results show that the proposed approach outperforms the state-of-the-art approaches for all the criteria, especially the RMSE (Root Mean Squared Error) has been reduced by about 13%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As computational power increases, multi-modality sensing has become more and more popular in recent years. Especially in the area of Autonomous Driving (AD), multiple sensors are combined to overcome the drawbacks of individual ones, which can provide redundancy for safety. Nowadays, most self-driving cars are equipped with lidar and cameras for both perception and mapping. Simulation systems have become essential to the development and validation of AD technologies. Instead of using computer graphics to create virtual driving scenarios, Li et al. <ref type="bibr" target="#b12">[11]</ref> proposed to augment real-world pictures with simulated traffic flow to create photorealistic simulation images and renderings. One key component in their pipeline is to remove those moving agents on the road to generate clean background street images. AutoRemover <ref type="bibr" target="#b29">[27]</ref> generated those kinds of data using the augmented platform and proposed a video inpainting method based on the deep learning techniques. Those map service companies, which display street-level panoramic views in their map Apps, also choose to place depth sensors in addition to image sensors on their capture vehicles. Due to privacy protection, those street view images have to be post-processed to blur human faces and vehicle license plates before posted for public access. There is a strong desire to totally remove those agents on the road for better privacy protection and more clear street images.</p><p>Significant progress has been made in image inpainting in recent years. The mainstream approaches <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b7">6,</ref><ref type="bibr" target="#b22">21]</ref> adopt the patch-based method to complete missing regions by sampling and pasting similar patches from known regions or other source images. The method has been naturally extended to video inpainting, where not only spatial coherence but also temporal coherence are preserved.</p><p>The basic idea behind video inpainting is that the missing regions/pixels within a frame are observed in some other frames of the same video. Under this observation, some prior works <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b25">23,</ref><ref type="bibr" target="#b26">24]</ref> use optical flow as guidance to fill the missing pixels either explicitly or implicitly. They are successfully applied in different scenarios with seamless inpainting results. However, flow computation suffers from textureless areas, no matter it's learning based or not. Furthermore, perspective changes in the video could also degrade the quality of optical flow estimation. These frame-wise flow errors are accumulated when we fill missing pixels from a temporally distant frame, resulting in distorted inpainting results, which will be shown in the experiment section.</p><p>The emergence of deep learning, especially Generative Adversarial Networks (GAN), has provided us a powerful tool for inpainting. For images, <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b27">25]</ref> formulate inpainting as a conditional image generation problem. Although formulated differently, GAN based inpainting approaches are essentially the same as the patch-based approach, since the spirit is still looking for similar textures in the training data and fill the holes. Therefore, they have to delicately choose their training data to match the domain of the input images. And domain adaptation is not an easy task once the input images come from different scenarios. Moreover, GAN-based approaches share the same problem as the patch-based methods that they are poor at handling perspective changes in images.</p><p>As image+depth sensors become standard for AD cars, we propose a method to inpaint street-view videos with the guidance of depth. Depending on the tasks, target objects are either manually labeled or automatically detected in color images, and then removed from their depth counterpart. A 3D map is built by stitching all point clouds together and projected back onto individual frames. Most of the frame pixels are assigned with a depth value via 3D projection and those remaining pixels get their depth by interpolation. With a dense depth map and known extrinsic camera parameters, we are able to sample colors from other frames to fill holes within the current frame. These colors serve as an initial guess for those missing pixels, followed by regularization enforcing spatial and photometric smoothness. After that, we apply color harmonization to make smooth and seamless blending boundaries. In the end, a moving average is applied along the optical flow to make the final inpainted video look smooth temporally.</p><p>Unlike learning-based methods, our approach cant inpaint occluded areas if they are never visible in the video. To solve this problem, we propose fusion inpainting, which makes use of multiple video clips to inpainting a target region. Compared to state-of-the-art inpainting approaches, we are able to preserve better details in the missing region with correct perspective distortion. Temporal coherence is implicitly enforced since the 3D map is consistent across all frames. We are even able to inpaint multiple video clips captured at different times by registering all the frames into a common 3D point map. Although our experiments are conducted on datasets captured from a self-driving car, the proposed method is not limited to this scenario only. It can be generalized to both indoor and outdoor scenarios, as long as we have synchronized image+depth data.</p><p>In this paper, we propose a novel video inpainting method with the guidance of 3D maps in AD scenarios. We avoid using deep learning-based methods so that our entire pipeline only runs on CPUs. This makes it easy to be generalized to different platforms and different use cases because it doesn't require GPUs and domain adaptation of training data. 3D map guided inpainting is a new direction for the inpainting community to explore, given that more and more videos are accompanied with depth data. The main contributions of this paper are listed as follows:</p><p>1. We propose a novel approach of depth guided video inpainting for autonomous driving; 2. We are the first to fuse multiple videos for inpainting, in order to solve the long time occlusion problem; 3. We collect a new dataset in the urban road with synchronized images and Lidar data including many challenge inpainting scenes such as long time occlusion; 4. Furthermore, we designed Candidate Color Sampling Criteria and Color Harmonization for inpainting. Our approach shows smaller RMSE compared with other state-of-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>The principle of inpainting is essentially filling the target holes by borrowing appearance information from known sources. The sources could be regions other than the hole in the same image, images from the same video or images/videos of similar scenarios. It's critical to reduce the search space for the right pixels. Following different cues, prior works can be categorized into 3 major classes: propagation-based inpainting, patch-based inpainting, and learning-based inpainting.</p><p>Propogation-based Inpainting. Propagation-based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">5]</ref> extrapolate boundary pixels around the holes for image completion. These approaches are successfully applied to regions of uniform colors. However, it has difficulties to fill large holes with rich texture variations. Thus, Propagation-based approaches usually repair small holes and scratches in an image.</p><p>Patch-based Inpainting. Patch-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">4,</ref><ref type="bibr" target="#b7">6,</ref><ref type="bibr" target="#b22">21]</ref> on the other hand, not only look at the boundary pixels but also search in the other regions/images for similar appearance in order to complete missing regions. This kind of approach has been extended to the temporal domain for video inpainting <ref type="bibr" target="#b14">[13,</ref><ref type="bibr" target="#b15">14,</ref><ref type="bibr" target="#b21">20]</ref>. Huang et al. <ref type="bibr" target="#b9">[8]</ref> jointly estimate optical flow and color in the missing regions to address the temporal consistency problem. In general, patchbased methods can better handle non-stationary visual data. As suggested by its name, Patch-based methods depend on reliable pixel matches to copy and paste image patches to missing regions. When a pixel match can't be robustly obtained, for example in cases of big perspective changes or illumination changes, the inpainting results are problematic.</p><p>Learning-based Inpainting. The success of deep learning techniques inspires recent works on applying it for image inpainting. Ren et al. <ref type="bibr" target="#b20">[19]</ref> adds a few feature maps in the new Shepard layers, achieving stronger results than a much deeper network architecture. Generative Adversarial Networks(GAN <ref type="bibr" target="#b8">[7]</ref>) was first introduced to generate novel photos. It's straightforward to extend it to inpainting by formulating inpainting as a conditional image generation problem <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b16">15,</ref><ref type="bibr" target="#b27">25]</ref>. Pathak et al. <ref type="bibr" target="#b16">[15]</ref> proposed context encoders, which is a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. The context encoders are trained to both understand the content of the entire image, and produce a plausible hypothesis for the missing parts. Iizuka et al. <ref type="bibr" target="#b10">[9]</ref> used global and local context discriminators to distinguish real images from fake ones. The global discriminator looks at the entire image to ensure it is coherent as a whole, while the local discriminator looks only at a small area centered at the completed region to ensure the local consistency of the generated patches. More recently, Yu et al. <ref type="bibr" target="#b27">[25]</ref> presented a contextual attention mechanism in a generative inpainting framework, which further improves the inpainting quality. For video inpainting, Xu et al. <ref type="bibr" target="#b26">[24]</ref> formulated an effective framework that is specially designed to exploit redundant information across video frames. They first synthesize a spatially and temporally coherent optical flow field across video frames, then the synthesized flow field is used to guide the propagation of pixels to fill up the missing regions in the video.</p><p>3 Proposed Approach <ref type="figure" target="#fig_0">Fig. 1</ref> shows a brief pipeline of our approach. A 3D map is first built by stitching all point clouds together, and projected back onto individual frames. With dense depth map and known extrinsic camera parameters, we are able to sample candidate colors from other frames to fill holes within current frame. Then, a belief propagation based regularization is applied to make sure pixel colors within the inpainting region are consistent with each other. It is followed by a color harmonization step which ensures that colors within inpainting region are consistent with outside regions. More details will be described in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">3D Depth Map</head><p>Dynamic Object Removal. We first remove the moving objects from the point cloud, only keep the background points in the final 3D map. It is straight-forward to do so once the calibration between the depth sensor and the image sensor is performed. All points that are projected in the bounding box of the image are removed. The bounding boxes can be automatically detected or manually labeled. Alternatively, we can use PointNet++ <ref type="bibr" target="#b19">[18]</ref> to detect and remove those typical moving objects directly from the point cloud.</p><p>3D Map Stitching. For lidar sensors, LOAM <ref type="bibr" target="#b28">[26]</ref> is a quite robust tool to fuse the multiple frames to build the 3D map. It is capable to match and track geometric features even with a sparse 16-beam lidar. For other dense depth sensors, such as Kinect, <ref type="bibr" target="#b11">[10]</ref> and <ref type="bibr" target="#b23">[22]</ref> proposed real-time solutions to reconstruct a 3D map which can be further down-sampled to generate the final point cloud with a reasonable resolution.</p><p>Camera Pose Refinement. The relative poses between depth sensor and image sensor can be calibrated in advance, but there are still some misalignments between the point cloud and image pixels, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Vibrations, inaccurate synchronization, and accumulative errors from point cloud stitching cause pose offset between the image sensor and depth sensor.</p><p>In order to produce seamless inpainting results, such offset should be compensated even if it's minor in most times. From the initial extrinsic calibration between the image sensor and depth sensor, we optimize their relative rotation R and translation T by minimizing the photometric projection error. The error is defined as:</p><formula xml:id="formula_0">E = p∈Ω |c(p) − c(q)| 2 ,<label>(1)</label></formula><p>where p is a pixel projection from 3D map. Ω is an area surrounding the target inpainting region, which is illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref> as the region between red and yellow boxes. q is original pixel in the image overlaid by p. The function c(·) returns the value of a pixel. Note that the colors and locations of a pixel are discrete values, making the error function E not continuous on R and T. We can't solve the following equation directly using the standard solvers, such as Levenberg-Marquardt algorithm or Gauss-Newton algorithm. Instead, we search discrete spaces of R and T to minimize E. However, R and T have 6 degrees of freedom (DoF) in total, making the searching space extremely large. We choose to fix T and only optimize R because R is dominant at determining projection location when the majority of the 3D map are distant points. Moreover, in most cases, we only need to move projection pixels slightly in vertical and horizontal directions in the image space, which are determined by pitch and yaw angles of the camera. We finally reduce our search space to 2 DoF, which significantly speed up the optimization process.</p><p>Depth Map. Once the camera pose is refined, we project the 3D map onto each image frame to generate the corresponding depth map. Note that some point clouds are captured far from the current image, which can be occluded and de-occluded during the projection process. Hence, we employ z-buffer to get the nearest depth. <ref type="figure">Fig. 3</ref>. A color image and its corresponding dense depth map. Note that the depth is only rendered for background points and all moving objects have been removed.</p><p>To get a fully dense depth map, we could definitely borrow some of the fancy algorithms (e.g. <ref type="bibr" target="#b3">[3]</ref>) that learn to produce dense depth maps from sparse ones. However, we find that the simple linear interpolation is good enough to generate the dense 3D map in our cases. We further apply a 5 × 5 median filter to remove some individual noise points. The final depth map is shown in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Candidate Color Sampling Criteria</head><p>As every pixel is assigned a depth value, it is possible to map a pixel from one image to other images. There are multiple choices of colors to fill in the pixels of the target inpainting region, a guideline should be followed to find the best candidate color. We have 2 principles to choose the best color candidate: 1) always choose from the frame that is closer to the current frame temporally and 2) always choose from the frame where the 3D background is closer to the camera. Please refer to <ref type="figure">Fig. 4</ref> for an example of our candidate selection criteria. The first requirement ensures our inpainting approach suffers less from perspective distortion and occlusion. And the second requirement is because image records more texture details when it's closer to objects, so that we can retain more details in the inpainting regions.</p><p>Under this guideline and the fact that sensors only move forwards during capture, our algorithm works by first searching forwards temporally to the end of video and then backwards until beginning. The first valid pixel is chosen as the candidate. And the valid pixel means its location doesn't fall into the target inpainting regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Regularization with Belief Propagation</head><p>At this point, every pixel gets color value individually. If the camera pose and depth value are 100% correct, we can generate perfect inpainting results with smooth boundaries and neighbors. However, it's not the case in the real world, especially, the depth map always carries errors. Therefore, we have to explicitly enforce some smoothness constraints.</p><p>We formulate the color selection as a discrete global optimization problem and solve it using belief propagation (BP). Before explaining the formulation, we first define the color space and neighbors of a target pixel. As shown in left <ref type="figure">Fig. 4</ref>. Color candidate selection criteria. Top row: a pixel finds its candidate colors in 2 later frames where road texture appears clear in both images. In this case, we choose the frame that is temporally close to the current frame, in order to minimize the impact of perspective change and potential occlusion or de-occlusion. Bottom row: a pixel finds its candidate colors in one previous frame and one later frame. In this case, we prefer the later frame over the previous one, since road texture is lost in the previous frame. image pair in <ref type="figure" target="#fig_2">Fig. 5</ref>, a target pixel (left red box) finds its candidate pixel (right red box) from a source image, due to depth inaccuracy, the true color might not lie exactly on the candidate pixel, but a small window around. So we collect all pixel colors from this small n by n window to form the color space for the target pixel. The right image pair in <ref type="figure" target="#fig_2">Fig. 5</ref> illustrates how to find out the expected colors of neighbors. Because of perspective changes, the 4 neighbors of a target pixel are not necessarily neighbors in the source image. Hence, we warp neighbor pixels into the source image by their depth value to sample the expected colors.</p><p>Let P be the set of pixels in the target inpainting region and L be a set of labels. The labels correspond to the indices of potential colors in the color space. A labeling function l assigns a l p ∈ L to each pixel p ∈ P . We assume that the labels should vary smoothly almost everywhere but may change dramatically at some places such as object boundaries. The quality of labeling is given by an energy function as</p><formula xml:id="formula_1">E = (p,q)∈N V (l p , l q ) + p∈P D p (l p ),<label>(2)</label></formula><p>where N are the number of edges in the four-connected image grid graph. V (l p , l q ) is the cost of assigning labels l p and l q to two neighboring pixels, and is normally referred to as the discontinuity cost. D p (l p ) is the cost of assigning label l p to pixel p, which is referred to as the data cost. Determining a labeling with minimum energy corresponds to the Maximum A Posteriori (MAP) estimation problem. We incorporate boundary smoothness constraint into the data cost as following:</p><formula xml:id="formula_2">D p (l p ) =                |C pl (l p ) − I(q)|, if p is left boundary pixel |C pr (l p ) − I(q)|, if p is right boundary pixel |C pt (l p ) − I(q)|, if p is top boundary pixel |C pb (l p ) − I(q)|, if p is bottom boundary pixel α, otherwise ,<label>(3)</label></formula><p>where C pl , C pr , C pt , C pb return expected colors of pixel p's left, right, top and bottom neighbors respectively. q is the neighbor pixel of p outside of the inpainting region in the target image, so it has known color, which is returned by the function I(q). Here we take the difference of true neighbor color and expected neighbor color as a measure of labeling quality. For those pixels not on the inpainting boundary, we give equal opportunities to all the labels by assigning a constant value of α. The discontinuity cost is defined as</p><formula xml:id="formula_3">V (l p , l q ) =          |C pl (l p ) − C q (l q )| + |C p (l p ) − C qr (l q )| if L |C pr (l p ) − C q (l q )| + |C p (l p ) − C ql (l q )| if R |C pt (l p ) − C q (l q )| + |C p (l p ) − C qb (l q )| if T |C pb (l p ) − C q (l q )| + |C p (l p ) − C qt (l q )| if B .<label>(4)</label></formula><p>Here, C p (·) and C q (·) fetch colors for p and q at label l p and l q . L, R, T, B stand for q is on left, on right, on top and on bottom respectively. For a pair of 2 neighboring pixels p and q, we compute differences between p's color and q's expected color of p and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Color Harmonization</head><p>Pixels from different frames may have different colors due to changing camera exposure time and white balance, causing color discontinuities <ref type="figure" target="#fig_3">(Fig. 6</ref>). We borrow Poisson image editing <ref type="bibr" target="#b18">[17]</ref> to solve these problems. Poisson image editing is originally proposed to clone an image patch from source image into a destination image with seamless boundary and original texture. It achieve this by solving the following minimization problem</p><formula xml:id="formula_4">min f Ω |∆f − v| with f | ∂Ω = f * | ∂Ω .<label>(5)</label></formula><p>Here all the notations are inherited from <ref type="bibr" target="#b18">[17]</ref>. Ω is the inpainting region with boundary ∂Ω. f * is color function of destination image and f is color function of the target inpainting region within destination image. ∆. = [∂./∂x, ∂./∂y] is gradient operator. v is the desired color gradient defined over Ω.</p><p>In our case, v is computed using the output from the belief propagation step, with one exception. If two neighboring pixels within Ω are from different frames, we set their gradient to 0. This guarantee color consistency within the inpainting regions. The effectiveness of this solution is demonstrated in <ref type="figure" target="#fig_3">Fig. 6</ref>. Note that the blank-pixel region is also filled up. Since blank pixels have 0 gradient values, solving the Poisson equation on this part is equivalent to smooth color interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Video Fusion</head><p>Our algorithm has an implicit assumption that the inpainting regions must be visible in some other frames. Otherwise, some pixels will remain blank, as can be seen from <ref type="figure" target="#fig_3">Fig. 6</ref>. Learning-based methods can hallucinate inpainting colors from their training data. In contrast, our approach can't inpaint occluded areas if they are never visible in the video, leaving blank pixels.</p><p>For small areas of blank pixels, a smooth interpolation is sufficient to fill the hole. However, in some cases, a vehicle in front could block a wide field of view for the entire video duration, leaving big blank holes. A simple interpolation will not be capable of handling this problem. A better way to address this issue would be capturing another video of the same scene, where the occluded parts become visible. Fortunately, it is straightforward to register newly captured frames into an existing 3D map using LOAM <ref type="bibr" target="#b28">[26]</ref>. Once new frames are registered and merged into the existing 3D map, inpainting is performed exactly the same way. Some of our results of video fusion can be found in the next section as well as in supplemental materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Temporal Smoothing</head><p>Finally, we compute both forward and backward optical flows for all the result frames. For every pixel in the target inpainting areas, we trace it into neighboring frames using the optical flow and replace its original color with average of colors sampled from neighbor frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Results</head><p>To our best knowledge, all the public datasets (including DAVIS Dataset <ref type="bibr" target="#b17">[16]</ref>) for video inpainting don't come with depth, which is a must for our algorithm. Autonomous driving dataset ApolloScape <ref type="bibr" target="#b13">[12]</ref> indeed have both camera images and point clouds, but it's not adopted by research community to evaluate video inpainting. Plus, its dataset was captured by a professional mapping Lidar RIEGL, which is not a typical setup for an autonomous driving car. Thus, we captured our own dataset and compare to previous works on our dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Inpainting Dataset</head><p>We use an autonomous driving car to collect large-scale datasets in urban streets. The data is generated from a variety of sensors, including Hesai Pandora all in one sensor (40-beam LiDAR, 4 mono cameras covering 360 degrees, 1 forwardfacing color camera), and a localization system working at 10 HZ. The LiDAR is synchronized with embedded frontal facing wide-angle color camera. We recorded a total of 5 hours length of RGB videos includes 100K synchronized 1280 × 720 images and point cloud. The dataset includes many challenging scenes e.g. background is occluded by large bus, shuttle or truck in the intersection and the Inputs Ours Yu <ref type="bibr" target="#b27">[25]</ref> Xu <ref type="bibr" target="#b26">[24]</ref> Huang <ref type="bibr" target="#b9">[8]</ref>  front car is blocking the front view all the time. For those long time occlusion scenarios, the background is missing in the whole video sequence. We captured these difficult streets/intersections more than once, providing us the data for video fusion inpainting. Our new dataset will be published with the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons</head><p>We qualitatively and quantitatively compare our results to three state-of-theart works: two video inpainting approaches <ref type="bibr" target="#b9">[8,</ref><ref type="bibr" target="#b26">24]</ref> and one image inpainting approach <ref type="bibr" target="#b27">[25]</ref>. For those two deep learning-based approaches <ref type="bibr" target="#b26">[24]</ref> and <ref type="bibr" target="#b27">[25]</ref>, we re-train their models on our dataset by randomly sampling missing regions on input frames to perform a fair comparison.</p><p>Qualitative Comparison. In <ref type="figure" target="#fig_4">Fig. 7</ref>, we compare our results with three other methods. It is clear that our method produces better results than others. Even though Huang <ref type="bibr" target="#b9">[8]</ref> got smooth inpainting results, almost all the texture details are missing in their results. As shown, Yu <ref type="bibr" target="#b27">[25]</ref> and Xu <ref type="bibr" target="#b26">[24]</ref> sometimes fill totally messy texture in the target regions. <ref type="figure">Fig. 8</ref> illustrates our capability to handle perspective change between source and target frames. Since our method is based on 3D geometry, perspective changes are inherently handled correctly. However, existing methods have a hard time overcoming this issue. They either fail to recover detailed texture or fail to place the texture in the right place.</p><p>Quantitative Comparison. To quantitatively compare our method with other methods, we manually labeled some background areas as the target inpainting regions and use them as the ground truth. We utilize four metrics for the <ref type="figure">Fig. 8</ref>. Top row: a patch from source image needs to be used to inpaint an occluded region in target image. Although there is significant perspective change from source to target images, our method produces geometrically and visually correct results. While other methods either fail to recover detailed texture or fail to place the texture in the right place.  <ref type="table">Table 1</ref>. Quantitative comparison with other methods, where the best results are highlighted in bold. To be clear, the values of "MAE" and "RMSE" are the lower the better while the values of "PSNR" and "SSIM" are the higher the better. evaluations: Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), Peak Signal to Noise Ratio (PSNR), and Structural Similarity Index (SSIM). Tab. 1 shows the evaluation results of the baseline methods and our method. Note that our method outperforms others on all four metrics. Our method reduce RMSE by 13% compared to SOTA method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>Poisson Image Blending <ref type="figure" target="#fig_5">Fig. 9</ref> shows the effectiveness of applying Poisson image blending. Visible seams are obvious at boundaries of pixels coming from different frames. This is because our capturing camera is working under auto exposure and auto white balance mode. A same object may have different color tones in different frames from the same video, not to mention videos captured on different days. Tab. 2 shows the quantitative results with and without Poisson color blending. It is clear that color blending indeed improves the results.</p><p>Strategies MAE RMSE PSNR SSIM no blending 9.410 17.484 21.783 0.911 blending 6.497 13.009 22.312 0.917 <ref type="table">Table 2</ref>. Ablation study on Poisson color blending, where the best results are highlighted in bold. To be clear, the values of "MAE" and "RMSE" are the lower the better while the values of "PSNR" and "SSIM" are the higher the better.</p><p>Strategies MAE RMSE PSNR SSIM no fusion 10.427 14.967 20.941 0.879 fusion 6.059 8.333 21.195 0.882 <ref type="table">Table 3</ref>. Ablation study on multiple video fusion, where the best results are highlighted in bold. To be clear, the values of "MAE" and "RMSE" are the lower the better while the values of "PSNR" and "SSIM" are the higher the better.</p><p>Video Fusion. <ref type="figure" target="#fig_5">Fig. 9</ref> shows fusion of 2 videos. 1st row shows four frames from a video and the 2nd row shows 4 frames from another video captured on a different day at the same traffic intersection. Here, our goal is to inpaint those foreground objects in the 2nd video. 3rd row shows output using video 2 only, where exists large blank regions. That is because front vehicles keep blocking certain areas during the entire capture time. It is clear that Poisson image blending is not capable of completing large blank holes. 4th row shows BP output after we fuse the 1st video into the 2nd one, where the blank holes The fusion of multiple videos for inpainting demonstrates another advantage of our proposed approach. For those existing video inpainting works, they haven't address the issue of long-time occlusion, neither did they proposed to fuse multiple videos for inpainting purpose. Please checkout video demos here: https://youtu.be/iOIxdQIzjQs .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose an automatic video inpainting algorithm that removes object from videos and synthesizes missing regions with the guidance of depth. It outperforms existing state-of-the-art inpainting methods on our inpainting dataset by preserving accurate texture details. The experiments indicate that our approach could reconstruct cleaner and better background images, especially in the challenging scenarios with long time occlusion scenes. Furthermore, our method may be generalized to any videos as long as depth exists, in contrast to those deep learning-based approaches whose success heavily depend on comprehensiveness and resemblance of training dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Frame-wise point clouds (a) are stitched into a 3D map (b) using LOAM. The 3D map is projected onto a frame (c) to generate a depth map. For each pixel in the target region (e), we use its depth (d) as guidance to sample colors from other frames (f). Final pixel values are determined by BP regularization and color harmonization to ensure photometric consistency. (g) shows the final inpainting result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The point cloud is projected into the target region (red box) with colors. The left image shows projection by calibration result. Obvious misalignment can be seen at boundaries. The right image shows projection by optimized rotation R, where points match much better with surrounding pixels. The region between yellow and red boxes is where we compare colors of projected 3D points and image pixels to optimize camera rotation matrix R.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Left image pair: potential color choices for a target pixel. a pixel within inpainting region find its candidate pixel (red box) from a source image. A small window of pixels around this candidate (yellow boxes) are all potential colors to fill the target pixel. Right image pair: the 4 neighbors of a target pixel are not necessarily neighbors in the source image due to perspective change. In order to get neighbor colors, we need to warp neighbor pixels into the source image by their depth value.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Left image: input image. Middle image: inpainting result. Note the color discontinuity in yellow box and blank pixels in red box. Right image: result after color harmonization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 .</head><label>7</label><figDesc>5 frames from different video clips are demonstrated to compare our results with others.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 9 .</head><label>9</label><figDesc>1st row: frames from video 1; 2nd row: frames from video 2 captured on a different day; 3rd row: results after Poisson color blending using video 2 only; 4th row: direct inpainting results by fusing both videos; 5th row: results after Poision color blending using both videos. are all gone. 5th row shows the final results after color blending and optical flow temporal smoothing. Tab. 3 demonstrates effectiveness of video fusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Methods MAE RMSE PSNR SSIM Yu [25] 10.961 16.848 20.821 0.850</figDesc><table><row><cell>Xu [24]</cell><cell>7.569 12.932 19.220 0.594</cell></row><row><cell cols="2">Huang [8] 6.924 11.017 20.022 0.762</cell></row><row><cell>Ours</cell><cell>6.135 9.633 21.631 0.895</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Filling-in by joint interpolation of vector fields and gray levels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Caselles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verdera</surname></persName>
		</author>
		<idno type="DOI">10.1109/83.935036</idno>
		<ptr target="https://doi.org/10.1109/83.935036" />
	</analytic>
	<monogr>
		<title level="j">Trans. Img. Proc</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1200" to="1211" />
			<date type="published" when="2001-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Simultaneous structure and texture image inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertalmio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Img. Proc</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="882" to="889" />
			<date type="published" when="2003-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TIP.2003.815261</idno>
		<ptr target="https://doi.org/10.1109/TIP.2003.815261" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Depth estimation via affinity learned with convolutional spatial propagation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="103" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Image Melding: Combining inconsistent images using patch-based synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of SIGGRAPH 2012)</title>
		<meeting>SIGGRAPH 2012)</meeting>
		<imprint>
			<publisher>TOG</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="1" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video Inpainting With Short-Term Windows: Application to Object Removal and Error Concealment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ebdelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Le Meur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="3034" to="3047" />
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/TIP.2015.2437193</idno>
		<ptr target="https://doi.org/10.1109/TIP.2015.2437193" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/383259.383296</idno>
		<ptr target="http://doi.acm.org/10.1145/383259.383296" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 28th Annual Conference on Computer Graphics and Interactive Techniques<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
	<note>SIGGRAPH &apos;01</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Weinberger, K.Q.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporally coherent completion of dynamic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">196</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Globally and Locally Consistent Image Completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of SIGGRAPH 2017)</title>
		<meeting>of SIGGRAPH 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Kinectfusion: Real-time 3d reconstruction and interaction using a moving depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Izadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Molyneaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hodges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2047196.2047270</idno>
		<ptr target="http://doi.acm.org/10.1145/2047196.2047270" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology</title>
		<meeting>the 24th Annual ACM Symposium on User Interface Software and Technology<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="559" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Aads: Augmented autonomous driving simulation using data-driven algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">C</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1126/scirobotics.aaw0863</idno>
		<ptr target="https://robotics.sciencemag.org/content/4/28/eaaw0863" />
	</analytic>
	<monogr>
		<title level="j">Science Robotics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">28</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Trafficpredict: Trajectory prediction for heterogeneous traffic-agents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Manocha</surname></persName>
		</author>
		<ptr target="https://arxiv.org/pdf/1811.02146.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6120" to="6127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards fast, generic video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fradet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/2534008.2534019</idno>
		<ptr target="http://doi.acm.org/10.1145/2534008.2534019" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th European Conference on Visual Media Production. pp. 7:1-7:8. CVMP &apos;13</title>
		<meeting>the 10th European Conference on Visual Media Production. pp. 7:1-7:8. CVMP &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Video inpainting of complex scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fradet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prez</surname></persName>
		</author>
		<idno type="DOI">10.1137/140954933</idno>
		<ptr target="https://doi.org/10.1137/140954933" />
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="724" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Poisson image editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pérez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gangnet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<idno type="DOI">http:/doi.acm.org/10.1145/1201775.882269</idno>
		<ptr target="http://doi.acm.org/10.1145/1201775.882269" />
	</analytic>
	<monogr>
		<title level="m">ACM SIG-GRAPH 2003 Papers</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="313" to="318" />
		</imprint>
	</monogr>
	<note>SIGGRAPH &apos;03</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Shepard convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/5774-shepard-convolutional-neural-networks.pdf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="901" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exemplar-based video inpainting without ghost shadow artifacts by maintaining temporal continuity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">C</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Hwang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TCSVT.2009.2013519</idno>
		<ptr target="http://dx.doi.org/10.1109/TCSVT.2009.2013519" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cir. and Sys. for Video Technol</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="347" to="360" />
			<date type="published" when="2009-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Summarizing visual data using bidirectional similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Caspi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Real-time visual odometry from dense rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Steinbrcker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<date type="published" when="2011-11" />
			<biblScope unit="page" from="719" to="722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCVW.2011.6130321</idno>
		<ptr target="https://doi.org/10.1109/ICCVW.2011.6130321" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video inpainting by jointly learning temporal structure and spatial details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 33th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep flow-guided video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.07892</idno>
		<title level="m">Generative image inpainting with contextual attention</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Loam: Lidar odometry and mapping in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems Conference</title>
		<imprint>
			<date type="published" when="2014-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.12588</idno>
		<title level="m">Autoremover: Automatic object removal for autonomous driving videos</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
