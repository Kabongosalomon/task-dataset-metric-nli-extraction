<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MAttNet: Modular Attention Network for Referring Expression Comprehension</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of North Carolina at Chapel Hill</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MAttNet: Modular Attention Network for Referring Expression Comprehension</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we address referring expression comprehension: localizing an image region described by a natural language expression. While most recent work treats expressions as a single unit, we propose to decompose them into three modular components related to subject appearance, location, and relationship to other objects. This allows us to flexibly adapt to expressions containing different types of information in an end-to-end framework. In our model, which we call the Modular Attention Network (MAttNet), two types of attention are utilized: languagebased attention that learns the module weights as well as the word/phrase attention that each module should focus on; and visual attention that allows the subject and relationship modules to focus on relevant image components. Module weights combine scores from all three modules dynamically to output an overall score. Experiments show that MAttNet outperforms previous state-of-the-art methods by a large margin on both bounding-box-level and pixel-level comprehension tasks. Demo 1 and code 2 are provided.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Referring expressions are natural language utterances that indicate particular objects within a scene, e.g., "the woman in the red sweater" or "the man on the right". For robots or other intelligent agents communicating with people in the world, the ability to accurately comprehend such expressions in real-world scenarios will be a necessary component for natural interactions.</p><p>Referring expression comprehension is typically formulated as selecting the best region from a set of proposals/objects O = {o i } N i=1 in image I, given an input expression r. Most recent work on referring expressions uses CNN-LSTM based frameworks to model P (r|o) <ref type="bibr" target="#b39">[19,</ref><ref type="bibr" target="#b31">11,</ref><ref type="bibr" target="#b52">32,</ref><ref type="bibr" target="#b40">20,</ref><ref type="bibr" target="#b38">18]</ref> or uses a joint vision-language embedding framework to model P (r, o) <ref type="bibr" target="#b42">[22,</ref><ref type="bibr" target="#b46">26,</ref><ref type="bibr" target="#b47">27]</ref>. During test- <ref type="figure">Figure 1</ref>: Modular Attention Network (MAttNet). Given an expression, we attentionally parse it into three phrase embeddings, which are input to three visual modules that process the described visual region in different ways and compute individual matching scores. An overall score is then computed as a weighted combination of the module scores.</p><p>ing, the proposal/object with highest likelihood/probability is selected as the predicted region. However, most of these work uses a simple concatenation of all features (target object feature, location feature and context feature) as input and a single LSTM to encode/decode the whole expression, ignoring the variance among different types of referring expressions. Depending on what is distinctive about a target object, different kinds of information might be mentioned in its referring expression. For example, if the target object is a red ball among 10 black balls then the referring expression may simply say "the red ball". If that same red ball is placed among 3 other red balls then location-based information may become more important, e.g., "red ball on the right". Or, if there were 100 red balls in the scene then the ball's relationship to other objects might be the most distinguishing information, e.g., "red ball next to the cat". Therefore, it is natural and intuitive to think about the com-prehension model as a modular network, where different visual processing modules are triggered based on what information is present in the referring expression.</p><p>Modular networks have been successfully applied to address other tasks such as (visual) question answering <ref type="bibr" target="#b22">[2,</ref><ref type="bibr" target="#b23">3]</ref>, visual reasoning <ref type="bibr" target="#b28">[8,</ref><ref type="bibr" target="#b32">12]</ref>, relationship modeling <ref type="bibr" target="#b30">[10]</ref>, and multi-task reinforcement learning <ref type="bibr" target="#b21">[1]</ref>. To the best our knowledge, we present the first modular network for the general referring expression comprehension task. Moreover, these previous work typically relies on an off-the-shelf language parser <ref type="bibr" target="#b44">[24]</ref> to parse the query sentence/question into different components and dynamically assembles modules into a model addressing the task. However, the external parser could raise parsing errors and propagate them into model setup, adversely effecting performance. Therefore, in this paper we propose a modular network for referring expression comprehension -Modular Attention Network (MAttNet) -that takes a natural language expression as input and softly decomposes it into three phrase embeddings. These embeddings are used to trigger three separate visual modules (for subject, location, and relationship comprehension, each with a different attention model) to compute matching scores, which are finally combined into an overall region score based on the module weights. Our model is illustrated in <ref type="figure">Fig. 1</ref>. There are 3 main novelties in MAttNet.</p><p>First, MAttNet is designed for general referring expressions. It consists of 3 modules: subject, location and relationship. As in <ref type="bibr" target="#b33">[13]</ref>, a referring expression could be parsed into 7 attributes: category name, color, size, absolute location, relative location, relative object and generic attribute. MAttNet covers all of them. The subject module handles the category name, color and other attributes, the location module handles both absolute and (some) relative location, and the relationship module handles subject-object relations. Each module has a different structure and learns the parameters within its own modular space, without affecting the others.</p><p>Second, MAttNet learns to parse expressions automatically through a soft attention based mechanism, instead of relying on an external language parser <ref type="bibr" target="#b44">[24,</ref><ref type="bibr" target="#b33">13]</ref>. We show that our learned "parser" attends to the relevant words for each module and outperforms an off-the-shelf parser by a large margin. Additionally, our model computes module weights which are adaptive to the input expression, measuring how much each module should contribute to the overall score. Expressions like "red cat" will have larger subject module weights and smaller location and relationship module weights, while expressions like "woman on left" will have larger subject and location module weights.</p><p>Third, we apply different visual attention techniques in the subject and relationship modules to allow relevant attention on the described image portions. In the subject mod-ule, soft attention attends to the parts of the object itself mentioned by an expression like "man in red shirt" or "man with yellow hat". We call this "in-box" attention. In contrast, in the relationship module, hard attention is used to attend to the relational objects mentioned by expressions like "cat on chair" or "girl holding frisbee". Here the attention focuses on "chair" and "frisbee" to pinpoint the target object "cat" and "girl". We call this "out-of-box" attention. We demonstrate both attentions play important roles in improving comprehension accuracy.</p><p>During training, the only supervision is object proposal, referring expression pairs, (o i , r i ), and all of the above are automatically learned in an end-to-end unsupervised manner, including the word attention, module weights, soft spatial attention, and hard relative object attention.</p><p>We demonstrate MAttNet has significantly superior comprehension performance over all state-of-the-art methods, achieving ∼10% improvements on bounding-box localization and almost doubling precision on pixel segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Referring Expression Comprehension: The task of referring expression comprehension is to localize a region described by a given referring expression. To address this problem, some recent work <ref type="bibr" target="#b39">[19,</ref><ref type="bibr" target="#b52">32,</ref><ref type="bibr" target="#b40">20,</ref><ref type="bibr" target="#b31">11,</ref><ref type="bibr" target="#b38">18]</ref> uses CNN-LSTM structure to model P (r|o) and looks for the object o maximizing the probability. Other recent work uses joint embedding model <ref type="bibr" target="#b42">[22,</ref><ref type="bibr" target="#b46">26,</ref><ref type="bibr" target="#b36">16,</ref><ref type="bibr" target="#b24">4]</ref> to compute P (o|r) directly. In a hybrid of both types of approaches, <ref type="bibr" target="#b53">[33]</ref> proposed a joint speaker-listener-reinforcer model that combined CNN-LSTM (speaker) with embedding model (listener) to achieve state-of-the-art results.</p><p>Most of the above treat comprehension as bounding box localization, but object segmentation from referring expression has also been studied in some recent work <ref type="bibr" target="#b29">[9,</ref><ref type="bibr" target="#b35">15]</ref>. These papers use FCN-style <ref type="bibr" target="#b37">[17]</ref> approaches to perform expression-driven foreground/background classification. We demonstrate that in addition to bounding box prediction, we also outperform previous segmentation results. Modular Networks: Neural module networks <ref type="bibr" target="#b23">[3]</ref> were introduced for visual question answering. These networks decompose the question into several components and dynamically assemble a network to compute an answer to the given question. Since their introduction, modular networks have been applied to several other tasks: visual reasoning <ref type="bibr" target="#b28">[8,</ref><ref type="bibr" target="#b32">12]</ref>, question answering <ref type="bibr" target="#b22">[2]</ref>, relationship modeling <ref type="bibr" target="#b30">[10]</ref>, multitask reinforcement learning <ref type="bibr" target="#b21">[1]</ref>, etc. While the early work <ref type="bibr" target="#b23">[3,</ref><ref type="bibr" target="#b32">12,</ref><ref type="bibr" target="#b22">2]</ref> requires an external language parser to do the decomposition, recent methods <ref type="bibr" target="#b30">[10,</ref><ref type="bibr" target="#b28">8]</ref> propose to learn the decomposition end-to-end. We apply this idea to referring expression comprehension, also taking an end-to-end approach bypassing the use of an external parser.</p><p>We find that our soft attention approach achieves better performance over the hard decisions predicted by a parser.</p><p>The most related work to us is <ref type="bibr" target="#b30">[10]</ref>, which decomposes the expression into (Subject, Preposition/Verb, Object) triples. However, referring expressions have much richer forms than this fixed template. For example, expressions like "left dog" and "man in red" are hard to model using <ref type="bibr" target="#b30">[10]</ref>. In this paper, we propose a generic modular network addressing all kinds of referring expressions. Our network is adaptive to the input expression by assigning both word-level attention and module-level weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>MAttNet is composed of a language attention network plus visual subject, location, and relationship modules. Given a candidate object o i and referring expression r, we first use the language attention network to compute a soft parse of the referring expression into three components (one for each visual module) and map each to a phrase embedding. Second, we use the three visual modules (with unique attention mechanisms) to compute matching scores for o i to their respective embeddings. Finally, we take a weighted combination of these scores to get an overall matching score, measuring the compatibility between o i and r.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Language Attention Network</head><p>Instead of using an external language parser <ref type="bibr" target="#b44">[24]</ref>[3] <ref type="bibr" target="#b22">[2]</ref> or pre-defined templates <ref type="bibr" target="#b33">[13]</ref> to parse the expression, we propose to learn to attend to the relevant words automatically for each module, similar to <ref type="bibr" target="#b30">[10]</ref>. Our language attention network is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. For a given expression r = {u t } T t=1 , we use a bi-directional LSTM to encode the context for each word. We first embed each word u t into a vector e t using an one-hot word embedding, then a bidirectional LSTM-RNN is applied to encode the whole expression. The final hidden representation for each word is the concatenation of the hidden vectors in both directions:</p><formula xml:id="formula_0">e t = embedding(u t ) h t = LSTM(e t , h t−1 ) h t = LSTM(e t , h t+1 ) h t = [ h t , h t ].</formula><p>Given H = {h t } T t=1 , we apply three trainable vectors f m where m ∈ {subj, loc, rel}, computing the attention on each word <ref type="bibr" target="#b49">[29]</ref> for each module:</p><formula xml:id="formula_1">a m,t = exp (f T m h t ) T k=1 exp (f T m h k )</formula><p>The weighted sum of word embeddings is used as the modular phrase embedding:  Different from relationship detection <ref type="bibr" target="#b30">[10]</ref> where phrases are always decomposed as (Subject, Preposition/Verb, Object) triplets, referring expressions have no such well-posed structure. For example, expressions like "smiling boy" only contain language relevant to the subject module, while expressions like "man on left" are relevant to the subject and location modules, and "cat on the chair" are relevant to the subject and relationship modules. To handle this variance, we compute 3 module weights for the expression, weighting how much each module contributes to the expressionobject score. We concatenate the first and last hidden vectors from H which memorizes both structure and semantics of the whole expression, then use another fully-connected (FC) layer to transform it into 3 module weights:</p><formula xml:id="formula_2">q m =</formula><formula xml:id="formula_3">[w subj , w loc , w rel ] = softmax(W T m [h 0 , h T ] + b m )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Visual Modules</head><p>While most previous work <ref type="bibr" target="#b52">[32,</ref><ref type="bibr" target="#b53">33,</ref><ref type="bibr" target="#b39">19,</ref><ref type="bibr" target="#b40">20]</ref> evaluates CNN features for each region proposal/candidate object, we use Faster R-CNN <ref type="bibr" target="#b41">[21]</ref> as the backbone net for a faster and more principled implementation. Additionally, we use ResNet <ref type="bibr" target="#b27">[7]</ref> as our main feature extractor, but also provide comparisons to previous methods using the same VGGNet features <ref type="bibr" target="#b43">[23]</ref> (in Sec. 4.2).</p><p>Given an image and a set of candidates o i , we run Faster R-CNN to extract their region representations. Specifically, we forward the whole image into Faster R-CNN and crop the C3 feature (last convolutional output of 3rd-stage) for each o i , following which we further compute the C4 feature (last convolutional output of 4th-stage). In Faster R-CNN, C4 typically contains higher-level visual cues for category prediction, while C3 contains relatively lower-level cues including colors and shapes for proposal judgment, making both useful for our purposes. In the end, we compute the matching score for each o i given each modular phrase embedding, i.e., S(o i |q subj ), S(o i |q loc ) and S(o i |q rel ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Subject Module</head><p>Our subject module is illustrated in <ref type="figure">Fig. 3</ref>. Given the C3 and C4 features of a candidate o i , we forward them to two </p><formula xml:id="formula_4">* &amp;'() MLP MLP L2%normlize L2%normlize</formula><p>Matching function <ref type="figure">Figure 3</ref>: The subject module is composed of a visual subject representation and phrase-guided embedding. An attribute prediction branch is added after the ResNet-C4 stage and the 1x1 convolution output of attribute prediction and C4 is used as the subject visual representation. The subject phrase embedding attentively pools over the spatial region and feeds the pooled feature into the matching function.</p><p>tasks. The first is attribute prediction, helping produce a representation that can understand appearance characteristics of the candidate. The second is the phrase-guided attentional pooling to focus on relevant regions within object bounding boxes. Attribute Prediction: Attributes are frequently used in referring expressions to differentiate between objects of the same category, e.g. "woman in red" or "the fuzzy cat". Inspired by previous work <ref type="bibr" target="#b50">[30,</ref><ref type="bibr" target="#b48">28,</ref><ref type="bibr" target="#b51">31,</ref><ref type="bibr" target="#b36">16,</ref><ref type="bibr" target="#b45">25]</ref>, we add an attribute prediction branch in our subject module. While preparing the attribute labels in the training set, we first run a template parser <ref type="bibr" target="#b33">[13]</ref> to obtain color and generic attribute words, with low-frequency words removed. We combine both C3 and C4 for predicting attributes as both low and high-level visual cues are important. The concatenation of C3 and C4 is followed with a 1 × 1 convolution to produce an attribute feature blob. After average pooling, we get the attribute representation of the candidate region. A binary cross-entropy loss is used for multi-attribute classification:</p><formula xml:id="formula_5">L attr subj = λ attr i j w attr j [log(p ij )+(1−y ij )log(1−p ij )]</formula><p>where w attr j = 1/ freq attr weights the attribute labels, easing unbalanced data issues. During training, only expressions with attribute words go through this branch.</p><p>Phrase-guided Attentional Pooling: The subject description varies depending on what information is most salient about the object. Take people for example. Sometimes a person is described by their accessories, e.g., "girl in glasses"; or sometimes particular clothing items may be mentioned, e.g., "woman in white pants". Thus, we allow our subject module to localize relevant regions within a bounding box through "in-box" attention. To compute spatial attention, we first concatenate the attribute blob and C4, then use a 1×1 convolution to fuse them into a subject blob, which consists of spatial grid of features V ∈ R d×G , where G = 14 × 14. Given the subject phrase embedding q subj , we compute its attention on each grid location:</p><formula xml:id="formula_6">H a = tanh(W v V + W q q subj ) a v = softmax(w T h,a H a )</formula><p>The weighted sum of V is the final subject visual representation for the candidate region o i :</p><formula xml:id="formula_7">v subj i = G i=1 a v i v i</formula><p>Matching Function: We measure the similarity between the subject representation v subj i and phrase embedding q subj using a matching function, i.e, S(o i |q subj ) = F ( v subj i , q subj ). As shown in top-right of <ref type="figure">Fig. 3</ref>, it consists of two MLPs (multi-layer perceptions) and two L2 normalization layers following each input. Each MLP is composed of two fully connected layers with ReLU activations, serving to transform the visual and phrase representation into a common embedding space. The inner-product of the two l2normalized representations is computed as their similarity score. The same matching function is used to compute the location score S(o i |q loc ), and relationship score S(o i |q rel ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Location Module</head><p>Our location module is shown in <ref type="figure">Fig. 4</ref>. Location is frequently used in referring expressions with about 41% expressions from RefCOCO and 36% expressions from Ref-COCOg containing absolute location words <ref type="bibr" target="#b33">[13]</ref>, e.g. "cat on the right" indicating the object location in the image.</p><formula xml:id="formula_8">Matching ! " # , % " &amp; , ! ' # , % ' &amp;( , )ℎ #&amp; same-type location difference concat score loc</formula><p>Loc. phrase embedding + ,-.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Figure 4: Location Module</head><p>Following previous work <ref type="bibr" target="#b52">[32]</ref>[33], we use a 5-d vector l i to encode the top-left position, bottom-right position and relative area to the image for the candidate object, i.e.,</p><formula xml:id="formula_9">l i = [ x tl W , y tl H , x br W , y br H , w·h W ·H ].</formula><p>Additionally, expressions like "dog in the middle" and "second left person" imply relative positioning among objects of the same category. We encode the relative location representation of a candidate object by choosing up to five surrounding objects of the same category and calculating their offsets and area ratio, i.e., δl ij = [</p><formula xml:id="formula_10">[ x tl ]ij wi , [ y tl ]ij hi , [ x br ]ij wi , [ y br ]ij hi , wj</formula><p>hj wihi ]. The final location representation for the target object is:</p><formula xml:id="formula_11">l loc i = W l [l i ; δl i ] + b l</formula><p>and the location module matching score between o i and q loc is S(o i |q loc ) = F ( l loc i , q loc ). While the subject module deals with "in-box" details about the target object, some other expressions may involve its relationship with other "out-of-box" objects, e.g., "cat on chaise lounge". The relationship module is used to address these cases. As in <ref type="figure" target="#fig_2">Fig. 5</ref>, given a candidate object o i we first look for its surrounding (up-to-five) objects o ij regardless of their categories. We use the average-pooled C4 feature as the appearance feature v ij of each supporting object. Then, we encode their offsets to the candidate object via</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Relationship Module</head><formula xml:id="formula_12">δm ij = [ [ x tl ]ij wi , [ y tl ]ij hi , [ x br ]ij wi , [ y br ]ij hi , wj</formula><p>hj wihi ]. The visual representation for each surrounding object is then:</p><formula xml:id="formula_13">v rel ij = W r [v ij ; δm ij ] + b r</formula><p>We compute the matching score for each of them with q rel and pick the highest one as the relationship score, i.e.,</p><formula xml:id="formula_14">S(o i |q rel ) = max j =i F ( v rel ij , q rel )</formula><p>This can be regarded as weakly-supervised Multiple Instance Learning (MIL) which is similar to <ref type="bibr" target="#b30">[10]</ref>[20].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Loss Function</head><p>The overall weighted matching score for candidate object o i and expression r is:</p><formula xml:id="formula_15">S(o i |r) = w subj S(o i |q subj ) + w loc S(o i |q loc ) + w rel S(o i |q rel ) (1)</formula><p>During training, for each given positive pair of (o i , r i ), we randomly sample two negative pairs (o i , r j ) and (o k , r i ), where r j is the expression describing some other object and o k is some other object in the same image, to calculate a combined hinge loss,</p><formula xml:id="formula_16">L rank = i [λ1max(0, ∆ + S(oi|rj) − S(oi|ri)) +λ2max(0, ∆ + S(o k |ri) − S(oi|ri))]</formula><p>The overall loss incorporates both attributes cross-entropy loss and ranking loss: L = L attr subj + L rank .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We use 3 referring expression datasets: RefCOCO, Re-fCOCO+ <ref type="bibr" target="#b33">[13]</ref>, and RefCOCOg <ref type="bibr" target="#b39">[19]</ref> for evaluation, all collected on MS COCO images <ref type="bibr" target="#b34">[14]</ref>, but with several differences. 1) RefCOCO and RefCOCO+ were collected in an interactive game interface, while RefCOCOg was collected in a non-interactive setting thereby producing longer expressions, 3.5 and 8.4 words on average respectively. 2) Re-fCOCO and RefCOCO+ contain more same-type objects, 3.9 vs 1.63 respectively. 3) RefCOCO+ forbids using absolute location words, making the data more focused on appearance differentiators.</p><p>During testing, RefCOCO and RefCOCO+ provide person vs. object splits for evaluation, where images containing multiple people are in "testA" and those containing multiple objects of other categories are in "testB". There is no overlap between training, validation and testing images. RefCOCOg has two types of data partitions. The first <ref type="bibr" target="#b39">[19]</ref> divides the dataset by randomly partitioning objects into training and validation splits. As the testing split has not been released, most recent work evaluates performance on the validation set. We denote this validation split as Re-fCOCOg's "val*". Note, since this data is split by objects the same image could appear in both training and validation. The second partition <ref type="bibr" target="#b40">[20]</ref> is composed by randomly partitioning images into training, validation and testing splits. We denote its validation and testing splits as RefCOCOg's "val" and "test", and run most experiments on this split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results: Referring Expression Comprehension</head><p>Given a test image, I, with a set of proposals/objects,   <ref type="table">Table 2</ref>: Ablation study of MAttNet using different combination of modules. The feature used here is res101-frcn.</p><formula xml:id="formula_17">O = {o i } N i=1 ,</formula><p>S(o i |r) for each proposal/object given the input expression r, and pick the one with the highest score. For evaluation, we compute the intersection-over-union (IoU) of the selected region with the ground-truth bounding box, considering IoU &gt; 0.5 a correct comprehension. First, we compare our model with previous methods using COCO's ground-truth object bounding boxes as proposals. Results are shown in <ref type="table">Table.</ref> 1. As all of the previous methods (Line 1-8) used a 16-layer VGGNet (vgg16) as the feature extractor, we run our experiments using the same feature for fair comparison. Note the flat fc7 is a single 4096-dimensional feature which prevents us from using the phrase-guided attentional pooling in <ref type="figure">Fig. 3</ref>, so we use average pooling for subject matching. Despite this, our results (Line 9) still outperform all previous state-of-the-art methods. After switching to the res101-based Faster R-CNN (res101-frcn) representation, the comprehension accuracy further improves another ∼3% (Line 10). Note our Faster R-CNN is pre-trained on COCO's training images, excluding those in RefCOCO, RefCOCO+, and RefCOCOg's vali-dation+testing. Thus no training images are seen during our evaluation 3 . Our full model (Line 11) with phrase-guided attentional pooling achieves the highest accuracy over all others by a large margin.</p><p>Second, we study the benefits of each module of MAt-tNet by running ablation experiments <ref type="table">(Table.</ref> 2) with the same res101-frcn features. As a baseline, we use the concatenation of the regional visual feature and the location feature as the visual representation and the last hidden output of LSTM-encoded expression as the language representation, then feed them into the matching function to obtain the similarity score (Line 1). Compared with this, a simple two-module MAttNet using the same features (Line 2) already outperforms the baseline, showing the advantage of modular learning. Line 3 shows the benefit of encoding location (Sec. 3.2.2). After adding the relationship module, the performance further improves (Line 4). Lines 5 and Line 6 show the benefits brought by the attribute subbranch and the phrase-guided attentional pooling in our subject module. We find the attentional pooling (Line 6) greatly improves on the person category (testA of RefCOCO and RefCOCO+), demonstrating the advantage of modular attention on understanding localized details like "girl with red hat".</p><p>Third, we tried training our model using 3 hard-coded phrases from a template language parser <ref type="bibr" target="#b33">[13]</ref>, shown in Line 7 of <ref type="table">Table.</ref> 2, which is ∼5% lower than our end-toend model (Line 6). The main reason for this drop is errors made by the external parser which is not tuned for referring expressions.</p><p>Fourth, we show results using automatically detected objects from Faster R-CNN, providing an analysis of fully automatic comprehension performance.    <ref type="figure" target="#fig_1">2)</ref> with a big margin. Besides, we show the performance when using the detector branch of Mask R-CNN <ref type="bibr" target="#b26">[6]</ref> (res101-mrcn) in Line 9, whose results are even better than using Faster R-CNN.</p><p>Finally, we show some example visualizations of comprehension using our full model in <ref type="figure">Fig. 6</ref> as well as visualizations of the attention predictions. We observe that our language model is able to attend to the right words for each module even though it is learned in a weakly-supervised manner. We also observe the expressions in RefCOCO and RefCOCO+ describe the location or details of the target object more frequently while RefCOCOg mentions the relationship between target object and its surrounding object more frequently, which accords with the dataset property. Note that for some complex expressions like "woman in plaid jacket and blue pants on skis" which contains several relationships (last row in <ref type="figure">Fig. 6</ref>), our language model is able to attend to the portion that should be used by the "in-box" subject module and the portion that should be used by the "out-of-box" relationship module. Additionally our subject module also displays reasonable spatial "in-box" attention, which qualitatively explains why attentional pooling <ref type="table">(Table.</ref> 2 Line 6) outperforms average pooling <ref type="table">(Table.</ref> 2 Line 5). For comparison, some incorrect comprehension are shown in <ref type="figure">Fig. 7</ref>. Most errors are due to sparsity in the training data, ambiguous expressions, or detection error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Segmentation from Referring Expression</head><p>Our model can also be used to address referential object segmentation <ref type="bibr" target="#b29">[9,</ref><ref type="bibr" target="#b35">15]</ref>. Instead of using Faster R- CNN as the backbone net, we now turn to res101-based Mask R-CNN <ref type="bibr" target="#b26">[6]</ref> (res101-mrcn). We apply the same procedure described in Sec. 3 on the detected objects, and use the one with highest matching score as our prediction. Then we feed the predicted bounding box to the mask branch to obtain a pixel-wise segmentation. We evaluate the full model of MAttNet and compare with the best results reported in <ref type="bibr" target="#b35">[15]</ref>. We use Precision@X (X ∈ {0.5, 0.6, 0.7, 0.8, 0.9}) 4 and overall Intersectionover-Union (IoU) as metrics. Results are shown in <ref type="table" target="#tab_5">Table. 4</ref> 4 Precision@0.5 is the percentage of expressions where the IoU of the predicted segmentation and ground-truth is at least 0.5. with our model outperforming state-of-the-art results by a large margin under all metrics 5 . As both <ref type="bibr" target="#b35">[15]</ref> and MAt-tNet use res101 features, such big gains may be due to our proposed model. We believe decoupling box localization (comprehension) and segmentation brings a large gain over FCN-style <ref type="bibr" target="#b37">[17]</ref> foreground/background mask classification <ref type="bibr" target="#b29">[9,</ref><ref type="bibr" target="#b35">15]</ref> for this instance-level segmentation problem, but a more end-to-end segmentation system may be studied in future work. Some referential segmentation examples are shown in <ref type="figure" target="#fig_4">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Our modular attention network addresses variance in referring expressions by attending to both relevant words and visual regions in a modular framework, and dynamically computing an overall matching score. We demonstrate our model's effectiveness on bounding-box-level and pixellevel comprehension, significantly outperforming state-ofthe-art. Acknowledgements: This research is supported by NSF Awards #1405822, 1562098, 1633295, NVidia, Google Research, Microsoft Research and Adobe Research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Training Details</head><p>We optimize our model using Adam with an initial learning rate of 0.0004 and with a batch size of 15 images (and all their expressions). The learning rate is halved every 8,000 iterations after the first 8,000-iteration warm-up. The word embedding size and hidden state size of the LSTM are set to 512. We also set the output of all MLPs and FCs within our model to be 512-dimensional. To avoid overfitting, we regularize the word-embedding and output layers of the LSTM in the language attention network using dropout with ratio of 0.5. We also regularize the two inputs (visual and language) of matching function using a dropout with a ratio of 0.2. For the constrastive pairs, we set λ 1 = 1.0 and λ 2 = 1.0 in the ranking loss L rank . Besides, we set λ attr = 1.0 for multi-label attribute cross-entropy loss L attr subj .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Computational Efficiency</head><p>During training, the full model of MAttNet converges at around 30,000 iterations, which takes around half day using single Titan-X(Pascal). At inference time, our fully automatic system goes through both Mask R-CNN and MAt-tNet, which takes on average 0.33 seconds for a forward, where 0.31 seconds are spent on Mask R-CNN and 0.02 seconds on MAttNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Attribute Prediction</head><p>Our full model is also able to predict attributes during testing. Our attribute labels are extracted using the template parser <ref type="bibr" target="#b33">[13]</ref>. We fetch the object name, color and generic attribute words from each expression, with low-frequency words removed. We use 50 most frequently used attribute words for training. The histograms for top-20 attribute words are shown in <ref type="figure" target="#fig_6">Fig. 9</ref>, and the quantitative analysis of our multi-attribute prediction results is shown in <ref type="table">Table.</ref> 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4. MAttNet + Grabcut</head><p>In Section 4.3, we show MAttNet could be extended to referential segmentation by using Mask R-CNN as the    <ref type="table">Table 5</ref>: Multi-attribute prediction on the validation split of each dataset.</p><p>backbone net. Actually, the mask branch of MAttNet could be any foreground-background decomposition method. The simplest replacement might be GrabCut. We show the results of MatNet+GrabCut in <ref type="table" target="#tab_8">Table 6</ref>. Note even though GrabCut is an inferior segmentation method, it still far outperforms previous state-of-the-art results <ref type="bibr" target="#b35">[15]</ref>. Thus, we believe the way of decoupling box localization (comprehension) and segmentation is more suitable for instance-level referential segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5. Mask R-CNN Implementation</head><p>Our implementation of Mask R-CNN 6 is based on the single-GPU Faster R-CNN implementation <ref type="bibr" target="#b25">[5]</ref>. For the mask branch, we follow the structure in the original pa-   <ref type="table">Table.</ref> 7. Both models are based on ResNet101 and were trained using same setting. In the main paper, we denote them as res101frcn and res101-mrcn respectively. It shows that Mask R-CNN has higher AP than Faster R-CNN due to the multitask training (with additional mask supervision).   We then compare our Mask R-CNN implementation with the original one <ref type="bibr" target="#b26">[6]</ref> in <ref type="table" target="#tab_11">Table 8</ref>. Note this is not a strictly fair comparison as our model was trained with fewer images. Overall, the AP of our implementation is ∼2 points lower. The main reason may due to the shorter 600-pixel edge setting and smaller training batch size. Even though, our pixel-wise comprehension results already outperform the state-of-the-art ones with a huge margin (see <ref type="table">Table.</ref> 4, and we believe there exists space for further improvements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6. More Examples</head><p>We show more examples of comprehension using our full model in <ref type="figure" target="#fig_7">Fig. 10 (RefCOCO)</ref>, <ref type="figure" target="#fig_8">Fig. 11</ref> (RefCOCO+) and <ref type="figure" target="#fig_1">Fig. 12</ref> (RefCOCOg). For each example, we show the input image (1st column), the input expression with our predicted module weights and word attention (2nd column), the subject attention (3rd column) and top-5 attributes (4th column), box-level comprehension (5th column), and pixelwise segmentation (6th column). As comparison, we also show some incorrect comprehension in <ref type="figure" target="#fig_10">Fig. 13</ref>.    </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>$%&amp; , . ()* , . +,( ] man in red holding controller on the right Word Attention man in red holding controller on the right man in red holding controller on the right ⨀ Bi-LSTM man in red holding controller on the right</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Language Attention Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Relationship Module</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :Figure 7 :</head><label>67</label><figDesc>with hands up"Expression="a man with a silver ring is holding a phone" Expression="woman in plaid jacket and blue pants on skis" Expression="bottom left bowl"Expression="suit guy under umbrella" Examples of fully automatic comprehension. The blue dotted boxes show our prediction with the relative regions in yellow dotted boxes, and the green boxes are the ground-truth. The word attention is multiplied by module weight.Expression="dude with 9"Expression="boy with striped shirt" Expression="man standing behind person hitting ball" Examples of incorrect comprehensions. Red dotted boxes show our wrong prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>and white horse" Expression="a woman with full black tops" Expression="woman with short red hair" Expression="right kid" Expression="left elephant" Examples of fully-automatic MAttNet referential segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Attribute histogram for three datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Examples of fully automatic comprehension onRefCOCO. The 1st column shows the input image. The 2nd column shows the expression, word attention and module weights. The 3rd column shows our predicted subject attention, and the 4th column shows its top-5 attributes. The 5th column shows box-level comprehension where the red dotted boxes show our prediction and yellow dotted boxes shows the relative object, and the green boxes are the ground-truth. The 6th column shows the segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Examples of fully automatic comprehension on RefCOCO+.The 1st column shows the input image. The 2nd column shows the expression, word attention and module weights. The 3rd column shows our predicted subject attention, and the 4th column shows its top-5 attributes. The 5th column shows box-level comprehension where the red dotted boxes show our prediction and yellow dotted boxes shows the relative object, and the green boxes are the ground-truth. The 6th column shows the segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Examples of fully automatic comprehension onRefCOCOg. The 1st column shows the input image. The 2nd column shows the expression, word attention and module weights. The 3rd column shows our predicted subject attention, and the 4th column shows its top-5 attributes. The 5th column shows box-level comprehension where the red dotted boxes show our prediction and yellow dotted boxes shows the relative object, and the green boxes are the ground-truth. The 6th column shows the segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Examples of incorrect comprehension on three datasets. The 1st column shows the input image. The 2nd column shows the expression, word attention and module weights. The 3rd column shows our predicted subject attention, and the 4th column shows its top-5 attributes. The 5th column shows box-level comprehension where the red dotted boxes show our prediction and yellow dotted boxes shows the relative object, and the green boxes are the ground-truth. The 6th column shows the segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Comparison with state-of-the-art approaches on ground-truth MS COCO regions.</figDesc><table><row><cell></cell><cell>RefCOCO</cell><cell></cell><cell></cell><cell>RefCOCO+</cell><cell></cell><cell cols="2">RefCOCOg</cell></row><row><cell>val</cell><cell>testA</cell><cell>testB</cell><cell>val</cell><cell>testA</cell><cell>testB</cell><cell>val</cell><cell>test</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Table.3 shows the ablation study of fully-automatic MAttNet. While perfor-</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>RefCOCO</cell><cell></cell><cell></cell><cell>RefCOCO+</cell><cell></cell><cell cols="2">RefCOCOg</cell></row><row><cell></cell><cell>detector</cell><cell>val</cell><cell>testA</cell><cell>testB</cell><cell>val</cell><cell>testA</cell><cell>testB</cell><cell>val</cell><cell>test</cell></row><row><cell>1 Speaker+Listener+Reinforcer [33]</cell><cell>res101-frcn</cell><cell>69.48</cell><cell>73.71</cell><cell>64.96</cell><cell>55.71</cell><cell>60.74</cell><cell>48.80</cell><cell>60.21</cell><cell>59.63</cell></row><row><cell>2 Speaker+Listener+Reinforcer [33]</cell><cell>res101-frcn</cell><cell>68.95</cell><cell>73.10</cell><cell>64.85</cell><cell>54.89</cell><cell>60.04</cell><cell>49.56</cell><cell>59.33</cell><cell>59.21</cell></row><row><cell>3 Matching:subj+loc</cell><cell>res101-frcn</cell><cell>72.28</cell><cell>75.43</cell><cell>67.87</cell><cell>58.42</cell><cell>61.46</cell><cell>52.73</cell><cell>64.15</cell><cell>63.25</cell></row><row><cell>4 MAttN:subj+loc</cell><cell>res101-frcn</cell><cell>72.72</cell><cell>76.17</cell><cell>68.18</cell><cell>58.70</cell><cell>61.65</cell><cell>53.41</cell><cell>64.40</cell><cell>63.74</cell></row><row><cell>5 MAttN:subj+loc(+dif)</cell><cell>res101-frcn</cell><cell>72.96</cell><cell>76.61</cell><cell>68.20</cell><cell>58.91</cell><cell>63.06</cell><cell>55.19</cell><cell>64.66</cell><cell>63.88</cell></row><row><cell>6 MAttN:subj+loc(+dif)+rel</cell><cell>res101-frcn</cell><cell>73.25</cell><cell>76.77</cell><cell>68.44</cell><cell>59.45</cell><cell>63.31</cell><cell>55.68</cell><cell>64.87</cell><cell>64.01</cell></row><row><cell>7 MAttN:subj(+attr)+loc(+dif)+rel</cell><cell>res101-frcn</cell><cell>74.51</cell><cell>77.81</cell><cell>68.39</cell><cell>62.13</cell><cell>66.33</cell><cell>55.75</cell><cell>65.33</cell><cell>65.19</cell></row><row><cell>8 MAttN:subj(+attr+attn)+loc(+dif)+rel</cell><cell>res101-frcn</cell><cell>76.40</cell><cell>80.43</cell><cell>69.28</cell><cell>64.93</cell><cell>70.26</cell><cell>56.00</cell><cell>66.67</cell><cell>67.01</cell></row><row><cell cols="3">9 MAttN:subj(+attr+attn)+loc(+dif)+rel res101-mrcn 76.65</cell><cell>81.14</cell><cell>69.99</cell><cell>65.33</cell><cell>71.62</cell><cell>56.02</cell><cell>66.58</cell><cell>67.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of MAttNet on fully-automatic comprehension task using different combination of modules. The features used here are res101-frcn, except the last row using res101-mrcn.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>RefCOCO</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell cols="2">Backbone Net</cell><cell>Split</cell><cell cols="5">Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9</cell><cell>IoU</cell></row><row><cell cols="2">D+RMI+DCRF [15]</cell><cell cols="2">res101-DeepLab</cell><cell>val</cell><cell>42.99</cell><cell>33.24</cell><cell>22.75</cell><cell>12.11</cell><cell>2.23</cell><cell>45.18</cell></row><row><cell cols="2">MAttNet</cell><cell cols="2">res101-mrcn</cell><cell>val</cell><cell>75.16</cell><cell>72.55</cell><cell>67.83</cell><cell>54.79</cell><cell>16.81</cell><cell>56.51</cell></row><row><cell cols="2">D+RMI+DCRF [15]</cell><cell cols="3">res101-DeepLab testA</cell><cell>42.99</cell><cell>33.59</cell><cell>23.69</cell><cell>12.94</cell><cell>2.44</cell><cell>45.69</cell></row><row><cell cols="2">MAttNet</cell><cell cols="2">res101-mrcn</cell><cell>testA</cell><cell>79.55</cell><cell>77.60</cell><cell>72.53</cell><cell>59.01</cell><cell>13.79</cell><cell>62.37</cell></row><row><cell cols="2">D+RMI+DCRF [15]</cell><cell cols="2">res101-DeepLab</cell><cell>testB</cell><cell>44.99</cell><cell>32.21</cell><cell>22.69</cell><cell>11.84</cell><cell>2.65</cell><cell>45.57</cell></row><row><cell cols="2">MAttNet</cell><cell cols="2">res101-mrcn</cell><cell>testB</cell><cell>68.87</cell><cell>65.06</cell><cell>60.02</cell><cell>48.91</cell><cell>21.37</cell><cell>51.70</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RefCOCO+</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell cols="2">Backbone Net</cell><cell>Split</cell><cell cols="5">Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9</cell><cell>IoU</cell></row><row><cell cols="2">D+RMI+DCRF [15]</cell><cell cols="2">res101-DeepLab</cell><cell>val</cell><cell>20.52</cell><cell>14.02</cell><cell>8.46</cell><cell>3.77</cell><cell>0.62</cell><cell>29.86</cell></row><row><cell cols="2">MAttNet</cell><cell cols="2">res101-mrcn</cell><cell>val</cell><cell>64.11</cell><cell>61.87</cell><cell>58.06</cell><cell>47.42</cell><cell>14.16</cell><cell>46.67</cell></row><row><cell cols="2">D+RMI+DCRF [15]</cell><cell cols="3">res101-DeepLab testA</cell><cell>21.22</cell><cell>14.43</cell><cell>8.99</cell><cell>3.91</cell><cell>0.49</cell><cell>30.48</cell></row><row><cell cols="2">MAttNet</cell><cell cols="2">res101-mrcn</cell><cell>testA</cell><cell>70.12</cell><cell>68.48</cell><cell>63.97</cell><cell>52.13</cell><cell>12.28</cell><cell>52.39</cell></row><row><cell cols="2">D+RMI+DCRF [15]</cell><cell cols="2">res101-DeepLab</cell><cell>testB</cell><cell>20.78</cell><cell>14.56</cell><cell>8.80</cell><cell>4.58</cell><cell>0.80</cell><cell>29.50</cell></row><row><cell cols="2">MAttNet</cell><cell cols="2">res101-mrcn</cell><cell>testB</cell><cell>54.82</cell><cell>51.73</cell><cell>47.27</cell><cell>38.58</cell><cell>17.00</cell><cell>40.08</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">RefCOCOg</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">Backbone Net</cell><cell cols="7">Split Pr@0.5 Pr@0.6 Pr@0.7 Pr@0.8 Pr@0.9</cell><cell>IoU</cell></row><row><cell>MAttNet</cell><cell cols="2">res101-mrcn</cell><cell>val</cell><cell>64.48</cell><cell>61.52</cell><cell></cell><cell>56.50</cell><cell>43.97</cell><cell>14.67</cell><cell>47.64</cell></row><row><cell>MAttNet</cell><cell cols="2">res101-mrcn</cell><cell>test</cell><cell>65.60</cell><cell>62.92</cell><cell></cell><cell>57.31</cell><cell>44.44</cell><cell>12.55</cell><cell>48.61</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of segmentation performance on RefCOCO, RefCOCO+, and our results on RefCOCOg. mance drops due to detection errors, the overall improvements brought by each module are consistent with Table. 2, showing the robustness of MAttNet. Our results also outperform the state-of-the-art [33] (Line 1,</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of referential segmentation performance between D+RMI+DCRF<ref type="bibr" target="#b35">[15]</ref> and MatNet+GrabCut.</figDesc><table><row><cell>per [6], with several differences: 1) We sample R = 256</cell></row><row><cell>regions from N = 1 image during each forward-backward</cell></row><row><cell>propagation due to the constraint of single GPU, while [6]</cell></row><row><cell>samples R = 128 regions from N = 16 images using 8</cell></row><row><cell>GPUs. 2) During training, the shorter edge of our resized</cell></row><row><cell>image is 600 pixels instead of 800 pixels, for saving mem-</cell></row><row><cell>ory. 3) Our model is trained on a union of COCO's 80k train</cell></row><row><cell>and 35k subset of val (trainval35k) images excluding the</cell></row><row><cell>val/test (valtest4k) images in RefCOCO, RefCOCO+ and</cell></row><row><cell>RefCOCOg.</cell></row><row><cell>We firstly show the comparison between Faster R-CNN</cell></row><row><cell>and Mask R-CNN on object detection in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>netAP bb AP bb</figDesc><table><row><cell></cell><cell></cell><cell>50</cell><cell>AP bb 75</cell></row><row><cell>res101-frcn</cell><cell>34.1</cell><cell>53.7</cell><cell>36.8</cell></row><row><cell cols="2">res101-mrcn 35.8</cell><cell>55.3</cell><cell>38.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7 :</head><label>7</label><figDesc>Object detection results.</figDesc><table><row><cell>net</cell><cell cols="2">AP AP 50 AP 75</cell></row><row><cell cols="2">res101-mrcn (ours) 30.7 52.3</cell><cell>32.4</cell></row><row><cell>res101-mrcn [6]</cell><cell>32.7 54.2</cell><cell>34.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Instance segmentation results.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Such constraint forbids us to evaluate on RefCOCOg's val* using the res101-frcn feature inTable 1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">There is no experiments on RefCOCOg's val/test splits in<ref type="bibr" target="#b35">[15]</ref>, so we show our performance only for reference inTable 4.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Our implementation: https://github.com/lichengunc/mask-faster-rcnn.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>man standing up&quot; 1. guy (0.91) 2. black (0.20) 3. white (0.04) 4. woman (0.04) 5. lady (0.02</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">woman second from left</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Expression=. man on far right&quot; 1. guy (0.58) 2. jacket (0.49) 3. woman (0.24) 4. black (0.24) 5. lady (0.12</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>second from right guy&quot; 1. guy (0.44) 2. blue (0.29) 3. woman (0.25) 4. girl (0.12) 5. shirt (0.11</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>pink donut on top&quot; 1. pink (0.64) 2. purple (0.15) 3. white (0.13) 4. baby (0.04) 5. brown (0.04) 1. food (0.80) 2. plate (0.42) 3. white (0.12) 4. brown (0.01) 5. black (0.01</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>bottom left bowl&quot; Expression=&quot;teddy bear left&quot; 1. food (0.80) 2. plate (0.42) 3. white (0.12) 4. brown (0.01) 5. black (0.01</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<title level="m">Expression &amp; Lang. attention Subj. attention Box localization Top-5 attributes Segmentation Input image</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>woman (0.44) 2. shirt (0.27) 3. girl (0.25) 4. lady (0.19) 5. guy (0.10</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>woman with sun glasses&quot; 1. girl (0.33) 2. pink (0.26) 3. shirt (0.24) 4. white (0.12) 5. guy (0.11</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>pink girl</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>man on far right&quot; 1. black (0.58) 2. guy (0.56) 3. shirt (0.43) 4. hand (0.25) 5. woman (0.18</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>red blue white phone case&quot; 1. red (0.16) 2. black (0.11) 3. white (0.11) 4. girl (0.04) 5. blue (0.02) 1. animal (0.32) 2. face (0.26) 3. white (0.20) 4. dark (0.11) 5. black (0.10</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>biggest lamb&quot; Expression=&quot;largest compute screen&quot; 1. white (0.94) 2. gray (0.13) 3. number (0.04) 4. old (0.04) 5. black (0.03</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<title level="m">Expression &amp; Lang. attention Subj. attention Box localization Top-5 attributes Segmentation Input image</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>guy (0.39) 2. black (0.24) 3. hand (0.14) 4. woman (0.07) 5. grey (0.05</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Expression=&quot;a woman with a blue headband holding a tennis racket</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>woman in plaid jacket and blue pants on skis&quot; 1. woman (0.45) 2. skier (0.40) 3. blue (0.16) 4. girl (0.12) 5. child (0.08</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>man wearing glasses sitting at table&quot; 1. guy (0.66) 2. woman (0.15) 3. boy (0.09) 4. young (0.08) 5. lady (0.08) 1. woman (0.80) 2. girl (0.17) 3. lady (0.13) 4. young (0.10) 5. boy (0.07</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>giraffe bending down&quot; 1. baby (0.40) 2. young (0.26) 3. brown (0.25) 4. white (0.05) 5. adult (0.02</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<title level="m">Expression &amp; Lang. attention Subj. attention Box localization Top-5 attributes Segmentation Input image</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>animal (0.50) 2. face (0.08) 3. pink (0.03) 4. middle (0.03) 5. baby (0.02</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>giraffe to far left&quot; 1. guy (0.40) 2. shirt (0.35) 3. white (0.15) 4. blue (0.05) 5. green (0.04</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>pointing and smiling&quot; 1. shirt (0.33) 2. woman (0.29) 3. girl (0.18) 4. white (0.16) 5. guy (0.15</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Expression=</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>red cover mustard&quot; 1. red (0.81) 2. full (0.03) 3. pink (0.02) 4. glass (0.01) 5. dark (0.01</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<title level="m">RefCOCO: RefCOCO+: RefCOCOg: Expression &amp; Lang. attention Subj. attention Box localization</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Modular multitask reinforcement learning with policy sketches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NAACL</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Query-guided regression network with context policy for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kovvuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">An implementation of faster rcnn with study for region sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02138</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Segmentation from natural language expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling relationship in referential expressions with compositional modular networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbacnh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Natural language object retrieval. CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Recurrent multimodal interaction for referring image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Referring expression generation and comprehension via attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Comprehension-guided referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Generation and comprehension of unambiguous object descriptions. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling context between objects for referring expression understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Reasoning about fine-grained attribute phrases using reference games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<idno>2017. 4</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning deep structurepreserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning two-branch neural networks for image-text matching tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03470</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image captioning and visual question answering based on attributes and external knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hierarchical attention networks for document classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01646</idno>
		<title level="m">Boosting image captioning with attributes</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image captioning with semantic attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A joint speakerlistener-reinforcer model for referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
