<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-supervised Learning for Single View Depth and Surface Normal Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huangying</forename><surname>Zhan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saroj</forename><surname>Chamara</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Weerasekera</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Garg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reid</surname></persName>
						</author>
						<title level="a" type="main">Self-supervised Learning for Single View Depth and Surface Normal Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we present a self-supervised learning framework to simultaneously train two Convolutional Neural Networks (CNNs) to predict depth and surface normals from a single image. In contrast to most existing frameworks which represent outdoor scenes as fronto-parallel planes at piecewise smooth depth, we propose to predict depth with surface orientation while assuming that natural scenes have piece-wise smooth normals. We show that a simple depth-normal consistency as a soft-constraint on the predictions is sufficient and effective for training both these networks simultaneously. The trained normal network provides state-of-the-art predictions while the depth network, relying on much realistic smooth normal assumption, outperforms the traditional self-supervised depth prediction network by a large margin on the KITTI benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Recovering 3D scene structure from image data is longstudied problem in the robotics and computer vision communities. Decades of research has been focused on recovering the 3D structure of the scene from multiple 2D images as a geometric inverse problem, but the recent surge in deep learning has produced promising solutions for recovering scene geometry even from a single RGB image. Supervised deep neural networks trained on large indoor RGBD data-sets like NYUD <ref type="bibr" target="#b0">[1]</ref> and SUN-RGBD <ref type="bibr" target="#b1">[2]</ref> can currently produce high fidelity depth and surface normal predictions from single RGB image.</p><p>While the availability of cheap range sensors like Kinect makes large-scale supervised training of single-view depth and normal predictions possible for indoor scenes, the absence of high quality, dense depth sensing outdoors makes supervised learning difficult. To address this problem, Garg et al. <ref type="bibr" target="#b2">[3]</ref> introduced a self-supervised learning paradigm where large stereo data-set can be used to train the single view depth prediction networks by using the traditional dense stereo loss function of <ref type="bibr" target="#b3">[4]</ref> as the supervisory signal. Various extensions of the self-supervised learning for depth estimation have been proposed in recent times, including the use of robust image alignment costs, Godard et al. <ref type="bibr" target="#b4">[5]</ref>. <ref type="bibr" target="#b5">[6]</ref> extended the framework to enable training depth predictors using monocular sequence whereas <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref> advocate using stereo sequences to train a network for unsupervised two-frame visual odometry, because this addresses the issue of metric scale (which is otherwise unobservable). <ref type="bibr" target="#b7">[8]</ref> additionally propose to learn good features to match and <ref type="bibr" target="#b8">[9]</ref> advocate to evaluate the quality of warp using Generative Adversarial Networks <ref type="bibr" target="#b9">[10]</ref>. Most of these self-supervised frameworks minimise a loss based on image data even though they are predicting a depth map. This means that they must regularise the predicted depth maps during training in regions where there is no strong photometric information; this is usually done by encouraging the predicted depth maps to be piece-wise smooth, or constant with depth discontinuities aligning image edges. These assumptions are rarely realistic and lead to fronto-parallel planar artifacts in the estimated structures in homogeneous regions.</p><p>In this work, we propose to address these issues by jointly learning two convolutional neural networks predicting depths and surface normals from a single input image in a self-supervised framework. Estimating surface normals in conjunction with depth-maps allows for a richer geometric reasoning where we can relax the piece-wise smooth/constant depth-map assumption to allow for smooth or planer surfaces in the scene. Similar to <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, in addition to surface prediction networks, we learn a two-frame relative camera pose estimation network to predict visual odometry which allows us to use both stereo and temporal information available in KITTI dataset for accurate depth prediction. Our training schema is described in detail in Section III and the test-time setup is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>Very closely related to our work are the recently published works of <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref>. In these works, the authors also advocate replacing the piece-wise smooth depth assumption with that of piece-wise smooth normals, however our proposed framework is different in the following aspects:</p><p>• Most importantly, unlike our proposed method, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> do not explicitly learn to predict surface normals from a single image. Instead, these methods estimate the normals from the predicted depths and propose to regularize them to iteratively refine the depth predictions. This amounts to imposing a hard constraint on the normals to be the function of depth, limiting the normal accuracy, since the normals computed from depth are bound to have severe depth discretization artifacts and are very noisy. We show in our experiments that the combination of a dedicated network for normals and a soft constraint between inverse depths and normals leads to better predictions. • Both <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> propose to regularize second order depth discontinuity along with the normal discontinuity which is redundant. We show that no additional prior on depth is required for learning state-of-the-art normals. • Both <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> use monocular setup for training whereas we use stereo information to produce metric visual odometry given a frame pair addressing depthtranslation scale ambiguity. • We introduce a depth and normal consistency term over time and penalize the inconsistent depth and normal predictions for two consecutive frames of the video sequences during training. This leads to improvement in accuracy of the estimations. In summary, the proposed method to the best of our knowledge, is the first self-supervised framework to jointly train for single view depth, single view normal, and two frame visual odometry prediction without depth/translation scale ambiguity. Our system produces state-of-the-art surface normal estimation and significantly better depth prediction compared to the corresponding depth smoothness based selfsupervised framework on KITTI benchmark dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>Depth estimation from a monocular color image is a longstanding problem in computer vision. Early work mainly aimed at estimating surface normals from a single image and in turn integrated them to form depth maps <ref type="bibr" target="#b12">[13]</ref>. Normals were estimated using shape from shading <ref type="bibr" target="#b13">[14]</ref>, shape-fromdefocus <ref type="bibr" target="#b14">[15]</ref> and other low-level image features <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref> or perspective geometry based reasoning using vanishing points and lines <ref type="bibr" target="#b17">[18]</ref>. In contrast, learning based approaches such as <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, especially those using neural nets <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref> in recent times have achieved state-of-the art performance in depth/surface normal prediction. In the following section we give a brief overview of both supervised and unsupervised methods for learning to predict geometry from single image, the latter of which is more relavent to this work.</p><p>Supervised learning of single-view geometry Supervised learning based methods for depth prediction rely upon availability of sensory depth data, such as that acquired by the Kinect sensor and LIDAR sensor (for indoor and outdoor scenes respectively). One of the early learning based methods to achieve reasonable success in single image depth estimation is Make3D <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref> which relied upon a set of handcrafted features to map patches from the input image to some feature space, and in turn learn to regress from that feature space to depth values. They also use an additional pairwise depth smoothness prior, modeled as either a Gaussian or Laplacian distribution with a data dependent learned variance, in order to regularize and globally optimize for a depth map, with the belief that the process of recovering a depth map requires global reasoning on the image.</p><p>Recently, deep learning-based methods dominate this area <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>. For example, <ref type="bibr" target="#b26">[27]</ref> train a multi-scale Convolutional Neural Network, operating at coarse and fine image resolutions, to regress a depth map from a single image, and in <ref type="bibr" target="#b20">[21]</ref> they extend their network to a three-scale architecture and regress for depth maps, normal maps, and semantic labels in real-time from a single image. The semantic label maps were predicted from a single RGB-D image as the additional depth channel improved results. In <ref type="bibr" target="#b29">[30]</ref> the latter work was extended to jointly predict depth, surface normals and surface curvature, which improved the results of all three tasks.</p><p>Liu et al. <ref type="bibr" target="#b30">[31]</ref> proposed to formulate depth estimation as a deep continuous Conditional Random Fields (CRF) learning problem. Given the continuous nature of the depth values, they learn the unary depth values and weightings for the pairwise smoothness potential functions via CNNs in an endto-end framework. <ref type="bibr" target="#b31">[32]</ref> used a fully convolutional network architecture based on ResNet <ref type="bibr" target="#b32">[33]</ref> with a novel upsampler for decoding the depth map at input resolution. Kendall et al. <ref type="bibr" target="#b28">[29]</ref> adapted the DenseNet architecture for several regression tasks including depth prediction, and showed that jointly predicting pixelwise depths and confidences, where the output is modeled as a multivariate Gaussian distribution, improves depth estimation results. <ref type="bibr" target="#b33">[34]</ref> combined shallow convolutional networks with regression forests to reduce the need for large training sets. Recently in <ref type="bibr" target="#b34">[35]</ref>, it was proposed that sharper predictions at depth boundaries can be achieved by emphasizing local depth error gradients. This same phenomenon was observed in <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b35">[36]</ref> that also emphasized local depth errors during training. Our inversedepth normal consistency terms also emphasizes local depth errors based on the predicted normals and achieves a similar effect but in a more implicit unsupervised fashion (i.e. with no dedicated sensory depth data).</p><p>Self-supervised learning of single-view geometry Recent work have started to incorporate multi-view geometry based loss functions for depth regression resulting in a self-supervised learning framework for inferring depth from single image. This stream of work aims to replace the more explicit sensory-data based ground truth supervision with a good image alignment loss between different views observing the same scene (by using stereo data or monocular videos for supervision). The ubiquity and relatively low price of RGB cameras and availability of large datasets of recorded videos increase the attractiveness of this approach.</p><p>Using stereo pairs for training, <ref type="bibr" target="#b2">[3]</ref>[5] deploy an autoencoder like framework where the authors propose to predict the disparity (inverse depth) of the left image, using which the right image of the stereo pair can be warped to synthesize the left image. The photometric difference between the input (left) image and the warped image is minimized to train the single view depth predictor. An inverse depth smoothness prior on the predicted depths is used to regularize the solution, encouraging piece-wise smooth depth maps. <ref type="bibr" target="#b5">[6]</ref> extended the above framework to jointly estimate depth and ego-motion using monocular videos -upto a scale. Methods like <ref type="bibr" target="#b6">[7]</ref> <ref type="bibr" target="#b7">[8]</ref> proposed to combine the advantages of using both spatial and temporal information available in KITTI sequences for improving depth predictions while solving the scaling ambiguity issue. A large body of work since have been targeted to use better loss functions, in particular the image alignment loss <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b8">[9]</ref> propose to use SSIM and GANs respectively for image matching. Enforcing temporal consistency in the predicted depths by aligning the backprojected depth maps via differentiable approximation of ICP has been studied in <ref type="bibr" target="#b36">[37]</ref>.</p><p>While most of the self supervised approaches have mainly focused on getting accurate depth-maps, little attention has been devoted to use other scene representations. We are aware of two recent works <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> which incorporate the surface orientation (normal) estimation for single view geometric understanding. Similar to <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b10">[11]</ref>[12] learn depth from monocular sequences using a self-supervised photometric loss but additionally they compute surface normals from the predicted depths using a weighted mean cross product <ref type="bibr" target="#b37">[38]</ref>. They propose to regularize the inverse depths and the normals computed from the depth predictions simultaneously. We believe that this is redundant and a separate normal prediction is beneficial then relying on the normals to be computed from predicted depth. The differences between our work and <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref> is detailed in section I and a extensive comparison of the proposed work with these methods is described in section IV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. FRAMEWORK FOR JOINT LEARNING OF DEPTHS AND SURFACE NORMALS</head><p>In this section, we present our system which consists of three CNNs. One each for per-pixel single-view depth prediction, for single view surface normal prediction and a pose-net which takes two images -consecutive images of a monocular video -as input to predict the camera motion (vehicle's egomotion in KITTI) between these two frames in metric units. Our system is trained in a self-supervised manner, which means no ground truth data (depths or surface normals) is required for training. Instead we use stereo sequences for training for depths and surface normals from a single image where two consecutive stereo-pairs</p><formula xml:id="formula_0">{I t L , I t R }, {I t−1 L , I t−1 R } form a single training instance.</formula><p>The goal is to predict the depth map D and surface normal mapN of I t L (the left image at time t) which we define as the reference image I for a particular training instance. At the same time we also want to predict T t→t−1 which is the relative pose (ego-motion) between the left/right image at time t and the left/right image at time t − 1.</p><p>Our proposed loss function to train these three networks jointly consists of six terms:</p><formula xml:id="formula_1">λ 1 L P + λ 2 L DN + λ 3 L N + λ 4 L N S + λ 5 L DC + λ 6 L N C ,<label>(1)</label></formula><p>where the λ's are the relative weights for losses used for training. L P denotes the photometric alignment cost involv-ing the scene's depth observed by the left camera at time t and t − 1 with the estimated egomotion, L DN enforces the estimated depths and normals to be consistent, L N enforces the predicted normals to face the camera and L N S is a smoothness prior which favors the predicted normals to be piece-wise smooth. Additionally, assuming the scene is rigid, two temporal geometric consistency terms L DC and L N C enforce the estimated depths and normals at the two time instances to be consistent given the egomotion. Each of these terms are elaborated in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Enforcing Multi-View Photometric Consistency</head><p>We enforce multi-view photometric consistency (both spatially and temporally) by minimizing the photometric cost that aligns image I t L and I t R (the left and right stereo images at time t) as well as the photometric cost that aligns image I t L and I t−1 L (the left images at time t and t − 1). It follows that the photometric loss based on dense image alignment can be written as follows,</p><formula xml:id="formula_2">L P = p |I t L (p) − W (I t R , p )| + |I t L (p) − W (I t−1 L , p )| (2)</formula><p>where, W (I t R , p ) and W (I t−1 L , p ) are the synthesized left images reconstructed from I t R and I t−1 L respectively. W (., .) is a differentiable bilinear interpolation function <ref type="bibr" target="#b38">[39]</ref> for indexing into a particular non-integer location in a given input image. The corresponding pixel p in I t R for a pixel p in I t L is defined by the camera intrinsics, K; the known stereo baseline, T L→R ; and the depth map, D as follows,</p><formula xml:id="formula_3">p = KT L→R D(p)K −1 p<label>(3)</label></formula><p>where p represents an integer pixel location in I t L (the reference image) in homogeneous coordinates. Similarly, the corresponding pixel p in I t−1 L for a pixel in p in I t L is given by, p = KT t→t−1 D(p)K −1 p</p><p>The relative camera transformation, T t→t−1 , is represented by 6 parameters in se3, and consists of rotation and translation components. Note that the depth CNN predicts inverse depth D inv which is better constrained, e.g. sky in infinite depth is zero is inverse depth. The depth is converted from our predicted inverse depth as follows: D = 1/(D inv + 10 −4 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Normal Smoothness Prior</head><p>Relying on photometric loss described in the previous section for learning depth suffers from well known aperture problem. A single pixel from a uniformly colored area in a image can be matched to many pixels with similar color intensity in the next view making the depth estimation ambiguous. To counter the problem, previous works <ref type="bibr" target="#b2">[3]</ref> [5] <ref type="bibr" target="#b7">[8]</ref> adopt an inverse depth/disparity smoothness as a prior. However, as explained in the introduction such disparity smoothness assumption is not very realistic and strong regularization of disparity discontinuity leads to fronto-parallel artifacts in the predictions. In this work we rely on smooth normal assumption whereby we apply edge-aware regularization to the discontinuities in the predicted surface normal by minimizing:</p><formula xml:id="formula_5">L N S = p |∂ xN (p)|e −|∂xI(p)| + |∂ yN (p)|e −|∂yI(p)| (5)</formula><p>where ∂ x (.) and ∂ y (.) are gradients in horizontal and vertical direction respectively. Similar to previous works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b7">[8]</ref>, we assume that the image edges are very good indicator of scene discontinuity, however we use them to guide the normal smoothness. Eqn. (5) allows for normal discontinuities at the areas where strong image gradient is present while penalizing the normal discontinuities in the homogeneous regions of the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Depth -Normal Consistency</head><p>For enforcing consistency of predicted depth with predicted surface normals, we use the inverse-depth-normal consistency term proposed in <ref type="bibr" target="#b39">[40]</ref> as a soft constraint in the form of a loss for training our networks. This loss is based on the geometric relationship between the predicted normal N(p) of the scene corresponding to point p in the reference image, its predicted depth D(p) and the predicted depth D(q) of p's neighbour q. The depth-normal consistency term can be written as:</p><formula xml:id="formula_6">N (p), D(q)X(q) − D(p)X(p) = 0<label>(6)</label></formula><p>where , is the dot product operator andX(p) = K −1 p, X(q) = K −1 q. Note thatN(p) is normalized to have unit magnitude and is expressed in Cartesian coordinates. Eqn. (6) can be simplified as:</p><formula xml:id="formula_7">D(q) N (p),X(q) − D(p) N (p),X(p) = 0<label>(7)</label></formula><p>By dividing the above equation by D(p)D(q) we get:</p><formula xml:id="formula_8">D inv (p) N (p),X(q) − D inv (q) N (p),X(p) = 0 (8)</formula><p>Finally, we minimize the following energy L DN , penalizing inconsistency between predicted inverse depths and normals:</p><formula xml:id="formula_9">L DN = G(p) p,q∈N (p) |D inv (p)c pq − D inv (q)c pp | (9) c pq = N (p),X(q) , c pp = N (p),X(p)<label>(10)</label></formula><p>In our experiments, the neighbourhood N (p) comprises just the pixel itself and its two neighbours immediately below and to the right. The image-edge based weight G(p) = e −α|∇I(p)| β 2 reduces regularization at image edges, under the assumption that these regions align with depth discontinuities. α and β are tunable parameters, which we use [α, β] = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b0">1]</ref>.</p><p>It is easy to note that, in the special case when c pq = c pp = −1, i.e. the normalN = (0, 0, −1) T is pointed directly at the camera, Eqn. (9) reduces to the traditionally used inverse depth smoothness prior in unsupervised learning methods such as <ref type="bibr" target="#b2">[3]</ref>[5] <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Fixing Surface Normal Direction Ambiguity</head><p>It is to be noted that the surface normal prediction network in our framework only rely upon the depth-normal consistency loss L DN . However, normals estimated from the depth maps using Eqn. (6) have directional ambiguity. i.e. given the depth map the computed surface normal can face the camera or be in the opposite direction. To fix this ambiguity, we first compute an approximated surface normalN c (p) from the predicted depth using mean cross product 1 and then penalize the deviation of the predicted normalsN(p) from these approximated normals my minimizing: </p><formula xml:id="formula_10">L N = 1 2N p ||N(p) −N c (p)|| 2 2 .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Enforcing Temporal Consistency of Predicted Geometry</head><p>Finally, while the accumulated loss terms defined in sections III-A-III-D makes the simultaneous learning of depth, normal and egomotion well posed, we enforce temporal consistency in our predictions to improve the accuracy of our predictions. Using the rigid scene assumption as the cameras move in space over time we want the predicted depths and normals at time t to be consistent with the respective predictions at time t − 1. This is done by correctly transforming the scene geometry (inverse depth and normal maps) from frame t to frame t − 1 much like the image warping over time as described in section III-A. In particular, we define two consistency-terms L DC and L N C :</p><formula xml:id="formula_11">L DC = p |D t inv (p) − W (D t−1 inv , p )| (12) L N C = p |N t (p) − W (N t−1 , p )|<label>(13)</label></formula><p>where,</p><formula xml:id="formula_12">[X t−1 (p), Y t−1 (p), D t−1 (p)] = T −1 t→t−1 D t−1 (p)K −1 p (14) N t−1 (p) = R −1 t→t−1N t−1 (p).<label>(15)</label></formula><formula xml:id="formula_13">D t−1 (p) = 1/D t−1 inv (p) andN t−1</formula><p>(p) in the above two equations are the transformed depths and normals from frame t − 1 to frame t (based on the predicted pose T t→t−1 where R t→t−1 is the rotation component) for use in L DC and L N C .</p><p>To further clarify, it is important to bring the depth/normals which are estimated in the camera reference frame at any given time step into the current reference frame before applying consistency of depth/normal over time. For depth consistency we transform the estimated 3D points (backprojection of the depth map) at time t − 1 to the camera reference frame of time t using the estimated pose T t→t−1 before warping. Similarly, the normals need to transformed from one reference frame to the other using the estimated rotation R t→t−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Network Architecture</head><p>For both the depth CNN and normal CNN, we use the same architecture except for the last prediction layer, which has a 1-channel output for the depth CNN while a 3-channel output for the normal CNN. Following previous works <ref type="bibr" target="#b2">[3]</ref>[5] <ref type="bibr" target="#b7">[8]</ref>, we use a fully convolutional neural network with skip-connections. The network consists of an encoder and a decoder. For the encoder, we use the ResNet50 <ref type="bibr" target="#b40">[41]</ref> variant (ResNet50-1by2), which has much less learnable parameters and also faster computation speed. For the decoder network, we have studied two architectures, including the simple bilinear upsampler used in <ref type="bibr" target="#b2">[3]</ref>[8] and bilinear upsampling with convolutions, i.e. upsample coarser feature maps, concatenate with features in ResNet50-1by2 (skip connection), and apply convolution (including batch normalization, and ReLU activation). Our experiments show that although both upsamplers have similar quantitative performance for depth estimation as observed in <ref type="bibr" target="#b7">[8]</ref>, the latter architecture is able to produce sharper predictions, especially useful for predicting surface normals. We use sigmoid activation at the end of the depth and normal network. We additionally apply a L2normalization on the surface normals to get unit normals. For the pose network, we use the one proposed in <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS AND EVALUATIONS</head><p>In this section we describe the details of our experimental evaluation of our proposed framework. We evaluate our framework on KITTI dataset <ref type="bibr" target="#b41">[42]</ref> <ref type="bibr" target="#b42">[43]</ref> and compare with prior art on both depth esimtation and surface normal estimation. Moreover, we present an ablation study to show the contribution of each component in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>We train our CNNs with the Caffe <ref type="bibr" target="#b43">[44]</ref> framework. Adam optimizer <ref type="bibr" target="#b44">[45]</ref> is used with the following settings, [β 1 , β 2 , ] = [0.9, 0.999, 10 −8 ]. The initial learning rate is 0.001 and is decreased to one tenth manually when the training loss converges. We found the loss weightings for Eqn. (1) via grid search and by referring to previous loss weightings adopted by <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b7">[8]</ref>, and are as follows: [λ 1 , λ 2 , λ 3 , λ 4 , λ 5 , λ 6 ] = [1, 13, 1, 0.7, 1, 0.01].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Depth Evaluation</head><p>We evaluate our depth estimation result on the Eigen split provided by <ref type="bibr" target="#b26">[27]</ref> for fair comparison with prior works. We follow the same depth evaluation protocol as in <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b4">[5]</ref>. Our results for depth estimation using our full framework (Stereo+Normal+Temporal) are presented in <ref type="table">Table I</ref> and is compared against relevant prior works and our own 'Baseline' network that are all trained using the photometric loss with inverse depth smoothness prior. Our full method performs best in all measures compared to all the baselines. This reaffirms the importance of the more realistic normal smoothness prior over the traditional inverse-depth smoothness prior.</p><p>We also compare against our method without temporal photometric and geometric (depth and normal) consistency (Stereo+Normal) which leads to less accurate results as seen in <ref type="table">Table I</ref>, signifying the importance of temporal geometric consistency especially for reconstructing far points in the scene.</p><p>In <ref type="figure" target="#fig_1">Fig. 2</ref> it is apparent that the depth maps predicted using our proposed framework are superior, particularly to reconstruct the road (Rows 1-3) and building in Row 3 when compared with the Baseline approach, and other prior works. While the results of <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b10">[11]</ref> are blurry our proposed methods is able to retain sharp edges with correct reconstruction of non-fronto parallel planes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Surface Normal Evaluation</head><p>There is no surface normal ground truth available in KITTI dataset. In particular, the depth ground truth in Eigen split provided by KITTI raw dataset is very sparse and unsuitable to generate surface normal. To have better evaluation on surface normal prediction, <ref type="bibr" target="#b10">[11]</ref>[12] use KITTI's official stereo split which contains 200 high quality disparity images from which reasonably high quality surface normals can be generated for evaluation. Following <ref type="bibr" target="#b10">[11]</ref>[12], we use the KITTI split for surface normal evaluation and inpaint the depth ground truth following the approach used in <ref type="bibr" target="#b0">[1]</ref>. We use the mean cross product to generate surface normal ground truth from these inpainted depths. As, 92 out of 200 images in KITTI split are used in the training set of Eigen split, we only use the remaining 108 images for evaluation. Moreover, we follow the depth evaluation protocol <ref type="bibr" target="#b4">[5]</ref> that only use the centre part of the prediction and evaluate normals only at the pixels where the ground truth depths exist to reduce the normal errors introduces due to inpainting.</p><p>The quantitative evaluation of unsupervised normal prediction frameworks is presented in <ref type="table">Table II</ref>, where we compare against surface normals estimated via different methods. The bottom-most row (Stereo+Normal+Temporal (CNN)) shows our best result. This is the normal predicted by the Normal CNN using our full loss function. We show that using inverse depth-normal consistency (Stereo+Normal) gives better surface normals than the inverse depth smoothness (Baseline) and the result is further improved by using temporal geometric consistency. On the bottom part of the table, '-(CNN)' are the surface normals predicted from CNNs. We can see that the surface normals predicted from the CNNs are better than the corresponding results which are computed from predicted  <ref type="bibr" target="#b4">[5]</ref>. depths in all cases, signifying the importance of a dedicated Normal CNN. <ref type="figure" target="#fig_1">Fig. 2 (bottom)</ref> compares the normals predicted by our framework with that of <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b45">[46]</ref> and our 'Baseline' approach (where the normals are computed using the mean cross product rule). It can be easily seen that while the inverse depth smoothness regularization produces detailed but very noisy normal maps, our predicted normals are of a significantly high quality preserving the normal edges while being largely smooth/constant on the smooth objects/road and buildings.</p><p>It is important to note that unlike the proposed method, our 'Baseline', <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref> do not have an explicit normal prediction network which allows deviation from the predicted depths. Our claim is that a dedicated network and a soft constraint allowing deviation from depth normal consistency is critical in good normal estimation performance. A clear visual indication is shown in <ref type="figure">Fig. 3</ref> where we compare the predicted normalsN by our Normal CNN with the oneŝ N c which are computed via the mean cross product of the corresponding depth predictions coming from our joint training framework. It can be seen that even after the joint training of depths and normals, computing the normals from the predicted depth leaves us with very noisy undesirable output. This effect is additionally quantified in <ref type="table">Table II</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this work we have proposed to simultaneously learn a single view depth and normal prediction network, along with a 2-frame visual odometry network in a self-supervised manner from stereo sequence training data. We show that these three networks can be learned together using a soft depth-normal consistency constraint while assuming the surface normals to be piece-wise smooth, to give state of the art surface normal predictions and significantly improved depth predictions when compared to prediction reliant on inverse-depth smoothness prior currently prevalent for selfsupervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. ACKNOWLEDGEMENT</head><p>This work was supported by the UoA Scholarship to HZ, the ARC Laureate Fellowship FL130100102 to IR and the Australian Centre of Excellence for Robotic Vision CE140100016.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>All authors are with the School of Computer Science, at the University of Adelaide, and Australian Centre for Robotic Vision Our test-time setup where depths and surface normals are predicted from a single image, and ego-motion is predicted from two views. At traintime, all three networks are trained in a self-supervised manner from stereo image sequence data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Qualitative comparison of depths and surface normals between different methods. The ground truth (GT) depths are inpainted from sparse LIDAR ground truth depths. The ground truth surface normals are computed from the inpainted ground truth depths, and are not reliable for all the points (especially the upper part of the images where the LIDAR depths are missing).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Fig. 3. Qualitative comparison between surface normals computed from CNN depths (Stereo+Normal+Temporal) and surface normals predicted from the Normal CNN, showing the importance of having a dedicated Normal CNN. Left: Groundtruth (GT); Middle: Computed normals from predicted depths; Right: Predicted normals.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>SUPERVISED DEPTH ESTIMATION PERFORMANCE COMPARISON ON KITTI DATASET (TRAIN&amp;TEST ON EIGEN SPLIT, 80M CAP). THE RESULTS ARE EVALUATED ON CROPPED REGION USED IN [5]. OUR ABLATION STUDY IS PRESENTED ON THE BOTTOM OF THE TABLE.</figDesc><table><row><cell>Method</cell><cell>Supervision</cell><cell></cell><cell cols="2">Error metric</cell><cell></cell><cell></cell><cell>Accuracy metric</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Abs Rel</cell><cell cols="2">SqRel RMSE</cell><cell>RMSE log</cell><cell cols="2">δ &lt; 1.25 δ &lt; 1.25 2</cell><cell>δ &lt; 1.25 3</cell></row><row><cell>Train set mean</cell><cell>Depth</cell><cell>0.361</cell><cell>4.826</cell><cell>8.102</cell><cell>0.377</cell><cell>0.638</cell><cell>0.804</cell><cell>0.894</cell></row><row><cell>Zhou et al. [6]</cell><cell>Mono.</cell><cell>0.208</cell><cell>1.768</cell><cell>6.856</cell><cell>0.283</cell><cell>0.678</cell><cell>0.885</cell><cell>0.957</cell></row><row><cell>Yang et al.[11]</cell><cell>Mono.</cell><cell>0.182</cell><cell>1.481</cell><cell>6.501</cell><cell>0.267</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Yang et al.[12]</cell><cell>Mono.</cell><cell>0.162</cell><cell>1.352</cell><cell>6.276</cell><cell>0.252</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Garg et al. [3]</cell><cell>Stereo</cell><cell>0.152</cell><cell>1.226</cell><cell>5.849</cell><cell>0.246</cell><cell>0.784</cell><cell>0.921</cell><cell>0.967</cell></row><row><cell>Godard et al. [5]</cell><cell>Stereo</cell><cell>0.148</cell><cell>1.344</cell><cell>5.927</cell><cell>0.247</cell><cell>0.803</cell><cell>0.922</cell><cell>0.964</cell></row><row><cell>Stereo+Inv.Depth.Smoothness (Baseline)</cell><cell>Stereo</cell><cell>0.150</cell><cell>1.409</cell><cell>5.800</cell><cell>0.249</cell><cell>0.800</cell><cell>0.923</cell><cell>0.964</cell></row><row><cell>Stereo+Normal</cell><cell>Stereo</cell><cell>0.135</cell><cell>1.194</cell><cell>5.718</cell><cell>0.237</cell><cell>0.816</cell><cell>0.929</cell><cell>0.968</cell></row><row><cell>Stereo+Normal+Temporal</cell><cell>Stereo</cell><cell>0.133</cell><cell>1.083</cell><cell>5.580</cell><cell>0.229</cell><cell>0.816</cell><cell>0.932</cell><cell>0.971</cell></row><row><cell></cell><cell></cell><cell cols="2">TABLE I</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SELF-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>EVALUATED ON KITTI SPLIT (108/200 SAMPLES, EXCLUDING 92 SAMPLES IN EIGEN SPLIT). WE EVALUATED ON CENTRE CROPPED REGION AS DEPTH EVALUATION IN</figDesc><table><row><cell>Method</cell><cell cols="2">Error metric</cell><cell cols="3">Accuracy metric</cell></row><row><cell></cell><cell>Mean</cell><cell>Median</cell><cell>11.25 •</cell><cell>22.5 •</cell><cell>30 •</cell></row><row><cell>Yang et al.[11]</cell><cell>37.44</cell><cell>24.32</cell><cell>0.275</cell><cell>0.477</cell><cell>0.560</cell></row><row><cell>Yang et al. [12]</cell><cell>35.69</cell><cell>22.33</cell><cell>0.293</cell><cell>0.502</cell><cell>0.585</cell></row><row><cell>Baseline (Computed)</cell><cell>36.03</cell><cell>24.00</cell><cell>0.283</cell><cell>0.481</cell><cell>0.565</cell></row><row><cell>Stereo+Normal (Computed)</cell><cell>33.43</cell><cell>21.15</cell><cell>0.305</cell><cell>0.519</cell><cell>0.607</cell></row><row><cell>Stereo+Normal+Temporal (Computed)</cell><cell>32.01</cell><cell>20.17</cell><cell>0.319</cell><cell>0.534</cell><cell>0.622</cell></row><row><cell>Stereo+Normal (CNN)</cell><cell>30.37</cell><cell>19.13</cell><cell>0.335</cell><cell>0.551</cell><cell>0.640</cell></row><row><cell>Stereo+Normal+Temporal (CNN)</cell><cell>30.23</cell><cell>19.11</cell><cell>0.336</cell><cell>0.551</cell><cell>0.638</cell></row><row><cell cols="2">TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SURFACE NORMAL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It should be noted that this approximation was used in<ref type="bibr" target="#b10">[11]</ref> to compute normals from depth maps.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Nathan Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>B G</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="740" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Determining optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">K</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Schunck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="185" to="203" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Undeepvo: Monocular visual odometry through unsupervised deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.06841</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of monocular depth estimation and visual odometry with deep feature reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Generative adversarial networks for unsupervised monocular depth prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Aleotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mattoccia</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Unsupervised learning of geometry with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Lego: Learning edge with geometry all at once by watching videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Normal Integration -Part I: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Durou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Quéau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Aujol</surname></persName>
		</author>
		<ptr target="https://hal.archives-ouvertes.fr/hal-01334349" />
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Shape from Shading: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-S</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Cryer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analaysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="690" to="706" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Depth from focus with your mobile phone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Suwajanakorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3497" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic photo pop-up</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM transactions on graphics (TOG)</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="577" to="584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Recovering surface layout from an image</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="172" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Single view metrology,&quot; in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Proceedings of the Seventh IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="434" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discriminatively Trained Dense Surface Normal Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladický</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8693</biblScope>
			<biblScope unit="page" from="468" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Data-driven 3D primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3392" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2650" to="2658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Designing Deep Networks for Surface Normal Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Marr revisited: 2d-3d alignment via surface normal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5965" to="5974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pixelnet: Towards a general pixel-level architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06694</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Make3d: Learning 3d scene structure from a single still image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="824" to="840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2366" to="2374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="842" to="857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">What uncertainties do we need in bayesian deep learning for computer vision?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5580" to="5590" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Joint prediction of depths, normals and surface curvature from rgb images using cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dharmasiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Spek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Drummond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1505" to="1512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="2024" to="2039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2016 Fourth International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="239" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5506" to="5514" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Revisiting single image depth estimation: Toward higher resolution maps with accurate object boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ozay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Okatani</surname></persName>
		</author>
		<idno>abs/1803.08673</idno>
		<ptr target="http://arxiv.org/abs/1803.08673" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5038" to="5047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from monocular video using 3d geometric constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahjourian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Angelova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05522</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Using cross-product matrices to compute the svd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerical Algorithms</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="61" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dense monocular reconstruction using surface normals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Latif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2017-05" />
			<biblScope unit="page" from="2524" to="2531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Unsupervised learning of geometry with edge-aware depth-normal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03665</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
