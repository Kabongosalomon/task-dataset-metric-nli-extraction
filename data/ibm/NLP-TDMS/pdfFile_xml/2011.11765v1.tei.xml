<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting Contrastive Self-Supervised Learning with False Negative Cancellation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Huynh</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maryam</forename><forename type="middle">Khademi</forename><surname>Google</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">TTI-Chicago</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Chicago</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting Contrastive Self-Supervised Learning with False Negative Cancellation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Self-supervised representation learning has witnessed significant leaps fueled by recent progress in Contrastive learning, which seeks to learn transformations that embed positive input pairs nearby, while pushing negative pairs far apart. While positive pairs can be generated reliably (e.g., as different views of the same image), it is difficult to accurately establish negative pairs, defined as samples from different images regardless of their semantic content or visual features. A fundamental problem in contrastive learning is mitigating the effects of false negatives. Contrasting false negatives induces two critical issues in representation learning: discarding semantic information and slow convergence. In this paper, we study this problem in detail and propose novel approaches to mitigate the effects of false negatives. The proposed methods exhibit consistent and significant improvements over existing contrastive learning-based models. We achieve new state-of-the-art performance on ImageNet evaluations, achieving 5.8% absolute improvement in top-1 accuracy over the previous state-of-the-art when finetuning with 1% labels, as well as transferring to downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With the universal adaptability of deep neural networks, representation learning has become the backbone of most modern AI agents, in which good pretrained representations have proven essential to improving performance on downstream tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b26">27]</ref>. While conventional approaches use labeled data to pretrain visual representations, there has been a recent surge in self-supervised representation learning <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b29">30]</ref>. In fact, selfsupervised visual representation learning has been closing the gap with, and in some cases even surpassing its supervised counterpart <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10]</ref>. Notably, most state-of-theart self-supervised visual representation learning methods are converging around, and fueled by, the central concept of contrastive learning <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>In contrastive learning, the embedding space is governed by two opposing forces, the attraction of positive pairs and repellence of negative pairs, effectively actualized through the contrastive loss. Without labels, recent breakthroughs in self-supervised visual representation learning rely on the instance discrimination task in which positive pairs are defined as different views of the same image, while negative pairs are formed by sampling views from different images, regardless of their semantic information <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>. Figure 1 illustrates this process. Positive pairs generated from different views of the same image are generally accurate since they are likely to carry similar semantic content or visual features. However, the creation of valid negative pairs is far more difficult. The common approach of defining negative pairs as samples from different images ignores their semantic content. For example, two images of a dog are considered a negative pair, as demonstrated in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>By contrasting these undesirable negative pairs, the architecture is encouraged to discard their common features in the learned embedding, which are indeed the common semantic content, e.g., dog features in the previous example. We define those undesirable negative samples as false negatives, i.e., negative pairs from the same semantic category. Besides disregarding the semantic information, false negatives also hinder the convergence of contrastive learningbased objectives due to the appearance of contradicting objectives. For instance, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the dog's head on the left is attracted to its fur (positive pair), but is repelled from similar fur of another dog image on the right (negative pair), creating contradicting objectives.</p><p>While recent efforts focus on improved architectures <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22]</ref> and data augmentation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">43]</ref>, relatively little work considers the effects of negative samples, especially that of false negatives. Most existing methods focus on mining hard negatives <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b24">25]</ref>, or most recently, increasing positive samples to counter-balance the negatives <ref type="bibr" target="#b11">[12]</ref>. However, there has been little effort to identify false negatives.</p><p>False negatives remain a fundamental problem in contrastive self-supervised learning. Without labels, this problem is very hard to adequately resolve, as it boils down to a chicken-and-egg problem, where we want to learn good semantic representations, but may need certain semantic information to start with. Nevertheless, in this paper, we attempt to study this problem in detail and propose novel ideas to overcome its limitations, as overviewed in <ref type="figure" target="#fig_1">Figure  2</ref>. Particularly, the contributions of the paper are as follows:</p><p>• We propose simple yet effective strategies to find potential false negatives in contrastive learning. Without any labels, the proposed method effectively finds false negatives with ∼40% accuracy among 1000 humandefined semantic categories on ImageNet <ref type="bibr" target="#b39">[40]</ref>. Our work is the first to address this problem. • We propose and study the effect of two different strategies to improve the contrastive loss based on the estimated false negatives, namely, false negative elimination and attraction. • We show that the proposed methods consistently and significantly improve over existing contrastive learning-based approaches across a wide range of settings, e.g., with or without momentum contrast <ref type="bibr" target="#b21">[22]</ref>. Further, we show that our methods are effectively complementary with the recent multi-crop augmentation strategy in terms of both accuracy and computational </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Early work on self-supervised learning employs proxy tasks to guide the learned embeddings, such as predicting the angle of a rotated image <ref type="bibr" target="#b18">[19]</ref>, the relative location of patches <ref type="bibr" target="#b14">[15]</ref>, or organizing shuffled patches to recover the original image much like solving a jigsaw puzzle <ref type="bibr" target="#b35">[36]</ref>. Other proxy tasks include recovering an image from a corrupted version <ref type="bibr" target="#b44">[45]</ref>, predicting part of the image from context <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b7">8]</ref> or generating one view of an image from another, e.g., split-brain auto-encoder <ref type="bibr" target="#b50">[50]</ref> or colorization <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b29">30]</ref>. While these approaches have been effective, the proxy tasks are fairly heuristic and lack generality.</p><p>Other works use clustering-based methods for selfsupervised representation learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b46">47]</ref>. Caron et al. <ref type="bibr" target="#b5">[6]</ref> iteratively improve the learned representations by clustering samples (e.g., k-means) and using these clusters as pseudo-labels. They then train the network to classify samples based on these pseudo-labels. Our attraction-based approach to false negative cancellation is similar to clustering methods, as they both attempt to group visually connected samples. However, the formulation and context differs.</p><p>We take a contrastive learning-based approach to selfsupervised representation learning. Earlier work in this area includes CPC <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b22">23]</ref>, Deep InfoMax <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b1">2]</ref>, and contrastive multiview coding <ref type="bibr" target="#b41">[42]</ref>. Recognizing that contrastive loss requires a large set of negative samples, PIRL <ref type="bibr" target="#b33">[34]</ref> maintains a memory bank of previous representation of all images, which limits scalability. MoCo v1 <ref type="bibr" target="#b21">[22]</ref> addresses this problem by maintaining a momentum encoder and a limited queue of previous samples. SimCLR v1 <ref type="bibr" target="#b8">[9]</ref> eschews a momentum encoder in favor of a large batch size, and proposes updates to the projection head and data augmentation.</p><p>Realizing the important role of negative samples in contrastive learning, a few recent methods have investigated ways to improve negative sampling. These include increasing the sampling of hard negatives (i.e., those that are close to the anchor) <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b24">25]</ref>, and using more positive samples to counter-balance the effects of undesirable negatives <ref type="bibr" target="#b11">[12]</ref>.</p><p>Inspired by contrastive learning, recent methods seek to predict one positive view from another. However, they are not categorized as contrastive learning as they have different formulations at their core and do not contrast against negative samples, a defining element of contrastive learning. SwAV <ref type="bibr" target="#b6">[7]</ref> is an online clustering-based method that employs swapping prediction of different views from an image, while BYOL <ref type="bibr" target="#b20">[21]</ref> employs a momentum encoder as a prediction target for another view from the main encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Contrastive Learning</head><p>The goal of contrastive learning is to learn a transformation that brings positive pairs "nearby" in an embedding space while pushing negative pairs apart. This is done by minimizing a contrastive loss that, for each anchor image i, measures the (negative) similarity between its embedding z i and that of its positive match z j relative to the similarity between the anchor embedding of k ∈ {1, . . . , M } negative matches:</p><formula xml:id="formula_0">l i = − log exp(sim(z i , z j )/τ ) M k=1 1 [k =i] exp(sim(z i , z k )/τ ) ,<label>(1)</label></formula><p>where sim(u, v) is a similarity function, e.g., the L 2 normalized cosine similarity sim(u, v) = u T v/ u v , and τ is a temperature parameter. Without known correspondence, self-supervised methods commonly define positive pairs as different augmentations of the same image and negative pairs as samples from different images. Consider a batch of N images, each augmented to form N positive pairs for a total of 2N images. Methods like SimCLR use samples from the same batch as negative pairs (i.e., M = 2N examples for each anchor i), while MoCo uses samples from a momentum encoder to avoid the use of a large batch size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">False Negative Cancellation</head><p>Regardless of whether the negative samples are drawn from the same batch or the output of a momentum encoder, they are samples from different images that may or may not possess similar semantic content or visual features. Consequently, it is possible that some samples k have the same semantic content as the anchor i, and are thus false negatives. As discussed earlier, false negatives give rise to two critical problems in contrastive learning: they discard semantic information and slow convergence.</p><p>Our method seeks to mitigate these problems. In particular, we propose a strategy to identify false negatives and an approach that uses these false negatives to improve contrastive learning. The following discussion describes how we might incorporate knowledge of false negatives in contrastive learning. We discuss the process of identifying false negatives in Section 3.2.3.</p><p>Supposing we can find the false negatives, we propose two strategies that use them to improve contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">False Negative Elimination</head><p>The simplest strategy for mitigating the effects of false negatives is to not contrast against them. This amounts to the following modification to the contrastive objective <ref type="formula" target="#formula_0">(1)</ref>:</p><formula xml:id="formula_1">l elim i = − log exp(sim(z i , z j )/τ ) 2N k=1 1 [k =i,k / ∈Fi] exp(sim(z i , z k )/τ ) ,<label>(2)</label></formula><p>where F i is the set of the detected false negatives with respect to an anchor i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">False Negative Attraction</head><p>While eliminating false negatives alleviates the undesireable effects of contrasting against them, it ignores information available in what are actually true positives. Minimizing the original contrastive loss (Eqn. 1) only seeks to attract an anchor to different views of the same image. Including true positives drawn from different images would increase the diversity of the training data and, in turn, has the potential to improve the quality of the learned embeddings. Indeed, Khosla et al. <ref type="bibr" target="#b25">[26]</ref> show that supervised contrastive learning (i.e., where an anchor is attracted to samples having the same semantic label) can be more effective than the traditional supervised cross-entropy loss. Thus, we propose to treat the false negatives that have been identified as true positives and attract the anchor to this set. This yields the following expression for the contrastive loss:</p><formula xml:id="formula_2">l att i = − 1 1 + |F i | log exp(sim(z i , z j )/τ ) 2N k=1 1 [k =i] exp(sim(z i , z k )/τ ) + f ∈Fi log exp(sim(z i , z f )/τ ) 2N k=1 1 [k =i] exp(sim(z i , z k )/τ )<label>(3)</label></formula><p>We note that compared to simply ignoring the detected false negatives <ref type="formula" target="#formula_1">(2)</ref>, the attraction strategy is more sensitive to the quality of the candidate false negatives. Encouraging embeddings that erroneously attract false positives can degrade the quality of the learned representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Finding False Negatives</head><p>Unfortunately, the process of identifying false negatives is fundamentally difficult, amounting to a chicken-and-egg problem-without labels, the learned semantic information could be used to establish valid and invalid correspondences, yet the correctness of these embeddings depends on the ability to identify correspondences. We propose a new approach to identify false negatives based on the following observations:</p><p>• False negatives are samples from different images with the same semantic content, therefore they should hold certain similarity (e.g., dog features). • A false negative may not be as similar to the anchor as it is to other augmentations of the same image, as each augmentation only holds a specific view of the object.</p><p>The above observations mean that we may be able to approximate a false negative with more augmented views of the anchor. As an example, consider <ref type="figure" target="#fig_2">Figure 3</ref> where we treat the picture of the dog's head on the left ("main views," in red) as the anchor image. The support views on the left are other augmented views generated from the same image and serve as positive matches. The picture of the dog's head on the far right ("main view," in blue) is not an augmented version of the anchor. Consequently, while it is similar to the anchor image, it would thus be treated as a negative match by contemporary self-supervised methods (i.e., it is a false negative). However, we see that this image is more similar to the augmented view of the anchor ("support view," in blue) than it is to the anchor with respect to the orientation of the dog's face. Similarly, the head and fur of the dog are expected to be a positive pair. While the false negative fur on the far right (orange) could look different than the head anchor, it should be more similar to the fur in the support views (orange).</p><p>Motivated by these observations, we propose a strategy for identifying candidate false negatives that follows as: where score i = {score m,i |m} is the set of scores for each negative sample with respect to anchor i.</p><p>For each element in the above procedure, there are several considerations one can make, including the choice of the similarity function, the strategy for aggregating scores, and the manner in which the most similar samples are defined.</p><p>In this work, we investigate the following options:</p><p>Similarity Function We use the cosine similarity function, since it is used in the contrastive loss during pretraining.</p><p>Aggregation Strategy We consider both mean aggregation, score m,i = 1 |S| |S| s=1 sim(z m , z s i ), and max aggregation, score m,i = max s∈{1,...,|S|} sim(z m , z s i ), and discuss their effects in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Screening Strategy</head><p>We consider two choices for the most similar samples, one that considers the top-k matches, best(score i ) = {z m |score m,i ∈ top(score i , k)}, and one that considers those above a threshold t, best(score i ) = {z m |score m,i &gt; t}. A top-k strategy may be preferred given information about the approximate number of false negatives, while thresholding may be better suited when a dynamic adaptation is expected. We also consider a strategy that combines top-k and and thresholding, best(score i ) = {z m |score m,i ∈ top(score i , k) &amp; score m,i &gt; t}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Ablation Studies</head><p>We use the same configuration as SimCLR v2 for pretraining and evaluation. The base encoder is ResNet-50 with a 3-layer MLP projection head. Data augmentation includes random crops, color distortion, and Gaussian blur. For each experiment, we pretrain for 100 epochs on the Im-ageNet ILSVRC-2012 training set, then freeze the encoder  and train a linear classifier on top, which is then evaluated on the ImageNet evaluation set. We pretrain on 128 Cloud TPUs with a batch size of 4096. We use the LARS optimizer with a learning rate of 6.4 and a cosine decay schedule, and a weight decay of 1 × 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">False Negative Cancellation Strategies</head><p>We evaluate the effects of various approaches to false negative mitigation, including the choice of cancellation strategy, aggregation score, and screening strategies, and draw the following conclusions.</p><p>False negative elimination consistently improves contrastive learning across crop sizes, and the gap is higher for bigger crops. <ref type="figure" target="#fig_4">Figure 4</ref> (left) demonstrates that the inclusion of false negative elimination yields top-1 accuracy that is strictly better than that of the SimCLR baseline accross the full range of crop ratios. We postulate that the bigger gap for larger crop sizes is due to the increased chance of having common semantic content in big crops, which leads to a higher ratio of false negatives. It is also worth noting that in <ref type="figure" target="#fig_4">Figure 4</ref> (left), we only eliminate a negligible number of two potential false negatives among 8190 (batch size 4096) negative samples for each anchor, but it could affect top-1 accuracy by as much as 1%. This supports the significant effect of false negatives in contrastive learning.</p><p>Having a support set helps in finding false negatives regardless of the cancellation strategy, with greater benefits with the attraction strategy. <ref type="figure" target="#fig_5">Figure 5</ref>     top-1 accuracy when we compare negative samples to a support set (Section 3.2.3) to the case in which we only compare negative samples to the anchor itself. The use of a support set results in larger performance gains when using false negative attraction (∼2%) compared to the false negative elimination strategy (∼0.2%). Further, while the elimination strategy improves performance relative to the Sim-CLR baseline whether or not a support set is used, attracting false negatives found without support set actually hurts performance ( <ref type="figure" target="#fig_5">Figure 5</ref>, right). This likely results from the fact that embeddings learned with attraction strategy are more sensitive to invalid false negatives (discussed next), justifying the use of a support set to reliably find false negatives.</p><p>The attraction strategy is much more sensitive to the quality of the found false negatives compared to the elimination strategy. This property is consistently confirmed through <ref type="figure" target="#fig_5">Figure 5</ref> (right) as previously discussed, and <ref type="figure" target="#fig_4">Figure 4</ref> (right), where the attraction strategy only works with more reliable false negatives, those having very high similarity scores, while the elimination method is not very sensitive to the thresholds.</p><p>Max aggregation significantly and consistently outperforms mean aggregation for the attraction strategy, while the gains are less pronounced with false negative elimination. <ref type="figure" target="#fig_6">Figure 6</ref> demonstrates that max aggregation outperforms mean aggregation for all support sizes and topk values in the attraction strategy, with a gap in some cases greater than 1%. This may be due to the fact that false negatives are similar to a strict subset of the support set, in which case considering all elements as in mean aggregation corrupts the similarity score. The difference is more pronounced for the attraction strategy, which is more sensitive to invalid false negatives.</p><p>Filtering by top-k tends to perform better than by a threshold, while a combination of both provides the best balance. As seen in <ref type="figure">Figure 7</ref>, the best choice of top-k is better than the best threshold. A strategy that combines the two approaches achieves greater accuracy, with the exception of false negative attraction at top-4, for which there is a negligible degradation in performance.</p><p>False negative attraction is superior to elimination when the detected false negatives are valid. As shown in <ref type="figure">Figure 8</ref> (left) and <ref type="table">Table 1</ref>, false negative elimination improves upon SimCLR by 1.02%, while false negative attraction results in a 1.75% improvement. These results use max aggregation, a support size of eight, top-4 for false negative attraction, and top-8 filtering for elimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">With Multi-crop</head><p>Caron et al. <ref type="bibr" target="#b6">[7]</ref> propose a multi-crop data augmentation strategy that increases the number of positive views attracted to each anchor image, improving the quality of the learned embeddings. Multi-crop is closely related and complementary to false negative attraction in multiple facets, from principle to computational efficiency. While multicrop attracts more positive samples, false negative attraction tries to attract samples that would otherwise be erroneously treated as negatives. The positive samples should be more reliable than the false negatives we attempt to find; however, multi-crop lacks semantic feature diversity, as it never attracts samples from different images. Because of these characteristics, multi-crop and false negative attraction offer complementary advantages. Furthermore, they can share computational overhead by using a common support set. Thus far, we have only used the support set to find false negatives, but they can also be used as additional positive views for multi-crop. In doing so, we may be able to  <ref type="table">Table 2</ref>. Complementary performance and computational efficiency of multi-crop and false negative attraction.  double the performance without noticeable overhead if the respective improvements are complementary. <ref type="figure" target="#fig_8">Figure 9</ref> and <ref type="table">Table 2</ref> demonstrate the advantages of using false negative cancellation together with multi-crop data augmentation. Adding multi-crop improves the performance of the SimCLR baseline by 2.09%, while incurring an additional 4.77 hours of computational overhead. Further adding false negative attraction on top of multi-crop yields a 1.92% absolute improvement in accuracy (i.e., similar to the 2.09% gain provided by multi-crop), while incurring only 0.1 hours of computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">With Momentum Encoders</head><p>Thus far, we have identified false negatives in the current batch from a single encoder. However, other methods may store negative samples in a memory bank, or encoded by a momentum encoder. Here, we investigate the behaviors of the proposed method in these settings. Momentum contrast <ref type="bibr" target="#b21">[22]</ref> employs two encoders, the main encoder and momentum encoder, and a memory bank where negative samples are stored alongside the samples from the current batch. This offers more options for finding false negatives, i.e., whether to use the support set from the main encoder or the momentum encoder, or whether to find negatives from samples in the current batch or all samples in memory. <ref type="figure">Figure 8 (right)</ref> indicates that it is better to generate the support set using samples from the momentum encoder compared to the main encoder. Further, finding false negatives in the memory bank yields greater top-1 accuracy than drawing false negatives from the current batch. <ref type="figure" target="#fig_8">Figure 9</ref> and <ref type="table" target="#tab_7">Table 3</ref> show that our method works across different configurations, with or without the presence of either momentum contrast or multi-crop. Notably, the performance gain from false negative cancellation in the presence   of momentum contrast is even higher at 2.27%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">With True Labels &amp; False Negatives Accuracy</head><p>We now consider the effectiveness of our false negative detection and cancellation strategy relative to the ideal setting in which we have access to ground-truth labels, which provides an upper-bound on performance. <ref type="figure" target="#fig_0">Figure 10</ref> presents the top-1 accuracy when using false negative cancellation combined with multi-crop data augmentation as well as the accuracy that results from false negative cancellation using ground-truth labels. As expected, cancelling false negatives helps substantially (8.18%) when true labels are used. However, we close half the gap by just using multi-crop and the false negatives our method finds, increasing top-1 accuracy by 4.01% over the SimCLR baseline.</p><p>To better understand the extent to which we are able to identify false negatives, <ref type="figure" target="#fig_0">Figure 10</ref> (right) plots the accuracy of the false negative detections over 100 epochs of pretraining. We see that the false negative detection accuracy steadily increases, reaching approximately 40% accuracy by 100 epochs. Note that the false negatives accuracy is computed based on human-defined semantic labels, with 1000 categories in ImageNet. The chance of finding a false negative for an anchor at random is just 0.1%. Accuracy (%) <ref type="figure" target="#fig_0">Figure 10</ref>. A visualization of (left) top-1 accuracy with false negative cancellation using detected vs. ground-truth labels and (right) the accuracy of false negative detection. <ref type="table" target="#tab_8">Table 4</ref> compares the computational cost of our false negative cancellation strategy compared to the SimCLR baseline. As expected, we can see that for the same number of epochs, the process of detecting and incorporating false negatives incurs additional computational time. However, to achieve the same accuracy, SimCLR requires more than three times the amount of computation (26.22 h and 1000 epochs) than our framework (7.50 h and 100 epochs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Computational Efficiency</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison with State-of-the-Art</head><p>We compare our improved model with false negative cancellation to other state-of-the-art methods on standard ImageNet evaluations and transfer to downstream tasks.</p><p>Pretraining Settings We use similar configurations as SimCLR v2. Specifically, we use ResNet-50 as the base encoder, with a 3-layer MLP projection head. We use a 65k memory buffer for the momentum encoder, with a momentum of 0.999. Following Tian et al. <ref type="bibr" target="#b42">[43]</ref>, we use random crops, color distortion, Gaussian blur, and RandAugment <ref type="bibr" target="#b13">[14]</ref> for data augmentation. We also adopt multi- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation Learning Contrastive learning</head><p>MoCo v1 <ref type="bibr" target="#b21">[22]</ref> 60.6 -PIRL <ref type="bibr" target="#b33">[34]</ref> 63.6 -PCL <ref type="bibr" target="#b31">[32]</ref> 65.9 -SimCLR v1 <ref type="bibr" target="#b8">[9]</ref> 69.3 89.0 MoCo v2 <ref type="bibr" target="#b10">[11]</ref> 71.   <ref type="table">Table 7</ref>. Transfer learning on classification task using ImageNet-pretrained ResNet models across 12 data sets.</p><p>Method AP50 Supervised 81.3 MoCo v2 <ref type="bibr" target="#b10">[11]</ref> 82.5 SwAV <ref type="bibr" target="#b6">[7]</ref> 82.6 FNC (ours) 82.8 crop <ref type="bibr" target="#b6">[7]</ref> with a support size of eight, which is shared with false negative cancellation. We use the attraction strategy with max aggregation for false negative cancellation, while the top-k is set to 10 and a threshold of 0.7 is used for filtering the scores. We pretrain for 1000 epochs on 128 Cloud TPUs with a batch size of 4096. We use the LARS optimizer with a learning rate of 6.4, a cosine schedule for decaying the learning rate, and a weight decay of 1 × 10 −4 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">ImageNet Evaluation</head><p>Our evaluation on ImageNet follows a protocol similar to that of SimCLR v2. Specifically, we conduct linear evaluation, in which a linear classifier is trained on top of the frozen pretrained features, and consider semi-supervised settings, in which we finetune the network with 1% and 10% labels available. We use a batch size of 1024 with a 0.16 learning rate, while weight decay is removed. We finetune for 90 epochs in linear evaluation, 60 epochs for 1%, and 30 epochs for 10% labels in semi-supervised settings. <ref type="table">Table 5</ref> presents the linear evaluation results. Our method achieves state-of-the-art performance for contrastive learning-based models with a top-1 accuracy of 74.4%, a 2.7% improvement over SimCLR v2 and a 1.4% boost from the previous best. Among all approaches, our method is second only to SwAV <ref type="bibr" target="#b6">[7]</ref>, a clustering-based method, and is even better than BYOL <ref type="bibr" target="#b20">[21]</ref>, a recent stateof-the-art method that only uses positive samples. <ref type="table" target="#tab_11">Table 6</ref> presents the results in the semi-supervised setting, in which our proposed model not only exceeds other contrastive learning-based methods, but also achieves the best performance among all models across all measures. Notably, in the 1% labels setting, our method significantly improves over the previous best to achieve 63.7% top-1 accuracy (a 5.8% absolute improvement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Transferring Features</head><p>Image Classification Following SimCLR, we perform the same evaluations on 12 classification datasets: Food <ref type="bibr" target="#b4">[5]</ref>, CIFAR10 <ref type="bibr" target="#b28">[29]</ref>, CIFAR100 <ref type="bibr" target="#b28">[29]</ref>, Birdsnap <ref type="bibr" target="#b3">[4]</ref>, SUN397 <ref type="bibr" target="#b45">[46]</ref>, Cars <ref type="bibr" target="#b27">[28]</ref>, Aircraft <ref type="bibr" target="#b32">[33]</ref>, VOC2007 <ref type="bibr" target="#b16">[17]</ref>, DTD <ref type="bibr" target="#b12">[13]</ref>, Pets <ref type="bibr" target="#b36">[37]</ref>, Caltech-101 <ref type="bibr" target="#b17">[18]</ref>, and Flowers <ref type="bibr" target="#b34">[35]</ref>. We follow the same setup as in SimCLR. As <ref type="table">Table 7</ref> demonstrates, our approach achieves a significant improvement in performance among contrastive learning-based methods (i.e., SimCLR v1 and SimCLR v2) in both settings. In linear evaluation, our method outperforms SimCLR v1 on all 12 datasets and is better than SimCLR v2 on all but one dataset. In finetuning, the proposed model outperforms both SimCLR v1 and v2 on all but one dataset, and matches that of BYOL, with each being superior on about half of the datasets.</p><p>Object Detection To further evaluate the transferability of the learned embeddings, we finetune the model on PASCAL VOC object detection. We use similar settings as in MoCo <ref type="bibr" target="#b21">[22]</ref>, where we finetune on the VOC trainval07+12 set using Faster R-CNN with a R50-C4 backbone, and evaluate on VOC test2007. We train for 34K iterations with batch size 16. The learning rate is set to 0.02, which is reduced by a factor of 10 after 20K and 28K iterations. As <ref type="table" target="#tab_12">Table 8</ref> reveals, our proposed method outperforms both MoCo v2, SwAV, and the supervised baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we address a fundamental problem in contrastive self-supervised learning that has not been adequately studied, identifying false negatives, and propose strategies to utilize this ability to improve contrastive learning frameworks. In addition to bringing novel insights to this topic through in-depth experimental analysis, our proposed method significantly boosts existing models, and sets new performance standards for contrastive self-supervised learning methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>False negatives in contrastive learning. Without knowledge of labels, automatically selected negative pairs could actually belong to the same semantic category, creating false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Overview of the proposed framework. Left: Original definition of the anchor, positive, and negative samples in contrastive learning. Middle: Identification of false negatives (blue). Right: false negative cancellation strategies, i.e. elimination and attraction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Finding false negatives with the support views. Negative samples (main views, right) may not have as reliable similarity with the anchor itself (red) as they do with other augmented views of the same image (support views). For instance, the dog's face in the support view (left, blue) is closer to the negative sample (right, blue) in terms of the facial orientation than the anchor (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>A comparison of top-1 accuracy (left) between false negative elimination and SimCLR; and (right) top-1 accuracy across filtering thresholds in false negative cancellation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>False negative cancellation with and without support set across top-k choices for different mitigation strategies. The dashed line denotes the performance of the SimCLR baseline. The results use mean aggregation in scoring potential false negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>False negative cancellation with mean and max aggregation across support sizes and top-k for the false negative (left) elimination and (right) attraction strategies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 .</head><label>9</label><figDesc>False negative cancellation with (left) multi-crop and (right) momentum encoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Base line SimC LR FN Canc ellati on + Multi -crop FN Canc ellati on With True Labe ls</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1. For each anchor i, generate a support set S i = {z s i } that contains other support views from the same image besides the two main views.2. Compute similarity scores, score s m,i = sim(z m , z s i ), between a negative sample z m and each sample z s i in the support set.3. Aggregate the computed scores for each negative sample, score m,i = aggregate s∈S (score s m,i ).</figDesc><table /><note>4. Define a set of potential false negatives F i as the neg- ative samples that are most similar to the support set based on the aggregated scores, F i = best(score i ),</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>contrasts the</figDesc><table><row><cell>Top-1 Accuracy (%)</cell><cell>66.4 66.6 66.8 67.0 67.2 67.4</cell><cell cols="2">False Negative Elimination Top-2 Top-4 Top-8</cell><cell>Top-1 Accuracy (%)</cell><cell>67.0 64.0 64.5 65.0 65.5 66.0 66.5</cell><cell>False Negative Attraction Top-2 Top-4 Top-8</cell></row><row><cell></cell><cell></cell><cell>Without Support</cell><cell>With Support</cell><cell></cell><cell></cell><cell>Without Support</cell><cell>With Support</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 .</head><label>3</label><figDesc>Top-1 accuracy improvement of false negative cancellation for different baselines.</figDesc><table><row><cell>Model</cell><cell cols="3">Epochs Time (h) Acc. (%)</cell></row><row><cell>SimCLR</cell><cell>100</cell><cell>2.63</cell><cell>66.41</cell></row><row><cell>SimCLR</cell><cell>1000</cell><cell>26.22</cell><cell>70.34</cell></row><row><cell>Improved Model</cell><cell>100</cell><cell>7.50</cell><cell>70.42</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 .</head><label>4</label><figDesc>Computational efficiency and accuracy.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 .</head><label>6</label><figDesc>ImageNet semi-supervised evaluation. Food CIFAR10 CIFAR100 Birdsnap SUN397 Cars Aircraft VOC2007 DTD Pets Caltech-101 Flowers</figDesc><table><row><cell>Linear evaluation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCLR v1 [9]</cell><cell>68.4</cell><cell>90.6</cell><cell>71.6</cell><cell>37.4</cell><cell>58.8</cell><cell>50.3</cell><cell>50.3</cell><cell>80.5</cell><cell>74.5 83.6</cell><cell>90.3</cell><cell>91.2</cell></row><row><cell>SimCLR v2 [10]</cell><cell>73.9</cell><cell>92.4</cell><cell>76.0</cell><cell>44.7</cell><cell>61.0</cell><cell>54.9</cell><cell>51.1</cell><cell>81.2</cell><cell>76.5 85.0</cell><cell>91.2</cell><cell>93.5</cell></row><row><cell>BYOL [21]</cell><cell>75.3</cell><cell>91.3</cell><cell>78.4</cell><cell>57.2</cell><cell>62.2</cell><cell>67.8</cell><cell>60.6</cell><cell>82.5</cell><cell>75.5 90.4</cell><cell>94.2</cell><cell>96.1</cell></row><row><cell>FNC (ours)</cell><cell>74.4</cell><cell>93.0</cell><cell>76.8</cell><cell>54.0</cell><cell>63.2</cell><cell>68.8</cell><cell>61.3</cell><cell>83.0</cell><cell>76.3 89.0</cell><cell>93.5</cell><cell>94.9</cell></row><row><cell>Finetuned</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SimCLR v1 [9]</cell><cell>88.2</cell><cell>97.7</cell><cell>85.9</cell><cell>75.9</cell><cell>63.5</cell><cell>91.3</cell><cell>88.1</cell><cell>84.1</cell><cell>73.2 89.2</cell><cell>92.1</cell><cell>97.0</cell></row><row><cell>SimCLR v2 [10]</cell><cell>88.2</cell><cell>97.5</cell><cell>86.0</cell><cell>74.9</cell><cell>64.6</cell><cell>91.8</cell><cell>87.6</cell><cell>84.1</cell><cell>74.7 89.9</cell><cell>92.3</cell><cell>97.2</cell></row><row><cell>BYOL [21]</cell><cell>88.5</cell><cell>97.8</cell><cell>86.1</cell><cell>76.3</cell><cell>63.7</cell><cell>91.6</cell><cell>88.1</cell><cell>85.4</cell><cell>76.2 91.7</cell><cell>93.8</cell><cell>97.0</cell></row><row><cell>FNC (ours)</cell><cell>88.3</cell><cell>97.7</cell><cell>86.8</cell><cell>76.3</cell><cell>64.2</cell><cell>92.0</cell><cell>88.5</cell><cell>84.7</cell><cell>76.0 90.9</cell><cell>93.6</cell><cell>97.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 .</head><label>8</label><figDesc>Transfer learning on Pascal VOC object detection.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We would like to express special thanks to Ting Chen and Mohammad Norouzi for helpful discussions regarding SimCLR and feedbacks on the project, Benjamin Louie for his dedicated helps during the internship, and Google Geo for the infrastructure and general support towards the project.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Selflabelling via simultaneous clustering and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Learning Representations (ICLR)</title>
		<meeting>Int&apos;l Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning representations by maximizing mutual information across views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Buchwalter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">545</biblScope>
			<biblScope unit="page" from="15" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cliquecnn: Deep unsupervised exemplar learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Bautista</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sanakoyeu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tikhoncheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3846" to="3854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale fine-grained visual categorization of birds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Food-101-Mining discriminative components with random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Machine Learning (ICML)</title>
		<meeting>Int&apos;l Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Machine Learning (ICML)</title>
		<meeting>Int&apos;l Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Debiased contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yen-Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Describing textures in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cimpoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Randaugment: Practical data augmentation with no separate search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l. Conf. on Computer Vision (ICCV)</title>
		<meeting>Int&apos;l. Conf. on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DeCAF: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Machine Learning (ICML)</title>
		<meeting>Int&apos;l Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The Pascal visual object classes (VOC) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning generative visual models from few training examples: An incremental Bayesian approach tested on 101 object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the CVPR Workshop on Generative-Model Based Vision</title>
		<meeting>the CVPR Workshop on Generative-Model Based Vision</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by predicting image rotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Learning Representations (ICLR)</title>
		<meeting>Int&apos;l Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altché</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">J</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Learning Representations (ICLR)</title>
		<meeting>Int&apos;l Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Learning Representations (ICLR)</title>
		<meeting>Int&apos;l Conf. on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hard negative mixing for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Sariyildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Supervised contrastive learning</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Satellite image-based localization via learned embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. on Robotics and Automation (ICRA)</title>
		<meeting>IEEE Int&apos;l Conf. on Robotics and Automation (ICRA)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
	<note>Singapore</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">3D object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S J</forename><surname>Dengand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. on Computer Vision Workshops</title>
		<meeting>IEEE Int&apos;l Conf. on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto.</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="840" to="849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning representations for automatic colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Prototypical contrastive learning of unsupervised representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04966</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the Indian Conf. on Computer Vision, Graphics and Image Processing</title>
		<meeting>of the Indian Conf. on Computer Vision, Graphics and Image essing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04592</idno>
		<title level="m">Contrastive learning with hard negative samples</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Fixmatch: Simplifying semi-supervised learning with consistency and confidence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.07685</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Machine Learning (ICML)</title>
		<meeting>Int&apos;l Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. on Machine Learning (ICML)</title>
		<meeting>Int&apos;l Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">SUN Database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc</title>
		<meeting>null</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Int&amp;apos;l Conf</surname></persName>
		</author>
		<title level="m">on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Unsupervised data augmentation for consistency training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12848</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="645" to="654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. on Computer Vision (ECCV)</title>
		<meeting>European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
