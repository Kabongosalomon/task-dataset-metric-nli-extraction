<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Taking A Closer Look at Domain Shift: Category-level Adversaries for Semantics Consistent Domain Adaptation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">Research School of Computer Science</orgName>
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="institution">Farsee2 Tech. Co</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science &amp; Technology</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center of Network and Computation</orgName>
								<orgName type="institution">Huazhong University of Science &amp; Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology Sydney</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Baidu Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Taking A Closer Look at Domain Shift: Category-level Adversaries for Semantics Consistent Domain Adaptation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider the problem of unsupervised domain adaptation in semantic segmentation. A key in this campaign consists in reducing the domain shift, i.e., enforcing the data distributions of the two domains to be similar. One of the common strategies is to align the marginal distribution in the feature space through adversarial learning. However, this global alignment strategy does not consider the category-level joint distribution. A possible consequence of such global movement is that some categories which are originally well aligned between the source and target may be incorrectly mapped, thus leading to worse segmentation results in target domain. To address this problem, we introduce a category-level adversarial network, aiming to enforce local semantic consistency during the trend of global alignment. Our idea is to take a close look at the category-level joint distribution and align each class with an adaptive adversarial loss. Specifically, we reduce the weight of the adversarial loss for category-level aligned features while increasing the adversarial force for those poorly aligned. In this process, we decide how well a feature is category-level aligned between source and target by a co-training approach. In two domain adaptation tasks, i.e., GTA5 → Cityscapes and SYN-THIA → Cityscapes, we validate that the proposed method matches the state of the art in segmentation accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation aims to assign each pixel of a photograph to a semantic class label. Currently, the achievement is at the price of large amount of dense pixel-level  <ref type="figure">Figure 1</ref>. (Best viewed in color.) Illustration of traditional and the proposed adversarial learning. The size of the solid gray arrow represents the weight of the adversarial loss. (a) Traditional adversarial learning ignores the semantic consistency when pursuing the marginal distribution alignment. As a result, the global movement might cause the well-aligned features (class A) to be mapped onto different joint distributions (negative transfer). (b) The proposed self-adaptive adversarial learning reweights the adversarial loss for each feature by a local alignment score. Our method reduces the influence of the adversaries when discovers a high semantic alignment score on a feature, and vice versa. As is shown, the proposed strategy encourages a category-level joint distribution alignment for both class A and class B.</p><p>annotations obtained by expensive human labor <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26</ref>]. An alternative would be resorting to simulated data, such as computer generated scenes <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, so that unlimited amount of labels are made available. However, models trained with the simulated images do not generalize well to realistic domains. The reason lies in the different data distributions of the two domains, typically known as do-main shift <ref type="bibr" target="#b35">[36]</ref>. To address this issue, domain adaptation approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b46">47]</ref> are proposed to bridge the gap between the source and target domains. A majority of recent methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b40">41]</ref> aim to align the feature distributions of different domains. Works along this line are based on the theoretical insights in <ref type="bibr" target="#b0">[1]</ref> that minimizing the divergence between domains lowers the upper bound of error on the target domain. Among this cohort of domain adaptation methods, a common and pivotal step is minimizing some distance metric between the source and target feature distributions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b38">39]</ref>. Another popular choice, which borrows the idea from adversarial learning <ref type="bibr" target="#b9">[10]</ref>, is to minimize the accuracy of domain prediction. Through a minimax game between two adversarial networks, the generator is trained to produce features that confuse the discriminator while the latter is required to correctly classify which domain the features are generated from.</p><p>Although the works along the path of adversarial learning have led to impressive results <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b34">35]</ref>, they suffer from a major limitation: when the generator network can perfectly fool the discriminator, it merely aligns the global marginal distribution of the features in the two domains ( i.e., P (F s ) ≈ P (F t ), where F s and F t denote the features of source and target domain in latent space) while ignores the local joint distribution shift, which is closely related to the semantic consistency of each category (i.e.,</p><formula xml:id="formula_0">P (F s , Y s ) = P (F t , Y t ),</formula><p>where Y s and Y t denote the categories of the features). As a result, the de facto use of the adversarial loss may cause those target domain features, which are already well aligned to their semantic counterpart in source domain, to be mapped to an incorrect semantic category (negative transfer). This side effect becomes more severe when utilize a larger weight on the adversarial loss.</p><p>To address the limitation of the global adversarial learning, we propose a category-level adversarial network (CLAN), prioritizing category-level alignment which will naturally lead to global distribution alignment. The cartoon comparison of traditional adversarial learning and the proposed one is shown in <ref type="figure">Fig. 1</ref>. The key idea of CLAN is two-fold. First, we identify those classes whose features are already well aligned between the source and target domains, and protect this category-level alignment from the side effect of adversarial learning. Second, we identify the classes whose features are distributed differently between the two domains and increase the weight of the adversarial loss during training. In this process, we utilize co-training <ref type="bibr" target="#b45">[46]</ref>, which enables high-confidence predictions with two diverse classifiers, to predict how well each feature is semantically aligned between the source and target domains. Specifically, if the two classifiers give consistent predictions, it indicates that the feature is predictive and achieves good semantic alignment. In such case, we reduce the influence of the adversarial loss in order to encourage the network to gen-erate invariant features that can keep semantic consistency between domains. On the contrary, if the predictions disagree with each other, which indicates that the target feature is far from being correctly mapped, we increase the weight of the adversarial loss on that feature so as to accelerate the alignment. Note that 1) Our adversarial learning scheme acts directly on the output space. By regarding the output predictions as features, the proposed method jointly promotes the optimization for both classifier and extractor; 2) Our method does not guarantee rigorous joint distribution alignment between domains. Yet, compared with marginal distribution alignment, our method can map the target features closer (or no negative transfer at worst) to the source features of the same categories. The main contributions are summarized below.</p><p>• By proposing to adaptively weight the adversarial loss for different features, we emphasize the importance of category-level feature alignment in reducing domain shift.</p><p>• Our results are on par with the state-of-the-art UDA methods on two transfer learning tasks, i.e., GTA5 <ref type="bibr" target="#b29">[30]</ref> → Cityscapes <ref type="bibr" target="#b7">[8]</ref> and SYNTHIA <ref type="bibr" target="#b30">[31]</ref> → Cityscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>This section will focus on adversarial learning and cotraining techniques for unsupervised domain adaptation, which form the two main motivations of our method.</p><p>Adversarial learning. Ben-David et al. <ref type="bibr" target="#b0">[1]</ref> had proven that the adaptation loss is bounded by three terms, e.g., the expect loss on source domain, the domain divergence, and the shared error of the ideal joint hypothesis on the source and target domain. Because the first term corresponds to the well-studied supervised learning problems and the third term is considered sufficiently low to achieve an accurate adaptation, the majority of recent works lay emphasis on the second term. Adversarial adaptation methods are good examples of this type of approaches and can be investigated on different levels. Some methods focus on the distribution shift in the latent feature space <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b34">35]</ref>. In an example, Hoffman et al. <ref type="bibr" target="#b14">[15]</ref> appended category statistic constraints to the adversarial model, aiming to improve semantic consistency in target domain. Other methods address the adaption problem on the pixel level <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b2">3]</ref>, which relate to the style transfer approaches <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b6">7]</ref> to make images indistinguishable across domains. A joint consideration of pixel and feature level domain adaptation is studied in <ref type="bibr" target="#b13">[14]</ref>. Besides alignment in the bottom feature layers, Tsai et al. <ref type="bibr" target="#b39">[40]</ref> found that aligning directly the output space is more effective in semantic segmentation. Domain adaptation in the output space enables the joint optimization for both prediction and representation, so our method utilizes this advantage.</p><p>Co-training. Co-training <ref type="bibr" target="#b45">[46]</ref> belongs to multi-view learning in which learners are trained alternately on two distinct views with confident labels from the unlabeled data. In UDA, this line of methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b24">25]</ref> are able to assign pseudo labels to unlabeled samples in the target domain, which enables direct measurement and minimization the classification loss on target domain. In general, co-training enforces the two classifiers to be diverse in the learned parameters, which can be achieved via dropout <ref type="bibr" target="#b32">[33]</ref>, consensus regularization <ref type="bibr" target="#b33">[34]</ref> or parameter diverse <ref type="bibr" target="#b42">[43]</ref>, etc. Similar to co-training, tri-training keeps the two classifiers producing pseudo labels and uses these pseudo labels to train an extra classifier <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b42">43]</ref>. Apart from assigning pseudo labels to unlabeled data, Saiko et al. <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> maximized the consensus of two classifiers for domain adaptation.</p><p>Our work does not follow the strategy of global feature alignment <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b37">38]</ref> or classifiers consensus maximization <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref>. Instead, category-level feature alignment is enforced through co-training. To our knowledge, we are making an early attempt to adaptively weight the adversarial loss for features in segmentation task according to the local alignment situation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Settings</head><p>We focus on the problem of unsupervised domain adaptation (UDA) in semantic segmentation, where we have access to the source data X S with pixel-level labels Y S , and the target data X T without labels. The goal is to learn a model G that can correctly predict the pixel-level labels for the target data X T . Traditional adversaries-based networks (TAN) consider two aspects for domain adaptation. First, these methods train a model G that distills knowledge from labeled data in order to minimize the segmentation loss in the source domain, formalized as a fully supervised problem:</p><formula xml:id="formula_1">L seg (G) = E[ (G(X S ), Y S )] ,<label>(1)</label></formula><p>where E[·] denotes statistical expectation and (·, ·) is an appropriate loss function, such as multi-class cross entropy. Second, adversaries-based UDA methods also train G to learn domain-invariant features by confusing a domain discriminator D which is able to distinguish between samples of the source and target domains. This property is achieved by minimaxing an adversarial loss:</p><formula xml:id="formula_2">L adv (G, D) = − E[log(D(G(X S )))] − E[log(1 − D(G(X T )))] .</formula><p>(2)</p><p>However, as mentioned above, there is a major limitation for traditional adversarial learning methods: even under perfect alignment in marginal distribution, there might be the negative transfer that causes the samples from different domains but of the same class label to be mapped farther away in the feature space. In some cases, some classes are already aligned between domains, but the adversarial loss might deconstruct the existing local alignment when pursuing the global marginal distribution alignment. In this paper, we call this phenomenon "lack of semantic consistency", which is a critical cause of performance degradation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Network Architecture</head><p>Our network architecture is illustrated in <ref type="figure">Fig. 2</ref>. It is composed of a generator G and a discriminator D. G can be any FCN-based segmentation network <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b3">4]</ref> and D is a CNN-based binary classifier with a fully-convolutional output <ref type="bibr" target="#b9">[10]</ref>. As suggested in the standard co-training algorithm <ref type="bibr" target="#b45">[46]</ref>, generator G is divided into feature extractor E and two classifiers C 1 and C 2 . E extracts features from input images; C 1 and C 2 classify features generated from E into one of the pre-defined semantic classes, such as car, tree and road. Following the co-training practice, we enforce the weights of C 1 and C 2 to be diverse through a cosine distance loss. This will provide us with the distinct views / classifiers to make semantic predictions for each feature. The final prediction map p is obtained by summing up the two diverse prediction tensors p <ref type="bibr" target="#b0">(1)</ref> and p <ref type="bibr" target="#b1">(2)</ref> and we call p an ensemble prediction.</p><p>Given a source domain image x s ∈ X S , feature extractor E outputs a feature map, which is input to classifiers C 1 and C 2 to yield the pixel-level ensemble prediction p. On the one hand, p is used to calculate a segmentation loss under the supervision of the ground-truth label y s ∈ Y S . On the other hand, p is input to D to generate an adversarial loss.</p><p>Given a target domain image x t ∈ X T , we also forward it to G and obtain an ensemble prediction p. Different from the source data flow, we additionally generate a discrepancy map out of p <ref type="bibr" target="#b0">(1)</ref> and p <ref type="bibr" target="#b1">(2)</ref> , denoted as M(p <ref type="bibr" target="#b0">(1)</ref> , p <ref type="bibr" target="#b1">(2)</ref> ), where M(·, ·) denotes some proper distance metric to measure the element-wise discrepancy between p (1) and p <ref type="bibr" target="#b1">(2)</ref> . When using the cosine distance as an example, M(p <ref type="bibr" target="#b0">(1)</ref> , p <ref type="bibr" target="#b1">(2)</ref> ) forms</p><formula xml:id="formula_3">a 1 × H × W shaped tensor with the (i th ∈ H, j th ∈ W ) element equaling to (1 − cos(p (1) i,j , p<label>(2)</label></formula><p>i,j )). Once D produces an adversarial loss map L adv , an element-wise multiplication is performed between L adv and M(p <ref type="bibr" target="#b0">(1)</ref> , p <ref type="bibr" target="#b1">(2)</ref> ). As a result, the final adaptive adversarial loss on a target sample takes the form as</p><formula xml:id="formula_4">H i=1 W j=1 (1−cos(p (1) i,j , p<label>(2)</label></formula><p>i,j ))×L advi,j , where {i, j} traverses over all the pixels on the map. In this manner, each pixel on the segmentation map is differently weighted w.r.t the adversarial loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training Objective</head><p>The proposed network is featured by three loss functions, i.e., the segmentation loss, the weight discrepancy loss and the self-adaptive adversarial loss. Given an image x ∈ X S  <ref type="figure">Figure 2</ref>. Overview of the proposed category-level adversarial network. It consists of a feature extractor E, two classifiers C1 and C2, and a discriminator D. C1 and C2 are fed with the deep feature map extracted from E and predict semantic labels for each pixel from diverse views. In source flow, the sum of the two prediction maps is used to calculate a segmentation loss as well as an adversarial loss from D. In target flow, the sum of the two prediction maps is forwarded to D to produce a raw adversarial loss map. Additionally, we adopt the discrepancy of the two prediction maps to produce a local alignment score map. This map evaluates the category-level alignment degree of each feature and is used to adaptively weight the raw adversarial loss map. of shape 3 × H × W and a label map y ∈ Y S of shape C × H × W where C is the number of semantic classes, the segmentation loss (multi-class cross-entropy loss) can be concretized from Eq. 1 as</p><formula xml:id="formula_5">L seg (G) = H×W i=1 C c=1 −y ic log p ic ,<label>(3)</label></formula><p>where p ic denotes the predicted probability of class c on pixel i. y ic denotes the ground truth probability of class c on the pixel i. If pixel i belongs to class c, y ic = 1, otherwise y ic = 0.</p><p>For the second loss, as suggested in the standard cotraining algorithm <ref type="bibr" target="#b45">[46]</ref>, the two classifiers C 1 and C 2 should have possibly diverse parameters in order to provide two different views on a feature. Otherwise, the training degenerates to self-training. Specifically, we enforce divergence of the weights of the convolutional layers of the two classifiers by minimizing their cosine similarity. Therefore, we have the following weight discrepancy loss:</p><formula xml:id="formula_6">L weight (G) = w 1 · w 2 w 1 w 2 ,<label>(4)</label></formula><p>where w 1 and w 2 are obtained by flattening and concatenating the weights of the convolution filters of C 1 and C 2 . Third, we adopt the discrepancy between the two predictions p <ref type="bibr" target="#b0">(1)</ref> and p <ref type="bibr" target="#b1">(2)</ref> as an indicator to weight the adversarial loss. The self-adaptive adversarial loss can be extended from the traditional adversarial loss (Eq. 2) as</p><formula xml:id="formula_7">L adv (G, D) = −E[log(D(G(X S )))] − E[(λ local M(p (1) , p (2) ) + ) log(1 − D(G(X T )))] ,<label>(5)</label></formula><p>where p <ref type="bibr" target="#b0">(1)</ref> and p <ref type="bibr" target="#b1">(2)</ref> are predictions made by C 1 and C 2 , respectively, M(·, ·) denotes the cosine distance, and λ local controls the adaptive weight for adversarial loss. Note that in Eq. 5, to stabilize the training process, we add a small number to the self-adaptive weight.</p><p>With the above loss terms, the overall loss function of our approach can be written as</p><formula xml:id="formula_8">L CLAN (G, D) =L seg (G) + λ weight L weight (G) + λ adv L adv (G, D) ,<label>(6)</label></formula><p>where λ weight and λ adv denote the hyper parameters that control the relative importance of the three losses. The training objective of CLAN is</p><formula xml:id="formula_9">G * , D * = arg min G max D L CLAN (G, D).<label>(7)</label></formula><p>We solve Eq. 7 by alternating between optimizing G and D until L CLAN (G, D) converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Analysis</head><p>The major difference between the proposed framework and traditional adversarial learning consists in two aspects: the discrepancy loss and the category-level adversarial loss. Accordingly, analysis will focus on the two differences.  First, the discrepancy (co-training) loss encourages E to learn domain-invariant semantics instead of the domain specific elements such as illumination. In our network, classifiers C 1 and C 2 1) are encouraged to capture possibly different characteristics of a feature, which is ensured by the discrepancy loss, and 2) are enforced to make the same prediction of any E output (no matter the source or target), which is required by the segmentation loss and the adversarial loss. The two forces actually require that E should capture the essential aspect of a pixel across the source and target domains, which, as we are aware of, is the pure semantics of a pixel, i.e., the domain-invariant aspect of a pixel. Without the discrepancy loss (co-training), force 1) is missing, and there is a weaker requirement for E to learn domain-invariant information. On the other side, in our simulated → real task, the two domains vary a lot at visual level, but overlap at semantic level. If C 1 and C 2 are input with visual-level features from E, their predictions should be inaccurate in target domain and tend to be different, which will be punished by large adversarial losses. As a result, once our algorithm converges, C 1 and C 2 will be input with semanticlevel features instead of visual-level features. That is, E is encouraged to learn domain-invariant semantics. Therefore, the discrepancy loss serves as an implicit contributing factor for the improved adaptation ability.</p><p>Second, in our major contribution, we extend the traditional adversarial loss with an adaptive weight [λ local M(p <ref type="bibr" target="#b0">(1)</ref> , p <ref type="bibr" target="#b1">(2)</ref> ) + ].</p><p>On the one hand, when M(p <ref type="bibr" target="#b0">(1)</ref> , p <ref type="bibr" target="#b1">(2)</ref> ) is large, feature maps of the same class do not have similar joint distributions between two domains: they suffer from the semantic inconsistency. Therefore, the weights are such assigned as to encourage G to fool D mainly on features that suffer from domain shift. On the other hand, when M(p <ref type="bibr" target="#b0">(1)</ref> , p <ref type="bibr" target="#b1">(2)</ref> ) is small, the joint distribution would have a large overlap across domains, indicating that the semantic inconsistency problem is not severe. Under this circumstance, G tends to ignore the adversarial punishment from D. From the view of D, the introduction of the adaptive weight encourages D to distill more knowledge from examples suffering from semantic inconsistency rather than those well-aligned classes. As a result, CLAN is able to improve category-level alignment degree in adversarial training. This could be regarded as an explicit contributing factor for the adaptation ability. We additionally give a contrastive analysis between traditional adversarial network (TAN) and CLAN on their adaptation result in <ref type="figure" target="#fig_3">Fig. 3.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We evaluate CLAN together with several state-of-the-art algorithms on two adaptation tasks, e.g., SYNTHIA [31] → Cityscapes <ref type="bibr" target="#b7">[8]</ref> and GTA5 <ref type="bibr" target="#b29">[30]</ref> → Cityscapes. Cityscapes is a real-world dataset with 5,000 street scenes. We use Cityscapes as the target domain. GTA5 contains 24,966 highresolution images compatible with the Cityscapes annotated classes. SYNTHIA contains 9400 synthetic images. We use SYNTHIA or GTA5 as the source domain. <ref type="table">Table 1</ref>. Adaptation from GTA5 <ref type="bibr" target="#b29">[30]</ref> to Cityscapes <ref type="bibr" target="#b7">[8]</ref>. We present per-class IoU and mean IoU. "V" and "R" represent the VGG16-FCN8s and ResNet101 backbones, respectively. "ST" and "AT" represent two lines of method, i.e., self training-and adversarial learning-based DA. We highlight the best result in each column in bold. To clearly showcase the effect of CLAN on infrequent classes, we highlight these classes in blue. Gain indicates the mIoU improvement over using the source only.  Source only R -75. <ref type="bibr" target="#b7">8</ref>   <ref type="table">Table 2</ref>. Adaptation from SYNTHIA <ref type="bibr" target="#b30">[31]</ref> to Cityscapes <ref type="bibr" target="#b7">[8]</ref>. We present per-class IoU and mean IoU for evaluation. CLAN and state-of-theart domain adaptation methods are compared. For each backbone, the best accuracy is highlighted in bold. To clearly showcase the effect of CLAN on infrequent classes, we highlight these classes in blue. Gain indicates the mIoU improvement over using the source only. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>We use PyTorch for implementation. We utilize the DeepLab-v2 <ref type="bibr" target="#b3">[4]</ref> framework with ResNet-101 <ref type="bibr" target="#b11">[12]</ref> pretrained on ImageNet <ref type="bibr" target="#b8">[9]</ref> as our source-only backbone for network G. We use the single layer adversarial DA method proposed in <ref type="bibr" target="#b39">[40]</ref> as the TAN baseline. For co-training, we duplicate two copies of the last classification module and arrange them in parallel after the feature extractor, as illustrated in <ref type="figure">Fig. 2</ref>. For a fair comparison to those methods with the VGG backbone, we also apply CLAN on VGG-16 based FCN8s <ref type="bibr" target="#b22">[23]</ref>. For network D, we adopt a similar structure with <ref type="bibr" target="#b28">[29]</ref>, which consists of 5 convolution layers with kernel 4 × 4 with channel numbers {64, 128, 256, 512, 1} and stride of 2. Each convolution layer is followed by a Leaky-ReLU <ref type="bibr" target="#b26">[27]</ref> parameterized by 0.2 except the last layer. Finally, we add an up-sampling layer to the last layer to rescale the output to the size of the input map, in order to match the size of local alignment score map. During training, we use SGD <ref type="bibr" target="#b1">[2]</ref> as the optimizer for G with a momentum of 0.9, while using Adam <ref type="bibr" target="#b19">[20]</ref> to optimize D with β 1 = 0.9, β 2 = 0.99. We set both optimizers a weight decay of 5e − 4. For SGD, the initial learning rate is set to 2.5e − 4 and decayed by a poly learning rate policy, where the initial learning rate is multiplied by (1 − iter max iter ) power with power = 0.9. For Adam, we initialize the learning rate to 5e − 5 and fix it during the training. We train the network for a total of 100k iterations. We use a crop of 512 × 1024 during training, and for evaluation we up-sample the prediction map by a factor of 2 and then evaluate mIoU. In our best model, the hyper-parameters λ weight , λ adv , λ local and are set to 0.01, 0.001, 40 and 0.4 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparative Studies</head><p>We present the adaptation results on task GTA5 → Cityscapes in <ref type="table">Table 1</ref> with comparisons to the state-of-the-art domain adaptation methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b48">49]</ref>. We observe that CLAN significantly outperforms the source-only segmentation method by +18.7% on VGG-16 and +6.6% on ResNet-101. Besides, CLAN also outperforms the stateof-the-art methods, which improves the mIOU by over +7% compared with MCD <ref type="bibr" target="#b33">[34]</ref>, CDA <ref type="bibr" target="#b43">[44]</ref> and CyCADA <ref type="bibr" target="#b13">[14]</ref>. Compared to traditional adversarial network (TAN) in the output space <ref type="bibr" target="#b39">[40]</ref>, CLAN brings over +1.6% improvement in mIOU in both architectures of VGG-16 and ResNet-101. In some infrequent classes which are prone to suffer from the side effect of global alignment, e.g., fence, traffic light and pole, CLAN can significantly outperform TAN. Besides, we also compare CLAN with the self training-based methods, among which CBST <ref type="bibr" target="#b48">[49]</ref> is the current state-of-the-art one. This series of explicit methods usually achieve higher mIoU then the implicit feature alignment. While in our experiment, we find that CLAN is on par with CBST. Some qualitative segmentation examples can be viewed in <ref type="figure" target="#fig_7">Fig. 5</ref>. <ref type="table">Table 2</ref> provides the comparative results on the task SYN-THIA → Cityscapes. On VGG-16, our final model yields 39.3% in terms of mIOU, which significantly improves the non-adaptive segmentation result by 19.1%. Besides, CLAN outperforms the current state-of-art method <ref type="bibr" target="#b14">[15]</ref> by 16.4% and [6] by 3.6%. On ResNet-101, CLAN brings 9.2% improvement to source only segmentation model. Compare to TAN <ref type="bibr" target="#b39">[40]</ref>, the use of adaptive adversarial loss also brings 1.9% gain in terms of mIOU. Likewise, CLAN is more effective on those infrequent classes which are prone to be negatively transferred, such as traffic light and sign, bringing over 3.2% improvement respectively. While on some prevalent classes, CLAN can also be par on with the baseline method. Note that on the "train" class, the improvement is not stable. This is due to the training samples that contain the "train" are very few. Finally, comparing with the self training-based method, CLAN outperforms CBST by 3.2% in terms of mIOU. These observations are in consistent with our t-SNE analysis in <ref type="figure" target="#fig_3">Fig. 3</ref>, which further verifies that CLAN can actually boost the category-level alignment in segmentation-based DA task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Feature Distribution</head><p>To further verify that CLAN is able to decrease the negative transfer effect for those well-aligned features, we designed an experiment to take a closer look at the categorylevel alignment degree of each class. Specifically, we randomly select 1K source and 1K target images and calculate the cluster center distance (CCD) {d e 1 ...d e n } of features of  the same class between two domains, where n = #class and e is training epoch. d e i is normalized by d e i /d 0 i (In this way, the CCD from the pre-trained model without any finetuning would be always normalized to 1). We report d e i in <ref type="figure" target="#fig_6">Fig. 4</ref> (left subfigure, taking the class "wall" as an example). First, we observe as training goes on, d e i is monotonically decreasing in CLAN while not being monotone in TAN, suggesting CLAN prevents the well-aligned features from being incorrectly mapped. Second, d e i converges to a smaller value in CLAN than TAN, suggesting CLAN achieves better feature alignment at semantic level.</p><p>We further report the final CCD of each class in <ref type="figure">Fig. 6</ref>. We can observe that CLAN can achieve a smaller CCD in most cases, especially in those infrequent classes which are prone to be negatively transferred. These quantitative results, together with the qualitative t-SNE <ref type="bibr" target="#b27">[28]</ref> analysis in <ref type="figure" target="#fig_3">Fig. 3</ref>, indicate that CLAN can preferably align the two domains in semantic level. Such category-aligned feature distribution usually makes the subsequent classification easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Parameter Studies</head><p>In this experiment, we aim to study two problems: 1) whether the adaptive adversarial loss would cause instability (vanishing gradient) during adversarial training and 2) how much the adaptive adversarial loss would effect the performance. For the problem 1), we utilize the loss of D to indicate the convergence performance and a stable adversarial training is achieved if D loss converges around 0.5. First, we test our model using λ local = 40, with varying over a range {0.1, 0.2, 0.4, 0.8}. We do not use any larger than 0.8 since CLAN would degrade into TAN in that case. In the experiment, our model suffers from poor convergence when utilize a very small , e.g., 0.1 or 0.2. It indicates that a proper choice of is between 0.2 and 0.8. Motivated by this observation, we then test our model using = 0.4 with varying λ local over a range {10, 20, 40, 80}. We observe that the convergence performance is not very sensitive to λ local since the loss of D converges to proper values in all the cases. The best performance is achieved when using λ local = 40 and = 0.4. Besides, we observe that the adaptation performance of CLAN can steadily outperform TAN when using  <ref type="figure">Figure 6</ref>. Quantitative analysis of the feature joint distributions. For each class, we show the distance of the feature cluster centers between source domain and target domain. These results are from 1) the model pre-trained on ImageNet <ref type="bibr" target="#b8">[9]</ref> without any fine-tuning, 2) the model fine-tuned with source images only, 3) the adapted model using TAN and 4) the adapted model using CLAN, respectively. parameters near the best value. We present the detailed performance variation in <ref type="figure" target="#fig_6">Fig. 4</ref> (right subfigure). By comparing both the convergence and segmentation results under these different parameter settings, we can conclude that our proposed adaptive adversarial weight can significantly effect and improve the adaptation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduce the category-level adversarial network (CLAN), aiming to address the problem of semantic inconsistency incurred by global feature alignment during unsupervised domain adaptation (UDA). By taking a close look at the category-level data distribution, CLAN adaptively weight the adversarial loss for each feature according to how well their category-level alignment is. In this spirit, each class is aligned with an adaptive adversarial loss. Our method effectively prevents the well-aligned features from being incorrectly mapped by the side effect of pure global distribution alignment. Experimental results validate the effectiveness of CLAN, which yields very competitive segmentation accuracy compared with state-of-the-art UDA approaches.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Non-adapted features (f) Adapted features (TAN) (g) Adapted features (CLAN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>A contrastive analysis of CLAN and traditional adversarial network (TAN). (a): A target image, and we focus on the poles and traffic signs in orange boxes. (b): A non-adapted segmentation result. Although the global segmentation result is poor, the poles and traffic signs can be correctly segmented. It indicates that some classes are originally aligned between domains, even without any domain adaptation. (c): Adapted result of TAN, in which a decent segmentation map is produced but poles and traffic signs are poorly segmented. The reason is that the global alignment strategy tends to assign a conservative prediction to a feature and would lead some features to be predicted to other prevalent classes<ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18]</ref>, thus causing those infrequent features being negatively transferred. (d): Adapted result from CLAN. CLAN reduces the weight of adversarial loss for those aligned features. As a result, the original well-segmented class are well preserved. We then map the high-dimensional features of (b), (c) and (d) to a 2-D space with t-SNE<ref type="bibr" target="#b27">[28]</ref> shown in (e), (f) and (g). The comparison of feature distributions further proves that CLAN can enforce category-level alignment during the trend of global alignment. (For a clear illustration, we only show 4 related classes, i.e., building in blue, traffic sign in orange, pole in red and vegetation in green.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) [14] V AT 85.6 30.7 74.7 14.4 13.0 17.6 13.7 5.8 74.6 15.8 69.9 38.2 3.5 72.3 16.0 5.0 0.1 3.6 0.0 29.2 11.3 Baseline (TAN) [40] V AT 87.3 29.8 78.6 21.1 18.2 22.5 21.5 11.0 79.7 29.6 71.3 46.8 6.5 80.1 23.0 26.9 0.0 10.6 0.3 35.0 17.1 CLAN V AT 88.0 30.6 79.2 23.4 20.5 26.1 23.0 14.8 81.6 34.5 72.0 45.8 7.9 80.5 26.6 29.9 0.0 10.7 0.0 36.6 18.7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Left: Cluster center distance variation as training goes on. Right: Mean IoU (see bars &amp; left y axis) and convergence performance (see lines &amp; right y axis) variation when training with different λ local and .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results of UDA segmentation for GTA5 → Cityscapes. For each target image, we show the non-adapted (source only) result, adapted result with CLAN and the ground truth label map, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>79.8 22.1 20.0 23.6 33.1 21.8 81.8 25.9 75.9 57.3 26.2 76.3 29.8 32.1 7.2 29.5 32.5 41.4 4.8 CLAN R AT 87.0 27.1 79.6 27.3 23.3 28.3 35.5 24.2 83.6 27.4 74.2 58.6 28.0 76.2 33.1 36.7 6.7 31.9 31.4 43.2 6.6</figDesc><table><row><cell></cell><cell>16.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 70.3 53.8 26.4 49.9 17.2 25.9 6.5 25.3 36.0 36.6 -</cell></row><row><cell>Baseline (TAN) [40]</cell><cell>R AT 86.5 25.9</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work is partially supported by the National Natural Science Foundation of China (No. 61572211).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="834" to="848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Co-training for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2456" to="2464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2011" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09020</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3213" to="3223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Filter pruning via geometric median for deep convolutional neural networks acceleration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03213</idno>
		<title level="m">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02649</idno>
		<title level="m">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Contrastive adaptation network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep adversarial attention alignment for unsupervised domain adaptation: the benefit of target expectation maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="401" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05192</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Semantic-aware gradgan for virtual-to-real urban scene adaption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01726</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02791</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Transfer learning from multiple source domains via consensus regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM conference on Information and knowledge management</title>
		<meeting>the 17th ACM conference on Information and knowledge management</meeting>
		<imprint>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Macro-micro adversarial network for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="418" to="434" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. icml</title>
		<meeting>icml</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V D</forename><surname>Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Asymmetric tritraining for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08400</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.01575</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Adversarial dropout regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02560</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation for semantic segmentation with gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06969</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Convolutional wasserstein distances: Efficient optimal transportation on geometric domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De Goes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Peyré</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Butscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">66</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.10349</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4068" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">A fully convolutional tribranch network (fctn) for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03694</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person reidentication</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Tri-training: Exploiting unlabeled data using three classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1529" to="1541" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Sim-real joint reinforcement transfer for 3d indoor navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via classbalanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">Vijaya</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
