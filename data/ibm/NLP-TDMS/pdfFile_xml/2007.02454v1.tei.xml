<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Self-Challenging Improves Cross-Domain Generalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyi</forename><surname>Huang</surname></persName>
							<email>zeyih@andrew</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haohan</forename><surname>Wang</surname></persName>
							<email>haohanw@cs</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
							<email>epxing@cs</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Self-Challenging Improves Cross-Domain Generalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T18:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>cross-domain generalization, robustness</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional Neural Networks (CNN) conduct image classification by activating dominant features that correlated with labels. When the training and testing data are under similar distributions, their dominant features are similar, leading to decent test performance. The performance is nonetheless unmet when tested with different distributions, leading to the challenges in cross-domain image classification. We introduce a simple training heuristic, Representation Self-Challenging (RSC), that significantly improves the generalization of CNN to the outof-domain data. RSC iteratively challenges (discards) the dominant features activated on the training data, and forces the network to activate remaining features that correlates with labels. This process appears to activate feature representations applicable to out-of-domain data without prior knowledge of new domain and without learning extra network parameters. We present theoretical properties and conditions of RSC for improving cross-domain generalization. The experiments endorse the simple, effective, and architecture-agnostic nature of our RSC method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Imagine teaching a child to visually differentiate "dog" from "cat": when presented with a collection of illustrations from her picture books, she may immediately answer that "cats tend to have chubby faces" and end the learning. However, if we continue to ask for more differences, she may start to notice other features like ears or body-size. We conjecture this follow-up challenge question plays a significant role in helping human reach the remarkable generalization ability. Most people should be able to differentiate "cat" from "dog" visually even when the images are presented in irregular qualities. After all, we did not stop learning after we picked up the first clue when we were children, even the first clue was good enough to help us recognize all the images in our textbook.</p><p>Nowadays, deep neural networks have exhibited remarkable empirical results over various computer vision tasks, yet these impressive performances seem unmet when the models are tested with the samples in irregular qualities (i.e.,  <ref type="figure">Fig. 1</ref>. The essence of our Representation Self-Challenging (RSC) training method: top two panels: the algorithm mutes the feature representations associated with the highest gradient, such that the network is forced to predict the labels through other features; bottom panel: after training, the model is expected to leverage more features for prediction in comparison to models trained convetionally.</p><p>out-of-domain data, samples collected from the distributions that are similar to, but different from the distributions of the training samples). To account for this discrepancy, technologies have been invented under the domain adaptation regime <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, where the goal is to train a model invariant to the distributional differences between the source domain (i.e., the distribution of the training samples) and the target domain (i.e., the distribution of the testing samples) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>As the influence of machine learning increases, the industry starts to demand the models that can be applied to the domains that are not seen during the training phase. Domain generalization <ref type="bibr" target="#b17">[18]</ref>, as an extension of domain adaptation, has been studied as a response. The central goal is to train a model that can align the signals from multiple source domains.</p><p>Further, Wang et al. extend the problem to ask how to train a model that generalizes to an arbitrary domain with only the training samples, but not the corresponding domain information, as these domain information may not be available in the real world <ref type="bibr" target="#b30">[31]</ref>. Our paper builds upon this set-up and aims to offer a solution that allows the model to be robustly trained without domain information and to empirically perform well on unseen domains.</p><p>In this paper, we introduce a simple training heuristic that improves crossdomain generalization. This approach discards the representations associated with the higher gradients at each epoch, and forces the model to predict with remaining information. Intuitively, in a image classification problem, our heuristic works like a "self-challenging" mechanism as it prevents the fully-connected layers to predict with the most predictive subsets of features, such as the most frequent color, edges, or shapes in the training data. We name our method Representation Self Challenging (RSC) and illustrate its main idea in <ref type="figure">Figure 1</ref>.</p><p>We present mathematical analysis that RSC induces a smaller generalization bound. We further demonstrate the empirical strength of our method with domain-agnostic cross-domain evaluations, following previous setup <ref type="bibr" target="#b30">[31]</ref>. We also conduct ablation study to examine the alignment between its empirical performance and our intuitive understanding. The inspections also shed light upon the choices of its extra hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We summarize the related DG works from two perspectives: learning domain invariant features and augmenting source domain data. Further, as RSC can be broadly viewed as a generic training heuristic for CNN, we also briefly discuss the general-purpose regularizations that appear similar to our method.</p><p>DG through Learning Domain Invariant Features: These methods typically minimize the discrepancy between source domains assuming that the resulting features will be domain-invariant and generalize well for unseen target distributions. Along this track, Muandet et al. employed Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b17">[18]</ref>. Ghifary et al. proposed a multi-domain reconstruction auto-encoder <ref type="bibr" target="#b9">[10]</ref>. Li et al. applied MMD constraints to an autoencoder via adversarial training <ref type="bibr" target="#b14">[15]</ref>.</p><p>Recently, meta-learning based techniques start to be used to solve DG problems. Li et al. alternates domain-specific feature extractors and classifiers across domains via episodic training, but without using inner gradient descent update <ref type="bibr" target="#b13">[14]</ref>. <ref type="bibr">Balaji et al.</ref> proposed MetaReg that learns a regularization function (e.g., weighted 1 loss) particularly for the networks classification layer, while excluding the feature extractor <ref type="bibr" target="#b0">[1]</ref>.</p><p>Further, recent DG works forgo the requirement of source domains partitions and directly learn the cross-domain generalizable representations through a mixed collection of training data. Wang et al. extracted robust feature representation by projecting out superficial patterns like color and texture <ref type="bibr" target="#b30">[31]</ref>. Wang et al. penalized models tendency in predicting with local features in order to extract robust globe representation <ref type="bibr" target="#b29">[30]</ref>. RSC follows this more recent path and directly activates more features in all source domain data for DG without knowledge of the partition of source domains.</p><p>DG through Augmenting Source Domain: These methods augment the source domain to a wider span of the training data space, enlarging the possibility of covering the span of the data in the target domain. For example, An auxiliary domain classifier has been introduced to augment the data by perturbing input data based on the domain classification signal <ref type="bibr" target="#b22">[23]</ref>. Volpi et al. developed an adversarial approach, in which samples are perturbed according to fictitious target distributions within a certain Wasserstein distance from the source <ref type="bibr" target="#b28">[29]</ref>. A recent method with state-of-the art performance is JiGen <ref type="bibr" target="#b3">[4]</ref>, which leverages self-supervised signals by solving jigsaw puzzles.</p><p>Key difference: These approaches usually introduce a model-specific DG model and rely on prior knowledge of the target domain, for instance, the tar-get spatial permutation is assumed by JiGen <ref type="bibr" target="#b3">[4]</ref>. In contrast, RSC is a modelagnostic training algorithm that aims to improve the cross-domain robustness of any given model. More importantly, RSC does not utilize any knowledge of partitions of domains, either source domain or target domain, which is the general scenario in real world application.</p><p>Generic Model Regularization: CNNs are powerful models and tend to overfit on source domain datasets. From this perspective, model regularization, e.g., weight decay <ref type="bibr" target="#b18">[19]</ref>, early stopping, and shake-shake regularization <ref type="bibr" target="#b7">[8]</ref>, could also improve the DG performance. Dropout <ref type="bibr" target="#b24">[25]</ref> mutes features by randomly zeroing each hidden unit of the neural network during the training phase. In this way, the network benefit from the assembling effect of small subnetworks to achieve a good regularization effect. Cutout <ref type="bibr" target="#b5">[6]</ref> and HaS <ref type="bibr" target="#b23">[24]</ref> randomly drop patches of input images. SpatialDropout <ref type="bibr" target="#b25">[26]</ref> randomly drops channels of a feature map. DropBlock <ref type="bibr" target="#b8">[9]</ref> drops contiguous regions from feature maps instead of random units. DropPath <ref type="bibr" target="#b10">[11]</ref> zeroes out an entire layer in training, not just a particular unit. MaxDrop <ref type="bibr" target="#b19">[20]</ref> selectively drops features of high activations across the feature map or across the channels. Adversarial Dropout <ref type="bibr" target="#b20">[21]</ref> dropouts for maximizing the divergence between the training supervision and the outputs from the network. <ref type="bibr" target="#b11">[12]</ref> leverages Adversarial Dropout <ref type="bibr" target="#b20">[21]</ref> to learn discriminative features by enforcing the cluster assumption.</p><p>Key difference: RSC differs from above methods in that RSC locates and mutes most predictive parts of feature maps by gradients instead of randomness, activation or prediction divergence maximization. This selective process plays an important role in improving the convergence, as we will briefly argue later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Notations: (x, y) denotes a sample-label pair from the data collection (X, Y) with n samples, and z (or Z) denotes the feature representation of (x, y) learned by a neural network. f (·; θ) denotes the CNN model, whose parameters are denoted as θ. h(·; θ top ) denotes the task component of f (·; θ); h(·; θ top ) takes z as input and outputs the logits prior to a softmax function; θ top denotes the parameters of h(·; θ top ). l(·, ·) denotes a generic loss function. RSC requires one extra scalar hyperparameter: the percentage of the representations to be discarded, denoted as p. Further, we use · to denote the estimated quantities, use· to denote the quantities after the representations are discarded, and use t in the subscript to index the iteration. For example, θ t means the estimated parameter at iteration t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Self-Challenging Algorithm</head><p>As a generic deep learning training method, RSC solves the same standard loss function as the ones used by many other neural networks, i.e.,</p><formula xml:id="formula_0">θ = arg min θ x,y ∼ X,Y l(f (x; θ), y),</formula><p>but RSC solves it in a different manner. At each iteration, RSC inspects the gradient, identifies and then mutes the most predictive subset of the representation z (by setting the corresponding values to zero), and finally updates the entire model.</p><p>This simple heuristic has three steps (for simplicity, we drop the indices of samples and assume the batch size is 1 in the following equations):</p><p>1. Locate: RSC first calculates the gradient of upper layers with respect to the representation as follows:</p><formula xml:id="formula_1">g z = ∂(h(z; θ top t ) y)/∂z,<label>(1)</label></formula><p>where denotes an element-wise product. Then RSC computes the (100 − p) th percentile, denoted as q p . Then it constructs a masking vector m in the same dimension of g as follows. For the i th element:</p><formula xml:id="formula_2">m(i) = 0, if g z (i) ≥ q p 1, otherwise<label>(2)</label></formula><p>In other words, RSC creates a masking vector m, whose element is set to 0 if the corresponding element in g is one of the top p percentage elements in g, and set to 1 otherwise. 2. Mute: For every representation z, RSC masks out the bits associated with larger gradients by:z</p><formula xml:id="formula_3">= z m<label>(3)</label></formula><p>3. Update: RSC computes the softmax with perturbed representation with</p><formula xml:id="formula_4">s = softmax(h(z; θ top t )),<label>(4)</label></formula><p>and then use the gradientg</p><formula xml:id="formula_5">θ = ∂l(s, y)/∂ θ t<label>(5)</label></formula><p>to update the entire model for θ t+1 with optimizers such as SGD or ADAM.</p><p>We summarize the procedure of RSC in Algorithm 1. No that operations of RSC comprise of only few simple operations such as pooling, threshold and element-wise product. Besides the weights of the original network, no extra parameter needs to be learned.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Theoretical Evidence</head><p>To expand the theoretical discussion smoothly, we will refer to the "dog" vs. "cat" classification example repeatedly as we progress. The basic set-up, as we introduced in the beginning of this paper, is the scenario of a child trying to learn the concepts of "dog" vs. "cat" from illustrations in her book: while the hypothesis "cats tend to have chubby faces" is good enough to classify all the animals in her picture book, other hypotheses mapping ears or body-size to labels are also predictive.</p><p>On the other hand, if she wants to differentiate all the "dogs" from "cats" in the real world, she will have to rely on a complicated combination of the features mentioned about. Our main motivation of this paper is as follows: this complicated combination of these features is already illustrated in her picture book, but she does not have to learn the true concept to do well in her finite collection of animal pictures.</p><p>This disparity is officially known as "covariate shift" in domain adaptation literature: the conditional distribution (i.e., the semantic of a cat) is the same across every domain, but the model may learn something else (i.e., chubby faces) due to the variation of marginal distributions.</p><p>With this connection built, we now proceed to the theoretical discussion, where we will constantly refer back to this "dog" vs. "cat" example.</p><p>Background As the large scale deep learning models, such as AlexNet or ResNet, are notoriously hard to be analyzed statistically, we only consider a simplified problem to argue for the theoretical strength of our method: we only concern with the upper layer h(·; θ top ) and illustrate that our algorithm helps improve the generalization of h(·; θ top ) when Z is fixed. Therefore, we can directly treat Z as the data (features). Also, for convenience, we overload θ to denote θ top within the theoretical evidence section.</p><p>We expand our notation set for the theoretical analysis. As we study the domain-agnostic cross-domain setting, we no longer work with i.i.d data. Therefore, we use Z and Y to denote the collection of distributions of features and labels respectively. Let Θ be a hypothesis class, where each hypothesis θ ∈ Θ maps Z to Y. We use a set D (or S) to index Z, Y and θ. Therefore, θ (D) denotes the hypothesis with minimum error in the distributions specified with D, but with no guarantees on the other distributions. e.g., θ (D) can be "cats have chubby faces" when D specifies the distribution to be picture book.</p><p>Further, θ denotes the classifier with minimum error on every distribution considered. If the hypothesis space is large enough, θ should perform no worse than θ (D) on distributions specified by D for any D.</p><p>e.g., θ is the true concept of "cat", and it should predict no worse than "cats have chubby faces" even when the distribution is picture book.</p><p>We use θ to denote any ERM and use θ RSC to denote the ERM estimated by the RSC method. Finally, following conventions, we consider l(·, ·) as the zeroone loss and use a shorthand notation L(θ; D) = E z,y ∼ Z(D),Y(D) l(h(z; θ), y) for convenience, and we only consider the finite hypothesis class case within the scope of this paper, which leads to the first formal result:</p><formula xml:id="formula_6">Corollary 1. If |e(z(S); θ RSC ) − e(z(S); θ RSC )| ≤ ξ(p),<label>(6)</label></formula><p>where e(·; ·) is a function defined as e(z; θ ) := E z,y ∼S l(f (z; θ ); y) and ξ(p) is a small number and a function of RSC's hyperparameter p;z is the perturbed version of z generated by RSC, it is also a function of p, but we drop the notation for simplicity. If Assumptions A1, A2, and A3 (See Appendix) hold, we have, with probability at least 1 − δ</p><formula xml:id="formula_7">L( θ RSC (S); S) − L(θ RSC (S); D) ≤ (2ξ(p) + 1) 2(log(2|Θ RSC |) + log(2/δ)) n</formula><p>As the result shows, whether RSC will succeed depends on the magnitude of ξ(p). The smaller ξ(p) is, the tighter the bound is, the better the generalization bound is. Interestingly, if ξ(p) = 0, our result degenerates to the classical generalization bound of i.i.d data.</p><p>While it seems the success of our method will depend on the choice of Θ to meet Condition 6, we will show RSC is applicable in general by presenting it forces the empirical counterpart ξ(p) to be small. ξ(p) is defined as</p><formula xml:id="formula_8">ξ(p) :=|h( θ RSC , z) − h( θ RSC ,z)|,</formula><p>where the function h(·, ·) is defined as</p><formula xml:id="formula_9">h( θ RSC , z) = (z,y)∼S l(f (z; θ RSC ); y).<label>(7)</label></formula><p>We will show ξ(p) decreases at every iteration with more assumptions:</p><p>A4: Discarding the most predictive features will increase the loss at current iteration. A5: The learning rate η is sufficiently small (η 2 or higher order terms are negligible).</p><p>Formally, Corollary 2. If Assumption A4 holds, we can simply denote</p><formula xml:id="formula_10">h( θ RSC (t),z t ) = γ t (p)h( θ RSC (t), z t ),</formula><p>where h(·, ·) is defined in Equation 7. γ t (p) is an arbitrary number greater than 1, also a function of RSC's hyperparameter p. Also, if Assumption A5 holds, we have:</p><formula xml:id="formula_11">Γ ( θ RSC (t + 1)) = Γ ( θ RSC (t)) − (1 − 1 γ t (p) )||g|| 2 2 η where Γ ( θ RSC (t)) := |h( θ RSC (t), z t ) − h( θ RSC (t),z t )|</formula><p>t denotes the iteration, z t (orz t ) denotes the features (or perturbed features) at iteration t, andg = ∂h( θ RSC (t),z t )/∂ θ RSC (t) <ref type="figure">Fig. 2</ref>. Γ ( θRSC(t)), i.e., "Loss Difference", plotted for the PACS experiment (details of the experiment setup will be discussed later). Except for the first epoch, Γ ( θRSC(t)) decreases consistently along the training process.</p><p>Notice that ξ(p) = Γ ( θ RSC ), where θ RSC is θ RSC (t) at the last iteration t. We can show that ξ(p) is a small number because Γ ( θ RSC (t)) gets smaller at every iteration. This discussion is also verified empirically, as shown in <ref type="figure">Figure 2</ref>.</p><p>The decreasing speed of Γ ( θ RSC (t)) depends on the scalar γ t (p): the greater γ t (p) is, the faster Γ ( θ RSC (t)) descends. Further, intuitively, the scale of γ t (p) is highly related to the mechanism of RSC and its hyperparameter p. For example, RSC discards the most predictive representations, which intuitively guarantees the increment of the empirical loss (Assumption A4).</p><p>Finally, the choice of p governs the increment of the empirical loss: if p is small, the perturbation will barely affect the model, thus the increment will be small; while if p is large, the perturbation can alter the model's response dramatically, leading to significant ascend of the loss. However, we cannot blindly choose the largest possible p because if p is too large, the model may not be able to learn anything predictive at each iteration.</p><p>In summary, we offer the intuitive guidance of the choice of hyperparamter p: for the same model and setting, the smaller p is, the smaller the training error will be; the bigger p is, the smaller the (cross-domain) generalization error (i.e., difference between testing error and training error) will be.</p><p>Therefore, the success of our method depends on the choice of p as a balance of the above two goals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Engineering Specification &amp; Extensions</head><p>For simplicity, we detail the RSC implementation on a ResNet backbone + FC classification network. RSC is applied to the training phase, and operates on the last convolution feature tensor of ResNet. Denote the feature tensor of an input sample as Z and its gradient tensor of as G. G is computed by back propagating the classification score with respect to the ground truth category. Both of them are of size [7 × 7 × 512]. Spatial-wise RSC: In the training phase, global average pooling is applied along the channel dimension to the gradient tensor G to produce a weighting matrix w i of size <ref type="bibr">[7 × 7]</ref>. Using this matrix, we select top p percentage of the 7 × 7 = 49 cells, and mute its corresponding features in Z. Each of the 49 cells correspond to a [1×1×512] feature vector in Z. After that, the new feature tensor Z new is forwarded to the new network output. Finally, the network is updated through back-propagation. We refer this setup as spatial-wise RSC, which is the default RSC for the rest of this paper.</p><p>Channel-wise RSC: RSC can also be implemented by dropping features of the channels with high-gradients. The rational behind the channel-wise RSC lies in the convolutional nature of DNNs. The feature tensor of size [7 × 7 × 512] can be considered a decomposed version of input image, where instead of the RGB colors, there are 512 different characteristics of the each pixels. The C characteristics of each pixel contains different statistics of training data from that of the spatial feature statistics.</p><p>For channel-wise RSC, global average pooling is applied along the spatial dimension of G, and produce a weighting vector of size <ref type="bibr">[1 × 512]</ref>. Using this vector, we select top p percentage of its 512 cells, and mute its corresponding features in Z. Here, each of the 512 cells correspond to a [7 × 7] feature matrix in Z. After that, the new feature tensor Z new is forwarded to the new network output. Finally, the network is updated through back-propagation.</p><p>Batch Percentage: Some dropout methods like curriculum dropout <ref type="bibr" target="#b16">[17]</ref> do not apply dropout at the beginning of training, which improves CNNs by learning basic discriminative clues from unchanged feature maps. Inspired by these methods, we randomly apply RSC to some samples in each batch, leaving the other unchanged. This introduces one extra hyperparameter, namely Batch Percentage: the percentage of samples to apply RSC in each batch. We also apply RSC to top percentage of batch samples based on cross-entropy loss. This setup is slight better than randomness.</p><p>Detailed ablation study on above extensions will be conducted in the experiment section below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We consider the following four data collections as the battleground to evaluate RSC against previous methods.  <ref type="bibr" target="#b29">[30]</ref>: 1000 classes with two domains. The protocol is to train on standard ImageNet <ref type="bibr" target="#b21">[22]</ref> training set and test on ImageNet-Sketch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation Study</head><p>We conducted five ablation studies on possible configurations for RSC on the PACS dataset <ref type="bibr" target="#b12">[13]</ref>. All results were produced based on the ResNet18 baseline in <ref type="bibr" target="#b3">[4]</ref> and were averaged over five runs.</p><p>(1) Feature Dropping Strategies <ref type="table" target="#tab_0">(Table 1)</ref>. We compared the two attention mechanisms to select the most discriminative spatial features. The "Top-Activiation" <ref type="bibr" target="#b19">[20]</ref> selects the features with highest norms, whereas the "Top-Gradient" (default in RSC) selects the features with high gradients. The comparison shows that "Top-Gradient" is better than "Top-Activation", while both are better than the random strategy. Without specific note, we will use "Top-Gradient" as default in the following ablation study.</p><p>(2) Feature Dropping Percentage (choice of p) ( <ref type="table" target="#tab_3">Table 2</ref>): We ran RSC at different dropping percentages to mute spatial feature maps. The highest average accuracy was reached at p = 33.3%. While the best choice of p is data-specific, our results align well with the theoretical discussion: the optimal p should be neither too large nor too small.</p><p>(3) Batch Percentage <ref type="table" target="#tab_4">(Table 3)</ref>: RSC has the option to be only randomly applied to a subset of samples in each batch.    <ref type="table">Table 4</ref>. Ablation study of Spatial-wise RSC verse Spatial+Channel RSC. We used the best strategy and parameter by  <ref type="table">Table 5</ref>. Ablation study of Dropout methods. "S" and "C" represent spatial-wise and channel-wise respectively. For fair comparison, results of above methods are report at their best setting and hyperparameters. RSC used the hyperparameters selected in above ablation studies:"Top-Gradient", Feature Dropping Percentage (33.3%) and Batch Percentage (33.3%).</p><p>is relatively constant. Nevertheless we still choose 33.3% as the best option on the PACS dataset.</p><p>(4) Spatial-wise plus Channel-wise RSC <ref type="table">(Table 4</ref>): In "Spatial+Channel", both spatial-wise and channel-wise RSC were applied on a sample at 50% probability, respectively. (Better options of these probabilities could be explored.) Its improvement over Spatial-wise RSC indicates that it further activated features beneficial to target domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PACS</head><p>backbone artpaint cartoon sketch photo Avg ↑  <ref type="table">Table 7</ref>. DG results on VLCS <ref type="bibr" target="#b26">[27]</ref> (Best in bold).</p><p>(5) Comparison with different dropout methods ( <ref type="table">Table 5</ref>): Dropout has inspired a number of regularization methods for CNNs. The main differences between those methods lie in applying stochastic or non-stochastic dropout mechanism at input data, convolutional or fully connected layers. Results shows that our gradient-based RSC is better. We believe that gradient is an efficient and straightforward way to encode the sensitivity of output prediction. To the best of our knowledge, we compare with the most related works and illustrate the impact of gradients. (a) Cutout <ref type="bibr" target="#b5">[6]</ref>. Cutout conducts random dropout on input images, which shows limited improvement over the baseline. (b) DropBlock <ref type="bibr" target="#b8">[9]</ref>. DropBlock tends to dropout discriminative activated parts spatially. It is better than random dropout but inferior to non-stochastic dropout methods in <ref type="table">Table 5</ref> such as AdversarialDropout, Top-Activation and our RSC. (c) Adver-sarialDropout <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b11">12]</ref>. AdversarialDropout is based on divergence maximization, while RSC is based on top gradients in generating dropout masks. Results show evidence that the RSC is more effective than AdversarialDropout. (d) Random and Top-Activation dropout strategies at their best hyperparameter settings.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Cross-Domain Evaluation</head><p>Through the following experiments, we used "Top-Gradient" as feature dropping strategy, 33.3% as Feature Dropping Percentages, 33.3% as Batch Percentage, and Spatial+Channel RSC. All results were averaged over five runs. In our RSC implementation, we used the SGD solver, 30 epochs, and batch size 128. The learning rate starts with 0.004 for ResNet and 0.001 for AlexNet, learning rate decayed by 0.1 after 24 epochs. For PACS experiment, we used the same data augmentation protocol of randomly cropping the images to retain between 80% to 100%, randomly applied horizontal flipping and randomly (10% probability) convert the RGB image to greyscale, following <ref type="bibr" target="#b3">[4]</ref>.</p><p>In <ref type="table">Table.</ref> 6,7,8, we compare RSC with the latest domain generalization work, such as Hex <ref type="bibr" target="#b30">[31]</ref>, PAR <ref type="bibr" target="#b29">[30]</ref>, JiGen <ref type="bibr" target="#b3">[4]</ref> and MetaReg <ref type="bibr" target="#b0">[1]</ref>. All these work only report results on different small networks and datasets. For fair comparison, we compared RSC to their reported performances with their most common choices of DNNs (i.e., AlexNet, ResNet18, and ResNet50) and datasets. RSC consistently outperforms other competing methods.</p><p>The empirical performance gain of RSC can be better appreciated if we have a closer look at the PACS experiment in <ref type="table">Table.</ref> 6. The improvement of RSC from the latest baselines <ref type="bibr" target="#b3">[4]</ref> are significant and consistent: 4.5 on AlexNet, 5.2 on ResNet18, and 4.5 on ResNet50. It is noticeable that, with both ResNet18 and ResNet50, RSC boosts the performance significantly for sketch domain, which is the only colorless domain. The model may have to understand the semantics of the object to perform well on the sketch domain. On the other hand, RSC performs only marginally better than competing methods in photo domain, which is probably because that photo domain is the simplest one and every method has already achieved high accuracy on it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Standard ImageNet Benchmark: With the impressive performance observed in the cross-domain evaluation, we further explore to evaluate the benefit of RSC with other benchmark data and higher network capacity.  <ref type="table" target="#tab_0">Table 10</ref>. Generalization results on ImageNet. Baseline was produced with official Pytorch implementation and their ImageNet models.</p><p>We conducted image classification experiments on the Imagenet database <ref type="bibr" target="#b21">[22]</ref>. We chose three backbones with the same architectural design while with clear hierarchies in model capacities: ResNet50, ResNet101, and ResNet152. All models were finetuned for 80 epochs with learning rate decayed by 0.1 every 20 epochs. The initial learning rate for ResNet was 0.01. All models follow extra the same training prototype in default Pytorch ImageNet implementation 1 , using original batch size of 256, standard data augmentation and 224 × 224 as input size.</p><p>The results in <ref type="table" target="#tab_0">Table 10</ref> shows that RSC exhibits the ability reduce the performance gap between networks of same family but different sizes (i.e., ResNet50 with RSC approaches the results of baseline ResNet101, and ResNet101 with RSC approaches the results of baseline ResNet151). The practical implication is that, RSC could induce faster performance saturation than increasing model sizes. Therefore one could scale down the size of networks to be deployed at comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduced a simple training heuristic method that can be directly applied to almost any CNN architecture with no extra model architecture, and almost no increment of computing efforts. We name our method Representation Selfchallenging (RSC). RSC iteratively forces a CNN to activate features that are less dominant in the training domain, but still correlated with labels. Theoretical and empirical analysis of RSC validate that it is a fundamental and effective way of expanding feature distribution of the training domain. RSC produced the state-of-the-art improvement over baseline CNNs under the standard DG settings of small networks and small datasets. Moreover, our work went beyond the standard DG settings, to illustrate effectiveness of RSC on more prevalent problem scales, e.g., the ImageNet database and network sizes up-to ResNet152.</p><p>Notice that this does not contradict with our cross-domain set-up: while Assumption A3 implies that data from any distribution of interest is i.i.d (otherwise the operation E A [] is not valid), the cross-domain difficulty is raised when only different subsets of A are used for train and test. For example, considering A to be a uniform distribution of [0, 1], while the train set is uniformly sampled from [0, 0.5] and the test set is uniformly sampled from (0.5, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2 Proof of Theoretical Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A2.1 Corollary 1</head><p>Proof. We first study the convergence part, where we consider a fixed hypothesis. We first expand which directly leads us to the fact that |L(θ RSC (S); S) − L(θ RSC (S); D)| has the expectation 0 (A4) and bounded by [0, ξ(p)].</p><p>For |L( θ RSC (S); S) − L(θ RSC (S); S)|, the strategy is relatively standard. We first consider the convergence of a fixed hypothesis θ RSC , then over n i.i.d samples, the empirical risk ( L(θ RSC )) will be bounded within [0, 1] with the expectation L(θ RSC ).</p><p>Before we consider the uniform convergence step, we first put the two terms together and apply the Hoeffding's inequality. When the random variable is with expectation L(θ RSC ) and bound [0, 1 + 2ξ(p)], we have: Rearranging these terms following standard tricks will lead to the conclusion. and h( θ RSC (t), z) − h( θ RSC (t + 1), z) = 1 γ t (p) ∂h( θ RSC (t),z) ∂ θ RSC (t)g η = 1 γ t (p) ||g|| 2 2 η</p><p>We write these terms back and get Γ ( θ RSC (t + 1)) = ( 1 γ t (p) − 1)||g|| 2 2 η + Γ ( θ RSC (t))</p><p>We can simply drop the absolute value sign because all these terms are greater than zero. Finally, we rearrange these terms and prove the conclusion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>|L( θ RSC (S); S) − L(θ RSC (S); D)|= |L( θ RSC (S); S) − L( θ RSC (S); D) + L( θ RSC (S); D) − L(θ RSC (S); D)| ≤ |L( θ RSC (S); S) − L(θ RSC (S); D)| + |L(θ RSC (S); D) − L(θ RSC (S); D)| We first consider the term |L(θ RSC (S); S) − L(θ RSC (S); D)|, where we can expand |L(θ RSC (S); S) − L(θ RSC (S); D)| ≤ 2|L(θ RSC (S); S) − L(θ RSC (S); O)|because of Assumption A4. Also, because of Assumption A4, if samples in S are perturbed versions of samples in O, then samples in O can also be seen as perturbed versions of samples in S, thus, Condition 6 can be directly re-written into: |L(θ RSC (S); S) − L(θ RSC (S); O)| ≤ ξ(p),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>P</head><label></label><figDesc>(| L(θ RSC ; S) − L(θ RSC ; D)| ≥ ) ≤ 2 exp(− 2n 2 (2ξ(p) + 1) 2 )Now, we consider the uniform convergence case, where we have:P( sup θRSC∈ΘRSC | L(θ RSC ; S) − L(θ RSC ; D)| ≥ ) ≤ 2|Θ RSC | exp(− 2n 2 (2ξ(p) + 1) 2 )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Algorithm 1 :</head><label>1</label><figDesc>RSC Update Algorithm Input: data set X, Y , percentage of representations to discard p, other configurations such as learning rate η, maximum number of epoches T , etc; Output: Classifier f (·; θ); random initialize the model θ0; while t ≤ T do for every sample (or batch) x, y do calculate z through forward pass; calculate gz with Equation 1; calculate qp and m as in Equation 2; generatez with Equation 3; calculate gradientg θ with Equation 4 and Equation 5; update θt+1 as a function of θt andg θ end end</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>-PACS [13]: seven classes over four domains (Artpaint, Cartoon, Sketches, and Photo). The experimental protocol is to train a model on three domains and test on the remaining domain. -VLCS [27]: five classes over four domains. The domains are defined by four image origins, i.e., images were taken from the PASCAL VOC 2007, LabelMe, Caltech and Sun datasets.</figDesc><table><row><cell>-Office-Home [28]: 65 object categories over 4 domains (Art, Clipart, Prod-</cell></row><row><cell>uct, and Real-World).</cell></row><row><cell>-ImageNet-Sketch</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 Table 1 .</head><label>31</label><figDesc>shows that the performance Feature Drop Strategies backbone artpaint cartoon sketch photo Avg ↑ 76.13 95.72 82.03 Top-Gradient ResNet18 81.23 77.23 77.56 95.61 82.91 Ablation study of Spatial-wise RSC on Feature Dropping Strategies. Feature Dropping Percentage 50.0% and Batch Percentage 50.0%.</figDesc><table><row><cell>Baseline [4]</cell><cell>ResNet18 78.96</cell><cell>73.93 70.59 96.28 79.94</cell></row><row><cell>Random</cell><cell>ResNet18 79.32</cell><cell>75.27 74.06 95.54 81.05</cell></row><row><cell cols="3">Top-Activation 76.05 Feature Dropping Percentage backbone artpaint cartoon sketch photo Avg↑ ResNet18 80.31</cell></row><row><cell>66.7%</cell><cell>ResNet18 80.11</cell><cell>76.35 76.24 95.16 81.97</cell></row><row><cell>50.0%</cell><cell>ResNet18 81.23</cell><cell>77.23 77.56 95.61 82.91</cell></row><row><cell>33.3%</cell><cell cols="2">ResNet18 82.87 78.23 78.89 95.82 83.95</cell></row><row><cell>25.0%</cell><cell>ResNet18 81.63</cell><cell>78.06 78.12 96.06 83.46</cell></row><row><cell>20.0%</cell><cell>ResNet18 81.22</cell><cell>77.43 77.83 96.25 83.18</cell></row><row><cell>13.7%</cell><cell>ResNet18 80.71</cell><cell>77.18 77.12 96.36 82.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of Spatial-wise RSC on Feature Dropping Percentage. We used "Top-Gradient" and fixed the Batch Percentage (50.0%) here.</figDesc><table><row><cell cols="3">Batch Percentage backbone artpaint cartoon sketch photo Avg↑</cell></row><row><cell>50.0%</cell><cell>ResNet18 82.87</cell><cell>78.23 78.89 95.82 83.95</cell></row><row><cell>33.3%</cell><cell>ResNet18 82.32</cell><cell>78.75 79.56 96.05 84.17</cell></row><row><cell>25.0%</cell><cell>ResNet18 81.85</cell><cell>78.32 78.75 96.21 83.78</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Ablation study of Spatial-wise RSC on Batch Percentage. We used "Top-Gradient" and fixed Feature Dropping Percentage (33.3%).</figDesc><table><row><cell>Method</cell><cell cols="2">backbone artpaint cartoon sketch photo Avg↑</cell></row><row><cell>Spatial</cell><cell>ResNet18 82.32</cell><cell>78.75 79.56 96.05 84.17</cell></row><row><cell cols="3">Spatial+Channel ResNet18 83.43 80.31 80.85 95.99 85.15</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>"Top-Gradient", Feature Dropping Percentage(33.3%) and Batch Percentage(33.3%). Top-Gradient(S+C) ResNet18 83.43 80.31 80.85 95.99 85.15</figDesc><table><row><cell>Method</cell><cell cols="2">backbone artpaint cartoon sketch photo Avg↑</cell></row><row><cell>Baseline [4]</cell><cell>ResNet18 78.96</cell><cell>73.93 70.59 96.28 79.94</cell></row><row><cell>Cutout[6]</cell><cell>ResNet18 79.63</cell><cell>75.35 71.56 95.87 80.60</cell></row><row><cell>DropBlock[9]</cell><cell>ResNet18 80.25</cell><cell>77.54 76.42 95.64 82.46</cell></row><row><cell cols="2">AdversarialDropout[21] ResNet18 82.35</cell><cell>78.23 75.86 96.12 83.07</cell></row><row><cell>Random(S+C)</cell><cell>ResNet18 79.55</cell><cell>75.56 74.39 95.36 81.22</cell></row><row><cell>Top-Activation(S+C)</cell><cell>ResNet18 81.03</cell><cell>77.86 76.65 96.11 82.91</cell></row><row><cell>RSC:</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>DG results on PACS<ref type="bibr" target="#b12">[13]</ref> (Best in bold).</figDesc><table><row><cell>VLCS</cell><cell cols="2">backbone Caltech Labelme Pascal Sun Avg ↑</cell></row><row><cell cols="2">Baseline[4] AlexNet 96.25</cell><cell>59.72 70.58 64.51 72.76</cell></row><row><cell cols="2">Epi-FCR[14] AlexNet 94.10</cell><cell>64.30 67.10 65.90 72.90</cell></row><row><cell>JiGen[4]</cell><cell>AlexNet 96.93</cell><cell>60.90 70.62 64.30 73.19</cell></row><row><cell>MASF[7]</cell><cell>AlexNet 94.78</cell><cell>64.90 69.14 67.64 74.11</cell></row><row><cell cols="2">RSC(ours) AlexNet 97.61</cell><cell>61.86 73.93 68.32 75.43</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>ResNet18 53.04 47.51 71.47 72.79 61.20 RSC(ours) ResNet18 58.42 47.90 71.63 74.54 63.12</figDesc><table><row><cell cols="2">Office-Home backbone Art Clipart Product Real Avg ↑</cell></row><row><cell>Baseline[4] ResNet18 52.15 45.86</cell><cell>70.86 73.15 60.51</cell></row><row><cell>JiGen[4]</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>DG results on Office-Home<ref type="bibr" target="#b27">[28]</ref> (Best in bold).</figDesc><table><row><cell cols="4">ImageNet-Sketch backbone Top-1 Acc ↑ Top-5 Acc ↑</cell></row><row><cell>Baseline[31]</cell><cell>AlexNet</cell><cell>12.04</cell><cell>24.80</cell></row><row><cell>Hex[31]</cell><cell>AlexNet</cell><cell>14.69</cell><cell>28.98</cell></row><row><cell>PAR [30]</cell><cell>AlexNet</cell><cell>15.01</cell><cell>29.57</cell></row><row><cell>RSC(ours)</cell><cell>AlexNet</cell><cell>16.12</cell><cell>30.78</cell></row><row><cell cols="4">Table 9. DG results on ImageNet-Sketch [30].</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>ImageNet backbone Top-1 Acc ↑ Top-5 Acc ↑ #Param. ↓</figDesc><table><row><cell>Baseline ResNet50</cell><cell>76.13</cell><cell>92.86</cell><cell>25.6M</cell></row><row><cell>RSC(ours) ResNet50</cell><cell>77.18</cell><cell>93.53</cell><cell>25.6M</cell></row><row><cell>Baseline ResNet101</cell><cell>77.37</cell><cell>93.55</cell><cell>44.5M</cell></row><row><cell>RSC(ours) ResNet101</cell><cell>78.23</cell><cell>94.16</cell><cell>44.5M</cell></row><row><cell>Baseline ResNet152</cell><cell>78.31</cell><cell>94.05</cell><cell>60.2M</cell></row><row><cell>RSC(ours) ResNet152</cell><cell>78.89</cell><cell>94.43</cell><cell>60.2M</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/pytorch/examples</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A1 Assumptions</head><p>A1: Θ is finite; l(·, ·) is zero-one loss for binary classification.</p><p>The assumption leads to classical discussions on the i.i.d setting in multiple textbooks (e.g., <ref type="bibr" target="#b15">[16]</ref>). However, modern machine learning concerns more than the i.i.d setting, therefore, we need to quantify the variations between train and test distributions. Analysis of domain adaptation is discussed <ref type="bibr" target="#b1">[2]</ref>, but still relies on the explicit knowledge of the target distribution to quantify the bound with an alignment of the distributions. The following discussion is devoted to the scenario when we do not have the target distribution to align.</p><p>Since we are interested in the θ instead of the θ (D), we first assume Θ is large enough and we can find a global optimum hypothesis that is applicable to any distribution, or in formal words:</p><p>This assumption can be met when the conditional distribution P(Y(D)|Z(D)) is the same for any D.</p><p>e.g., The true concept of "cat" is the same for any collection of images.</p><p>The challenge of cross-domain evaluation comes in when there exists multiple optimal hypothesis that are equivalently good for one distribution, but not every optimal hypothesis can be applied to other distributions. e.g., For the distribution of picture book, "cats have chubby faces" can predict the true concept of "cat". A model only needs to learn one of these signals to reduce training error, although the other signal also exists in the data.</p><p>The follow-up discussion aims to show that RSC can force the model to learn multiple signals, so that it helps in cross-domain generalization.</p><p>Further, Assumption A2 can be interpreted as there is at least some features z that appear in every distributions we consider. We use i to index this set of features. Assumption A2 also suggests that z i is i.i.d. (otherwise there will not exist θ ) across all the distributions of interest (but z is not i.i.d. because z −i , where −i denotes the indices other than i, can be sampled from arbitrary distributions). e.g., z is the image; z i is the ingredients of the true concept of a "cat", such as ears, paws, and furs; z −i is other features such as "sitting by the window".</p><p>We use O to specify the distribution that has values on the i th , but 0s elsewhere. We introduce the next assumption: </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="998" to="1008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recnorm: Simultaneous normalisation and classification applied to speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Bridle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="234" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Csurka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05374</idno>
		<title level="m">Domain adaptation for visual applications: A comprehensive survey</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Devries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.04552</idno>
		<title level="m">Improved regularization of convolutional neural networks with cutout</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Domain generalization via model-agnostic learning of semantic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kamnitsas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Glocker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13580</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gastaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07485</idno>
		<title level="m">Shake-shake regularization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dropblock: A regularization method for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ghiasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10727" to="10737" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain generalization for object recognition with multi-task autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Bastiaan Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2551" to="2559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07648</idno>
		<title level="m">Fractalnet: Ultra-deep neural networks without residuals</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Drop to adapt: Learning discriminative features for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">G</forename><surname>Jeong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deeper, broader and artier domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5542" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00113</idno>
		<title level="m">Episodic training for domain generalization</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jialin Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5400" to="5409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Mitchell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>McGraw Hill</publisher>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="870" to="877" />
			<pubPlace>Burr Ridge, IL</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Curriculum dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morerio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3544" to="3552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Simplifying neural networks by soft weight-sharing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="473" to="493" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Analysis on the dropout effect in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="189" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adversarial dropout for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">C</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Piratla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jyothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sarawagi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.10745</idno>
		<title level="m">Generalizing across domains via cross-gradient training</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE international conference on computer vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3544" to="3553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unbiased look at dataset bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: CVPR. vol</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Generalizing to unseen domains via adversarial data augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5334" to="5344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning robust global representations by penalizing local predictive power</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning robust representations by projecting superficial statistics out</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep visual domain adaptation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
