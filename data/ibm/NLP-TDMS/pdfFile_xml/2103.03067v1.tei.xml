<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TPCN: Temporal Point Cloud Networks for Motion Forecasting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosheng</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">‡</forename><forename type="middle">Tongyi</forename><surname>Cao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">DEEPROUTE.AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TPCN: Temporal Point Cloud Networks for Motion Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T19:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose the Temporal Point Cloud Networks (TPCN), a novel and flexible framework with joint spatial and temporal learning for trajectory prediction. Unlike existing approaches that rasterize agents and map information as 2D images or operate in a graph representation, our approach extends ideas from point cloud learning with dynamic temporal learning to capture both spatial and temporal information by splitting trajectory prediction into both spatial and temporal dimensions. In the spatial dimension, agents can be viewed as an unordered point set, and thus it is straightforward to apply point cloud learning techniques to model agents' locations. While the spatial dimension does not take kinematic and motion information into account, we further propose dynamic temporal learning to model agents' motion over time. Experiments on the Argoverse motion forecasting benchmark show that our approach achieves state-of-the-art results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Motion forecasting in autonomous driving concerns future trajectories of agents, including vehicles and pedestrians. For a self-driving car, the predicted future trajectories of surrounding traffic participants serve as key information to plan its future trajectories. A self-driving car should be able to predict the distribution or a few possible future trajectories of each agent as the future is full of uncertainty, given the relevant sensor input information in the past.</p><p>Traditional methods for motion forecasting <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39]</ref> are based on kinematic constraints and road map information with handcrafted rules. Though these approaches are sufficient in many simple situations, they fail to capture the rich behavior strategies and interaction in complex urban scenarios. Great progress has been made to explore the power of data-driven methods in motion forecasting with deep learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25]</ref>. These methods encode the agents (e.g., vehicles, pedestrians, and cyclists) and highdefinition map (HD map) information by rasterizing the corresponding elements (lanes, crosswalks) as lines and poly- ‡ Part of the work was done during an internship at DEEPROUTE.AI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Spatial Module Temporal Module</head><p>Predicted trajectories Input (agents + map) <ref type="figure">Figure 1</ref>. A high-level illustration of our approach. Red points represent the agent history trajectory points, while blue points are discrete map lane points. We use a spatial module based on point cloud learning to extract geometric features and a temporal module to extract sequential features. Both modules utilize the output of the other module and propagate mutually to output future trajectory points. gons with different colors. A standard image backbone network <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref> is then applied to the rasterized image to extract the map and agent features and perform motion prediction.</p><p>However, the rasterized image is an overly complex representation for environment and agent history and requires significantly more computation and data to train and deploy. More succinct representations have been explored to avoid this heavy process. VectorNet <ref type="bibr" target="#b9">[10]</ref> proposes a vector representation to exploit the spatial locality of individual road components with graph neural networks. LaneConv <ref type="bibr" target="#b18">[19]</ref> constructs a lane graph from vectorized map data and proposes LaneGCN to capture the topology and long dependency of the agents and map information. Both Vector-Net <ref type="bibr" target="#b9">[10]</ref> and LaneConv <ref type="bibr" target="#b18">[19]</ref> can be viewed as extensions of graph neural network in prediction with strong capability to extract spatial locality. However, both works fail to fully utilize the temporal information of agents with less focus on temporal feature extraction.</p><p>In this work, we extend ideas from 3D point cloud learning to the motion forecasting task. Previous work on point cloud network focuses on spatial points. We extend the met-ric space to the joint spatial-temporal space and represent the agents' history observations and map data as points in this space. Since the raw input data of prediction is a set of points that contain different agents with historical observations and map data, spatial and temporal learning will be two key components in prediction learning. Ignoring either information will lead to information loss and reduce the model's capability of context learning.</p><p>In order to combine spatial and temporal learning in a flexible and unified framework, we propose Temporal Point Cloud Networks (TPCN). Compared with GCN based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>, our TPCN does not manually specify the interaction structures (e.g., connectivity in the graph) and avoid the complex correlation learning process. TPCN models the prediction learning task as joint learning between a spatial module and a temporal module. In the spatial module, note that the waypoints and map points have very similar properties as point clouds, both being sparse and permutation invariant, and have a strong geometric correlation. Thus, point cloud learning strategies can be effective for spatial feature extractions. Instead of directly applying works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36</ref>] whose computation cost is high, we propose our novel spatial learning layer, namely Dual-representation Spatial Learning to obtain pointwise and voxelwise features through point cloud learning. Meanwhile, we propose Dynamic Temporal Learning in the temporal module to effectively extract the time-series information and motion estimation. Compared with traditional Hard Temporal Learning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>, our dynamic temporal learning layer naturally handles variant time length of different agents in the same sequence without the need to pad the history. By switching between the two modules, the spatial features and temporal features from these two modules are propagated mutually, each module taking the features of the other module as input. As such, spatial learning will utilize the temporal information (e.g., motion state), while the temporal learning will have some spatial guidance (e.g., map information), namely Joint Learning. <ref type="figure">Fig. 1</ref> illustrates the overall architecture of our approach. Note that we model the selection of multi-modal trajectories problem as displacement regression rather than classification.</p><p>In summary, our contributions reside as follows:</p><p>• We propose a novel and flexible architecture for prediction learning, which models the complex process as joint spatial and temporal learning. Dualrepresentation Spatial Learning for feature extraction of waypoints and map data is proposed as the spatial module. Meanwhile, we propose novel Dynamic Temporal Learning, consisting of Multi-interval Learning and Instance Pooling.</p><p>• We propose displacement prediction for multi-modal trajectories selection, which alleviates the hard assign-ment in classification through regression.</p><p>• Extensive experiments are conducted on the largescale Argoverse motion forecasting benchmark to show the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Most existing works on prediction can be roughly divided into three categories according to their representation and architecture. Rasterization based methods. Rasterization BEV images are the most common and direct ways to represent the structure of map and neighborhood relationships among agents. Some methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b24">25]</ref> render the HD map elements (junctions, lanes) as BEV images with different colors according to their types. In the format of images, a series of standard convolution layers or backbones <ref type="bibr" target="#b11">[12]</ref> can be applied to simplify the prediction task as trajectories selection and offset regression problems. Furtherly, some works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b24">25]</ref> propose to use anchor trajectories with human prior knowledge based on motion constraint to make the results more consistent with the current dynamic state and alleviate the difficulties in multi-modal prediction. However, these approaches have internal limitations since the performance is highly related to the spatial resolutions of the rasterized images. The temporal information can not be represented or modeled in the rendered images intuitively. GCN based methods. Graph Convolutional Network (GCN) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b29">30]</ref> nowadays gains its popularity in processing non-structural data and dealing with correlation relationship. Compared with traditional CNN, GCN shows its great promise in capturing the spatial locality on both euclidean and non-euclidean structure. With adjacency matrices, GCN focuses on learning the relationship between graph nodes and vertices. VectorNet <ref type="bibr" target="#b9">[10]</ref> introduces a novel vector representation and applies a graph neural network to predict the intent of vehicles. M Liang et al. <ref type="bibr" target="#b18">[19]</ref> proposes LaneGCN based on GCN, which is a specialized version designed for lane graphs. In order to capture the complex topology of HD map effectively, it combines with multiscale dilated convolution. Social-STGCNN <ref type="bibr" target="#b23">[24]</ref> models interactions as a graph by defining a novel kernel function to learn spatial and temporal patterns from pedestrian trajectories and behavior. However, GCN based methods are often faced with an efficiency problem when dealing with large scale scenarios which contain lots of nodes and vertices. Hybird Methods. In order to provide more interpretable and kinematic constraints, some works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b36">37]</ref> decouple the task as two-stage. Firstly, they discretize search space via uniform sampling or based on HD map to generate some proposals. Compared with anchor trajectories, these proposals can be more stable and more informative to capture the uncertainty of the multi-modal prediction. Sec-  ondly, they encode the HD map and agents via vector representation or rasterized images to furtherly refine each proposal. To some extent, two-stage methods can fully incorporate expert knowledge or classical planning or prediction approaches. On the other hand, it means the final outputs of refinement networks have a strong dependency on the quality of the proposals, which requires a reasonable sampling strategy or mature planning module.</p><formula xml:id="formula_0">Ins: 0 Ins: 1 t = 1 t = 0 t = 2 t = 0 t = 1 X Y Ins T 0 ! ! " ! 1 0 1 ! " " " 0 1 2 ! # " # 0 0 3 ! $ " $ 0 2 4 ! % " % 1 1 5 ! &amp; " &amp; 2 0 6 ! ' " '</formula><p>In comparison with these methods above, our TPCN has a novel representation and architecture, including spatial learning based on dual-representation point cloud learning and dynamic temporal information learning. We split the task into submodules to capture both spatial and temporal information effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The overall network architecture of our TPCN approach consists of two modules: 1) Dual-representation Spatial Learning and 2) Dynamic Temporal Learning. The Dualrepresentation Spatial Learning serves as the spatial module to model spatial features and the Dynamic Temporal Learning is the temporal module to extract temporal features. Both modules are integrated to propagate features mutually in spatial and temporal dimensions to achieve Joint Learning. As shown in <ref type="figure">Fig. 1</ref>, each module takes the pointwise features of the other module as input to generate corresponding pointwise output features, which is a natural foundation for fusing pointwise context information across multiple domains.</p><p>Typically, the motion prediction task will contain agents' data and environment information encoded with map data. We define an agent instance as an agent with a set of trajectory points. While map data refers to static objects without temporal information, a map instance can be described as an ordered point set for one specific element (e.g., a piece of lane centerline points). Thus, agent data will be represented by {p i,1 , p i,2 , . . . , p i,Ti }, where p i,t means the i-th agent's coordinate at time t, and T i is the time sequence length for i-th agent. Meanwhile, we represent map data in the for-</p><formula xml:id="formula_1">mat of {p i,1 , p i,2 , . . . , p i,Ni }, where p i,j is the j-th point of i-th map element instance with N i points in total.</formula><p>Voxelization. Given a grid size s, we can construct the mapping from a point</p><formula xml:id="formula_2">p i = (x i , y i ) to its voxel index v i : v i = ( x i /s , y i /s ) ,<label>(1)</label></formula><p>where · is a floor function. Thus we can build a hash table for the conversion between point coordinate space and voxel coordinate space</p><formula xml:id="formula_3">{p i , v i }.</formula><p>Instance Time Indexing. Apart from voxel and point spaces, we formulate the temporal space indexing system to address the dynamic and different sequential lengths of different agents. We represent all the instances over time as <ref type="figure" target="#fig_1">Fig. 2</ref> for an example. A special case is that t i is zero for all static instances in the map data.</p><formula xml:id="formula_4">{m i }, where the i-th element m i = (ins i , t i ) is an instance time index referring to the t i -th trajectory point of instance ins i . See</formula><p>Both voxelization and instance time indexing aim to provide space mapping or hashing: voxelization maps from the Cartesian coordinate system to a structural grid representation and instance time indexing maps from an index to a trajectory point of an instance. These mapping or hashing systems provide convenience for feature transformation between different spaces or representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dual-representation Spatial Learning</head><p>For the spatial module, we choose a point cloud learning approach to retrieve the spatial features of waypoints and map data with their locality and spatial geometry. Inspired by recent multiple representations learning <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32]</ref> that shows advantages over a single representation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b32">33]</ref>, we propose a dual-representation method to leverage the merits of mutually complementary information between voxel and point representations. The overall architecture of the spatial module is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>.</p><p>Pointwise Feature Learning. Pointwise features maintain geometric information and neighborhood relationship for interactions among points. To this end, we utilize the hierarchical feature learning from PointNet++ <ref type="bibr" target="#b26">[27]</ref> for pointwise feature extraction at different levels of the local neighborhood to exploit more local structure and correlation.</p><p>Point-Voxel Feature Propagation. Inspired by HVNet <ref type="bibr" target="#b35">[36]</ref>, we can transform pointwise features to voxel space by scattering operations. In this process, we maintain the hash table for each point, which stores the key and value pairs to map a Cartesian coordinate to a voxel index. Then, following the Feature Transformation Propagation algorithm (FTP) 1, we put all the points that share the same voxel index into the same cluster (each voxel may contain more than one point). Finally, we calculate the mean features over the points in each cluster as final features for the corresponding voxel, which is similar to average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Feature Transformation Propagation</head><p>Require: All pointwise features P F with corresponding indexing hash table H 1: clusters = {} 2: for each P F i ∈ P F do <ref type="bibr">3:</ref> clusters.at(H i ).append(P F i ) 4: end for 5: for each H i ∈ H do <ref type="bibr">6:</ref> clusters.at(H i ) = mean(clusters.at(H i )) 7: end for 8: return clusters Voxelwise Feature Learning. Voxelwise features have a strong capability to extract semantic context information <ref type="bibr" target="#b19">[20]</ref>. Most existing works apply 2D or 3D CNN to the rasterized or voxelized images for feature extraction in order to exploit the structural data with the current popular backbone <ref type="bibr" target="#b11">[12]</ref>. One of the key parameters for these methods is the grid size. Smaller grid size leads to less information loss but brings higher computation cost and latency. Meanwhile, compact 2D or 3D tensor of rasterized images neglect the sparsity of the input data and involve plenty of non-activated regions that may mess up feature learning.</p><p>Therefore, we employ the sparse convolution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b34">35]</ref> as our feature extractor to afford a smaller grid size for fine-grained voxelwise features. Furthermore, we build a Sparse BottleNeck network with skip connections, which replaces the bottleneck blocks with sparse convolutions in ResNet <ref type="bibr" target="#b11">[12]</ref>. Stacking Sparse BottleNeck layers not only quickly expands the receptive field at a low computational cost but also keeps the activation sparse.</p><p>Voxel-Point Feature Propagation. With voxelwise features, feature propagation from voxel representation to point representation can be performed by the naive nearest neighbor interpolation. PVCNN <ref type="bibr" target="#b19">[20]</ref> interpolates the pointwise features with corresponding neighboring voxelwise features. Since the interpolation weights are based on the physical distance to the neighboring grids accordingly, we extend the weights to be learnable by applying MLP to distance embedding that also concatenates associated voxelwise features.</p><p>Dual-representation Fusion. With pointwise and voxelwise features, we fuse these two types of features by feature concatenation. Thus, we obtain the features with dual representations and higher context information, which will be passed to the next stage of dynamic temporal learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dynamic Temporal Learning</head><p>In a motion prediction task, different agents have different lengths of observed past trajectories due to the different lifespan of each agent. Existing methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b18">19]</ref> pad the agents' data whose size is smaller than a given size T with zero in order to process data with the same length. We name this operation as Hard Temporal Learning (HTL). HTL has two main drawbacks: 1) padding data will introduce extra unnecessary computation cost, especially when the agents only appear in very few shots; 2) processing padded data will lead to the feature confusion problem, especially when invalid padding data involves the feature propagation. HTL forces the network to capture useless information.</p><p>We propose Dynamic Temporal Learning to address these limitations. Instead of padding, we only preserve the originally provided information without the requirement for a fixed time buffer size for each agent data. Therefore, we can retain each agent with a dynamic time sequence length. Furthermore, the Dynamic Temporal Learning consists of the following two parts.</p><p>Multi-interval Learning. The time interval is a key factor for time series data feature learning since it determines the time window size, similar to the receptive field in a CNN. Inspired by the multi-scale or multi-resolution hierarchy that has been proven its effectiveness in capturing local correlations and context, we also exploit Multiinterval Learning with prediction data. However, the main challenge for our application lies in the dynamic property. As a result, we utilize the Instance Time Indexing system and previously introduced FTP 1, and then perform high ef-  <ref type="figure" target="#fig_8">Figure 4</ref>. An example of multi-interval learning with an interval size of 2. ⊕ means tensor concatenation and points stand for the sequential historical waypoint data for one specific instance.</p><p>ficient Multi-interval Learning (MIL) according to the following Multi-interval Learning algorithm 2. Given a set of different intervals, we first regroup the instance time indexing to ensure that each point with the same instance ID and timestamp within the same interval will be clustered into the same group. Consequently, we employ the FTP algorithm 1 to obtain the mean features of the corresponding interval and map to pointwise features by the following two steps; 1) slicing by using the inverse hash or indexing table to gather the pointwise features and 2) concatenating the input tensor as a shortcut connection. Note that the output feature of the current time interval will be passed to the next level as input to progressively and aggressively capture features at increasingly larger intervals over a multi-resolution hierarchy. <ref type="figure" target="#fig_8">Fig. 4</ref> illustrates a special case for Multi-interval Learning with an interval size of 2. O p = concat(O p , F t ) 8: end for 9: return O p Instance Pooling. Instance-level features that aggregate information of an instance over time is a crucial component beyond the point, voxel, and time level features, as the fundamental data in this task is composed of instances as men-tioned. Moreover, instance-level features can capture the long-range or large time interval dependency under some scenarios. For example, when the start point and endpoint of a lane centerline are far away from each other, it is hard to design a suitable architecture to handle this dependency or correlation. To this end, we propose our Instance Pooling (Ins-Pool) to provide a more flexible way for the instance feature extraction. In this process, the Instance Pooling can be viewed as a special case of Multi-interval Learning as it applies pooling operation along with the set of points with the same instance ID to extract global entity features.</p><p>For implementations of the above algorithm 1 and 2, we optimize and utilize GPU-based scatter, gather and hash table operations to increase runtime efficiency by parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Displacement Prediction and Learning</head><p>A prediction header is built to predict the final forecasting that takes the features from fusion features of spatial and temporal modules as input. Most existing works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b18">19]</ref> predict K possible trajectories with their confidence scores respectively to model the multi-modal property. The loss is often split into regression and classification parts. During training, the loss will only be backpropagated at the trajectory that has the minimum displacement error at the endpoint. However, loss of classification for trajectory selection based on positive and negative samples assignment is not reasonable and too handcrafted, especially when two trajectories have very close displacement error. Inspired by the popular concept of IoU prediction <ref type="bibr" target="#b15">[16]</ref> in object detection, we predict the final displacement error rather than the trajectory's confidence. Therefore, we alleviate the problem of classification that requires hard assignments by displacement regression. We define the output of our prediction header as τ disp and τ reg :</p><formula xml:id="formula_5">τ disp = {d 0 , d 1 , . . . , d K−1 },<label>(2)</label></formula><formula xml:id="formula_6">τ k reg = {(x k 1 , y k 1 ), (x k 2 , y k 2 ), . . . , (x k T , y k T )},<label>(3)</label></formula><p>where τ k reg is the k-th predicted trajectory among K possible trajectories, which contains T waypoints with 2D (x, y) vector representation. τ disp is the predicted displacement error at endpoint associated with each possible trajectory.</p><p>Loss Functions. We sum the trajectory regression L reg and trajectory displacement L disp as final loss for training.</p><p>For trajectory regression, we follow the previous routine that we choose the best trajectory k * whose displacement error with ground-truth trajectory is minimum:</p><formula xml:id="formula_7">L reg = 1 T T i=1 ρ(x k * i , x gt i ) + ρ(y k * i , y gt i ),<label>(4)</label></formula><p>where (x gt i , y gt i ) represents the ground-truth coordinate at timestamp i, and ρ is a smooth L 1 loss function:</p><formula xml:id="formula_8">L disp = 1 K K i=1 ρ(d i , d * i ),<label>(5)</label></formula><p>where d * i is the ground-truth displacement between the kth trajectory and ground-truth trajectory. For inference, we sort the trajectories according to their predicted displacement in an ascending order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we conduct extensive experiments on Argoverse <ref type="bibr" target="#b4">[5]</ref>, which is one of the largest public motion forecasting datasets with rich HD map information. We also evaluate the proposed modules with ablation studies to show their effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>Dataset. Argoverse <ref type="bibr" target="#b4">[5]</ref> is a public motion forecasting dataset. It has more than 300K 5-second sequences collected in Pittsburgh and Miami. For each sequence, the sampling rate is 10Hz, meaning that the interval of the same object appears in the next timestamp is about 0.1s. There are multiple objects with centroid coordinate of time series trajectories within one sequence, with each object tagged as one of the three types, agent, AV, and others. Moreover, each sequence has only one object, tagged with type agent which is required to be predicted the next 3 seconds future horizon in this challenge. We name this agent as the target agent and other vehicles, including AV as other agents similar to <ref type="bibr" target="#b9">[10]</ref>. The whole sequences can be split into training, validation, and test set, with 205942, 39472, and 78143 sequences, respectively. The training and validation sets provide the full 5 seconds trajectories for each target agent data. For the test set, only the first 2 seconds trajectories are given. In addition to trajectories data, we could query the map data represented by lane centerlines points via a given location and city name.</p><p>Metrics. Following the previous works, we also adopt the widely used metrics Average Displacement Error (ADE) and Final Displacement Error (FDE) as criteria. ADE is the average displacement error with ground-truth labels over the entire time steps, and FDE is defined as the displacement error at the endpoint. For multi-modal prediction evaluation on Argoverse, minADE, and minFDE are also used since it allows multiple forecasted trajectories. During the evaluation, it selects K trajectories and computes minimum ADE and minimum FDE as metrics. Miss Rate(MR) is also considered in this task, which is the percentage of the predicted trajectories within a certain threshold (2m) of ground truth according to endpoint error. We take minADE, minFDE, MR for K=1 and K=6 as evaluation metrics in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data split</head><p>Speed distribution(%) [0, 5) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10)</ref>  Data Augmentation. As shown in Tab. 1, the speed distribution of target agents varies on different data splits. The validation set has a larger proportion of high-speed agents, while the average speed of agents on the test set is lower. Therefore, we apply global random scaling with the scaling ratio between [0.8, 1.25]. Global random scaling can simulate agents' dynamics at different speeds, which can improve the model's generalization ability. Besides that, we apply randomly point dropout with probability 0.9 and points location perturbation under normal distribution with mean 0 and standard deviation 0.2.</p><p>Experiment details. We apply some similar standard preprocessing steps as previous works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b8">9]</ref>. First, we translate all the point data to be centered by the coordinate of agent data at t = 0. We use the orientation between agent location at t = 0 and its previous location as the positive x-axis. Then, we set the range starting from [−48, −48] to [48, 48] and filter the points outside the region. For voxelization, we set the grid size g to 0.2m to keep the balance between efficiency and performance. The intervals for Multi-interval Learning are predefined as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16</ref>] that is enough for capturing sequential information. TPCN is trained for 36 epochs using a batch size of 32 with Adam <ref type="bibr" target="#b16">[17]</ref> optimizer with an initial learning rate of 0.001. Besides that, the learning rate decays at every 10 epochs in a ratio of 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation study</head><p>Component study. We conduct an ablation study on the Argoverse validation set to evaluate and analyze the contributions of our proposed components to the final performance. We take the spatial module without dual representations as our baseline. And then, we add other components gradually, as shown in Tab. 1. According to the results, we can draw some conclusions. First, spatial feature extraction based on point cloud learning that takes the unordered set of agents and map points is proven to be effective. With the only spatial module, our model has achieved a strong baseline compared with state-of-the-art methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b9">10]</ref> that have provided results on the validation set. Second, spatial and temporal modules are both indispensable parts of our model. Spatial module models the geometric information, neighborhood relationship, and interaction among points. The temporal module handles the time series features and focuses more on single instance feature learning. With both  modules on, features propagate between spatial and temporal dimensions, thus we achieve the best performance on the validation set. Third, point and voxel features are important feature compensation between each other due to the observation that when we apply dual-representation learning, the performance greatly outperforms single representation. Multi-interval learning plays a crucial role in temporal learning to capture sequential information, with about 10% improvement on displacement metrics. Furthermore, Instance Pooling retrieves the global instance features that address the long-range dependency problem, which leads to better performance.</p><p>Displacement prediction. We also evaluate the effectiveness of the proposed displacement prediction compared with typical classification. The experiment is based on the model with both spatial and temporal modules. Tab. 4 shows the displacement prediction with regression loss performs better than classification with cross-entropy loss in the aspect of trajectory selection. Displacement prediction gets rid of hard or manual assignment and converts the classification problem into a regression problem, which helps to converge to better results. It is worth noting that this change will not affect results for K = 6, demonstrating that trajectory regression and selection are independent tasks. Data composition. Since there are mainly three types of data (agent, non-agent vehicles, map) in the Argoverse dataset, we conduct experiments to see whether our TPCN can extract the corresponding features, including map topology relationship and locality. During this experiment, we train our model by removing some specific kinds of data.  <ref type="table">Table 4</ref>. Ablation study on loss design. Classification refers to predict scores and use cross-entropy loss to optimize. Displacement is the displacement prediction with regression loss.</p><p>As shown in Tab. 5, we see that our TPCN can model the internal relationship among different types of input data. Map information brings useful topology of the road networks and semantic guidance since most of the driving behavior is based on lane keep and lane changes. Meanwhile, non-agents vehicles provide interaction under some decisions (i.e., nudge, overtake). Lacking any one of the data will lead to a significant performance drop for our TPCN. Impact of data augmentation. We conduct an experiment to verify our data augmentation strategy in prediction task. As shown in Tab. 1, the data augmentation improves TPCN in terms of all metrics, especially for minFDE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluation</head><p>Quantitative results. We compare our model with other methods that achieve the state-of-the-art in Argoverse motion forecasting leaderboard. As shown in Tab. 3, we see that our TPCN improves the metrics for K = 1 by a large margin and outperforms the existing approaches in minADE 1 , minFDE 1 and MR 1 without any complex postprocessing. Moreover, our TPCN is the first method which achieves minADE 1 less than 1.7m, minFDE 1 less than 3.7m and MR 1 less than 0.59. In contrast to existing methods <ref type="bibr" target="#b4">[5]</ref> that ignore the temporal information or just use 1D CNN or LSTM to encode agents' temporal features, we mutually propagate the spatial and temporal features in order to maintain both locality and temporality. Furthermore, our  proposed spatial module can effectively capture better map information compared with LaneConv <ref type="bibr" target="#b18">[19]</ref>. Finally, we ranked 1st, 1st, 1st, 2nd, 3rd, 5th on the leaderboard according to the metrics, respectively. Quantitative results. We present some multi-modal prediction trajectories on several hard cases shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. Despite the noise of the input trajectory, TPCN can generate feasible, reasonable, and smooth trajectories with map constraints. For the multi-modality under junction scenarios, TPCN is able to capture the topology of the road network and give possible trajectories along with the lane's successors or neighborhood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose our TPCN that serves as a novel and flexible architecture for prediction learning. TPCN models the motion forecasting problem as joint temporal point cloud learning, consisting of both spatial and temporal modules. In the spatial module, TPCN takes the merit of dual-representation learning in point clouds to maintain better locality and geometric relationships. The temporal module utilizes the proposed Multi-interval Learning and Instance Pooling to capture more fine-grained sequential information. Both modules are learned and propagated mutually to obtain better context information for prediction learning. Experiments on the Argoverse motion forecasting benchmark show the effectiveness of our TPCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">The Detailed Network Architecture</head><p>We provide the detailed network architecture of our TPCN in <ref type="figure">Fig. 1</ref>. The network structure consists of 4 spatial modules and 4 dynamic temporal learning layers. Before the prediction header, we calculate the mean features and remove map instances, which are not required for predictions. Each spatial module takes the output features of the dynamic temporal learning layer as input except the first module. The dynamic temporal learning layer will take the output features of the spatial module as input as well. In the spatial module, the point representation utilizes PointNet++ with neighborhood radius of [0.2m, 0.4m, 0.8m, 1.6m], while the voxel representation uses Sparse BottleNeck as feature extractor. Further, although we use PointNet++ in our point representation learning, we do not apply any sampling or downsampling for the points. We keep all the points in this process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Interval Selection</head><p>To select the best intervals for Multi-interval Learning, we also do some experiments for the selection of this parameter. As shown in Tab. 1, we can see that the performance of our TPCN increases with more intervals. Moreover, the larger interval seems to have a larger impact on our TPCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Hyperparameters Tuning</head><p>Since we have trajectory regression loss and displacement prediction loss, we need to tune the weight of these two losses. We tried the uncertain loss to learn the weight of different losses. However, the uncertain loss does not improve the performance on the validation set, even leading to a slight decay of the performance.</p><p>As a result, we tried different weights, including [0.25, 0.5, 0.75, 1] and finally choose 1 as the weight factor. It means that the two losses or tasks should be treated equally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Qualitative Results</head><p>In this section, we provide more qualitative visual results of our TPCN on the Argoverse validation/test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Qualitative Analysis</head><p>We provide more visual results of our TPCN on the Argoverse validation set in <ref type="figure" target="#fig_2">Fig. 3. Besides, Fig. 4</ref>   <ref type="figure">Figure 1</ref>. The overall structure of our TPCN. M is the total input points number, including points of N map instances and points of S agent instances. Note each instance has multiple points. K is the number of multi-modal output trajectories. T is the number of prediction timestamps.</p><p>results on the Argoverse test set without ground-truth labels. Generally, these qualitative results demonstrate the effectiveness of our TPCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Failure Cases</head><p>We provide several failure cases on the validation set. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, there are several reasons that cause failure:</p><p>• The ground-truth labels are not completely correct.</p><p>Since the data of Argoverse is obtained from tracking, there may be some id switches, leading to the sudden perturbation of the agents' location (e.g., the fourth example in the first row of <ref type="figure" target="#fig_1">Fig. 2</ref>). Although TPCN does not output a similar trajectory as ground truth, the predicted trajectories are more reasonable and stable without large jerk.</p><p>• The multi-modality nature of the motion prediction task. There are some cases that the TPCN tends to output the trajectories with more motion constraints rather than map constraints. The first example in the second row of <ref type="figure" target="#fig_1">Fig. 2</ref>     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>An example that shows the instance time indexing system. The points with the same color belong to the same instance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Dual-representation Spatial Learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 2 4 :F t = MLP(O p ) 5 : 6 :</head><label>2456</label><figDesc>Multi-interval Learning Require: All pointwise features P F with instance time indexing m and the predefined set of time intervals T Ensure: All pointwise output features O p 1: O p = P F 2: for each t ∈ T do 3: m * = (m[0], m[1]/t ) O = FTP(F t , m * ) O p = slice(O, m * ) 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>The motion forecasting results on the Argoverse validation set. The past trajectory of the target agent is in yellow, predicted trajectory in green and ground truth in red, respectively. The figures demonstrate the effectiveness of our TPCN on scenarios including left-turn, right-turn, lane change, and so on.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 .</head><label>2</label><figDesc>Failure cases on the Argoverse validation set. The target agent's past trajectory is in yellow, predicted trajectory in green and ground truth in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 .</head><label>3</label><figDesc>The motion forecasting results on the Argoverse validation set. The target agent's past trajectory is in yellow, predicted trajectory is in green, and ground truth is in red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>The motion forecasting results on the Argoverse test set. The target agent's past trajectory is in yellow and predicted trajectory in green.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc><ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15)</ref> ≥ 15 Speed distribution on different data splits.</figDesc><table><row><cell>Train</cell><cell>26.4</cell><cell>42.4</cell><cell>26.7</cell><cell>4.5</cell></row><row><cell>Val</cell><cell>18.6</cell><cell>38.7</cell><cell>29.8</cell><cell>12.9</cell></row><row><cell>Test</cell><cell>33.4</cell><cell>52.6</cell><cell>13.1</cell><cell>0.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of each component on the Argoverse validation set. Point and Voxel represent pointwise feature learning and voxelwise feature learning, respectively. The temporal module includes Multi-interval Learning (MIL) and Instance Pooling (Ins-Pool).</figDesc><table><row><cell>Spatial Point Voxel MIL Ins-Pool Temporal</cell><cell>Aug</cell><cell>minADE1</cell><cell>minFDE1</cell><cell>MR1</cell><cell>minADE6</cell><cell>minFDE6</cell><cell>MR6</cell></row><row><cell></cell><cell></cell><cell>1.75</cell><cell>4.00</cell><cell>0.66</cell><cell>1.01</cell><cell>1.88</cell><cell>0.28</cell></row><row><cell></cell><cell></cell><cell>1.63</cell><cell>3.65</cell><cell>0.62</cell><cell>0.94</cell><cell>1.70</cell><cell>0.23</cell></row><row><cell></cell><cell></cell><cell>1.50</cell><cell>3.31</cell><cell>0.57</cell><cell>0.85</cell><cell>1.42</cell><cell>0.16</cell></row><row><cell></cell><cell></cell><cell>1.38</cell><cell>3.02</cell><cell>0.52</cell><cell>0.76</cell><cell>1.19</cell><cell>0.13</cell></row><row><cell></cell><cell></cell><cell>1.36</cell><cell>2.98</cell><cell>0.51</cell><cell>0.74</cell><cell>1.18</cell><cell>0.12</cell></row><row><cell></cell><cell></cell><cell>1.34</cell><cell>2.95</cell><cell>0.50</cell><cell>0.73</cell><cell>1.15</cell><cell>0.11</cell></row></table><note>"Aug" refers to data augmentation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The results of our method and top performing approaches on the Argoverse test set.</figDesc><table><row><cell></cell><cell>Models</cell><cell></cell><cell cols="2">minADE1 minFDE1</cell><cell>MR1</cell><cell cols="2">minADE6 minFDE6</cell><cell>MR6</cell></row><row><cell></cell><cell>Argoverse Baseline [5]</cell><cell></cell><cell>2.96</cell><cell>6.81</cell><cell>0.81</cell><cell>2.34</cell><cell>5.44</cell><cell>0.69</cell></row><row><cell cols="3">Argoverse Baseline (NN) [5]</cell><cell>3.45</cell><cell>7.88</cell><cell>0.87</cell><cell>1.71</cell><cell>3.29</cell><cell>0.54</cell></row><row><cell></cell><cell>Jean (1st) [5, 23]</cell><cell></cell><cell>1.74</cell><cell>4.24</cell><cell>0.68</cell><cell>0.98</cell><cell>1.42</cell><cell>0.13</cell></row><row><cell></cell><cell>uulm-mrm (2nd) [5]</cell><cell></cell><cell>1.90</cell><cell>4.19</cell><cell>0.63</cell><cell>0.94</cell><cell>1.55</cell><cell>0.22</cell></row><row><cell></cell><cell>LaneConv [19]</cell><cell></cell><cell>1.71</cell><cell>3.78</cell><cell>0.59</cell><cell>0.87</cell><cell>1.36</cell><cell>0.16</cell></row><row><cell></cell><cell>TNT [38]</cell><cell></cell><cell>1.77</cell><cell>3.91</cell><cell>0.59</cell><cell>0.94</cell><cell>1.54</cell><cell>0.13</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell>1.66</cell><cell>3.69</cell><cell>0.588</cell><cell>0.87</cell><cell>1.38</cell><cell>0.158</cell></row><row><cell>Metrics</cell><cell cols="3">Loss Classification Displacement</cell><cell></cell><cell></cell><cell></cell></row><row><cell>minADE1</cell><cell>1.44</cell><cell>1.34</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>minFDE1</cell><cell>3.14</cell><cell>2.95</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MR1</cell><cell>0.54</cell><cell>0.50</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>minADE6</cell><cell>0.74</cell><cell>0.73</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>minFDE6</cell><cell>1.14</cell><cell>1.15</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MR6</cell><cell>0.11</cell><cell>0.11</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study of data composition on the Argoverse validation set. Here, agents refer to other agents, and maps refer to map points data.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>shows the</figDesc><table><row><cell></cell><cell></cell><cell>K trajectories</cell></row><row><cell></cell><cell></cell><cell>S x K x T x 2</cell></row><row><cell cols="2">Prediction Header</cell><cell></cell></row><row><cell cols="2">Agent Instances</cell><cell>K displacements</cell></row><row><cell>Map Instances</cell><cell>S x 128</cell><cell>S x K</cell></row><row><cell>N x 128</cell><cell></cell><cell></cell></row><row><cell cols="2">Mean over Instance</cell><cell></cell></row><row><cell cols="2">M x 128</cell><cell></cell></row><row><cell>…</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">Instance Pooing</cell></row><row><cell cols="2">Dynamic Temporal Learning</cell><cell cols="2">Multi-interval Learning</cell></row><row><cell></cell><cell></cell><cell cols="2">(2, 4, 6, 8,16)</cell></row><row><cell cols="2">Spatial Module</cell><cell></cell></row><row><cell cols="2">Dynamic Temporal Learning</cell><cell cols="2">concatenate</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Voxel To Point</cell></row><row><cell cols="2">Spatial Module</cell><cell>PointNet++ radius:(0.2, 0.4, 0.8, 1.6)</cell><cell>Sparse BottleNeck</cell></row><row><cell cols="2">Input: M x 2</cell><cell></cell><cell>Point To Voxel</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 1 .</head><label>1</label><figDesc>demonstrates this phenomenon. The agent has no prior motion states about left-turn action. Thus TPCN predicts the lane-keeping behavior and ignores the map constraints. Therefore, this can be the further work of our TPCN. Ablation study of intervals on Argoverse validation set.</figDesc><table><row><cell>Intervals</cell><cell>minADE1</cell><cell>minFDE1</cell><cell>MR1</cell><cell>minADE6</cell><cell>minFDE6</cell><cell>MR6</cell></row><row><cell>[2]</cell><cell>1.47</cell><cell>3.36</cell><cell>0.57</cell><cell>0.84</cell><cell>1.43</cell><cell>0.16</cell></row><row><cell>[2, 4]</cell><cell>1.44</cell><cell>3.28</cell><cell>0.55</cell><cell>0.82</cell><cell>1.39</cell><cell>0.15</cell></row><row><cell>[2, 4, 6]</cell><cell>1.43</cell><cell>3.17</cell><cell>0.53</cell><cell>0.79</cell><cell>1.33</cell><cell>0.13</cell></row><row><cell>[2, 4, 6, 8]</cell><cell>1.39</cell><cell>3.07</cell><cell>0.51</cell><cell>0.76</cell><cell>1.24</cell><cell>0.12</cell></row><row><cell>[2, 4, 6, 8, 16]</cell><cell>1.34</cell><cell>2.95</cell><cell>0.50</cell><cell>0.73</cell><cell>1.15</cell><cell>0.11</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An empirical evaluation of generic convolutional and recurrent networks for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.01271</idno>
		<idno>1803. 2</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhijit</forename><surname>Ogale</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.03079</idno>
		<title level="m">Chauffeurnet: Learning to drive by imitating the best and synthesizing the worst</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Intentnet: Learning to predict intention from raw sensor data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="947" to="956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mayank</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05449</idno>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Argoverse: 3d tracking and forecasting with rich maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Fang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Lambert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patsorn</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jagjeet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hartnett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">De</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="8748" to="8757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multimodal trajectory predictions for autonomous driving using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henggang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang-Chieh</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Han</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tzu-Kuo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Short-term motion prediction of traffic actors for autonomous driving using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nemanja</forename><surname>Djuric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladan</forename><surname>Radosavljevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henggang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang-Chieh</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Han</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.05819</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alán</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Tpnet: Trajectory proposal network for motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangji</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinhong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6797" to="6806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vectornet: Encoding hd maps and agent dynamics from vectorized representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congcong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="11525" to="11533" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">3d semantic segmentation with submanifold sparse convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9224" to="9232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Vehicle trajectory prediction based on motion model and maneuver recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Houenou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Bonnifait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Véronique</forename><surname>Cherfaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE/RSJ international conference on intelligent robots and systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="4363" to="4369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Acquisition of localization confidence for accurate object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borui</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayuan</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="784" to="799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Temporal convolutional networks for action segmentation and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Lea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rene</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory D</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="156" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning lane graph representations for motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13732</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointvoxel cnn for efficient 3d deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="965" to="975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast and Furious: Real Time End-to-End 3D Detection, Tracking and Motion Forecasting With a Single Convolutional Net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3569" to="3577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harshayu</forename><surname>Girase</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shreyas</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Hui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.02025</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>It is not the journey but the destination: Endpoint conditioned trajectory prediction</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-head attention for multi-modal joint vehicle motion forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Mercat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><forename type="middle">El</forename><surname>Zoghby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Sandou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominique</forename><surname>Beauvois</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo Pita</forename><surname>Gil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9638" to="9644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Social-stgcnn: A social spatio-temporal graph convolutional neural network for human trajectory prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abduallah</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Kun Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Claudel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="14424" to="14432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Covernet: Multimodal behavior prediction using trajectory sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tung</forename><surname>Phan-Minh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><forename type="middle">Corina</forename><surname>Grigore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Freddy</forename><forename type="middle">A</forename><surname>Boulton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">M</forename><surname>Wolff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="14074" to="14083" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Interaction-aware probabilistic behavior prediction in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Constantin</forename><surname>Hubmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Löchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darius</forename><surname>Burschka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3999" to="4006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Pointvoxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains. IEEE signal processing magazine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="83" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Searching efficient 3d architectures with sparse point-voxel convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haotian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanrui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.16100</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Kpconv: Flexible and deformable convolution for point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugues</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Emmanuel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatriz</forename><surname>Deschaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Marcotegui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Goulette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Vehicle trajectory prediction by integrating physics-and maneuver-based approaches using interactive multiple models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guotao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijun</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Electronics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="5999" to="6008" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hvnet: Hybrid voxel network for lidar based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuangjie</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongyi</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end interpretable neural motion planner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abbas</forename><surname>Sadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8660" to="8669" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balakrishnan</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.08294</idno>
		<title level="m">Target-driven trajectory prediction</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Making bertha drive-an autonomous journey on a historic route</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julius</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Schreiber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henning</forename><surname>Lategahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Strauss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thao</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Appenrodt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><forename type="middle">G</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Intelligent transportation systems magazine</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="8" to="20" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
