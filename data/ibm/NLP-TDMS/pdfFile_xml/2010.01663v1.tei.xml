<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image and Volumetric Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Jeya</forename><forename type="middle">Maria</forename><surname>Jose Valanarasu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE, Ilker</roleName><forename type="first">Vishwanath</forename><forename type="middle">A</forename><surname>Sindagi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Vishal</forename><forename type="middle">M Patel</forename><surname>Hacihaliloglu</surname></persName>
						</author>
						<title level="a" type="main">KiU-Net: Overcomplete Convolutional Architectures for Biomedical Image and Volumetric Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>JOURNAL OF L A T E X CLASS FILES, VOL. X, NO. X, XXXX 2020 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T08:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Brain Anatomy Segmentation</term>
					<term>Brain Tumor Segmentation</term>
					<term>Liver Segmentation</term>
					<term>Gland Segmentation</term>
					<term>Nerve Segmentation</term>
					<term>Medical Image Segmentation</term>
					<term>Deep Learning</term>
					<term>Overcomplete Representations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most methods for medical image segmentation use U-Net or its variants as they have been successful in most of the applications. After a detailed analysis of these "traditional" encoder-decoder based approaches, we observed that they perform poorly in detecting smaller structures and are unable to segment boundary regions precisely. This is in spite of the fact that these approaches propagate low-level features to the output through skip connections. This issue can be attributed to the increase in receptive field size as we go deeper into the encoder. The extra focus on learning high level features causes the U-Net based approaches to learn less information about low-level features which are crucial for detecting small structures. To overcome this issue, we propose using an overcomplete convolutional architecture where we project our input image into a higher dimension such that we constrain the receptive field from increasing in the deep layers of the network. We design a new architecture for image segmentation-KiU-Net which has two branches: (1) an overcomplete convolutional network Kite-Net which learns to capture fine details and accurate edges of the input, and (2) U-Net which learns high level features. Furthermore, we also propose KiU-Net 3D which is a 3D convolutional architecture for volumetric segmentation. We perform a detailed study of KiU-Net by performing experiments on five different datasets covering various image modalities like ultrasound (US), magnetic resonance imaging (MRI), computed tomography (CT), microscopic and fundus images. The proposed method achieves a better performance as compared to all the recent methods with an additional benefit of fewer parameters and faster convergence. Additionally, we also demonstrate that the extensions of KiU-Net based on residual blocks and dense blocks result in further performance improvements. The implementation of KiU-Net can be found here: https://github.com/jeya-maria-jose/KiU-Net-pytorch</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>M EDICAL image segmentation plays a pivotal role in computer-aided diagnosis systems which are helpful in making clinical decisions. Segmenting a region of interest like an organ or lesion from a medical image or a scan is critical as it contains details like the volume, shape and location of the region of interest. Automatic methods proposed for medical image segmentation help in aiding radiologists for making fast and labor-less annotations. Early methods were based on traditional pattern recognition techniques like statistical modeling and edge detection filters. Later, machine learning approaches using hand-crafted features based on the modality and type of segmentation task were developed. Recently, the state of the art methods for medical image segmentation for most modalities like magnetic resonance imaging (MRI), computed tomography (CT) and ultrasound (US) are based on deep learning. As convolutional neural networks (CNNs) extract data-specific features which are rich in quality and effective in representing the image and the region of interest, deep learning reduces the hassle of extracting manual features from the image. Most of the architectures developed for semantic segmentation in both computer vision and medical image analysis are encoder-decoder type convolutional networks. Seg-Net <ref type="bibr" target="#b0">[1]</ref> was the first such type of network that was widely recognized. In the encoder block of Seg-Net, every convolutional layer is followed by a max-pooling layer which causes the input image to be projected onto a lower dimension similar to an undercomplete auto-encoder. The receptive field size of the filters increases with the depth arXiv:2010.01663v1 [eess.IV] 4 Oct 2020 of the network thereby enabling it to extract high-level features in the deeper layers. The initial layers of the encoder extract low-level information like edges and small anatomical structures while the deeper layers extract high-level information like objects (in the case of vision datasets) and organs/lesions (in the case of medical imaging datasets). A major breakthrough in medical image segmentation was brought by U-Net <ref type="bibr" target="#b1">[2]</ref> where skip connections were introduced between the encoder and decoder to improve the training and quality of the features used in predicting the segmentation. U-Net has became the backbone of almost all the leading methods for medical image segmentation in recent years. Subsequently, many more networks were proposed which built on top of U-Net architectures. U-Net++ <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> proposed using nested and dense skip connection for further reducing the semantic gap between the feature maps of the encoder and decoder. UNet3+ <ref type="bibr" target="#b4">[5]</ref> proposed using full-scale skip connections where skip connections are made between different scales. 3D U-Net <ref type="bibr" target="#b5">[6]</ref> and V-Net <ref type="bibr" target="#b6">[7]</ref> were proposed as extensions of U-Net for volumetric segmentation in 3D medical scans. In other extensions of U-Net like Res-UNet <ref type="bibr" target="#b7">[8]</ref> and Dense-UNet <ref type="bibr" target="#b8">[9]</ref>, the convolutional blocks in encoder and decoder consisted of residual connections <ref type="bibr" target="#b9">[10]</ref> and dense blocks <ref type="bibr" target="#b10">[11]</ref> respectively. It can be noted that all the above extensions of U-Net used the same encoder-decoder architecture and their contributions were either in skip connections, using better convolutional layer connections or in applications.</p><formula xml:id="formula_0">(a) (b) (c) (d) (e) (f) (g)</formula><p>The main problem with the above family of networks is that they lack focus in extracting features for segmentation of small structures. As the networks are built to be more deeper, more high-level features get extracted. Even though the skip connections facilitate transmission of local features to the decoder, from our experiments we observed that they still fail at segmenting small anatomical landmarks with blurred boundaries. Although U-Net and its variants are good at segmenting large structures, they fail when the segmentation masks are small or have noisy boundaries which can be seen in <ref type="figure" target="#fig_0">Fig 1 and 2</ref>. As mentioned earlier, Unet and its variants belong to undercomplete convolutional architectures which is what causes the network to focus on high-level features. To this end, we propose using overcomplete convolutional architectures for segmentation. We call our overcomplete architecture Kite-Net (Ki-Net) which transforms the input to higher dimensions (in the spatial sense). Note that Kite-Net does not follow the traditional encoder-decoder style of architecture, where the inputs are mapped to lower dimensional embeddings (in the spatial sense). Compared to the use of max-pooling layers in the traditional encoder and upsampling layers in the traditional decoder, Kite-Net has upsampling layers in the encoder and max-pooling layers in the decoder. This ensures that the receptive field size of filters in the deep layers of the network does not increase like in U-Net. This ensures that the Kite-Net is able to extract fine details of boundaries as well as small structures even in the deeper layers. Although Kite-Net extracts high quality low-level features, the lack of filters extracting high-level features makes Kite-Net not perform on par with U-Net when the dataset consists of both small and large structure annotations. Hence, we propose a multi-branch network, KiU-Net, where one branch is overcomplete (Ki-Net) and another is undercomplete (U-Net). Furthermore, we propose to effectively combine the features across the two branches using a novel cross-residual fusion strategy which results in efficient learning of KiU-Net.</p><p>Note that, we presented a preliminary version of this work in <ref type="bibr" target="#b11">[12]</ref> at MICCAI 2020 where we:</p><p>• Proposed an overcomplete convolutional architectures (Kite-Net) for segmentation and studied how it is different from undercomplete architectures like U-Net.</p><p>• Proposed a novel architecture KiU-Net which combines feature maps of both under-complete and overcomplete deep networks such that the network learns to segment both small and large segmentation masks effectively.</p><p>• Evaluated the performance of the proposed method for brain anatomy segmentation from 2D US scans where we achieved significantly better performance as compared to recent methods <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>.</p><p>In the current work, we propose additional improvements over our earlier work. Specifically, we make the following contributions:</p><p>• KiU-Net 3D which is an extension of KiU-Net for volumetric segmentation. Here, we use a 3D convolution-based overcomplete network for efficient extraction of low-level information.</p><p>• Res-KiUNet and Dense-KiUNet architectures where we use residual connections and dense blocks respectively for improving the learning of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>We evaluate the performance of the proposed architecture for volumetric and image segmentation across 5 datasets: Brain Tumor Segmentation (BraTS), Liver Tumor Segmentation (LiTS), Gland Segmentation (GlaS), Retinal Images vessel Tree Extraction (RITE) and Brain Anatomy segmentation. These datasets individually correspond to 5 different modalities: ultrasound (US), magnetic resonance imaging (MRI), computed tomography (CT), microscopic and fundus images. With these additional experiments on multiple datasets, we demonstrate that the proposed method generalizes well to different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In this section, we briefly review the deep learning works proposed for medical image segmentation. We mainly focus on methods that deal with datasets which we conduct our experiments on.</p><p>For brain ventricle segmentation from US scans, methods based on U-Net and residual architectures have been investigated <ref type="bibr" target="#b15">[16]</ref>. Wang et al. <ref type="bibr" target="#b13">[14]</ref> proposed a PSP-net based method for this task. In <ref type="bibr" target="#b12">[13]</ref>, the authors explored using uncertainty to further improve the predictions. For brain tumor segmentation for MRI scans, a lot of methods have been proposed based on 2D U-Net and its variations <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>. Many other methods based on Res-Net <ref type="bibr" target="#b9">[10]</ref>, pixel-net <ref type="bibr" target="#b20">[21]</ref> and PSP-net <ref type="bibr" target="#b21">[22]</ref> have been proposed for brain tumor segmentation in <ref type="bibr" target="#b22">[23]</ref>. 3D convolution based methods have been proved to be better for segmentation of brain tumor when compared to training 2D convolution networks on individual 2D slices of MRI scans and then combining them back together to get 3D segmentation. So, 3D U-Net based methods have been proposed in many recent works for brain tumor segmentation. Most of the top performing methods in the BraTS challenge used 3D U-Net as their backbone network <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Out of these methods, regularization on a 3D U-Net backbone architecture <ref type="bibr" target="#b26">[27]</ref> gives the best performance for BraTS test dataset (2018). A simple 3D U-Net with just proper hyper parameter tuning is a close second <ref type="bibr" target="#b27">[28]</ref>. U-Net based methods have also been adopted for kidney tumor segmentation from CT scans <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b30">[30]</ref>, <ref type="bibr" target="#b31">[31]</ref>, cell/nuclei segmentation from microscopic images <ref type="bibr" target="#b32">[32]</ref>, <ref type="bibr" target="#b33">[33]</ref>, <ref type="bibr" target="#b34">[34]</ref>, <ref type="bibr" target="#b35">[35]</ref> and retina nerve segmentation from fundus images <ref type="bibr" target="#b36">[36]</ref>, <ref type="bibr" target="#b37">[37]</ref>.</p><p>LiTS challenge which deals with liver segmentation and liver lesion segmentation has been one of the leading medical image segmentation datasets in recent times. Several works <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b38">[38]</ref>, <ref type="bibr" target="#b39">[39]</ref>, <ref type="bibr" target="#b40">[40]</ref> that perform well for this task are also based on U-Net. There are many more deep learning works for medical image segmentation which can be found in review papers for medical image segmentation <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b42">[42]</ref>. Unlike the above methods which are encoder-decoder based convolutional (undercomplete) networks designed for specific applications, in this work we propose a new architecture which uses overcomplete representations for segmentation. Also, we show that our proposed network is not application specific but a generic solution that can be used for segmentation in any modality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this section, we first discuss the issues with the U-Net family of architectures and motivate why we propose using overcomplete representations. Later, we describe the proposed architectures in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">KiU-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Issues with traditional encoder-decoder networks</head><p>In the dataset that we collected for Brain Anatomy Segmentation from US images, the segmentation masks are heterogeneous in terms of the size of the structures. For example it can be seen from <ref type="figure" target="#fig_2">Fig 3 that</ref> while one image has a large segmentation label, the other one has tiny segmentation mask. U-Net and its variants yield relatively good performance for this dataset as seen in <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>. However, in our experiments we observed that these methods fail to detect tiny structures in most of the cases. This does not cause much decrement in terms of the overall dice accuracy for the prediction since the datasets predominantly contain images with large structures. However, it is crucial to detect tiny structures with a high precision since it plays an important role in diagnosis. Furthermore, even for the large structures, U-Net based methods result in erroneous boundaries especially when the boundaries are blurry as seen in <ref type="figure" target="#fig_1">Fig 2 (b)</ref>,(c).</p><p>In order to clearly illustrate these observations, we evaluated U-Net on the Brain Anatomy Segmentation from US images dataset and the results are shown in <ref type="figure" target="#fig_1">Fig 2.</ref> It can be observed from the bottom row of this figure <ref type="figure" target="#fig_1">(Fig 2 (b)</ref>,(c)) that U-Net fails to detect the tiny structures. Further, the first row demonstrates that in the case of large structures, although U-Net produces an overall good prediction, it is unable to accurately segment out the boundaries. Additionally, we also made similar observations when U-Net based 3D architecture was used for volumetric segmentation of lesion. Specifically, the predictions from U-Net are blurred as it fails to segment the surface perfectly especially when the surface of the tumor is not smooth and has a high curvature (see <ref type="figure" target="#fig_0">Fig 1(a)</ref>).</p><p>To gain a further understanding of why U-Net based models are unable to segment small structures and boundaries accurately, we analyze the network architecture in detail. In each convolutional block of the encoder, the input set of features to that block get downsampled due to maxpooling layer. This makes sure that the encoder projects the input image to a lower dimension in spatial sense. This combination of convolution layer and max-pooling layer in the encoder causes the receptive field of the filters in deep layers of encoder to increase. With an increased receptive field, the deeper layers focus on high level features and thus are unable to extract features for segmenting small masks or fine edges. The only convolution filters that capture lowlevel information are the first few layers. Also, it can be noted that in a traditional "encoder-decoder" architecture, the network is designed such that the number of filters increases as we go deeper in the network. That is, for a typical convnet, more than 95% of the filters present in the network focus on extracting high level information. For example, the traditional UNet architecture has a 5 layer deep encoder with 64, 128, 256, 512 and 1024 filters. <ref type="figure" target="#fig_3">From Fig 4 (a)</ref>, we can observe that only the first layer captures low level information like edges and small anatomy. So, effectively the number of filters that capture low level information becomes 64 64+128+256+512+1024 = 3.22% which implies 96.78% of the filters work on learning high-level features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Overcomplete Networks</head><p>In signal processing, overcomplete representations <ref type="bibr" target="#b43">[43]</ref> were first explored for making dictionaries such that the number of basis functions can be more than the number of input signal samples. This enables a higher flexibility for capturing structure in the data. In <ref type="bibr" target="#b43">[43]</ref>, the authors show that overcomplete bases work as a better approximators of any underlying statistical distribution of a data. It has also been widely used for reconstruction of signals under the presence of noise and for source separation in a mixture of signals. It is mainly popular for these tasks because of its greater robustness in the presence of noise when compared to undercomplete representations. For denoising autoencoders <ref type="bibr" target="#b44">[44]</ref>, models with overcomplete hidden layer expression were observed to perform better as they are more useful feature detectors. The authors note that the proposed idea of denoising autoencoders in fact improved the feature detecting ability of an overcomplete fully connected network and it performs better than the standard bottleneck architectures for that task. Overcomplete representations have also been backed in the field of neuroscience where it is solves the problem of maintaining a stable sensory percept in the absence of time-invariant persistent activity <ref type="bibr" target="#b45">[45]</ref>.</p><p>The idea of overcomplete networks (in the spatial sense) in the convnet-deep learning era has been unexplored. To this end, we propose Kite-Net which is an overcomplete version of U-Net. In Kite-Net, the encoder projects the input image into a spatially higher dimension. This is achieved by incorporating bilinear upsampling layers in the encoder. This form of the encoder constrains the receptive field from increasing like in U-Net as we carefully select the kernel size of the filters and upsampling coefficient such that the deep layers learn to extract fine details and features to segment small masks effectively. Furthermore, in the decoder, each conv block has a conv layer followed by a max-pooling layer. In <ref type="figure" target="#fig_6">Fig 6,</ref> we can see how the receptive field is constrained in our proposed Kite-net when compared to U-Net.</p><p>To analyze this in detail, let I be the input image, F 1 and F 2 be the feature maps extracted from first and second conv blocks respectively. Let the initial receptive field of the conv filter be k × k on the image. In an undercomplete network, the receptive field size change due to max-pooling layer is dependent on two variables: pooling coefficient and stride of the pooling filter. For convenience, the pooling coefficient and stride is both set as 2 in our network. Considering this configuration, the receptive field of conv block 2 (to which F 1 is forwarded) on the input image would be 2 × k × 2 × k. Similarly, the receptive field of conv block 3 (to which F 2 is forwarded) would be 4×k ×4×k. This increase in receptive field can be generalized for an i th layer in an undercomplete network as follows:</p><formula xml:id="formula_1">RF (w.r.t I) = 2 2 * (i−1) × k × k.</formula><p>In comparison, the proposed overcomplete network has an upsampling layer with a coefficient 2 in the conv blocks replacing the max-pooling layer. As the upsampling layer actually works opposite to that of max-pooling layer, the   receptive field of conv bock 2 on the input image now would be 1 2 × k × 1 2 × k. Similarly, the receptive field of conv block 3 now would be 1 4 × k × 1 4 × k. This increase in receptive field can be generalized for i th layer in the overcomplete network as follows:</p><formula xml:id="formula_2">RF (w.r.t I) = ( 1 2 ) 2 * (i−1) × k × k.</formula><p>Note that the above calculations are based on a couple of assumptions. We assume that the pooling coefficient and stride are both set as 2 in both overcomplete and undercomplete network. Also, we consider that the receptive field change caused by the conv layer in both undercomplete and overcomplete networks would be the same and do not consider in our calculations. This can be justified as we have maintained the conv kernel size to 3 × 3 with stride 1 and padding 1 throughout our network and this setting does not actually affect the receptive as much as maxpooling or upsampling layer does. The above explanations are illustrated in <ref type="figure" target="#fig_6">Fig 6</ref>.</p><p>In <ref type="figure" target="#fig_3">Fig 4,</ref> we visualize the feature maps of both U-Net and Kite-Net. The first row corresponds to the features extracted from the first layer of encoder, second row corresponds to the second layer and third row corresponds to the third layer. From this, it can be observed that Kite-Net extracts more finer details over smaller regions when compared to U-Net. Although Kite-Net learns low-level features better than U-Net, it does not learn any high-level features. Due to this, Kite-Net is unable to segment out any large masks present in the input image. To overcome this, we propose KiU-Net, which efficiently combines both Kite-Net and U-Net while achieving the best of both networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Architecture Details</head><p>KiU-Net is a two-branch network where one branch is Kite-Net and the other is U-Net. This network exploits the lowlevel fine edge capturing power of Kite-Net as well the high-level shape capturing power of U-Net. KiU-Net has been illustrated in <ref type="figure" target="#fig_5">Fig 5.(a)</ref>. The input image is forwarded to both Kite-Net and U-Net in parallel. Each conv block in the encoder of Kite-Net has a conv 2D layer followed by a bilinear upsampling layer with coeffecient of two and ReLU activation. Each conv block in the encoder of U-Net has a conv 2D layer followed by a max-pooling layer with kernel size of two and ReLU activation. In each conv block in the decoder of Kite-Net, we have a conv 2D layer followed by a max-pooling layer with kernel size of two and ReLU   activation. Similarly for U-Net, we have a conv 2D layer followed by a bilinear upsampling layer with coeffecient of two and ReLU activation in its decoder. We have 3 such conv blocks in encoder and decoder in both Kite-Net and U-Net branches of KiU-Net. We then add the output feature maps of Kite-Net and U-Net and forward them through a 1 × 1 conv 2D layer to obtain the prediction. All the conv layers in our network (except for the last layer) are of 3 × 3 size with stride 1 and padding 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Cross residual feature block (CRFB)</head><p>In order to further exploit the capacity of the two networks, we propose to combine the features of the two networks at multiple scales through a novel cross residual feature block (CRFB). That is, at each level in the encoder and decoder of KiU-Net, we combine the respective features using a CRFB. As we know that the features learned by U-Net and Kite-Net are different from each other, this characteristic can be used to further improve the training of the individual networks. So, we try to learn the complementary features from both the networks which will further improve the quality of features learned by the individual networks. The CRFB block is illustrated in <ref type="figure" target="#fig_5">Fig 5.(b)</ref>. Denoting the features maps from U-Net as F i U and F i Ki as the feature maps from Ki-Net after i th block in KiU-Net, the crossresidual features R i U and R i Ki are first extracted using a conv block. The conv block that F i Ki is forwarded to has a combination of conv 2D layer and max-pooling layer. The conv block that F i U i is forwarded through has a combination of conv 2D layer and upsampling layer. These cross-residual features are then added to the original features F i U and</p><formula xml:id="formula_3">F i Ki to obtain the complementary featuresF i U andF i Ki , F i U = F i U + R i Ki andF i Ki = F i Ki + R i U .</formula><p>With this kind of complementary feature extraction from the two networks, we observe a considerable improvement in the segmentation performance of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">KiU-Net 3D</head><p>With the success of KiU-Net for 2D segmentation, we explore its usage for volumetric segmentation from 3D medical scans. Specifically, we propose KiU-Net 3D which is a two-branch network similar to KiU-Net but contains Kite-Net 3D and U-Net 3D as its branches. The input will be a scan of shape H × W × S where H, W are the height and width and S is the number of slices of 2D images in the scan. So, in our proposed KiU-Net 3D, we use 3D convolutions in the place of 2D convolutions to work on the voxel space of the scans.</p><p>In the encoder of Kite-Net 3D branch, every conv block has a conv 3D layer followed by a trilinear upsampling layer with coeffecient of two and ReLU activation. In decoder, every conv block has a conv 3D layer followed by a 3D max-pooling layer with coeffecient of two and ReLU activation. Similarly, in the encoder of U-Net 3D branch, every conv block has a conv 3D layer followed by a 3D maxpooling layer with coefficient of two and ReLU activation. In decoder, every conv block has a conv 3D layer followed by a trilinear upsampling layer with coefficient of two and ReLU activation. We have CRFB block across each layer in KiU-Net 3D similar to KiU-Net. The difference in the CRFB block architecture of KiU-Net 3D is that we have conv 3D layers and trilinear upsampling instead of conv 2D layers and bilinear upsampling like in KiU-Net. The output of both the branches are then added and forwarded to 1×1×1 conv 3D layer to get the prediction voxel. All the conv layers in our network (except for the last layer) have 3 × 3 × 3 kernel sizes with stride 1 and padding 1. KiU-Net 3D is illustrated in <ref type="figure" target="#fig_8">Fig 7.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we describe the experimental settings and the datasets that we use for 2D medical image segmentation and 3D medical volumetric segmentation to evaluate and compare the proposed KiU-Net and KiU-Net 3D networks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">KiU-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Datasets</head><p>Brain Anatomy Segmentation (US): Intraventricular hemorrhage (IVH) which results in the enlargement of brain ventricles is one of the main causes of preterm brain injury. The main imaging modality used for diagnosis of brain disorders in preterm neonates is cranial US because of its safety and cost-effectiveness. Also, absence of septum pellucidum is an important biomarker for septo-optic dysplasia diagnosis. Automatic segmentation of brain ventricles and septum pellucidum from these US scans is essential for accurate diagnosis and prognosis of these ailments. After obtaining institutional review board (IRB) approval, US scans were collected from 20 different premature neonates (age &lt; 1 year). The total number of images collected were 1629 with annotations out of which 1300 were allocated for training and 329 for testing. Before processing, each image was resized to 128 × 128 resolution.</p><p>Gland Segmentation (Microscopic): Accurate segmentation of glands is important to obtain reliable morphological statistics. Histology images in the form of Haematoxylin and Eosin (H&amp;E) stained slides are generally used for gland segmentation <ref type="bibr" target="#b46">[46]</ref>. Gland Segmentation (GLAS) dataset contains a total of 165 images out of which 85 are taken for training and 80 for testing. We pre-process the images by resizing them to 128 × 128 resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retinal Nerve Segmentation (Fundus):</head><p>Extraction of arteries and veins on retinal fundus images are essential for delineation of morphological attributes of retinal blood vessels, such as length, width, patterns and angles. These attributes are then utilized for the diagnosis and treatment of various ophthalmologic diseases such as diabetes, hypertension, arteriosclerosis and chorodial neovascularization <ref type="bibr" target="#b47">[47]</ref>. To perform retinal nerve segmentation, we use Retinal Images vessel Tree Extraction (RITE) dataset <ref type="bibr" target="#b48">[48]</ref> which is a subset of the DRIVE dataset. RITE dataset contains 40 images split into 20 for training and 20 for testing. We pre-process the images by resizing them to 128 × 128 resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Training and Implementation</head><p>As the purpose of these experiments are to demonstrate the effectiveness of proposed architecture, we do not use any application specific loss function or metric loss functions in our experiments. We use a binary cross-entropy loss between the prediction and ground truth to train the network. The cross-entropy loss is defined as follows:</p><formula xml:id="formula_4">L CE(p,p) = − 1 wh w−1 x=0 h−1 y=0</formula><p>(p(x, y) log(p(x, y)))+</p><p>(1 − p(x, y))log(1 −p(x, y)), where w and h are the dimensions of image, p(x, y) corresponds to the image andp(x, y) denotes the output prediction at a specific pixel location (x, y). We set the batch-size as 1 and learning rate as 0.001. We use Adam optimizer for training. We use these hyperparameters uniformly across all the experiments. We train our networks for 300 epochs or until convergence depending on the dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">KiU-Net 3D</head><p>For volumetric segmentation, we use two widely used public datasets: BraTS challenge dataset and LiTS challenge dataset for our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Datasets</head><p>Brain Tumor Segmentation (MRI): Segmentation of the tumor is important for volume estimation of gliomas which is essential for planning clinical treatment. Brain Tumor Segmentation (BraTS) challenge has a curated collection of MRI scans with expert annotations of brain tumor. It contains multimodal MRI scans of confirmed cases of glioblastoma (GBM/HGG) and low grade glioma (LGG). The modalities present in these scans are native (T1), post-contrast T1 (T1ce), T2-weighted (T2) and T2 attenuated inversion recovery (T2-FLAIR) <ref type="bibr" target="#b49">[49]</ref>, <ref type="bibr" target="#b50">[50]</ref>, <ref type="bibr" target="#b51">[51]</ref>. The annotations are provided for four classes -enhancing tumor, peritumoral edema and the necrotic and nonenhancing tumor core. We used the 2019 version of the BraTS challenge training data for training our the baselines and our proposed. It has a total of 335 MRI scans. For quantitative comparisons, we use the validation dataset provided by BraTS 2019 challenge which has 125 scans. For qualitative comparisons, we use the new 33 scans added to the training set of BraTS 2020 challenge dataset as the validation datasets do not come with ground truth annotations. Each MRI scan contains 155 slices each of dimensions 255 × 255. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Training and Implementation</head><p>For training the KiU-Net 3D, we use the cross entropy loss between the prediction and the input scan. Since, the scan can be viewed as a 3D voxel, the cross entropy loss can be formulated as follows: <ref type="bibr">TABLE 1</ref> Comparison of quantitative metrics for Brain US, GLAS and RITE datasets between KiU-Net, U-Net++ <ref type="bibr" target="#b2">[3]</ref>, U-Net <ref type="bibr" target="#b1">[2]</ref> and Seg-Net <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset</head><p>Metrics Seg-Net <ref type="bibr">[</ref>   where w, h are the dimensions of each slice while l is the number of slices in the scan, p(z, x, y) corresponds to the input 3D scan andp(z, x, y) denotes the output prediction at a specific pixel location (z, x, y) and C corresponds to the number of classes found in the dataset. The above loss formulation suits the multi-class segmentation framework of BraTS dataset where C = 4. For both the datasets, we use a learning rate of 0.0001 with batch size 1 and use Adam optimizer. We do voxel-wise training (similar to patch-wise training on 2D) for the BraTS dataset with voxel shapes 128 × 128 × 50.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RESULTS</head><p>In this section, we discuss the results and outcomes of the experiments that we conducted using KiU-Net and KiU-Net 3D. First, we discuss the quantitative evaluations where the proposed method is compared with other recent approaches using metrics that are widely used for medical image segmentation. Next, we provide qualitative results where we visualize sample predictions to analyze why the proposed method's performance is superior as compared to the other approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Quantitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">KiU-Net</head><p>Following existing approaches like <ref type="bibr" target="#b3">[4]</ref>, we use Dice Index (F1-score) and Jaccard Similarity score (mIOU) for evaluat-ing and comparing the proposed method (KiU-Net) on the medical image segmentation datasets:</p><formula xml:id="formula_5">Dice =</formula><p>2T P 2T P + F P + F N , Jaccard = T P T P + F P + F N , where TP, FP and FN correspond to the number of pixels that are true positives, false positives and false negatives respectively of the prediction when compared with the ground truth. <ref type="table">Table 1</ref> shows the results for the experiments on all the 3 datasets for image segmentation. As it can be observed, we compare KiU-Net with some of the widely used backbone architectures for medical image segmentation like Seg-Net, U-Net and U-Net++. KiU-Net not only performs better than the other networks but also achieves a significant boost in terms of performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">KiU-Net 3D</head><p>For 3D volumetric segmentation, we adopt the performance metrics used in the BraTS challenge and LiTS challenge. For brain tumor segmentation from MRI scans, we report the Dice accuracy of enhancing tumor (ET), whole tumor (WT) and tumor core (TC). We also report the Hausdorff distance for all these three classes. More details about these metrics for brain tumor segmentation can be found in <ref type="bibr" target="#b51">[51]</ref>. Similarly for liver segmentation in LiTS dataset, we report metrics such as Dice score, Jaccard index, volume overlap error (VOE), false negative rate (FNR), false positive rate (FPR), average symmetric surface distance (ASSD) and maximum symmetric surface distance (MSD). The details of these metrics can be found in <ref type="bibr" target="#b52">[52]</ref>. For both these datasets we compare KiU-Net 3D with U-Net 3D and Seg-Net 3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image Seg-Net U-Net U-Net++ KiU-Net GT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Retina Nerve Segmentation RITE Dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gland Segmentation GLAS Dataset</head><p>Brain Anatomy Segmentation Brain US Dataset <ref type="figure" target="#fig_11">Fig. 8</ref>. Comparison of qualitative results between SegNet, UNet , UNet++ and KiU-Net for Brain anatomy segmentation using brain US dataset, gland segmentation using GLAS dataset and retina nerve segmentation using RITE dataset.</p><p>Additionally, we also conduct experiments with slice-wise training of the scans using 2D versions of all these networks. <ref type="table" target="#tab_3">Tables 2 and 3</ref> report the quantitative metrics comparison for BraTS dataset and LiTS dataset respectively. We observe that KiU-Net outperforms other methods. Note that we have used the same pipeline for all these experiments for a fair comparison. Further, since the primary goal of these experiments is to show that KiU-Net can act as a better backbone network architecture, we do not perform any preprocessing or post-processing which can improve the performance further irrespective of the network architecture. In the first row, we can observe that KiU-Net is able to segment the small ventricles accurately and the other "traditional" networks fail to do so. Similarly from second, third and fourth rows, we can observe that the predictions of KiU-net are able to segment out the edges significantly better when compared to the other networks. In the predictions of RITE dataset which has very low number of images to train, our network performs reasonably well as compared to others learns, thus demonstrating that low level features for nerve segmentation are used effectively in our network. <ref type="figure" target="#fig_12">Fig 9</ref> illustrates the predictions of KiU-Net 3D for volumetric segmentation experiments. The first two rows correspond to the results for BraTS dataset and the bottom two rows correspond to the results for LiTS dataset. Note that the first row and third rows correspond to segmentation prediction of a single slice of the scan where as the second and fourth row correspond to the 3D segmentation prediction of the scan. The 3D segmentation results are visualized using ITKSnap <ref type="bibr" target="#b53">[53]</ref> where each scan prediction consists of 155 2D images in the case of BraTS dataset and 48 2D images in the case of LiTS dataset. From visualizations of the brain tumor segmentation task, it can be observed that KiU-Net is able to segment the surface and edges of the tumor significantly better than any other network and is more closer to the ground truth. For BraTS dataset, the red regions correspond to tumor core, yellow regions correspond to non-enhancing tumor and the green regions correspond to edema. From the second row for BraTS dataset, it can be observed that the tumor surface of the ground truth has sharp edges. While all the other methods smooth out these edges, KiU-Net predicts the sharp edges of the surface of the tumor more precisely. While these results are focused on demonstrating the superiority of KiU-Net in segmenting small lesions, the results on the LiTS dataset show that the proposed method is equally effective in segmenting larger regions. From the bottom two rows of <ref type="figure" target="#fig_12">Fig 9,</ref> we can observe that KiU-Net performs better than other networks in segmenting large masks as well. Based on this, we would like to point out that even though KiU-Net focuses more on low-level features when compared to UNet or UNet++, its performance is on-par/better as compared to them while segmenting large masks as well. We also observe that performing 2D segmentation on individual 2D images and then combining them to form a 3D scan does not work well. This can be observed from the first 3 images from the last row where the surfaces are not smooth while the ground truth looks smooth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Qualitative Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">KiU-Net</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">KiU-Net 3D</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">DISCUSSIONS</head><p>As we propose a generic solution to image and volumetric segmentation, we believe it is important to study the computational complexity and convergence trends of the proposed network. In this section, we provide these details and compare the proposed method with other approaches in terms of number of parameters and rate of convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Number of Parameters</head><p>Seg-Net, U-Net and U-Net++ have a 5 layer deep encoder and decoder. The number of filters in each block of these networks increase gradually as we go deeper in the network. For example, U-Net uses this sequence of filters for its 5 layers -64, 128, 256, 512 and 1024. Although KiU-Net is a multi-branch network, we limit the complexity of our network by using fewer layers and filters. Specifically, we use a 3 layer deep network with 32, 64 and 128 respectively as the number of filters. Due to this, the number of parameters in our network is significantly fewer as compared   to other methods. In <ref type="table" target="#tab_4">Table 4</ref>, we tabulate the number of parameters for KiU-Net and other recent networks and it can be observed that KiU-Net has ∼10× lesser parameters as compared to U-Net and ∼40× fewer parameters as compared to SegNet. Further, it is important to note that the other approaches have have resulted in higher complexity in an attempt to improve the performance, however, our approach is able to obtain better performance while have significantly fewer parameters.</p><formula xml:id="formula_6">(a) (b) (c) (d) (e) (f) (g)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Convergence</head><p>The convergence of loss function is an important characteristic associated with a network. A faster convergence means is always beneficial as it results in significantly lower training complexity. <ref type="figure" target="#fig_0">Fig 11 compares</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Ablation Study</head><p>We conduct an ablation study to analyze the effectiveness of different blocks in the proposed method (KiU-Net). For these experiments, we use the brain anatomy segmentation US dataset. We start with the basic undercomplete ("traditional") encoder-decoder convolutional architecture (UC) and overcomplete convolutional architecture (OC). Note that these networks do not contain any skip connections <ref type="figure" target="#fig_0">Fig. 11</ref>. Comparison of convergence of loss function between KiU-Net, UNet++ <ref type="bibr" target="#b2">[3]</ref>, UNet <ref type="bibr" target="#b1">[2]</ref> and SegNet <ref type="bibr" target="#b0">[1]</ref>. The convergence of KiU-Net is faster when compared to all other methods.</p><p>(SK). Next, we add skip connections to the UC and OC baselines. These networks are basically U-Net (UC+skip connections) and Kite-Net (OC+skip connections). We then fuse both these networks by adding the feature map output of both the networks at the end. This is in fact KiU-Net without the CRFB block. Finally, we show the performance of our proposed architecture -KiU-Net. <ref type="table" target="#tab_5">Table 5</ref> shows the results of all these ablation experiments. It can be observed that the performance improves with addition of each block to the network. Please note that the performances of OC and Kite-Net are lower because these predictions contain only the edges of the masks and do not contain any high-level information. <ref type="figure" target="#fig_0">Fig 10 illustrates</ref> the qualitative improvements after adding each major block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Further Improvements</head><p>As it is clear from the above discussions that KiU-Net is a good backbone architecture for both image and volumetric segmentation, we experiment with other variants of KiU-Net which result in further improvements. In this section, we describe these variants and present the detailed results corresponding to each of these variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRFB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input Image Prediction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CRFB</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv2D</head><p>Max-Pooling   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.1">Res-KiUNet</head><p>In Res-KiUNet, we employ residual connections in both the branches of KiU-Net. We use residual learning in every conv block at each level in both the encoder and decoder part of both branches. If x and y are the input and output of each conv block (F ()) of our network, the residual connection can be formulated as follows: y = F (x) + x. We illustrate the architecture details of Res-KiUNet in <ref type="figure" target="#fig_0">Fig  13</ref> where the residual connections are denoted using red arrows. Residual connections are helpful in efficient learning  of the network since we can propagate gradients to initial layers faster and thus solving the problem of vanishing gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.2">Dense-KiUNet</head><p>In Dense-KiUNet, we employ dense blocks after every conv layer in both the branches. We use a dense block of 4 conv layers where the input consists of k feature maps. Each conv layer outputs k/4 feature maps which is concatenated with the input to all the next conv layers. The output of all these conv layers are then concatenated to obtain k output feature maps. This is added with the input and sent to the next layer in the network. The output of the dense block is forwarded to a max-pooling layer in the encoder of undercomplete branch and in the decoder of overcomplete branch. In the encoder of overcomplete branch and in the decoder of undercomplete branch, the output of the dense block is forwarded to an upsampling layer. <ref type="figure" target="#fig_0">Fig 12 illustrates</ref> the architecture details of Dense-KiUNet and the dense block we have used.</p><p>To evaluate both Res-KiUNet and Dense KiUNet, we conduct experiments on the GlaS dataset and report the dice and Jaccard metrics in <ref type="table" target="#tab_8">Table 6</ref>. Further, we visualize the results of these experiments in <ref type="figure" target="#fig_0">Fig 14.</ref> It can be observed that Dense-KiUNet and Res-KiUNet provide further improvements in performance as compared to KiU-Net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION</head><p>In this paper, we proposed KiU-Net and KiU-Net 3D for image and volumetric segmentation, respectively. These are two-branch networks consisting of an undercomplete and an overcomplete autoencoder. Our novelty lies in proposing overcomplete convolutional architecture (Kite-Net) for learning small masks and finer details of surfaces and edges more precisely. The two branches are effectively fused using a novel cross-residual feature fusion method that results effective training. Further, we experiment with different variants of the proposed method like Res-KiUNet and Dense-KiUNet. We conduct extensive experiments for image and volumetric segmentation on 5 datasets spanning over 5 different modalities. We demonstrate that the proposed method performs significantly better when compared to the recent segmentation methods. Furthermore, we also show that our network comes with additional benefits such as lower model complexity and faster convergence. <ref type="table" target="#tab_9">Tables 7 and 8</ref> show the configuration of the Ki-Net and U-Net branch of our KiU Network. H and W are 128 each for the ultrasound train and test images.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A ARCHITECTURE DETAILS</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>A sample brain tumor segmentation prediction using (a) U-Net 3D, (b) KiU-Net 3D, and (c) Ground-Truth for BraTS dataset. KiU-Net 3D results in better segmentation of fine details when compared to U-Net 3D as it focuses more on low-level information effectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>(a) Input B-Mode Ultrasound Image. Predictions from (b) U-Net, (d) KiU-Net (ours), (f) Ground Truth. (c),(e) and (g) are the zoomed in patches from (b),(d) and (f) respectively. The boxes in the original images correspond to the zoomed in portion for the zoomed images. Our method segments small anatomy and edges better than U-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Sample images from Brain US data. Top row: B-Mode US scan. Bottom row: Ground truth. The structures present in the dataset are of varying sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Feature maps extracted from the encoder of (a) U-Net (b) Kite-Net. Top row: Feature maps extracted from the first layer. Middle row: Feature maps extracted from the second layer. Bottom row: Feature maps extracted from the third layer. It can be observed that while the features extracted from U-Net are coarse and focus on high-level features in the deeper layers, Kite-Net extracts fine details like edges in its deep layers. Also, the feature maps of Kite-Net in the deeper layers are of a higher resolution as we perform upsampling in the feature space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>(a) Architecture details of KiU-Net for 2D image segmentation. In KiU-Net, the input image is forwarded to the two branches of KiU-Net: Kite-Net and U-Net which have CRFB blocks connecting them at each level. The feature maps from the last layer of both the branches are added and passed through 1 × 1 2D conv to get the prediction. In CRFB, residual features of Kite-Net are learned and added to the features of U-Net to forward the complementary features and vice-versa. (b) Details of Cross Residual Fusion Block (CRFB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Effect on receptive field change due to architecture type as seen in (a) U-Net (b) Kite-Net. The receptive field of Kite-Net is constrained when compared to U-Net in the deeper layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>(a) Architecture details of KiU-Net 3D for 3D volumetric segmentation. (b) Details of Cross Residual Fusion Block (CRFB) for KiU-Net 3D. In KiU-Net 3D, the input 3D voxel is forwarded to the two branches of KiU-Net 3D: Kite-Net 3D and U-Net 3D which have 3D CRFB blocks connecting them at each level. The feature maps from the last layer of both the branches are added and passed through 1 × 1 3D conv to get the prediction. In CRFB, the residual features of Kite-Net 3D are learned and added to the features of U-Net 3D to forward the complementary features to U-Net and vice-versa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Liver Segmentation (CT): Accurate segmentation of liver and liver lesions from abdominal CT scans are essential for diagnosis, treatment and surgical planning. Liver Tumor Segmentation Challenge (LiTS) dataset [52] contains contrastenhanced abdominal CT scans along with annotations of liver and liver lesions. The training set contains 130 CT scans and the testing set contains 70 CT scans. For our experiments, we focus on liver segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>L</head><label></label><figDesc>CE(p,p) = − , x, y) log(p(z, x, y)),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig 8</head><label>8</label><figDesc>illustrates the predictions of KiU-Net along with Seg-Net, U-Net and U-Net++ for all 3 medical image segmentation datasets we used for evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 .</head><label>9</label><figDesc>Comparison of qualitative results between SegNet, UNet , KiU-Net, SegNet 3D, UNet 3D and KiU-Net 3D for brain tumor segmentation using BraTS dataset and Liver segmentation using LiTS dataset. The first and third row correspond to the prediction from a 2D slice in the MRI brain scan and abdominal CT scan respectively. The second and third rows visualize the 3D volume segmentation predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 10 .</head><label>10</label><figDesc>Qualitative results of ablation study on test images. (a) B-Mode input US image. (b) Ground Truth annotation. Prediction of segmentation masks by (c) UC -Under-complete architecture (d) OC -Over-complete architecture (e) UC + SK (under-complete architecture with skip connections) (f) UC + OC with SK (combined architecture with skip connections) (g) KiU-Net (ours). The change in quality of the predictions can be observed after each addition to the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 12 .</head><label>12</label><figDesc>Details of Dense-KiU-Net architecture. The input image is forwarded to the two branches of Dense-KiUNet where each branch has dense blocks at each level. The feature maps are added at the last layer and forwarded through a 1 × 1 conv 2D layer to get the prediction . In the right side of the figure, dense block architecture has been visualized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 13 .</head><label>13</label><figDesc>Details of Res-KiU-Net architecture. The input image is forwarded to the two branches of Res-KiUNet where each branch has residual connections at each level. The feature maps are added at the last layer and passed through a 1 × 1 conv 2D layer to get the prediction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig. 14 .</head><label>14</label><figDesc>(a) Input Image, Prediction using (b) KiU-Net (c) Res-KiUNet (d) Dense KiU-Net (e) Ground Truth for GLAS dataset. It can be observed that the predictions of Res-KiUNet and Dense-KiUNet are better in terms of quality when compared to KiU-Net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Comparison of quantitative metrics for brain tumor segmentation in BraTS dataset.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">1] U-Net [2] U-Net++ [3] KiU-Net</cell></row><row><cell></cell><cell cols="2">Brain US</cell><cell>Dice Jaccard</cell><cell>0.8279 0.7502</cell><cell>0.8537 0.7931</cell><cell>0.8659 0.7995</cell><cell>0.8943 0.8326</cell></row><row><cell></cell><cell>GLAS</cell><cell></cell><cell>Dice Jaccard</cell><cell>0.7861 0.6596</cell><cell>0.7976 0.6763</cell><cell>0.8005 0.6893</cell><cell>0.8325 0.7278</cell></row><row><cell></cell><cell>RITE</cell><cell></cell><cell>Dice Jaccard</cell><cell>0.5223 0.3914</cell><cell>0.5524 0.3111</cell><cell>0.5410 0.3724</cell><cell>0.7517 0.6037</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE 2</cell><cell></cell><cell></cell></row><row><cell>Type</cell><cell>Network</cell><cell cols="7">Dice-ET Dice-WT Dice-TC Hausdorff95-ET Hausdorff95-WT Hausdorff95-TC</cell></row><row><cell></cell><cell>Seg-Net [1]</cell><cell cols="2">0.4994</cell><cell>0.7611</cell><cell>0.6887</cell><cell>65.6867</cell><cell>20.3247</cell><cell>20.0050</cell></row><row><cell>Slice wise</cell><cell>U-Net [2]</cell><cell cols="2">0.5264</cell><cell>0.8083</cell><cell>0.7032</cell><cell>17.5458</cell><cell>13.9467</cell><cell>19.2653</cell></row><row><cell></cell><cell>KiU-Net</cell><cell cols="2">0.6637</cell><cell>0.8612</cell><cell>0.7061</cell><cell>9.4176</cell><cell>12.7896</cell><cell>13.0401</cell></row><row><cell></cell><cell>Seg-Net 3D [1]</cell><cell cols="2">0.5599</cell><cell>0.8062</cell><cell>0.7073</cell><cell>10.0037</cell><cell>10.4584</cell><cell>10.7513</cell></row><row><cell>Voxel wise</cell><cell>U-Net 3D [6]</cell><cell cols="2">0.6711</cell><cell>0.8448</cell><cell>0.7059</cell><cell>10.2162</cell><cell>13.0005</cell><cell>15.0856</cell></row><row><cell></cell><cell>KiU-Net 3D</cell><cell cols="2">0.7321</cell><cell>0.8760</cell><cell>0.7392</cell><cell>6.3228</cell><cell>8.9424</cell><cell>9.8929</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Comparison of quantitative metrics for liver segmentation in LiTS dataset.</figDesc><table><row><cell>Type</cell><cell>Network</cell><cell>Dice</cell><cell cols="2">Jacard VOE</cell><cell>FNR</cell><cell>FPR</cell><cell>ASSD</cell><cell>MSD</cell></row><row><cell></cell><cell>Seg-Net [1]</cell><cell cols="2">0.7656 0.6265</cell><cell cols="5">0.3734 0.0673 0.3061 7.6310 45.3610</cell></row><row><cell>Slice-wise</cell><cell>U-Net [2]</cell><cell cols="2">0.7723 0.6350</cell><cell cols="5">0.3649 0.0688 0.0688 7.6968 48.7151</cell></row><row><cell></cell><cell>KiU-Net</cell><cell cols="2">0.8035 0.6412</cell><cell cols="5">0.2956 0.0605 0.2045 6.8452 42.5421</cell></row><row><cell></cell><cell cols="3">Seg-Net 3D [1] 0.8789 0.7904</cell><cell cols="5">0.2091 0.0428 0.1666 3.9452 42.0544</cell></row><row><cell cols="2">Voxel-wise U-Net 3D [6]</cell><cell>0.9346</cell><cell cols="6">0.8828 0.1171 0.0622 0.0549 1.9450 33.8872</cell></row><row><cell></cell><cell>KiU-Net 3D</cell><cell>0.9423</cell><cell cols="6">0.8946 0.1053 0.0548 0.0505 1.7711 29.9831</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 4</head><label>4</label><figDesc>Comparison of number of parameters</figDesc><table><row><cell>Network</cell><cell cols="4">Seg-Net [1] U-Net [2] U-Net++ [3] KiU-Net</cell></row><row><cell>No. of Parameters</cell><cell>12.5M</cell><cell>3.1M</cell><cell>9.0M</cell><cell>0.29M</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 5</head><label>5</label><figDesc>Ablation Study</figDesc><table><row><cell>Metrics</cell><cell>UC</cell><cell>OC</cell><cell cols="4">UC+SK OC+SK UC+OC+SK KiU-Net</cell></row><row><cell>Dice</cell><cell cols="2">0.82 0.56</cell><cell>0.85</cell><cell>0.60</cell><cell>0.86</cell><cell>0.89</cell></row><row><cell>Jaccard</cell><cell cols="2">0.75 0.43</cell><cell>0.79</cell><cell>0.47</cell><cell>0.78</cell><cell>0.83</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6</head><label>6</label><figDesc>Comparison of performance metrics for Res-KiUNet and Dense KiUNet using GLAS dataset.</figDesc><table><row><cell cols="4">Performance Metrics KiU-Net Res-KiUNet Dense-KiUNet</cell></row><row><cell>Dice</cell><cell>0.8325</cell><cell>0.8385</cell><cell>0.8431</cell></row><row><cell>Jaccard</cell><cell>0.7278</cell><cell>0.7303</cell><cell>0.7422</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7</head><label>7</label><figDesc>Configuration of the Ki-Net branch of KiU-Net.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 8</head><label>8</label><figDesc>Configuration of the U-Net branch of KiU-Net.</figDesc><table><row><cell>Block name</cell><cell>Layer</cell><cell>Kernel size/Scale Factor</cell><cell>Filters</cell><cell>Padding</cell><cell>Input size</cell><cell>Output size</cell></row><row><cell></cell><cell>Conv1</cell><cell>3 × 3</cell><cell>32</cell><cell>1</cell><cell>1 × H × W</cell><cell>32 × H × W</cell></row><row><cell></cell><cell>MaxPooling</cell><cell>2 × 2</cell><cell>-</cell><cell>-</cell><cell>32 × H × W</cell><cell>32 × H/2 × W/2</cell></row><row><cell></cell><cell>ReLU</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32 × H/2 × W/2</cell><cell>32 × H/2 × W/2</cell></row><row><cell></cell><cell>Conv2</cell><cell>3 × 3</cell><cell>64</cell><cell>1</cell><cell>32 × H/2 × W/2</cell><cell>64 × H/2 × W/2</cell></row><row><cell>Encoder</cell><cell>MaxPooling</cell><cell>2 × 2</cell><cell>-</cell><cell>-</cell><cell>64 × H/2 × W/2</cell><cell>64 × H/4 × W/4</cell></row><row><cell></cell><cell>ReLU</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>64 × H/4 × W/4</cell><cell>64 × H/4 × W/4</cell></row><row><cell></cell><cell>Conv3</cell><cell>3 × 3</cell><cell>128</cell><cell>1</cell><cell>64 × H/4 × W/4</cell><cell>128 × H/4 × W/4</cell></row><row><cell></cell><cell>MaxPooling</cell><cell>2 × 2</cell><cell>-</cell><cell>-</cell><cell>128 × H/4 × W/4</cell><cell>128 × H/8 × W/8</cell></row><row><cell></cell><cell>ReLU</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>128 × H/8 × W/8</cell><cell>128 × H/8 × W/8</cell></row><row><cell></cell><cell>Conv1</cell><cell>3 × 3</cell><cell>128</cell><cell>1</cell><cell>512 × H/32 × W/32</cell><cell>128 × H/32 × W/32</cell></row><row><cell></cell><cell>Upsampling</cell><cell>2 × 2</cell><cell>-</cell><cell>-</cell><cell>128 × H/32 × W/32</cell><cell>128 × H/16 × W/16</cell></row><row><cell></cell><cell>ReLU</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>128 × H/16 × W/16</cell><cell>128 × H/16 × W/16</cell></row><row><cell></cell><cell>Conv2</cell><cell>3 × 3</cell><cell>64</cell><cell>1</cell><cell>128 × H/16 × W/16</cell><cell>64 × H/16 × W/16</cell></row><row><cell>Decoder</cell><cell>Upsampling</cell><cell>2 × 2</cell><cell>-</cell><cell>-</cell><cell>64 × H/16 × W/16</cell><cell>64 × H/8 × W/8</cell></row><row><cell></cell><cell>ReLU</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>64 × H/8 × W/8</cell><cell>64 × H/8 × W/8</cell></row><row><cell></cell><cell>Conv3</cell><cell>3 × 3</cell><cell>32</cell><cell>1</cell><cell>64 × H/8 × W/8</cell><cell>32 × H/8 × W/8</cell></row><row><cell></cell><cell>Upsampling</cell><cell>2 × 2</cell><cell>-</cell><cell>-</cell><cell>32 × H/8 × W/8</cell><cell>32 × H/4 × W/4</cell></row><row><cell></cell><cell>ReLU</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>32 × H/4 × W/4</cell><cell>32 × H/4 × W/4</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block name</head><p>Layer Kernel size/Scale Factor Filters Padding Input size Output size</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unet++: A nested u-net architecture for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unet++: Redesigning skip connections to exploit multiscale features in image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M R</forename><surname>Siddiquee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tajbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1856" to="1867" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unet 3+: A full-scale connected unet for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Iwamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1055" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">net: learning dense volumetric segmentation from sparse annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Içek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Lienkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
	<note>3d u-</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-A</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 fourth international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Weighted res-unet for highquality retina vessel segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 9th International Conference on Information Technology in Medicine and Education (ITME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="327" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hdenseunet: hybrid densely connected unet for liver and tumor segmentation from ct volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2663" to="2674" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Kiu-net: Towards accurate segmentation of biomedical images using overcomplete representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04878</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to segment brain anatomy from 2d ultrasound with less data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M J</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yasarla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic real-time cnn-based neonatal brain ventricles segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">G</forename><surname>Cuccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hacihaliloglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="716" to="719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1125" to="1134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Automatic segmentation of the cerebral ventricle in neonates using deep learning with 3d reconstructed freehand ultrasound imaging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sciolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sdika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Quetin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Delachartre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Ultrasonics Symposium (IUS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic segmentation of brain tumor from 3d mr images using segnet, u-net, and psp-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="226" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Three pathways u-net for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pre-conference proceedings of the 7th medical image computing and computer-assisted interventions (MICCAI) BraTS Challenge</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2018</biblScope>
			<biblScope unit="page" from="119" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Brain tumor detection and segmentation using deep learning u-net on multi modal mri</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Fridman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pre-Conference Proceedings of the 7th MICCAI BraTS Challenge</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="135" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep convolutional neural networks using u-net for automatic brain tumor segmentation in multimodal mri volumes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kermi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mahmoudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Khadir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Pixelnet: Representation of the pixels, by the pixels, and for the pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06506</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2881" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Glioma prognosis: segmentation of the tumor and survival prediction using shape, geometric and clinical information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="142" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">S3d-unet: separable 3d u-net for brain tumor segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="358" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning radiomics algorithm for gliomas (drag) model: a novel approach using 3d unet based deep convolutional neural network for predicting survival in gliomas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Baid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Talbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moiyadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="369" to="379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Brain tumor segmentation and survival prediction using 3d attention unet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vibashan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">J M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wijethilake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Utkarsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="262" to="272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d mri brain tumor segmentation using autoencoder regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Myronenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="311" to="320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">No new-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Isensee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kickingereder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bendszus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International MICCAI Brainlesion Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automatic segmentation of kidney and renal tumor in ct images based on 3d fully convolutional neural network with pyramid pooling module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Dillenseger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Coatrieux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 24th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3790" to="3795" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">3d unet-based kidney and kidney tumer segmentation with attentive feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Bisc-unet: A fine segmentation framework for kidney and renal tumor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Dillenseger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-L</forename><surname>Coatrieux</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Ric-unet: An improved neural network based on unet for nuclei segmentation in histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ieee Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="21420" to="21428" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mc-unet: Multi-scale convolution unet for bladder cancer cell segmentation in phase-contrast microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1197" to="1199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-net: deep learning for cell counting, detection, and morphometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Falk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bensch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ö</forename><surname>Içek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdulkadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Marrakchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Böhm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deubner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jäckel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Seiwald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="67" to="70" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mimo-net: A multi-input multi-output convolutional neural network for cell segmentation in fluorescence microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E A</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pelengaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 14th International Symposium on Biomedical Imaging</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="337" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Alom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yakopcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Asari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.06955</idno>
		<title level="m">Recurrent residual convolutional neural network based on u-net (r2u-net) for medical image segmentation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mdan-unet: Multi-scale and dual attention enhanced nested u-net architecture for segmentation of optical coherence tomography images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Cai-unet for segmentation of liver lesion in ct image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Society for Optics and Photonics</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11313</biblScope>
			<biblScope unit="page">1131325</biblScope>
		</imprint>
	</monogr>
	<note>Medical Imaging 2020: Image Processing</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Automatic couinaud segmentation from ct volumes on liver using glc-unet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Machine Learning in Medical Imaging</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="274" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Ra-unet: A hybrid deep attention-aware network to extract liver and tumor in ct scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01328</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep learning techniques for medical image segmentation: Achievements and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Hesamian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kennedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of digital imaging</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="582" to="596" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A A</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghafoorian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Van Der Laak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">I</forename><surname>Sánchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning overcomplete representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lewicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="365" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Over-complete representations on recurrent neural networks can support persistent percepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Druckmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Chklovskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="541" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Gland segmentation in colon histology images: The glas challenge contest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Matuszewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bruni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sanchez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="489" to="502" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Ridge-based vessel segmentation in color images of the retina</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Staal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niemeijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Viergever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="501" to="509" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automated separation of binary overlapping trees in low-contrast color retinal images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Abràmoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Garvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="436" to="443" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">The multimodal brain tumor image segmentation benchmark (brats)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kalpathy-Cramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Porz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Slotboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wiest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1993" to="2024" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Advancing the cancer genome atlas glioma mri collections with expert segmentation labels and radiomic features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sotiras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Freymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Davatzikos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">170117</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall survival prediction in the brats challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bakas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reyes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jakab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rempfler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Crimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Shinohara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rozycki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.02629</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bilic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Christ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chlebus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hesser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04056</idno>
		<title level="m">The liver tumor segmentation benchmark (lits)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Yushkevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Piven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Cody</forename><surname>Hazlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1116" to="1128" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
