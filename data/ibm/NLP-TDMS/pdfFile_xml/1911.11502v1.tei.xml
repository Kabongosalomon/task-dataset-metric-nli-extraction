<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hearing Lips: Improving Lip Reading by Distilling Speech Recognizers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchao</forename><surname>Wang</surname></persName>
							<email>xinchao.wang@stevens.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stevens Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Hou</surname></persName>
							<email>houpeng.hp@alibaba-inc.com</email>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haihong</forename><surname>Tang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingli</forename><surname>Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hearing Lips: Improving Lip Reading by Distilling Speech Recognizers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T06:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Lip reading has witnessed unparalleled development in recent years thanks to deep learning and the availability of largescale datasets. Despite the encouraging results achieved, the performance of lip reading, unfortunately, remains inferior to the one of its counterpart speech recognition, due to the ambiguous nature of its actuations that makes it challenging to extract discriminant features from the lip movement videos. In this paper, we propose a new method, termed as Lip by Speech (LIBS), of which the goal is to strengthen lip reading by learning from speech recognizers. The rationale behind our approach is that the features extracted from speech recognizers may provide complementary and discriminant clues, which are formidable to be obtained from the subtle movements of the lips, and consequently facilitate the training of lip readers. This is achieved, specifically, by distilling multigranularity knowledge from speech recognizers to lip readers. To conduct this cross-modal knowledge distillation, we utilize an efficacious alignment scheme to handle the inconsistent lengths of the audios and videos, as well as an innovative filtering strategy to refine the speech recognizer's prediction. The proposed method achieves the new state-of-the-art performance on the CMLR and LRS2 datasets, outperforming the baseline by a margin of 7.66% and 2.75% in character error rate, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Lip reading, also known as visual speech recognition, aims at predicting the sentence being spoken, given a muted video of a talking face. Thanks to the recent development of deep learning and the availability of big data for training, lip reading has made unprecedented progress with much performance enhancement <ref type="bibr" target="#b4">Chung et al. 2017;</ref><ref type="bibr" target="#b10">Zhao, Xu, and Song 2019)</ref>.</p><p>In spite of the promising accomplishments, the performance of the video-based lip reading remains considerably lower than its counterpart, the audio-based speech recognition, for which the goal is also to decode the spoken text and therefore can be treated as a heterogeneous modality sharing the same underlying distribution as lip reading. Given the same amount of training data and model architecture, Copyright c 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. the performance discrepancy is as large as 10.4% vs. 39.5% in terms of character error rate for speech recognition and lip reading, respectively <ref type="bibr" target="#b4">(Chung et al. 2017)</ref>. This is due to the intrinsically ambiguous nature of lip actuations: several seemingly-identical lip movements may produce different words, making it highly challenging to extract discriminant features from the video of interest and to further dependably predict the text output.</p><p>In this paper, we propose a novel scheme, Lip by Speech (LIBS), that utilizes speech recognition, for which the performances are in most cases gratifying, to facilitate the training of the more challenging lip reading. We assume a pre-trained speech recognizer is given, and attempt to distill knowledge concealed in the speech recognizer to the target lip reader to be trained.</p><p>The rationale for exploiting knowledge distillation (Hinton, Vinyals, and Dean 2015) for this task lies in that, acoustic speech signals embody information complementary to that of the visual ones. For example, utterances with subtle movements, which are challenging to be distinguished visually, are in most cases handy to be recognized acoustically <ref type="bibr" target="#b9">(Wolff et al. 1994)</ref>. By imitating the acoustic speech features extracted by the speech recognizer, the lip reader is expected to enhance its capability to extract discriminant visual features. To this end, LIBS is designed to distill knowledge at multiple temporal scales including sequence-level, context-level, and frame-level, so as to encode the multigranularity semantics from the input sequence.</p><p>Nevertheless, distilling knowledge from a heterogeneous modality, in this case the audio sequence, confronts two major challenges. The first lies in the fact that, the two modalities may feature different sampling rates and are thus asynchronous, while the second concerns the imperfect speechrecognition predictions. To this end, we employ a crossmodal alignment strategy to synchronize the audio and video data by finding the correspondence between them, so as to conduct the fine-grained knowledge distillation from audio features to visual ones. To enhance the speech predictions, on the other hand, we introduce a filtering technique to refine the distilled features, so that useful features can be filtered for knowledge distillation.</p><p>Experimental results on two large-scale lip reading datasets, CMLR <ref type="bibr" target="#b10">(Zhao, Xu, and Song 2019)</ref> and LRS2 <ref type="bibr" target="#b0">(Afouras et al. 2018)</ref>, show that the proposed approach outperforms the state of the art. We achieve a character error rate of 31.27%, a 7.66% enhancement over the baseline on the CMLR dataset, and one of 45.53% with 2.75% improvement on LRS2. It is noteworthy that when the amount of training data shrinks, the proposed approach tends to yield an even greater performance gain. For example, when only 20% of the training samples are used, the performance against the baseline has an 9.63% boost on the CMLR dataset.</p><p>Our contribution is therefore an innovative and effective approach to enhancing the training of lip readers, achieved by distilling multi-granularity knowledge from speech recognizers. This is to our best knowledge the first attempt along this line and, unlike existing feature-level knowledge distillation methods that work on Convolutional Neural Networks <ref type="bibr" target="#b8">(Romero et al. 2014;</ref><ref type="bibr" target="#b5">Gupta, Hoffman, and Malik 2016;</ref><ref type="bibr" target="#b8">Hou et al. 2019)</ref>, our strategy handles Recurrent Neural Networks. Experiments on several datasets show that the proposed method leads to the new state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Lip Reading  proposes the first deep learning-based, end-to-end sentence-level lipreading model. It applies a spatiotemporal CNN with Gated Recurrent Unit (GRU) <ref type="bibr" target="#b4">(Cho et al. 2014)</ref> and Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b4">(Graves et al. 2006</ref><ref type="bibr" target="#b4">). (Chung et al. 2017</ref> introduces the WLAS network utilizing a novel dual attention mechanism that can operate over visual input only, audio input only, or both. <ref type="bibr" target="#b0">(Afouras et al. 2018</ref>) presents a seq2seq and a CTC architecture based on self-attention transformer models, and are pre-trained on a non-publicly available dataset. <ref type="bibr" target="#b8">(Shillingford et al. 2018</ref>) designs a lipreading system that uses a network to output phoneme distributions and is trained with CTC loss, followed by finite state transducers with language model to convert the phoneme distributions into word sequences. In <ref type="bibr" target="#b10">(Zhao, Xu, and Song 2019)</ref>, a cascade sequenceto-sequence architecture (CSSMCM) is proposed for Chinese Mandarin lip reading. CSSMCM explicitly models tones when predicting characters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech Recognition</head><p>Sequence-to-sequence models are gaining popularity in the automatic speech recognition (ASR) community, since it folds separate components of a conventional ASR system into a single neural network. (Chorowski et al. 2014) combines sequence-to-sequence with attention mechanism to decide which input frames be used to generate the next output element. <ref type="bibr" target="#b3">(Chan et al. 2016</ref>) proposes a pyramid structure in the encoder, which reduces the number of time steps that the attention model has to extract relevant information from.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Knowledge Distillation</head><p>Knowledge distillation is originally introduced for a smaller student network to perform better by learning from a larger teacher network <ref type="bibr" target="#b7">(Hinton, Vinyals, and Dean 2015)</ref>. The teacher network has previously been trained, and the parameters of the student network are going to be estimated. In <ref type="bibr" target="#b8">(Romero et al. 2014)</ref>, the knowledge distillation idea is applied in image classification, where a student network is required to learn the intermediate output of a teacher network. In <ref type="bibr" target="#b5">(Gupta, Hoffman, and Malik 2016)</ref>, knowledge distillation is used to teach a new CNN for a new image modality (like depth images), by teaching the network to reproduce the mid-level semantic representations learned from a well-labeled image modality. (Kim and Rush 2016) propose a sequence-level knowledge distillation method for neural machine translation at the output level. Different from these work, we perform feature-level knowledge distillation on Recurrent Neural Networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>Here we briefly review the attention-based sequence-tosequence model <ref type="bibr" target="#b1">(Bahdanau, Cho, and Bengio 2015)</ref>. Let x = [x 1 , ..., x I ], y = [y 1 , ..., y K ] be the input and target sequence with a length of I and K respectively. Sequence-to-sequence model parameterizes the probability p(y|x) with an encoder neural network and a decoder neural network. The encoder transforms the input sequence x 1 , ..., x I into a sequence of hidden state h x 1 , ..., h x I and produces the fixed-dimensional state vector s x , which contains the semantic meaning of the input sequence. We also called s x the sequence vector in this paper.</p><formula xml:id="formula_0">h x i = RNN(x i , h x i−1 ),<label>(1)</label></formula><formula xml:id="formula_1">s x = h x I .<label>(2)</label></formula><p>The decoder computes the probability of the target sequence conditioned on the outputs of the encoder. Specifically, given the input sequence and previously generated target sequence y &lt;k , the conditional probability of generating the target y k at timestep k is decided by:</p><formula xml:id="formula_2">p(y k |y &lt;k , x) = g(y k−1 , h d k , c x k ), h d k = RNN(h d k−1 , y k−1 , c x k ),<label>(3)</label></formula><p>where g is the softmax function, h d k is the hidden state of decoder RNN at timestep k, and c x k is the context vector calculated by an attention mechanism. Attention mechanism allows the decoder to attend to different parts of the input sequence at each step of output generation.</p><p>Concretely, the context vector is calculated by weighting each encoder hidden state h x i according to the similarity distribution α k :</p><formula xml:id="formula_3">c x k = I i=1 α ki h x i ,<label>(4)</label></formula><p>The similarity distribution α k signifies the proximity between h d k−1 and each h x i , and is calculated by:</p><formula xml:id="formula_4">α ki = exp(f (h d k−1 , h x i )) I j=1 exp(f (h d k−1 , h x j ))</formula><p>.  Knowledge is distilled at sequence-, context-, and frame-level to enable the features of multi-granularity to be transferred from teacher network to student. KD is short for knowledge distillation.</p><p>f calculates the unnormalized similarity between h d k−1 and h x i , usually in the following ways:</p><formula xml:id="formula_6">f (h d k−1 , h x i ) =    (h d k−1 ) T h x i , dot (h d k−1 ) T W h x i , general v t tanh(W [h d k−1 , h x i ]), concat<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed Method</head><p>The framework of LIBS is illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>. Both the speech recognizer and the lip reader are based on the attention-based sequence-to-sequence architecture. For an input video,</p><formula xml:id="formula_7">x v = [x v 1 , ..., x v J ] represents its video frame sequence, y = [y 1 , ..., y K ] is the target character se- quence. The corresponding audio frame sequence is x a = [x a 1 , ..., x a I ].</formula><p>A pre-trained speech recognizer reads in the audio frame sequence x a , and outputs the predicted character sequenceỹ = [ỹ 1 , ...,ỹ L ]. It should be noted that the sentence predicted by speech recognizer is imperfect, and L may not equal to K. At the same time, the encoder hidden states h a = [h a 1 , ..., h a I ], sequence vector s a , and context vectors c a = [c a 1 , ..., c a L ] can also be obtained. They are used to guide the training of the lip reader.</p><p>The basic lip reader is trained to maximize conditional probability distribution p(y|x v ), which equals to minimize the loss function:</p><formula xml:id="formula_8">L base = − K k=1 log p(y k |y &lt;k , x v ).<label>(7)</label></formula><p>The encoder hidden states, sequence vector and context vectors of the lip reader are denoted as</p><formula xml:id="formula_9">h v = [h v 1 , ..., h v J ], s v , and c v = [c v 1 , ..., c v K ]</formula><p>, respectively. The proposed method LIBS aims to minimize the loss function:</p><formula xml:id="formula_10">L = L base + λ 1 L KD1 + λ 2 L KD2 + λ 3 L KD3 , (8) where L KD1 , L KD2</formula><p>, and L KD3 constitute the multigranularity knowledge distillation, and work at sequencelevel, context-level and frame-level respectively. λ 1 , λ 2 and λ 3 are the corresponding balance weights. Details are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sequence-Level Knowledge Distillation</head><p>As mentioned before, the sequence vector s x contains the semantic information of the input sequence. For a video frame sequence x v and its corresponding audio frame sequence x a , their sequence vectors s a and s v should be the same, because they are different expressions of the same thing. Therefore, the sequence-level knowledge distillation is denoted as :</p><formula xml:id="formula_11">L KD1 = s a − t(s v ) 2 2 .</formula><p>(9) t is a simple transformation function (for example a linear or affine function), which embeds features into a space with the same dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Context-Level Knowledge Distillation</head><p>When decoder predicting a character at a certain timestep, the attention mechanism uses context vector to summarize the input information that is most relevant to the current output. Therefore, if the lip reader and speech recognizer predict the same character at j-th timestep, the context vectors c v j and c a j should contain the same information. Naturally, the context-level knowledge distillation should push c v j and c a j to be the same. However, due to the imperfect speech-recognition predictions, it's possible thatỹ j and y j may not be the same. Simply making c v j and c a j similar would hinder the performance of lip reader. This requires choosing the correct characters from the speech-recognition predictions, and using the corresponding context vectors for knowledge distillation. Besides, in current attention mechanism, the context vectors are built upon the RNN hidden state vectors, which act as representations of prefix substrings of the input sentences, given the sequential nature of RNN computation <ref type="bibr" target="#b9">(Wu et al. 2018</ref>). Thus, even if there are same characters in the predicted sentence, their corresponding context vectors are different because of their different positions.</p><p>Based on these findings, a Longest Common Subsequence (LCS) 1 based filtering method is proposed to refine the distilled features. LCS is used to compare two sequences. Common subsequences with same order in the two sequences are found, and the longest sequence is selected. The most important aspects of LCS are that the common subsequence is not necessary to be contiguous, and it retains the relative position information between characters. Formally speaking, LCS computes the common subsequence betweenỹ = [ỹ 1 , ...,ỹ L ] and y = [y 1 , ..., y K ], and obtains the subscripts of the corresponding characters inỹ and y:</p><formula xml:id="formula_12">I a 1 , ..., I a M , I v 1 , ..., I v M = LCS(ỹ 1 , ...,ỹ L , y 1 , ..., y K ), M ≤ min(L, K),<label>(10)</label></formula><p>where I a 1 , ..., I a M and I v 1 , ..., I v M are the subscripts in the sentence predicted by speech recognizer and the ground truth sentence, respectively. Please refer to the supplementary material for details. It's worth noting that when the sentence is Chinese, two characters are defined to be the same if they have the same Pinyin. Pinyin is the phonetic symbol of Chinese character, and homophones account for more than 85% among all Chinese characters.</p><p>Context-level knowledge distillation only calculate on these common characters:</p><formula xml:id="formula_13">L KD2 = 1 M M i=1 c a I a i − t(c v I v i ) 2 2 .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame-Level Knowledge Distillation</head><p>Furthermore, we hope that the speech recognizer can teach the lip reader more finely and explicitly. Specifically, knowledge is distilled at frame-level to enhance the discriminability of each video frame feature. If the correspondence between video and audio is known, then it is sufficient to directly match the video frame feature with the corresponding audio feature. However, due to the 1 https://en.wikipedia.org/wiki/Longest common subsequence problem different sampling rates, video sequence and audio sequence have inconsistent length. Besides, since blanks may appear at the beginning or end of the data, there is no guarantee that video and audio are strictly synchronized. Therefore, it is impossible to specify the correspondence artificially. This problem is solved by first learning the correspondence between video and audio, then performing the frame-level knowledge distillation.</p><p>As the hidden states of RNN providing higher-level semantics and are easier to correlated than the original input feature (Sterpu, Saam, and Harte 2018), the alignment between audio and video is learned on the hidden states of the audio encoder and video encoder. Formally speaking, for each audio hidden state h a i , the most similar video frame feature is calculated by a way similar to the attention mechanism:</p><formula xml:id="formula_14">h v i = J j=1 β ji h v j ,<label>(12)</label></formula><p>β ji is the normalized similarity between h a i and video encoder hidden states h v j :</p><formula xml:id="formula_15">β ji = exp((h v j ) T W h a i ) J k=1 exp((h v k ) T W h a i )</formula><p>.</p><p>Sinceh v i contains the most similar information to audio feature h a i and the acoustic speech signals embody information complementary to the visual ones, makingh v i and h a i the same enhances lip reader's capability to extract discriminant visual feature. Thus, the frame-level knowledge distillation is defined as:</p><formula xml:id="formula_17">L KD3 = 1 I I i=1 h a i −h v i 2 2 .<label>(14)</label></formula><p>The audio and video modalities can have two-way interactions. However, in the preliminary experiment, we found that video attending audio leads to inferior performance. So, only audio attending video is chosen to perform the framelevel knowledge distillation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Datasets CMLR 2 <ref type="bibr" target="#b10">(Zhao, Xu, and Song 2019)</ref>: it is currently the largest Chinese Mandarin lip reading dataset. It contains over 100,000 natural sentences from China Network Television website, including more than 3,000 Chinese characters and 20,000 phrases. LRS2 3 <ref type="bibr" target="#b0">(Afouras et al. 2018)</ref>: it contains more than 45,000 spoken sentences from BBC television. LRS2 is divided into development (train/val) and test sets according to the broadcast date. The dataset has a "pre-train" set that contains sentences annotated with the alignment boundaries of every word.</p><p>We follow the provided dataset partition in experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>For experiments on LRS2 dataset, we report the Character Error Rate (CER), Word Error Rate (WER) and BLEU <ref type="bibr" target="#b8">(Papineni et al. 2002)</ref>. The CER and WER are defined as ErrorRate = (S + D + I)/N , where S is the number of substitutions, D is the number of deletions, I is the number of insertions to get from the reference to the hypothesis and N is the number of characters (words) in the reference. BLEU is a modified form of n-gram precision to compare a candidate sentence to one or more reference sentences.</p><p>Here, the unigram BLEU is used. For experiments on CMLR dataset, only CER and BLEU are reported, since the Chinese sentence is presented as a continuous string of characters without demarcation of word boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Strategy</head><p>Same as (Chung et al. 2017), curriculum learning is employed to accelerate training and reduce over-fitting. Since the training sets of CMLR and LRS2 are not annotated with the word boundaries, the sentences are grouped into subsets according to the length. We start training on short sentences and then make the sequence length grow as the network trains. Scheduled sampling ) is used to eliminate the discrepancy between training and inference. The sampling rate from the previous output is selected from 0.7 to 1 for CMLR dataset, and from 0 to 0.25 for LRS2 dataset. For fair comparisons, decoding is performed with beam search of width 1 for CMLR and 4 for LRS2, in a similar way to <ref type="bibr" target="#b3">(Chan et al. 2016</ref>). However, preliminary experimental results show that the sequence-to-sequence based model is hard to achieve reasonable results on the LRS2 dataset. This is because even the shortest English sentence contains 14 characters, which is still difficult for the decoder to extract relevant information from all input steps at the beginning of the training. Therefore, a pre-training stage is added for LRS2 dataset as in <ref type="bibr" target="#b0">(Afouras et al. 2018)</ref>. When pre-training, the CNN pretrained on word excerpts from the MV-LRS (Chung and Zisserman 2017) dataset is used to extract visual features for the pre-train set. The lip reader is trained on these frozen visual features. Pre-training starts with a single word, then gradually increases to a maximum length of 16 words. After that, the model is trained end-to-end on the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details Lip Reader</head><p>CMLR: The input images are 64 × 128 in dimension. VGG-M model <ref type="bibr" target="#b4">(Chatfield et al. 2014</ref>) is used to extract visual features. Lip frames are transformed into gray-scale, and the VGG-M network takes every 5 lip frames as an input, moving 2 frames at each timestep. We use a two-layer bidirectional <ref type="bibr">GRU (Cho et al. 2014</ref>) with a cell size of 256 for the encoder and a two-layer uni-directional GRU with a cell size of 512 for the decoder. For character vocabulary, characters that appear more than 20 times are kept.</p><p>[sos], <ref type="bibr">[eos] and [pad]</ref> are also included. The final vocabulary size is 1,779. The initial learning rate was 0.0003 and decreased by 50% every time the training error did not improve for 4 epochs. Warm-up <ref type="bibr" target="#b6">(He et al. 2016</ref>) is used to prevent over-fitting. . The initial learning rate was 0.0008 for pre-training, 0.0001 for training, and decreased by 50% every time the training error did not improve for 3 epochs.</p><p>The balance weights used in both datasets are shown in <ref type="table" target="#tab_1">Table 1</ref>. The values are obtained by conducting a grid search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Speech Recognizer</head><p>The datasets used to train speech recognizers are the audio of the CMLR and LRS2 datasets, plus additional speech data: aishell (Bu et al. 2017) for CMLR, and LibriSpeech (Panayotov et al. 2015) for LRS2. The 240-dimensional fbank feature is used as the speech feature, sampled at 16kHz and calculated over 25ms windows with a step size 10ms. For LRS2 dataset, the speech recognizer and lip reader have the same architecture. For CMLR dataset, specifically, three different speech recognizer architectures are considered to verify the generalization of LIBS. Teacher 1: It contains 2 layers of bi-directional GRU for encoder with a cell size of 256, 2 layers of uni-directional GRU for decoder with a cell size 512. In other words, it has the same architecture as lip reader. Teacher 2: The cell size of both encoder and decoder is 512. Others remain the same as Teacher 1. Teacher 3: The encoder contains 3 layers of pyramid bidirectional GRU <ref type="bibr" target="#b3">(Chan et al. 2016)</ref>. Others remain the same as Teacher 1. It's worth noting that Teacher 2 and the lip reader have different feature dimensions, and Teacher 3 reduces the audio time resolution by 8 times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>Effect of different teacher models. To evaluate the generalization of the proposed multi-granularity knowledge distillation method, we compare the effects of LIBS on the CMLR dataset under different teacher models. Since WAS <ref type="bibr" target="#b4">(Chung et al. 2017</ref>) and the baseline lip reader (trained without knowledge distillation) have the same sequence-tosequence architecture, WAS is trained using the same training strategy as LIBS, and is used interchangeably with baseline in the paper. As can be seen from <ref type="table" target="#tab_2">Table 2</ref>, LIBS substantially exceeds the baseline under different teacher model ar-  chitectures. It is worth noting that although the performance of Teacher 2 is better than that of Teacher 1, the corresponding student network is not. This is because the feature dimensions of Teacher 2 speech recognizer and lip reader are different. This implies that distill knowledge directly in the same dimensional feature space can achieve better results. In the following experiments, we analyze the lip reader learned from Teacher 1 on the CMLR dataset.</p><p>Effect of the multi-granularity knowledge distillation. <ref type="table" target="#tab_3">Table 3</ref> shows the effect of the multi-granularity knowledge distillation on CMLR and LRS2 datasets. Comparing WAS, WAS +L KD1 , WAS +L KD1 + L KD2 and LIBS, all metrics are increasing along with adding different granularity of knowledge distillation. The increasing results show that each granularity of knowledge distillation is able to contribute to the performance of LIBS. However, the smaller and smaller extent of the increase does not indicate that the sequence-level knowledge distillation has greater influence than the frame-level knowledge distillation. When only one granularity of knowledge distillation is added, WAS +L KD2 shows the best performance. This is due to the design that the context-level knowledge distillation is directly acting on the features used to predict characters. On the CMLR dataset, LIBS exceeds WAS by a margin of 7.66% in CER. However, the margin is not that large on the LRS2 dataset, only 2.75%. This may be caused by the differences in the training strategy. On LRS2 dataset, CNN  Effect of different amount of training data. Compared with lip video data, the speech data is easier to collect. We evaluate the effect of LIBS in the case of limited lip video data on CMLR dataset. As mentioned before, the sentences are grouped into subsets according to the length, and only the first subset is used to train the lip reader. The first subset is about 20% of the full training set, which contains 27,262 sentences, and the number of characters in each sentence does not exceed 11. It can be seen from the <ref type="table" target="#tab_4">Table 4</ref>, when the training data is limited, LIBS tends to yield an even greater performance gain: the improvement on CER increases from 7.66% to 9.63%, and from 5.86 to 7.96 on BLEU.</p><p>Comparison with state-of-the-art methods.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visualization</head><p>Attention visualization. The attention mechanism generates explicit alignment between the input video frames and the generated character outputs. Since the correspondence between the input video frames and the generated character outputs is monotonous in time, whether alignment has a diagonal trend is a reflection of the performance of the model <ref type="bibr" target="#b9">(Wang et al. 2017)</ref>. <ref type="figure" target="#fig_2">Figure 2</ref> visualizes the alignment of the video frames and the corresponding outputs with different granularities of knowledge distillation on the test set of LRS2 dataset. Comparing <ref type="figure" target="#fig_2">Figure 2</ref>(a) with <ref type="figure" target="#fig_2">Figure 2(b)</ref>, adding sequence-level knowledge distillation improves the quality of the end part of the generated sentence. This indicates that the lip reader enhances its understanding of the semantic information of the whole sentence. Adding context-level knowledge distillation <ref type="figure" target="#fig_2">(Figure 2(c)</ref>) allows the attention at each decoder step to be concentrated around the corresponding video frames, reducing the focus on unrelated frames. This also makes the predicted characters more accurate. Finally, the frame-level knowledge distillation <ref type="figure" target="#fig_2">(Figure 2(d)</ref>) further improves the discriminability of the video frame features, making the attention more focused. The quality and the comprehensibility of the generated sentence is increased along with adding different levels of knowledge distillation.</p><p>Saliency maps. Saliency visualization technique is employed to verify that LIBS enhances lip reader's ability to extract discriminant visual features, by showing areas in the video frames the model concentrated most when predicting. <ref type="figure" target="#fig_3">Figure 3</ref> shows saliency visualisations for the baseline model and LIBS respectively, based on <ref type="bibr" target="#b8">(Smilkov et al. 2017)</ref>. Both the baseline model and LIBS can correctly focus on the area around the mouth, but the salient regions for baseline model are more scattered compared with LIBS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we propose LIBS, an innovative and effective approach to training lip reading by learning from a pretrained speech recognizer. LIBS distills speech-recognizer knowledge of multiple granularities, from sequence-, context-, and frame-level, to guide the learning of the lip reader. Specifically, this is achieved by introducing a novel filtering strategy to refine the features from the speech recognizer, and by adopting a cross-modal alignment-based method for frame-level knowledge distillation to account for the sampling-rate inconsistencies between the two sequences. Experimental results demonstrate that the proposed LIBS yields a considerable improvement over the state of the art, especially when the training samples are limited. In our future work, we look forward to adopting the same framework to other modality pairs such as speech and sign language.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>The framework of LIBS. The student network deals with lip reading, and the teacher handles speech recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>LRS2:</head><label></label><figDesc>The input images are 112 × 112 pixels covering the region around the mouth. The CNN used to extract visual features is based on (Stafylakis and Tzimiropoulos 2017), with a filter width of 5 frames in 3D convolutions. The encoder contains 3 layers of bi-directional LSTM (Hochreiter and Schmidhuber 1997) with a cell size of 256, and the decoder contains 3 layers of uni-directional LSTM with a cell size of 512. The output size of lip reader is 29, containing 26 letters and tokens for [sos], [eos], [pad]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Alignment between the video frames and the predicted characters with different levels of the proposed multigranularity knowledge distillation. The vertical axis represents the video frames and the horizontal axis represents the predicted characters. The ground truth sentence is set up by the government.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Saliency maps for WAS and LIBS. The places where the lip reader has learned to attend are highlighted in red .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>The balance weights employed in CMLR and LRS2 datasets.</figDesc><table><row><cell>Dataset</cell><cell>λ1</cell><cell>λ2</cell><cell>λ3</cell></row><row><cell>CMLR</cell><cell>10</cell><cell>40</cell><cell>10</cell></row><row><cell>LRS2</cell><cell>2</cell><cell>10</cell><cell>10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The performance of LIBS when using different teacher models on the CMLR dataset.</figDesc><table><row><cell>Model</cell><cell>BLEU</cell><cell>CER</cell></row><row><cell>WAS</cell><cell>64.13</cell><cell>38.93%</cell></row><row><cell>Teacher 1</cell><cell>90.36</cell><cell>9.83%</cell></row><row><cell>LIBS</cell><cell>69.99</cell><cell>31.27%</cell></row><row><cell>Teacher 2</cell><cell>90.95</cell><cell>9.23%</cell></row><row><cell>LIBS</cell><cell>66.66</cell><cell>34.94%</cell></row><row><cell>Teacher 3</cell><cell>87.73</cell><cell>12.40%</cell></row><row><cell>LIBS</cell><cell>66.58</cell><cell>34.76%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Effect of the proposed multi-granularity knowledge distillation.</figDesc><table><row><cell>Methods</cell><cell>BLEU</cell><cell>CER</cell><cell>WER</cell></row><row><cell></cell><cell>CMLR</cell><cell></cell><cell></cell></row><row><cell>WAS</cell><cell>64.13</cell><cell>38.93%</cell><cell>-</cell></row><row><cell>WAS +LKD1</cell><cell>67.23</cell><cell>34.42%</cell><cell>-</cell></row><row><cell>WAS +LKD2</cell><cell>68.24</cell><cell>33.17%</cell><cell>-</cell></row><row><cell>WAS +LKD3</cell><cell>66.31</cell><cell>35.30%</cell><cell>-</cell></row><row><cell>WAS +LKD1 + LKD2</cell><cell>68.53</cell><cell>32.95%</cell><cell>-</cell></row><row><cell>LIBS</cell><cell>69.99</cell><cell>31.27%</cell><cell>-</cell></row><row><cell></cell><cell>LRS2</cell><cell></cell><cell></cell></row><row><cell>WAS</cell><cell>39.72</cell><cell>48.28%</cell><cell>68.19%</cell></row><row><cell>WAS +LKD1</cell><cell>41.00</cell><cell>46.04%</cell><cell>66.59%</cell></row><row><cell>WAS +LKD2</cell><cell>41.23</cell><cell>46.01%</cell><cell>66.31%</cell></row><row><cell>WAS +LKD3</cell><cell>41.18</cell><cell>46.91%</cell><cell>66.65%</cell></row><row><cell>WAS +LKD1 + LKD2</cell><cell>41.55</cell><cell>45.97%</cell><cell>65.93%</cell></row><row><cell>LIBS</cell><cell>41.91</cell><cell>45.53%</cell><cell>65.29%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The performance of LIBS when trained with different amount of training data on the CMLR dataset.</figDesc><table><row><cell>Percentage of Training Data</cell><cell>Metrics</cell><cell>WAS</cell><cell>LIBS</cell><cell>Improv</cell></row><row><cell>100%</cell><cell>CER BLEU</cell><cell cols="3">38.93% 31.27% 7.66% ↓ 64.13 69.99 5.86 ↑</cell></row><row><cell>20%</cell><cell>CER BLEU</cell><cell cols="3">60.13% 50.50% 9.63% ↓ 42.69 50.65 7.96 ↑</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Performance comparison with other existing frameworks on the CMLR and LRS2 datasets. initial value so that better video frame feature can be extracted during the training process. To verify this, we compare WAS and LIBS trained without the pre-training stage. The CER of WAS and LIBS are 67.64% and 62.91% respectively, with a larger margin of 4.73%. This confirms the hypothesis that LIBS can help to extract more effective visual features.</figDesc><table><row><cell>Methods</cell><cell>BLEU</cell><cell>CER</cell><cell>WER</cell></row><row><cell></cell><cell>CMLR</cell><cell></cell><cell></cell></row><row><cell>WAS</cell><cell>64.13</cell><cell>38.93%</cell><cell>-</cell></row><row><cell>CSSMCM</cell><cell>-</cell><cell>32.48%</cell><cell>-</cell></row><row><cell>LIBS</cell><cell>69.99</cell><cell>31.27%</cell><cell>-</cell></row><row><cell></cell><cell>LRS2</cell><cell></cell><cell></cell></row><row><cell>WAS</cell><cell>39.72</cell><cell>48.28%</cell><cell>68.19%</cell></row><row><cell>TM-seq2seq</cell><cell>-</cell><cell>-</cell><cell>49.8%</cell></row><row><cell>CTC/Attention</cell><cell>-</cell><cell>42.1%</cell><cell>63.5%</cell></row><row><cell>LIBS</cell><cell>41.91</cell><cell>45.53%</cell><cell>65.29%</cell></row><row><cell cols="4">is first pre-trained on the MV-LRS dataset. Pre-training gives</cell></row><row><cell>CNN a good</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>shows the experimental results compared with other frame-</cell></row><row><cell>works: WAS (Chung et al. 2017), CSSMCM (Zhao, Xu,</cell></row><row><cell>and Song 2019), TM-seq2seq (Afouras et al. 2018) and</cell></row><row><cell>CTC/attention (Petridis et al. 2018). TM-seq2seq achieves</cell></row><row><cell>the lowest WER on the LRS2 dataset due to its transformer</cell></row><row><cell>self-attention architecture (Vaswani et al. 2017). Since LIBS</cell></row><row><cell>is designed for the sequence-to-sequence architecture, per-</cell></row><row><cell>formance may be improved by replacing RNN with trans-</cell></row><row><cell>former self-attention block. Note that, despite the excellent</cell></row><row><cell>performance of CSSMCM, which is designed for Chinese</cell></row><row><cell>Mandarin lip reading, LIBS still exceeds it by a margin of</cell></row><row><cell>1.21% in CER.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.vipazoo.cn/CMLR.html 3 http://www.robots.ox.ac.uk/ ∼ vgg/data/lip reading/lrs2.html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep audiovisual speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Afouras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lipnet: Sentence-level lipreading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Assael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural machine translation by jointly learning to align and translate. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
	<note>Scheduled sampling for sequence prediction with recurrent neural networks. 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Listen, attend and spell: A neural network for large vocabulary conversational speech recognition</title>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoderdecoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Chatfield</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<idno>arXiv:1412.1602</idno>
	</analytic>
	<monogr>
		<title level="m">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In International Conference on Machine learning</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note type="report_type">First results. arXiv preprint</note>
	<note>Proceedings of the IEEE conference on computer vision and pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cross modal distillation for supervision transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoffman</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik ;</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinyals</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dean ; Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
	</analytic>
	<monogr>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="1997" />
			<biblScope unit="volume">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to steer by mimicking features from heterogeneous auxiliary networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<idno>arXiv:1706.03825</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Proceedings of the 2018 on International Conference on Multimodal Interaction</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Lipreading by neural networks: Visual preprocessing, learning, and sensory integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Vaswani</surname></persName>
		</author>
		<idno>Wolff et al. 1994</idno>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="4006" to="4010" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A cascade sequence-to-sequence model for chinese mandarin lip reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Song ; Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1908.04917</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
