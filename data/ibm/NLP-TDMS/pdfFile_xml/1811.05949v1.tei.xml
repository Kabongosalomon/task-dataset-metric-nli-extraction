<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Jointly Learning to Label Sentences and Tokens</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Rei</surname></persName>
							<email>marek.rei@cl.cam.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">The ALTA Institute Computer Laboratory</orgName>
								<orgName type="institution">University of Cambridge United Kingdom</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Søgaard</surname></persName>
							<email>soegaard@di.ku.dk</email>
							<affiliation key="aff1">
								<orgName type="department">CoAStaL DIKU Department of Computer Science</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Jointly Learning to Label Sentences and Tokens</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T17:00+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning to construct text representations in end-to-end systems can be difficult, as natural languages are highly compositional and task-specific annotated datasets are often limited in size. Methods for directly supervising language composition can allow us to guide the models based on existing knowledge, regularizing them towards more robust and interpretable representations. In this paper, we investigate how objectives at different granularities can be used to learn better language representations and we propose an architecture for jointly learning to label sentences and tokens. The predictions at each level are combined together using an attention mechanism, with token-level labels also acting as explicit supervision for composing sentence-level representations. Our experiments show that by learning to perform these tasks jointly on multiple levels, the model achieves substantial improvements for both sentence classification and sequence labeling.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Language composition is a core requirement for many Natural Language Processing tasks, as it allows systems to construct the meaning of a phrase or sentence by combining individual words. Many languages are highly compositional and the correct predictions often depend on long sequences of context, therefore it is crucial that the learned composition functions are dynamic and accurate. LSTMs <ref type="bibr" target="#b12">(Hochreiter and Schmidhuber, 1997)</ref> have become a common approach for constructing text representations, providing a method for iteratively combining word embeddings and learning practical composition functions <ref type="bibr" target="#b33">(Sutskever, Vinyals, and Le, 2014;</ref><ref type="bibr" target="#b13">Huang, Xu, and Yu, 2015;</ref><ref type="bibr" target="#b14">Kim et al., 2016;</ref><ref type="bibr" target="#b20">Melis, Dyer, and Blunsom, 2017)</ref>. Convolutional networks have also been explored as an alternative, operating over fixed window sizes and allowing for more computation to be parallelized <ref type="bibr" target="#b15">(Kim, 2014;</ref><ref type="bibr" target="#b5">Dauphin et al., 2017;</ref><ref type="bibr" target="#b8">Gehring et al., 2017)</ref>. Each of these can be further extended with an attention-based architecture, giving the model more flexibility and allowing it to dynamically decide which areas of the text should receive more focus for a given task <ref type="bibr" target="#b2">(Bahdanau, Cho, and Bengio, 2015;</ref><ref type="bibr" target="#b36">Yang et al., 2016)</ref>.</p><p>Most contemporary neural architectures are trained endto-end, expecting the models to discover the necessary func-Accepted for publication at the 33rd AAAI Conference on Artificial Intelligence <ref type="bibr">(AAAI 2019)</ref> tions for language composition and attention. In practice, the variability in natural language is very large and the available annotated datasets are often too small to learn everything automatically. Furthermore, the patterns discovered in the data might not always correspond to the behaviour that we expect or desire our models to exhibit. Therefore, we would benefit from having methods for directly supervising the composition functions based on previous knowledge and available annotation, while still taking advantage of the end-to-end supervision signal of the main task. Recent work has shown that alignment annotation can be used to guide the attention function in neural machine translation models <ref type="bibr" target="#b18">(Liu et al., 2016)</ref>. However, similar approaches could also be extended beyond alignment, making them applicable to learning better composition functions for a wide variety of tasks.</p><p>In this paper, we investigate how supervised objectives at different granularities can be combined in order to learn better overall language representations and composition functions. The proposed model uses a self-attention mechanism for constructing sentence-level representations for text classification, learning to dynamically control the focus at each word position. We modify the attention component to also behave as a token labeling system, allowing us to directly supervise the attention values based on existing token-level annotation. Auxiliary objectives using language modeling are also investigated, which regularize the composition functions towards the language style of the training domain.</p><p>The joint labeling objective encourages the model to apply more attention to the same areas as the human annotators, making the system more robust to noise in the training data and the model behaviour more intelligible for human users. The token-level predictions directly reflect the internal decisions of the composition model, making them wellsuited for interpreting the model and analysing the output. The token labeling component itself also benefits from the sentence-level objective, as this performs task-specific regularization and compensates for noisy or missing labels. The approach is evaluated on three different language analysis tasks where labeling can be performed either on sentences or tokens: grammatical error detection, uncertainty detection and sentiment detection. The joint labeling architecture is able to return predictions for both full sentences and individual tokens, and the combined model is shown to improve performance on both levels of granularity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Architecture</head><p>The text classification model takes as input a sequence of tokens, such as a sentence, and first predicts a relevance score for each token. These scores are used as weights in a selfattention mechanism that constructs a combined representation and predicts an overall label for the input text. The attention module is optimised as a sequence labeling system, explicitly teaching it how to recognize the most important areas in the text. This ties together the label predictions on different levels, encouraging the objectives to work together and improve performance on both tasks. The architecture is based on the zero-shot sequence labeling framework by <ref type="bibr" target="#b26">Rei and Søgaard (2018)</ref> which we extend with additional objectives and joint supervision on multiple levels. We will first describe the core architecture of the model and then provide details on different objective functions for optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Composition Model</head><p>The tokens given as input to the system are first broken down into individual characters, and vector representations are constructed for each word by composing character embeddings with a bi-directional LSTM. The last hidden vectors from each of the LSTMs are then concatenated and passed through a layer with tanh activation. Next, we combine the character-based representation m i with a pre-trained word embedding w i by concatenating them together, following <ref type="bibr" target="#b16">Lample et al. (2016)</ref>. This allows the model to learn character-level features while also taking advantage of highquality embeddings trained on large unannotated corpora. Vector x i will now represent the i-th word in downstream modules of sentence composition and analysis. During training, all of these components are updated end-to-end, including word embeddings, character embeddings and the character-level LSTMs.</p><p>The word representations x i are given as input to a bidirectional LSTM which operates over the words in the sentence, moving in both directions:</p><formula xml:id="formula_0">− → h i =LST M (x i , − − → h i−1 ) (1) ← − h i =LST M (x i , ← − − h i+1 )<label>(2)</label></formula><p>The hidden states from each direction are concatenated at every token position and passed through a linear layer, resulting in vector h i that represents the word at position i, but is also conditioned on the whole surrounding sentence:</p><formula xml:id="formula_1">h i =[ − → h i ; ← − h i ]<label>(3)</label></formula><formula xml:id="formula_2">h i =tanh(W h h i + b h )<label>(4)</label></formula><p>Labels are then predicted for each word by passing h i through a non-linear layer and then to a single output neuron with sigmoid activation:</p><formula xml:id="formula_3">e i =tanh(W e h i + b e ) (5) a i =σ(W a e i + b a )<label>(6)</label></formula><p>The value ofâ i is between 0 and 1, representing the confidence of the i-th token having a positive label. In order to</p><formula xml:id="formula_4">h i x i h i h i red w i m i r e d e i a i c i,1 c i,2 c i,3 d i,1 d i,2 d i,3 d i,3 d i,2 d i,1</formula><p>Figure 1: The model architecture for one specific token position, taking the word 'red' as input and returning a tokenlevel predictionâ i together with a weighted vector representation.</p><p>predict a label for the overall sentence, we construct a sentence representation using a self-attention mechanism, similar to <ref type="bibr" target="#b36">Yang et al. (2016)</ref>, with the token-level predictions functioning as attention weights:</p><formula xml:id="formula_5">a i =â i N k=1â k (7)</formula><p>where a i is the attention weight, normalized to sum up to 1 over all values in the sentence. These weights are then used for combining the context-conditioned hidden representations from Equation 4 into a single sentence representation s:</p><formula xml:id="formula_6">s = N i=1 a i h i<label>(8)</label></formula><p>Finally, we return a sentence-level prediction based on this representation:</p><formula xml:id="formula_7">y = σ(W y tanh(W y s + b y ) + b y )<label>(9)</label></formula><p>The architecture takes advantage of soft-attention <ref type="bibr" target="#b29">(Shen and Lee, 2016)</ref>, where the weights are calculated using the logistic function instead of an exponential function. This provides a smoother distribution over the sentence and allows the same value to also represent the token-level prediction. The aim of an attention mechanism is to allow the model to choose which areas of the sentence it should focus on. By using token-level predictions as attention weights, we are explicitly tying together the predictions on both levels. For example, in order to decide whether the sentence has positive sentiment, the model will assign higher attention weights to tokens that it has identified as being positive.</p><p>The baseline model is only optimized as a sentence-level classifier, minimizing the mean squared error between the predicted sentence-level scoreŷ (t) and the gold-standard sentence label y (t) :</p><formula xml:id="formula_8">L sent = t (ŷ (t) − y (t) ) 2 (10)</formula><p>The experiments in this paper focus on binary classification, but the model could also be extended for multi-label classification tasks in the future. In the next sections, we will explore different auxiliary objective functions that improve the optimization of this architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Attention</head><p>The model uses shared values for token-level predictions and attention weights. When optimizing for sentence classification, the network will learn to focus on informative areas of the sentence by itself. However, for some tasks we can take advantage of available token-level annotation and present this as an additional training signal. By providing token-level supervision to the sentence classification model, we are able to teach it to focus on the most relevant areas and thereby improve the quality of sentence representations along with overall performance.</p><p>To achieve this, we add an extra objective function for directly optimizing the predicted word scoreâ i :</p><formula xml:id="formula_9">L tok = t i (â (t) i − a (t) i ) 2 (11) whereâ (t)</formula><p>i is the predicted score for word i in sentence t, and a (t) i is the binary gold-standard label for the same word. This objective is commonly used in a sequence labeling architecture and it optimizes the model to assign correct labels to every token. However, since the token-level predictions are directly tied to attention weights in our composition function, this objective also teaches the model to focus on the most relevant areas in the text when composing sentence representations. <ref type="bibr" target="#b28">Rei (2017)</ref> proposed an auxiliary objective for sequence labeling, where the architecture is also trained as a language model. This method regularizes the network, while also providing a richer training signal for the language composition functions, specializing them for the given domain and writing style. The optimization is performed concurrently with the other tasks, using the same training set, which means we are able to improve performance without any additional data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language Modeling Objective</head><p>This objective has not been previously investigated beyond token labeling, therefore we integrate the language modeling objective into our network and investigate whether it also improves performance when composing sentencelevel representations. Each of the hidden LSTM states is passed through a specialized layer and then mapped to a probability distribution over the vocabulary of words using the softmax function:</p><formula xml:id="formula_10">− → q i =tanh( − → W q − → h i + − → b q ) (12) ← − q i =tanh( ← − W q ← − h i + ← − b q ) (13) P (w i+1 | − → q i ) =sof tmax( − → W q − → q i + − → b q ) (14) P (w i−1 | ← − q i ) =sof tmax( ← − W q ← − q i + ← − b q )<label>(15)</label></formula><p>where − → q i and ← − q i are hidden layers that allow the representation to specialize for the language modeling task. The model is then optimized to predict the next word in the sequence based on the forward-moving LSTM, and the previous word in the sequence based on the backward-moving LSTM, while the combination of both is used for assigning labels to each token:</p><formula xml:id="formula_11">L LM = − N −1 i=1 log(P (w i+1 | − → q i )) − N i=2 log(P (w i−1 | ← − q i ))<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Character-level LM Objective</head><p>We further extend the idea of using an auxiliary language modeling objective, and apply it to the character-based representations. Learning character-level composition functions and embeddings can be difficult, and learning to predict surrounding words provides the model with additional morphological information. However, the character-LSTMs compose only single words, and using them to predict the next or previous word would only be equivalent to a unigram language model. In order to provide some additional context to the model, we instead optimize the network to predict words in the sentence based on the character-level hidden states of the surrounding words from both sides.</p><formula xml:id="formula_12">g = [ − → d i−1,R ; ← − d i−1,1 ; − → d i+1,R ; ← − d i+1,1 ]<label>(17)</label></formula><formula xml:id="formula_13">g i = tanh( ← − W g g + b g ) (18) P (w i |g i ) = sof tmax( − → W g g i + b g )<label>(19)</label></formula><p>where − → d i,j is the left-to-right character-based representation for character j in word i, g contains the concatenation of the four hidden states from the bi-directional character-level LSTMs from the previous and next word in the sequence. The model is then optimized by minimizing the negative loglikelihood of predicting the middle word in the sequence:</p><formula xml:id="formula_14">L char = − N −1 i=2 log(P (w i |g i ))<label>(20)</label></formula><p>Both the word-level and character-level LM objectives introduce additional parameters to the model, in order to predict the probability distribution over surrounding words. However, these parameters are only required during training and can be omitted during testing, resulting in the same model architecture as the baseline, with the same number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attention Range Objective</head><p>Finally, we also consider a secondary method for joining the sentence-level objective with token labeling. For this, we make the assumptions that 1) only some, but not all, tokens in the sentence can have a positive label, and 2) that there are positive tokens in a sentence only if the overall sentence is positive. We can then construct a loss function that encourages the model to optimize for these constraints:</p><formula xml:id="formula_15">L attn = t (min(â (t) i ) − 0) 2 + t (max(â (t) i ) − y (t) ) 2 (21) where min(â (t) i )</formula><p>is the minimum value of all the attention weights in sentence t and max(â (t) i ) is the corresponding maximum value. The first part of the equation pushes the minimum unnormalized attention weights in a sentence towards 0, satisfying the constraint that all tokens in a sentence should not have a positive token-level label. The second component then optimizes for the maximum unnormalized attention weight in a sentence to be equal to the gold label for that sentence, which is either 0 or 1, incentivizing the network to only assign large attention weights to tokens in positive sentences. While these assumptions are not always true, they provide a method of connecting the sentence-and token-level optimization even when the token-level annotation is noisy or missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>We evaluate the joint labeling framework on three different tasks and datasets. The CoNLL 2010 shared task <ref type="bibr" target="#b6">(Farkas et al., 2010)</ref> dataset investigates the detection of uncertain language, also known as hedging. Speculative language is a common tool in scientific writing, allowing scientists to guide research beyond the evidence, and roughly 19.44% of sentences in biomedical papers contain hedge cues <ref type="bibr" target="#b35">(Vincze et al., 2008)</ref>. Automatic detection of these cues is important for downstream tasks such as information extraction and literature curation, as typically only definite information should be extracted and curated.</p><p>The shared task consisted of two separate subtasks: 1) detection of uncertainty in text by predicting a binary label for a sentence, and 2) detection of the location of individual cue tokens and their semantic scope. The dataset is annotated for both hedge cues (keywords indicating uncertainty) and scopes (the area of the sentence where the uncertainty applies). In our experiments, the models aim to detect hedge cues on the token level and the presence of speculative language on the sentence level. The joint labeling framework encourages the model to classify sentences based on detected hedge cues, while also using the sentence-level objective to improve the cue detection itself.</p><p>We also evaluate the model error detection, where the goal is to identify tokens which need to be edited in order to produce a grammatically correct sentence. This task is an important component in automated systems for language teaching and assessment, with recent work focusing on error detection as a supervised sequence labeling task <ref type="bibr" target="#b27">(Rei and Yannakoudakis, 2016;</ref><ref type="bibr">Kaneko, Sakaizawa, and Komachi, 2017;</ref><ref type="bibr" target="#b28">Rei, 2017)</ref>. In turn, classifying sentences as correct or ungrammatical is necessary for developing tutoring systems for language learners <ref type="bibr" target="#b1">(Andersen et al., 2013;</ref><ref type="bibr" target="#b4">Daudaravicius et al., 2016)</ref> and detecting errors in machine-generated text.</p><p>For error detection on both levels, we use the First Certificate in English (FCE, <ref type="bibr" target="#b37">Yannakoudakis, Briscoe, and Medlock (2011)</ref>) dataset, containing error-annotated short essays written by language learners. <ref type="bibr" target="#b27">Rei and Yannakoudakis (2016)</ref> converted the original error correction annotation to a sequence labeling dataset, which we use in our experiments. The model is optimized to detect incorrect sentences, while also identifying the location of the error tokens, with the joint model combining both objectives together.</p><p>Finally, we convert the Stanford Sentiment Treebank (SST, <ref type="bibr" target="#b30">Socher, Perelygin, and Wu (2013)</ref>) to a sequence labeling format, in order to evaluate on the task of sentiment detection. Sentiment analysis is generally a three-way classification task, whereas the attention framework is designed for binary classification. Therefore, we construct two separate binary tasks from this dataset -the detection of positive and negative sentiment.</p><p>In order to assign sentiment labels to individual tokens, we traverse the treebank annotation bottom-up. The sentiment label is assigned to the minimum span of tokens containing that sentiment, but can be overwritten by subsuming phrases of up to length 3 with the opposing sentiment. For example, in the phrase 'This movie is good', only the token 'good' will be labeled as positive, whereas in the phrase 'This movie is not good' the tokens 'not good' are labeled as negative. The model is optimized to detect the presence of positive/negative sentiment on the sentence level, while also labeling the individual tokens with the corresponding sentiment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details</head><p>We combine all the different objective functions together using weighting parameters. This allows us to control the importance of each objective and prevent the model from overspecializing to auxiliary goals. The final objective that we minimize during training is then:</p><formula xml:id="formula_16">L =Λ sent · L sent + Λ tok · L tok + Λ LM · L LM + Λ char · L char + Λ attn · L attn<label>(22)</label></formula><p>For the baseline system we set Λ sent = 1 and all the other weights to 0, optimizing only for the sentence-level label prediction. When using the full system, we use Λ sent = Λ tok = 1, Λ LM = Λ char = 0.1 and Λ attn = 0.01. The sentence-and token-labeling objectives are part of the main task and therefore set to 1; the language modeling objective weights were chosen following <ref type="bibr" target="#b28">Rei (2017)</ref>, and Λ attn was chosen based on experiments on the development set.</p><p>Tokens were lowercased, while the character-level component receives input with the original capitalization. Word embeddings were set to size 300, pre-loaded from publicly available Glove embeddings <ref type="bibr" target="#b24">(Pennington, Socher, and Manning, 2014</ref>) and fine-tuned during training. The word-level LSTMs are size 300 and character-level LSTMs size 100;  the hidden combined representation h i was set to size 200; the attention weight layer e i was set to size 100. The model was optimized using AdaDelta <ref type="bibr" target="#b39">(Zeiler, 2012)</ref> with learning rate 1.0. The network weights were randomly initialized using the uniform Glorot initialization method <ref type="bibr" target="#b9">(Glorot and Bengio, 2010)</ref>. Dropout <ref type="bibr" target="#b31">(Srivastava et al., 2014)</ref> with probability 0.5 was applied to word representations w i and the composed representations h i after the LSTMs. Training was stopped if performance on the development set had not improved for 7 epochs and the best model was used for evaluation. The code for running these experiments will be made publicly available. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence Classification Experiments</head><p>We first evaluate the architectures on different sentence classification tasks. <ref type="table" target="#tab_0">Table 1</ref> contains results for detecting speculative language and grammatical errors on the sentence level. <ref type="table" target="#tab_1">Table 2</ref> presents the results for the two subtasks of sentiment classification. <ref type="table">Table 3</ref> contains fine-grained results, evaluating each of the proposed objectives in isolation. This acts as an ablation test, allowing us to determine whether the individual modifications benefit overall performance.</p><p>BiLSTM-LAST is our baseline architecture, commonly used for similar text composition tasks <ref type="bibr" target="#b34">(Tang, Qin, and Liu, 2015;</ref><ref type="bibr" target="#b22">Neelakantan, Le, and Sutskever, 2016)</ref>. It processes the input with a bi-directional LSTM, concatenates the hidden states from both directions and uses these to predict the sentence-level label. BiLSTM-ATTN is the architecture using attention, based on <ref type="bibr" target="#b36">Yang et al. (2016)</ref>, which is optimized only using the sentence-level objective. BiLSTM-JOINT is the full multi-level model, composing the sentences with self-attention and incorporating all of the proposed objectives during the training process.</p><p>Analyzing the results, we find that the attention-based architecture itself already gives consistent improvements, with BiLSTM-ATTN outperforming BiLSTM-LAST in all settings.  <ref type="table">Table 3</ref>: Comparing sentence classification performance when each of the auxiliary objective functions is added to BiLSTM-ATTN in isolation.</p><p>The self-attention framework allows the model to dynamically choose the areas of the sentence where to focus, delivering better results on all datasets. The proposed auxiliary objectives also provide consistent improvements in F 1 score when combined with the BiLSTM-ATTN model for sentence classification. Integrating the token-labeling objective with the attention gives the biggest improvements overall, showing that explicitly supervising the attention function allows the model to learn better sentence representations. With over 25% error reduction in F 1 , the token-level objective is particularly beneficial for the sentiment analysis datasets, as it teaches the model to detect key phrases in the text. The findings also indicate that the language modeling objective is indeed beneficial in the text classification setting. By training the BiLSTM components as separate language models, we are providing the model with a richer optimization signal and a natural method of regularization which automatically matches the given corpus style. Applying a language modeling objective on the characters also delivers a separate improvement in performance. Character-based neural models need sufficient amounts of data to train, whereas human-annotated training resources can be very limited in size. By including this auxiliary objective, we are able to optimize the model to learn informative character-level features without requiring additional training data. The improvements of the attention range objective varied depending on the dataset, but we found it to give small yet consistent improvements when added on top of other objectives in the combination system. Incentivizing the model to make token-level predictions in a suitable range is also beneficial in settings when only sentence-level annotation is available.</p><p>Finally, combining all the objectives together gave the best and most consistent overall performance across the board. The BiLSTM-JOINT model achieves the highest F 1 score on three out of four datasets. The exception is negative sentiment detection, only because the token-level objective proved to be particularly important on that dataset. By teaching the model to focus in the right areas in the text and predict unseen words in the context, we are able to obtain a more robust and accurate sentence classification system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Token Labeling Experiments</head><p>In this section, we investigate how the BiLSTM-JOINT architecture performs as a token-level sequence labeler. We use the same model and the same training objectives, except for a small difference in the early stopping criterion: in previous experiments, we stopped training based on the sentencelevel performance on the development data, whereas now we use the corresponding token-level measure. In general, we found that the model requires more epochs before reaching its optimal performance on the sequence labeling tasks, likely due to the datasets having relatively fewer unique instances for sentence-level classification.</p><p>While regular sequence labeling models are trained using token-annotated examples, our model is able to also take advantage of sentence-annotated data. Collecting human annotation on the sentence level can be considerably easier and cheaper for many tasks, compared to labeling individual tokens. Experiments in Figures 2 and 3 measure sequence labeling performance as the percentage of available tokenannotated sentences is varied, with the remaining examples only having sentence-level labels. In the absence of tokenlevel annotation, our model will still continue to optimize the sequence labeling components -the sentence classification objective encourages the model to focus on relevant tokens in the sentence, and the attention range objective adjusts the output values into the correct scale. Remarkably, this training signal is strong enough to achieve reasonable sequence labeling performance even without any token-annotated data (0% on the scale), learning to label individual words based only on the sentence-level annotation. Using only 20% of the token-level annotation, the joint model achieves performance comparable to a regular sequence tagging model trained on the full dataset. The system also benefits from the auxiliary objectives when all the token-level annotation is available, performing 1.3% better on speculative language detection and a substantial 7% better on error detection. <ref type="table" target="#tab_4">Table 4</ref> investigates error detection performance in more detail, using the BiLSTM-JOINT model trained on FCE and evaluating it on external error detection datasets. The CoNLL 2014 shared task dataset <ref type="bibr" target="#b23">(Ng et al., 2014)</ref> contains 1,312 sentences, written by higher-proficiency learn- ers on more technical topics. They have been manually corrected by two separate annotators, and we report results on each of these annotations. JFLEG <ref type="bibr" target="#b21">(Napoles, Sakaguchi, and Tetreault, 2017)</ref> contains a broad range of language proficiency levels and focuses more on fluency edits, making the text more native-sounding, in addition to grammatical corrections. We use F 0.5 as the main evaluation measure for error detection -high precision is more important for practical error detection applications, therefore F 0.5 was established as the main measure by the CoNLL 2014 shared task. We compare our system to the sequence labeling model by <ref type="bibr" target="#b28">Rei (2017)</ref>, which currently has the best reported error detection results on FCE and CoNLL14 when using the dedicated training set. <ref type="bibr">2</ref> The results show substantial improvements and BiLSTM-JOINT achieves new state-of-the-art results without using any additional training data. Optimizing for sentence composition and language modeling, along with the regular token labeling, provides a more robust system and considerably higher F 0.5 scores on all the benchmarks. The main impact comes from large improvements in precision, with the additional objectives encouraging the   <ref type="formula" target="#formula_0">(2017)</ref>, BiLSTM-ATTN supervised only for sequence labeling, and BiLSTM-JOINT optimized with all the auxiliary objectives.</p><p>model to learn more informative features, which is ideal for the task of error detection, where high-precision predictions are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>In recent years, researchers have explored hierarchical neural models for tasks such as part-of-speech tagging <ref type="bibr" target="#b25">(Plank, Goldberg, and Søgaard, 2016)</ref>, modeling text coherence <ref type="bibr" target="#b17">(Li, Luong, and Jurafsky, 2015)</ref>, and character-based language modeling <ref type="bibr" target="#b11">(Hermans and Schrauwen, 2013</ref>  <ref type="bibr" target="#b38">Zaidan and Eisner (2008)</ref> also describe a generative model for fine-and coarse-grained sentiment analysis. <ref type="bibr" target="#b10">Harel and Mannor (2011)</ref> present an algorithm for learning from what they call multiple outlooks that is also similar in spirit to our work. Their algorithm takes advantage of the multiple outlooks by matching moments of the empirical distributions to find an optimal mapping between them. However, they do not consider the outlooks at different hierarchical levels. Most recently, <ref type="bibr" target="#b3">Barrett et al. (2018)</ref> extended the model described in this paper and used human attention from gaze recordings to train the composition function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we investigated how supervised objectives at different granularities can be combined in order to learn better overall language representations and composition functions. The model learns to jointly label text at different granularities, allowing these objectives to benefit from each other. We proposed an attention-based model for sentence classification that also behaves as a token labeling system, allowing us to directly supervise the attention values based on existing token-level annotations. The joint labeling objective encourages the model to apply more attention to the same areas as the human annotators, making the system more robust to noise in the training data and the model behavior more intelligible for human users. In return, the sentence-level objective provides task-specific regularization for the token labeling component and compensates for noisy or missing labels.</p><p>We also experimented with auxiliary objectives that further assist the model in learning better composition functions that are shared between both tasks. Training the network to predict surrounding words regularizes the model, while also specializing the language composition functions towards the given domain and writing style. The language modeling objective can be further extended to characterbased representations, providing the character composition model with an additional informative training signal. Finally, an attention range constraint can be used to connect the labeling objectives on both levels and encourage the attention weights to be in a reasonable range.</p><p>The experiments were performed on three different tasks where labeling is needed both on sentences and tokens -uncertainty detection, sentiment detection and grammatical error detection. Evaluation of the joint labeling model showed consistent improvements at labeling both whole sentences and individual tokens, compared to optimizing for these tasks individually. For sequence labeling, the model was also able to use much less training data for comparable results, and even performed reasonably well without any token-level annotations. The joint labeling model with the auxiliary objectives achieved the best and most consistent results on all datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Sequence labeling F 1 on CoNLL-10 cue detection when varying the amount of training data that is tokenannotated. Sequence labeling F 0.5 on FCE error detection when varying the amount of token-annotated training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Sentence classification results on CoNLL 2010 and FCE datasets. BiLSTM-LAST uses the last hidden states; LSTM-ATTN uses the attention-based composition while only optimizing for sentence classification; BiLSTM-JOINT is the full multilevel model, receiving supervision on both sentences and tokens.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>CoNLL 2010</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>FCE</cell><cell></cell><cell></cell></row><row><cell></cell><cell>DEV F1</cell><cell>Acc</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>DEV F1</cell><cell>Acc</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>BiLSTM-LAST</cell><cell>90.17</cell><cell>94.95</cell><cell>85.66</cell><cell>81.87</cell><cell>83.67</cell><cell>84.69</cell><cell>77.75</cell><cell>78.55</cell><cell>92.55</cell><cell>84.95</cell></row><row><cell>BiLSTM-ATTN</cell><cell>89.88</cell><cell>94.98</cell><cell>85.34</cell><cell>82.68</cell><cell>83.87</cell><cell>84.96</cell><cell>78.12</cell><cell>78.75</cell><cell>92.87</cell><cell>85.21</cell></row><row><cell>BiLSTM-JOINT</cell><cell>91.30</cell><cell>95.97</cell><cell>87.63</cell><cell>86.76</cell><cell>87.17</cell><cell>86.14</cell><cell>80.08</cell><cell>82.27</cell><cell>90.14</cell><cell>86.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell>SST-neg</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>SST-pos</cell><cell></cell><cell></cell></row><row><cell></cell><cell>DEV F1</cell><cell>Acc</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>DEV F1</cell><cell>Acc</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>BiLSTM-LAST</cell><cell>88.27</cell><cell>84.19</cell><cell>87.59</cell><cell>90.37</cell><cell>88.95</cell><cell>93.82</cell><cell>88.76</cell><cell>90.72</cell><cell>96.85</cell><cell>93.67</cell></row><row><cell>BiLSTM-ATTN</cell><cell>88.99</cell><cell>85.32</cell><cell>88.18</cell><cell>91.43</cell><cell>89.77</cell><cell>93.95</cell><cell>89.33</cell><cell>91.47</cell><cell>96.59</cell><cell>93.96</cell></row><row><cell>BiLSTM-JOINT</cell><cell>91.62</cell><cell>88.42</cell><cell>92.45</cell><cell>90.98</cell><cell>91.71</cell><cell>96.37</cell><cell>93.30</cell><cell>96.22</cell><cell>95.97</cell><cell>96.09</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Sentence classification results on Stanford sentiment treebank, separated into negative and positive sentiment detection.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc><ref type="bibr" target="#b28">Rei (2017)</ref> 58.88 28.92 48.48 17.68 19.07 17.86 27.62 21.18 25.88 ---BiLSTM-ATTN 60.73 22.33 45.07 21.69 11.42 18.16 34.13 12.76 25.22 69.86 19.32 45.74 BiLSTM-JOINT 65.53 28.61 52.07 25.14 15.22 22.14 37.72 16.19 29.65 72.53 25.04 52.52</figDesc><table><row><cell></cell><cell>FCE</cell><cell></cell><cell></cell><cell cols="2">CoNLL14-TEST1</cell><cell></cell><cell cols="2">CoNLL14-TEST2</cell><cell></cell><cell>JFLEG</cell><cell></cell></row><row><cell>P</cell><cell>R</cell><cell>F0.5</cell><cell>P</cell><cell>R</cell><cell>F0.5</cell><cell>P</cell><cell>R</cell><cell>F0.5</cell><cell>P</cell><cell>R</cell><cell>F0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Sequence labeling results on error detection datasets. Comparing the best system from Rei</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Higher results have been reported, but only using various additional annotated datasets.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Many languages, one parser. TACL</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Developing and testing a self-assessment and tutoring system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ø</forename><forename type="middle">E</forename><surname>Andersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Barker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Parish</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural Machine Translation by Jointly Learning to Align and Translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Sequence classification with human attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bingel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hollenstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>CoNLL</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A Report on the Automatic Evaluation of Scientific Writing Shared Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Daudaravicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Banchs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Volodina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Napoles</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="11" to="53" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<title level="m">Language Modeling with Gated Convolutional Networks. ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The CoNLL-2010 shared task: learning to detect hedges and their scope in natural language text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vincze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Móra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Csirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>CoNLL</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploring the utility of joint morphological and syntactic learning from child-directed speech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Frank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarats</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<title level="m">Convolutional Sequence to Sequence Learning. ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>AIS-TATS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Learning from Multiple Outlooks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Training and Analyzing Deep Recurrent Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schrauwen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long Short-term Memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sakaizawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Komachi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
	</analytic>
	<monogr>
		<title level="m">Bidirectional LSTM-CRF Models for Sequence Tagging</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<title level="m">Character-Aware Neural Language Models. AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks for Sentence Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<title level="m">Neural Architectures for Named Entity Recognition. NAACL-HLT</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A Hierarchical Neural Autoencoder for Paragraphs and Documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Neural machine translation with supervised attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Utiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Finch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sumita</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>COL-ING</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Structured Models for Fine-to-Coarse Sentiment Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Neylon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Reynar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">On the State of the Art of Evaluation in Neural Language Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Melis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">JFLEG: A Fluency Corpus and Benchmark for Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Napoles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tetreault</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The CoNLL-2014 Shared Task on Grammatical Error Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hadiwinoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">H</forename><surname>Susanto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bryant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoNLL-ST</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GloVe: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multilingual part-of-speech tagging with bidirectional long short-term memory models and auxiliary loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Zero-shot sequence labeling: Transferring knowledge from sentences to tokens</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Søgaard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NAACL-HLT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Compositional Sequence Labeling Models for Error Detection in Learner Writing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannakoudakis</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Semi-supervised Multitask Learning for Sequence Labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Neural attention models for sequence classification: Analysis and application to key term extraction and dialogue act detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>INTER-SPEECH</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Dropout : A Simple Way to Prevent Neural Networks from Overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Hierarchical structured model for fine-to-coarse manifesto text analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NAACL-HLT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Sequence to sequence learning with neural networks. NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Document modeling with gated recurrent neural network for sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The BioScope corpus: biomedical texts annotated for uncertainty, negation and their scopes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vincze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Szarvas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Móra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Csirik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Suppl</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Hierarchical Attention Networks for Document Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>HLT</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A New Dataset and Method for Automatically Grading ESOL Texts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yannakoudakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Briscoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Medlock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Modeling Annotators: A Generative Approach to Learning from Annotator Rationales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Zaidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">ADADELTA: An Adaptive Learning Rate Method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.5701</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Rationaleaugmented convolutional neural networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Marshall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Wallace</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
