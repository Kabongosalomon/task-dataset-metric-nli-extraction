<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Patch Refinement -Localized 3D Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lehner</surname></persName>
							<email>lehner@ml.jku.at</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Machine Learning</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mitterecker</surname></persName>
							<email>mitterecker@ml.jku.at</email>
							<affiliation key="aff1">
								<orgName type="department">Institute for Machine Learning Johannes</orgName>
								<orgName type="institution">Kepler University Linz</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Adler</surname></persName>
							<email>adler@ml.jku.at</email>
							<affiliation key="aff2">
								<orgName type="department">Institute for Machine Learning Johannes</orgName>
								<orgName type="institution">Kepler University Linz</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hofmarcher</surname></persName>
							<email>hofmarcher@ml.jku.at</email>
							<affiliation key="aff3">
								<orgName type="department">Institute for Machine Learning Johannes</orgName>
								<orgName type="institution">Kepler University Linz</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
							<email>nessler@ml.jku.at</email>
							<affiliation key="aff4">
								<orgName type="department">Institute for Machine Learning Johannes</orgName>
								<orgName type="institution">Kepler University Linz</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
							<email>hochreit@ml.jku.at</email>
							<affiliation key="aff5">
								<orgName type="department">Institute for Machine Learning Johannes</orgName>
								<orgName type="institution">Kepler University Linz</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Patch Refinement -Localized 3D Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T09:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We introduce Patch Refinement a two-stage model for accurate 3D object detection and localization from point cloud data. Patch Refinement is composed of two independently trained Voxelnet-based networks, a Region Proposal Network (RPN) and a Local Refinement Network (LRN). We decompose the detection task into a preliminary Bird's Eye View (BEV) detection step and a local 3D detection step. Based on the proposed BEV locations by the RPN, we extract small point cloud subsets ("patches"), which are then processed by the LRN, which is less limited by memory constraints due to the small area of each patch. Therefore, we can apply encoding with a higher voxel resolution locally. The independence of the LRN enables the use of additional augmentation techniques and allows for an efficient, regression focused training as it uses only a small fraction of each scene. Evaluated on the KITTI 3D object detection benchmark, our submission from January 28, 2019, outperformed all previous entries on all three difficulties of the class car, using only 50 % of the available training data and only LiDAR information.</p><p>Despite the recent advancement of image-only 3D detectors <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b8">9]</ref> point cloud information from LiDAR sensors is a requirement to achieve highly accurate 3D localization. Unfortunatly, LiDAR data is represented as an unordered, sparse set of points in a continuous 3D space and can not be directly processed by standard convolutional neural networks (CNNs). Deep CNNs operating on multiple discretized 2D Bird's Eye View (BEV) maps achieved early success in both 3D <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref> and</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Object detection and localization is one of the key challenges in autonomous driving <ref type="bibr" target="#b3">[4]</ref> and robotics <ref type="bibr" target="#b16">[17]</ref>. Compared to the performance of 2D object detection in images the results in 3D object detection lag behind considerably, mainly caused by increased difficulty of the localization task. Instead of fitting axis-aligned rectangles around the part of objects that is visible in the image plane, the main challenge in 3D detection and localization is the amodal oriented 3D bounding box prediction in 3D space, which includes the occluded or truncated parts of objects.</p><p>BEV <ref type="bibr" target="#b18">[19]</ref> detection. Simply discretizing point clouds is inevitably linked to a loss of information. Current state-of-the-art 3D object detection is largely based on the seminal work PointNet <ref type="bibr" target="#b12">[13]</ref>. Pointnets are used in combination with 2D image detectors to perform 3D localization on point cloud subsets <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>. Two-stage approaches <ref type="bibr" target="#b13">[14]</ref> use PointNets in an initial segmentation stage and a subsequent localization stage. Voxelnet <ref type="bibr" target="#b19">[20]</ref> introduces Voxel Feature Encoding (VFE) layers which utilize PointNet to learn an embedding of local geometry within voxels, which can then be processed by 3D and 2D convolutional stages. Others <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b7">8]</ref> add further improvements to Voxelnet.</p><p>Motivation Voxelnet is a single-stage model, as such it applies VFE-encoding with a uniform resolution on the whole scene, although such a resolution is only necessary at locations that contain objects. As a consequence, it is severely limited by memory constraints, especially during training, where only a batch size of 2 can be processed by a GPU with 11 GB of memory. Given the typical sparseness of objects within a LiDAR scene, only a few subsets contain viable information to train regression <ref type="bibr" target="#b2">[3]</ref>. But in the case of Voxelnet Batch Normalization layers <ref type="bibr" target="#b5">[6]</ref> break this local independence.</p><p>Patch Refinement To be able to train a small detector focused on 3D bounding box regression, we decompose the task into a preliminary BEV detection step and a local 3D detection step, similar to the two-stage approach of R-CNN <ref type="bibr" target="#b4">[5]</ref>. Object sizes are bound and unaffected by the distance to the sensor. We construct a Local Refinement Network (LRN) that operates on small subsets of points within a fixed-sized cuboid, which we term "patches". The RPN does not have to perform warping and independence of the LRN can be achieved by training with some noise to account for proposed locations that are slightly offset. We favor an independent approach because it enables additional augmentation options, the higher resolution features have to be calculated in any case and it allows us to evaluate the regression ability of the LRN without the influence of an RPN. <ref type="figure">Figure 1</ref>: Left: A visualization of extracted subsets ("patches") -highlighted in green with red bounding boxes -which serve as input to our Local Refinement Network (LRN). A patch contains the points within a cuboid centered at the object. Right: patches constructed to train the LRN, the same object is placed on different surfaces and occurs in different orientations.</p><p>Our work comprises the following key contributions:</p><p>• We demonstrate that it is a viable approach to decompose the 3D object detection task in autonomous driving scenarios into a preliminary BEV detection followed by a local 3D detection via two independently trained networks.</p><p>• We show that it takes only a few simple modifications to utilize the Voxelnet <ref type="bibr" target="#b19">[20]</ref> architecture either as an efficient Region Proposal Network or as a Local Refinement Network that is capable to perform highly accurate 3D bounding box regression and is not limited to fixed input space.</p><p>• We report the beneficial effect of adding the corner bounding box parametrization of AVOD <ref type="bibr" target="#b6">[7]</ref> as auxiliary regression targets, even without applying the proposed decoding algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>MV3D <ref type="bibr" target="#b1">[2]</ref> and AVOD <ref type="bibr" target="#b6">[7]</ref> are two-stage models that fuse image and point cloud information and perform regression on the resulting 2D BEV feature maps, projections of point clouds and camera information. While this enables the use of 2D CNNs, these approaches cannot capture the full geometric complexity of the 3D input scene, due to information loss caused by discretization.</p><p>Frustum PointNets <ref type="bibr" target="#b11">[12]</ref> projects the proposals of an image-based 2D detector onto the point cloud.</p><p>The resulting frustum is then further processed by a sequence of PointNets. It demonstrates that accurate amodal bounding box prediction can be performed without context information, which is actively removed by a segmentation PointNet. Noticeably, detection scores are calculated without taking the LiDAR representation into account.</p><p>Voxelnet <ref type="bibr" target="#b19">[20]</ref> applies a 3D grid to divide the input space into voxels. Followed by a sparse voxel-wise input encoding via a sequence of PointNet-based VFE layers. This enables the network to learn structures within voxels and to embed the point cloud into a structured representation while retaining the most important geometric information in the data. Which are subsequently processed by 3D and 2D CNNs. Both our RPN and LRN are based on Voxelnet, modified to better accomplish their respective tasks.</p><p>SECOND <ref type="bibr" target="#b17">[18]</ref> modifies Voxelnet by replacing the costly dense 3D convolutions with efficient sparse convolutions, reducing both run-time and memory consumption considerably. Furthermore, they propose ground truth sampling, an augmentation technique that populates training scenes with additional objects from other scenes. Besides speeding-up training by increasing the average number of objects per frame, this augmentation provides strong protection against overfitting on context information <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>PointPillars <ref type="bibr" target="#b7">[8]</ref> proposes a VoxelNet-based model without 3D convolutional middle layers. Instead, they apply a voxel grid with a vertical resolution of one and the encoding of the vertical information is solely performed within the VFE-layers. Further optimized towards speed, the model achieves the highest frame rates within the pool of current 3D object detection models. Our RPN follows a similar design, but we use a vertical resolution of two and concatenation.</p><p>PointRCNN <ref type="bibr" target="#b13">[14]</ref> is a two-stage approach utilizing PointNets, that introduces a novel LiDAR-only bottom-up 3D proposal generation first stage, followed by a second stage to refine predictions. Similar to our model it follows the R-CNN approach and pools the relevant subset of the input point cloud for each proposal. Unlike our LRN, the second stage of Point R-CNN reuses higher-level features and relies on the RPN to transform the proposals into a canonical representation.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Local Refinement Network</head><p>We follow the Voxelnet approach and apply a 3D voxel grid to the input, grouping points to voxels. This is followed by a VFE stage and then by a reduction step from 3D to 2D BEV feature maps. These are then processed by our 2D convolutional backbone network.</p><p>Grouping Points into Voxels We use the efficient sparse encoding algorithm of Voxelnet that processes only non-empty voxels. Although we train only on small regions of the input scene, we preserve absolute (global) point coordinates. This way our model can learn that objects farther from the sensor are typically represented with fewer measurements than nearby objects.</p><p>Voxel Feature Encoding While the grouping algorithm of Voxelnet processes only non-empty voxels, the input to the VFE layers consists of dense tensors with a large proportion of padded zeros (roughly 90 % with the default setting of at most 35 points per voxel). As it has a regularizing effect on the running mean and variance this zero-padding alleviates the use of Batch Normalization (BN) <ref type="bibr" target="#b5">[6]</ref>. We relinquish the use of zero padding and remove the BN in our VFE-stage. Instead, we apply per-sample normalization. Overall, our modified VFE layer requires less memory while also increasing the predictive performance of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reduction Network</head><p>The activation tensor resulting from the VFE stage is still three dimensional. Via multiple 3D-convolutional layers, the vertical resolution of the activation tensor is reduced to one resulting in 2D feature maps. The result is a 2D representation in BEV with vertical information encoded locally. <ref type="figure">Figure 3</ref>: Architecture of the backbone network. The network is designed in a way that regression is performed on feature maps of higher resolution and limited receptive fields, while detection is performed on higher level feature maps with a larger receptive field. Before regression, we concatenate feature maps A and X and before detection we concatenate B and C.</p><p>Backbone Network <ref type="figure">Figure 3</ref> depicts the architecture of our backbone network. It uses multiple blocks of 2D convolutional layers to generate intermediate feature maps of different resolutions. We then apply transposed convolutional layers to get feature maps with equal dimensions before the output layers, where feature maps are concatenated and combined with 1 × 1 convolutions at each anchor position. The resulting feature maps are labeled A, B, C, and X in <ref type="figure">Figure 3</ref>. For the regression head, the receptive field of A has been chosen to cover the majority of objects while the receptive field of X is slightly larger to cover outliers. Restricting the receptive field of the regression head reduces distractions from the environment when predicting exact bounding boxes. In contrast, we perform the detection based on higher-level feature maps that have a larger receptive field.</p><p>Loss We use a variant of residual box parametrization with a direction classifier <ref type="bibr" target="#b17">[18]</ref> and sine and cosine encoding for orientation. From the vector g = (x, y, z, l, h, w, θ) representing the three box-center coordinates x, y, z, height h, width w, length l and yaw θ around the y-axis of an oriented 3D bounding box, we calculate residual regression targets u = (∆x, ∆y, ∆z b , ∆z t , ∆w, ∆h, ∆l, ∆η, ∆ζ) . In ∆z b , ∆z t , the subscripts b and t denote bottom and top respectively, between a ground truth vector g and an anchor vector a from the same vector space as described in Equation <ref type="formula" target="#formula_0">(1)</ref>:</p><formula xml:id="formula_0">∆x = x g − x a √ l 2 + w 2 ∆z b = z g − h g 2 − z a + h a 2 ∆y = y g − y a √ l 2 + w 2 ∆z t = z g + h g 2 − z a − h a 2 ∆l = log l g l a ∆w = log w g w a ∆h = log h g h a ∆ζ = |cos(θ g − θ a )| ∆η = sin (θ g − θ a ) δ = 1 if cos (θ g − θ a ) &gt; 0 0 otherwise ,<label>(1)</label></formula><p>where h g denotes the component h of the vector g (the notation for the other components follows respectively).</p><p>Additionally, we use the box parametrization of AVOD <ref type="bibr" target="#b6">[7]</ref> as auxiliary regression targets. We transform ground truth boxes and anchor boxes into a 2D corner representation in BEV (c, d) for the xand y-coordinates of each corner resulting in eight variables</p><formula xml:id="formula_1">f = (c 1 , c 2 , c 3 , c 4 , d 1 , d 2 , d 3 , d 4 ) ,</formula><p>where the subscripts enumerate corners. Then we calculate the targets v = f g − f a . While we do not use these additional regression parameters during inference, the added training task improves performance considerably. We use axis-aligned 2D Intersection over Union (IoU) in BEV as a similarity measure between the ground truth boxes rotated to the nearest axis and the corresponding anchor type. For detection, positive anchors have to surpass an IoU threshold of 0.6, while anchors with an IoU below 0.45 are treated as negatives. For regression, the positive threshold is set to 0.45. Our default choice for training the detection head is the balanced sampling of N total anchors with a ratio of 3 to 1 of negative and positive anchors. For detection and direction we use the binary cross-entropy loss function, denoted L cls and for the regression targets we use the smooth L1 loss (also known as Huber loss), denoted L reg . Balancing is achieved via the hyperparameters α, β, γ, with the default values of 1, 1 and 2 respectively.</p><p>Let i and j denote the sampled positive and negative detection anchors and let p denote the sigmoid activation of the classification output. Further, let k denote the positive regression anchors. N pos_reg is the total number of positive regression anchors. The overall loss function is given as in Equation <ref type="formula" target="#formula_2">(2)</ref>.</p><formula xml:id="formula_2">L = α 1 N total i L cls (p pos_cls i , 1) + β 1 N total j L cls (p neg_cls j , 0) + γ 1 N pos_reg k L reg (u k , u * k ) + L reg (v k , v * k ) + L cls (h k , h * k )<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Patch Construction</head><p>The independence of the LRN enables us to construct patches from ground truth annotations.</p><p>Surface Sampling Inspired by the beneficial effect of ground truth sampling in SECOND, we try to achieve protection against overfitting on context information in a similar way. First, we create lists of objects and related surrounding areas (surfaces) in the training set. We then remove points within the slightly enlarged bounding boxes of the objects present in the respective surface. We then rotate each surface to align its center with the depth axis in front of the sensor car. Finally, we sort both object and surface lists based on the absolute distance to the sensor. During training, we combine objects with surfaces that appear in a similar distance to the sensor. To ensure that the object lines up with the surface, we look up the vertical coordinate of the original object and place the augmented object at the corresponding height. We apply surface sampling based on the difficulty levels easy, moderate and hard with the respective probabilities of 100%, 80% and 60%.</p><p>Global &amp; Per-Object Augmentation We then proceed with standard augmentation techniques. Scaling and mirroring of the patch and the object individually. Contrary to <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b17">18]</ref> we do not apply per-object rotation and vertical translation. As per-object rotation introduces self-occlusion artifacts and vertical translation creates unreasonable patches where objects appear below ground or fly above it. Especially in the context of self-driving, the training data for car objects is heavily biased, with a strong peak for parallel orientation to the ego-vehicle. A major benefit of working on a per-object basis without a fixed input space is the ability to use an augmentation for a full range of rotations in the forward view around the global z-axis by [− π 2 , π 2 ]. Therefore, every object is learned to be recognized in every possible orientation inside the patch, respecting the global position in which the so rotated object would happen to appear. Consequently, both the perpendicular and the parallel anchor types are trained equally well.</p><p>Random Cropping -LRN Detection Objective At this stage, we have an augmented object placed somewhere upon a surface. To achieve robustness against imperfect proposals by an actual RPN and to create a training task for the detection head, we sample noise from a circular area ([−π, π] and [0, 3] meters). Finally, we crop the patch at the BEV location that is determined by the object center and the offset from the sampled noise. Therefore, the objective of the detection head of the LRN will be to revert this offset and determine the correct object center within the small anchor grid. A task that is closely related to regression and designed to achieve a correlation between detection score and the ability to accurately regress an object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Region Proposal Network</head><p>Our RPN is a slim version of the LRN network, the main source of simplification is the circumvention of the vertical reduction stage. Instead of multiple 3D convolutional layers, we use a vertical voxel resolution of two and concatenate the two resulting feature planes. While this change reduces the 3D detection results considerably, the BEV detection results remain nearly unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Inference</head><p>During inference, we take the proposals of an RPN to extract patches. To increase the similarity with our training task, we remove points within the slightly enlarged bounding box of additional proposals falling within the patch. Similar to training we rotate the patch upon the depth axis (improvement of +0.15 AP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Data Set We evaluate our method on the KITTI 3D object detection benchmark <ref type="bibr" target="#b3">[4]</ref>, which provides samples of LiDAR point clouds as well as RGB camera images. The data set consists of 7,481 training samples and 7,518 test samples. Detections are evaluated on three difficulty levels (easy, moderate, hard). Average precision (AP) is used as the metric, successful detections for the class car require an IoU of at least 70,%.</p><p>Experimental Setup We subdivide the original training samples into a 3,712 samples train set and a 3,769 samples validation set as described in <ref type="bibr" target="#b0">[1]</ref>, which we use for all our experiments and the submission to the test server. Furthermore, we use a non-maximum suppression threshold of 0.01. We train our model on a single 1080Ti GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Implementation Details</head><p>Region Proposal Network For our RPN we use the input space and BEV resolution of Voxelnet, a vertical resolution of 2, the described loss and a backbone configuration of ABC/AX. To compensate for the smaller receptive fields due to the circumvented 3D convolutions, we add 2 layers to the convolutional block 1. We first pre-train the RPN on patches with the local objective and only afterward train it for its final task as an RPN, simply by changing its input to whole point clouds and decreasing the learning rate by a factor of 0.1 and the batch size to 4. The RPN adapts to the new input within a few epochs only. Due to the increased imbalance between foreground and background objects, present when processing whole point clouds, we choose Focal Loss <ref type="bibr" target="#b10">[11]</ref> with default parameters α = 0.25 and γ = 2 to train the detection head.</p><p>Local Refinement Network For the width x and depth y dimensions, we chose voxel sizes of 0.15 meters and patch sizes of 9.6 meters. For the vertical dimension, we chose a voxel height of <ref type="bibr">4 19</ref> meters. In order to reduce the vertical dimension from 19 to 1, we use a sequence of 4 3D convolutional layers with a kernel-size of 3, vertical strides of (2,1,2,1), and no padding. To compensate for the smaller receptive fields due to the increased resolution we add 5 layers to the convolutional block 1 and 2 layers to convolutional block 2. The convolutional block related to the extended feature maps X consists of 6 3x3 convolutions without initial down-sampling. The voxel resolution is 64 × 64 × 19 in the encoding stage. The regression and detection heads operate on a 32 × 32 anchor grid. We train with a batch size of 32 samples for 5 million samples with Adam Optimizer, an initial learning rate of 10 −4 and after one million samples we multiply the learning rate by 0.8 every 500, 000 samples. <ref type="table">Table 1</ref> presents the results of our method on the KITTI test set using the Average Precision (AP) metric. Consuming only LiDAR data and 50 % of the training data, our submission outperformed all previous methods for 3D object detection on cars. Noticeably, on the easy difficulty, the effect of the local training objective of the detection head is most prominent. A comparison of the precision-recall curves provided by the KITTI benchmark showed that our model has a distinct advantage to better avoid high ranked false positive detections that do not pass the 70 % IoU threshold.   <ref type="table">Table 3</ref>: A comparison of different RPNs, refined by the LRN Augmentation We analyze the effectiveness of our augmentation strategies: surface sampling and additional global rotation. <ref type="table" target="#tab_1">Table 2</ref> shows two strong performance drops when either of those is not employed. For surface sampling, this drop is expected, since we fully rely on the construction of artificial patches to avoid the model to overfit on context information. The drop caused by reduced global rotation could be related to an increased orientation bias. The data set comprises largely objects parallel to the sensor car and few objects in the perpendicular orientation. As we rotate all objects upon the depth axis and sample rotation only from [ −π 4 , π 4 ], the number of perpendicular objects is further reduced. Overall, both augmentation strategies are vital components of the training procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation on the KITTI Test Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Auxiliary Regression Targets</head><p>In our experiments without auxiliary regression targets, we observe slower, more unstable learning. Additionally, with auxiliary regression targets, the model becomes more decisive in rejecting false positives and preserves the level of recall with fewer proposals. <ref type="table" target="#tab_1">Table 2</ref> shows a decrease in performance if auxiliary regression targets are omitted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone Modification</head><p>We validate our architecture design against variants (see <ref type="figure">Figure 3</ref>), in which we modified the connections of the regression head and detection head to the feature maps A, B, C, and X. In the standard variant, the detection head is connected to B and C, while the regression head is connected to A and X. We denote this backbone as BC/AX. As a first variant, the backbone ABC/AX uses an additional feature map for detection, which leads to earlier overfitting of the detection head. A comparison of the backbone variant C/AX and backbone variant B/AX using only one feature map for detection, suggests that the higher level map C is of greater importance. The backbone variant BC/A relinquishes the use of the additional regression map, which has a negative effect on the performance on easy and moderate difficulty levels. The backbone variant "BC/AX red." uses fewer layers in the convolutional blocks 1 and 2, which achieves almost identical performance. <ref type="table" target="#tab_1">Table 2</ref> shows the decrease in performance for the different backbone variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Patch Extraction</head><p>We study the influence of additional objects present in a patch. We calculate thresholds based on the percentile of the detection scores over the validation set. We then remove additional objects only if their respective detection score exceeds a given threshold. We observe that removing those objects where the RPN is most confident is of greater importance and that the detection of objects of difficulty hard is affected the most from additional objects within a patch. We conclude that the effect is caused by distraction effects of additional objects with distinct features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Additional Experiments</head><p>Refinement of Other Models We study how our LRN performs when the region proposals are provided by other detection models. To this end, we construct validation patches based on the predictions generated by two other models. Our experiments show only marginal differences between our RPN and two state-of-the-art detection models, namely SECOND v1.5 and PointRCNN (see <ref type="table">Table 3</ref>). The results underline that the regression capability of the RPN is of low importance. Additionally, we compare our RPN to proposals created from ground truth labels. The gap increases with difficulty and suggests further improvement can be achieved via an RPN that is more capable to distinguish objects described by a low amount of LiDAR points, e.g. a fusion-based RPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Adaptation</head><p>We further investigate the option of a two-phase training procedure for our lightweight RPN. As the patches occur at their original distance to the sensor, we train a detector that can operate on whole scenes. First, we pre-train by concentrating on the domain of patches only, then we switch to the domain of whole scenes. The pre-trained RPN surpasses the moderate 3D-scores of Voxelnet (65.46) after only one additional epoch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have proposed Patch Refinement, a two-stage model for 3D object detection from sparse LiDAR point clouds. We demonstrate that a modified Voxelnet is capable of highly accurate local bounding box regression and a simplified Voxelnet is an adequate choice for an RPN to complement such an LRN. As the LRN operates on local point cloud subsets only, it can refine the proposals of an arbitrary RPN on demand. Further improvements regarding accuracy may be attainable by using a ground plane estimation algorithm and the integration of image information in the RPN stage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2</head><label>2</label><figDesc>depicts the inference procedure, which follows the original R-CNN<ref type="bibr" target="#b4">[5]</ref> approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The region proposal network (RPN) identifies the locations of objects in Bird's Eye View (BEV). Based on these proposed locations subsets are extracted from the point cloud and processed by the local refinement network (LRN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies. 3D object detection scores, averaged over three LRN checkpoints on the validation set.VFE-layersWe trained our network with VFE-layers as proposed in Voxelnet, with zero-padding and BN. In this case, we observe a large performance drop overall difficulty levels when used in our model. We hypothesize that the reason is that typically the batch statistics of those features are highly variable due to a low number of points and a varying input space. Additionally, we applied BN without zero-padding. This performs poorly both on whole scenes, as well as on patches.</figDesc><table><row><cell>RPN</cell><cell cols="3">RPN -AP Score 3D</cell><cell cols="3">Refined -AP Score 3D</cell></row><row><cell></cell><cell>Easy</cell><cell>Moderate</cell><cell>Hard</cell><cell cols="3">Easy Moderate Hard</cell></row><row><cell>Ours</cell><cell>87.88</cell><cell>74.31</cell><cell cols="2">68.09 89.61</cell><cell>79.04</cell><cell>77.96</cell></row><row><cell>SECOND v1.5</cell><cell>89.15</cell><cell>78.80</cell><cell cols="2">77.47 89.44</cell><cell>78.97</cell><cell>78.10</cell></row><row><cell>PointRCNN</cell><cell>89.10</cell><cell>78.71</cell><cell cols="2">77.79 89.58</cell><cell>79.09</cell><cell>78.06</cell></row><row><cell>Ground Truth</cell><cell>100.00</cell><cell>100.00</cell><cell cols="2">100.00 89.58</cell><cell>79.31</cell><cell>78.79</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaustav</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-view 3d object detection network for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1907" to="1915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vote3deep: Fast object detection in 3d point clouds using efficient convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><forename type="middle">Zeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><forename type="middle">Hay</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1355" to="1361" />
		</imprint>
	</monogr>
	<note>Robotics and Automation (ICRA</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>ICML&apos;15</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melissa</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stereo r-cnn based 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaojie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7644" to="7652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-task multi-sensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoshuai</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Roarnet: A robust 3d object detection based on region approximation refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwook</forename><surname>Paul Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masayoshi</forename><surname>Tomizuka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2510" to="2515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Divyansh</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8445" to="8453" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Enhancing semantic segmentation for robotics: The power of 3-d entangled forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Prankl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="56" />
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxing</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pixor: Real-time 3d object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7652" to="7660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oncel</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
