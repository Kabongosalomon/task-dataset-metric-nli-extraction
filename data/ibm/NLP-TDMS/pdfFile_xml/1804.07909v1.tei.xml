<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Refine Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihai</forename><surname>Fieraru</surname></persName>
							<email>mfieraru@mpi-inf.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
							<email>khoreva@mpi-inf.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Pishchulin</surname></persName>
							<email>leonid@mpi-inf.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
							<email>schiele@mpi-inf.mpg.de</email>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Informatics Saarland Informatics Campus</orgName>
								<address>
									<settlement>Saarbrücken</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Refine Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Multi-person pose estimation in images and videos is an important yet challenging task with many applications. Despite the large improvements in human pose estimation enabled by the development of convolutional neural networks, there still exist a lot of difficult cases where even the state-of-the-art models fail to correctly localize all body joints. This motivates the need for an additional refinement step that addresses these challenging cases and can be easily applied on top of any existing method. In this work, we introduce a pose refinement network (PoseRefiner) which takes as input both the image and a given pose estimate and learns to directly predict a refined pose by jointly reasoning about the input-output space. In order for the network to learn to refine incorrect body joint predictions, we employ a novel data augmentation scheme for training, where we model "hard" human pose cases. We evaluate our approach on four popular large-scale pose estimation benchmarks such as MPII Single-and Multi-Person Pose Estimation, PoseTrack Pose Estimation, and PoseTrack Pose Tracking, and report systematic improvement over the state of the art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of human pose estimation is to correctly localize and estimate body poses of all people in the scene. Human poses provide strong cues and have shown to be an effective representation for a variety of tasks such as activity recognition, motion capture, content retrieval and social signal processing. Recently, human pose estimation performance has improved significantly due to the use of deep convolutional neural networks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b17">18]</ref> and availability of large-scale datasets <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Although great progress has been made, the problem remains far from being solved. There still exist a lot of challenging cases, such as person-person occlusions, close proximity of similar looking people, rare body configura-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Initial Refined Pose</p><p>Pose Estimate Output Pose <ref type="figure">Figure 1</ref>: Example of the proposed pose refinement. Starting from an image and an estimated body pose (central), our refinement method PoseRefiner outputs a denoised body pose (right). The system learns to fuse the appearance of the person and an estimation of its pose structure in order to better localize each body joint. It is trained to specifically target common errors of human pose estimation methods, e.g. merges of body joints of different people in close proximity and confusion between right/left joints.</p><p>tions, partial visibility of people and cluttered backgrounds. Despite the great representational power, current deep learning-based approaches are not explicitly trained to address such hard cases and often output incorrect body pose predictions such as spurious body configurations, merges of body joints of different people, confusion between similarly looking left and right limbs, or missing joints. In this work, we propose a novel human pose refinement approach that is explicitly trained to address such hard cases. Our simple yet effective pose refinement method can be applied on top of any body pose prediction computed by an arbitrary human pose estimation approach, and thus is complementary to current approaches <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23]</ref>. As we demonstrate empirically, the proposed pose refinement allows to push the state of the art on several standard benchmarks of single-and multi-person pose estimation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>, as well as articulated pose tracking <ref type="bibr" target="#b1">[2]</ref>. In more detail, given an RGB image and a body pose estimate, we aim to output a refined human pose by exploiting the dependencies between the image and the inherent structure of the provided body pose (see <ref type="figure">Figure 1</ref>). This makes it easier for the network to identify what is wrong with input prediction and find a way to refine it. We employ a fully convolutional ResNet-based architecture and propose an elaborate data augmentation scheme for training. To model challenging cases, we propose a novel data augmentation procedure that allows to synthesize possible input poses and make the network learn to identify the erroneous body joint predictions and to refine them. We refer to the proposed approach as PoseRefiner.</p><p>We evaluate our approach on four human pose estimation benchmarks, namely MPII Single Person <ref type="bibr" target="#b2">[3]</ref>, MPII Multi-Person <ref type="bibr" target="#b2">[3]</ref>, PoseTrack Multi-Person Pose Estimation <ref type="bibr" target="#b1">[2]</ref>, and PoseTrack Multi-Person Pose Tracking <ref type="bibr" target="#b1">[2]</ref>. We report consistent improvement after applying the proposed refinement network to pose predictions given by various stateof-the-art approaches <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref> across different datasets and tasks, showing the effectiveness and generality of the proposed framework. With our refinement network, we improve the best reported results for multi-person pose estimation and pose tracking on MPII Human Pose and PoseTrack datasets.</p><p>In summary, our contributions are as follows:</p><p>• We introduce an effective post-processing technique for body joint refinement in human pose estimation tasks, that works on top of any existing human body pose estimation approach. Our proposed pose refinement network is efficient due to its feed-forward architecture, simple and end-to-end trainable.</p><p>• We propose a training data augmentation scheme for error correction, which enables the network to identify the erroneous body joint predictions and to learn a way to refine them.</p><p>• We show that our refinement model allows to systematically improve over various state-of-the-art methods and achieve top performing results on four different benchmarks.</p><p>The rest of the paper is organized as follows. Section 2 provides an overview of the related work and positions the proposed approach with respect to earlier work. Section 3 describes the proposed pose refinement network and data augmentation for error correction of human body pose estimation. Experimental results are presented in Section 4. Section 5 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Our proposed approach is related to previous work on single-and multi-person pose estimation, articulated track-ing as well as refinement/error correction methods, as described next. Single-person pose estimation. Classical methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">38]</ref> formulate single person pose estimation as a pictorial structure or graphical model problem and predict body joint locations using only hand-designed features. More recent methods <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b45">46]</ref> rely on localizing body joints by employing convolutional neural networks (CNNs), which contributed to large improvement in human pose estimation. <ref type="bibr" target="#b42">[43]</ref> directly predicts joint coordinates via a cascade of CNN pose regressors, while further improvement in the performance is achieved by predicting heatmaps of each body joint <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b29">30]</ref> and using very deep CNNs with multi-stage architectures <ref type="bibr" target="#b44">[45]</ref>. Our method is complementary to current approaches, as it is able to use their predictions as input and further improve their results, see Section 4.2 for details. Multi-person pose estimation. Compared to single person pose estimation, multi-person pose estimation requires parsing of the full body poses of all people in the scene and is a much more challenging task due to occlusions, various articulations and interactions between people. Multiperson pose estimation methods can be grouped into two types: top-down and bottom-up approaches.</p><p>Top-down approaches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b15">16</ref>] employ a person detector and then perform single-person pose estimation for each detected person. These methods highly depend on the reliability of the person detector and are known to have trouble recovering poses of people in close proximity to each other and/or with overlapping body parts. Thus, the output predictions of top-down methods might benefit from an additional refinement step proposed in this work.</p><p>Bottom-up methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b26">27]</ref> first predict all body joints and then group them into full poses of different people. Instead of applying person detection, these methods rely on context information and inter body joint relationships. However, modeling the joint relationships might not be as reliable, causing mistakes like failure to disambiguate poses of different people or grouping body parts of the same person into different clusters. Our refinement approach can particularly help in this scenario, as it re-estimates joint locations by taking the structure of the body pose into account.</p><p>Articulated pose tracking. Most articulated pose tracking methods rely on a two-stage framework, which first employs a per-frame pose estimator and then smooths the predictions over time. <ref type="bibr" target="#b40">[41]</ref> proposes a model combining a CNN and a CRF to jointly optimize per-frame predictions with the CRF, smoothing the predictions over space and time. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26</ref>] employ a bottom-up strategy, they first detect body joints of all people in all frames of the video and then an integer program optimization is solved grouping joints into people over time. <ref type="bibr" target="#b13">[14]</ref> applies 3D Mask R- <ref type="figure">Figure 2</ref>: The overview of our PoseRefiner system. We take as input an image I and an initial estimate of a person body pose P input . The input pose is encoded as n binary channels, where n is the number of joints, which are stacked together with the image RGB channels and used as an input to a fully convolutional network. The network learns to predict likelihood heatmaps for each joint type, as well as offset vectors to recover from the downscaled spatial resolution. The output pose P output is a refined estimate of the initial input. CNN <ref type="bibr" target="#b18">[19]</ref> over short video clips, producing a tubelet with body joints per person, and then performs a lightweight optimization to link the predictions over time. <ref type="bibr" target="#b25">[26]</ref> extends the work of <ref type="bibr" target="#b5">[6]</ref> by rethinking the network architecture and developing a redundant part affinity fields (PAFs) mechanism, while <ref type="bibr" target="#b36">[37]</ref> employs a geometric tracker to match the predicted poses frame-by-frame. All of these approaches heavily rely on accurate pose estimation in a single frame. We show in Section 4.4 that by refining the initial pose hypothesis in individual frames we are able to significantly improve pose tracking over time. Refinement/error correction. Another group of work <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b27">28]</ref> aims to refine labels from the initial estimate by jointly reasoning about input-output space. <ref type="bibr" target="#b6">[7]</ref> proposes to iteratively estimate residual corrections which are added to the initial prediction. In a similar spirit <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b4">5]</ref> use RNN-like architectures to sequentially refine the results and <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28]</ref> employ cascade CNNs with refinement stages. <ref type="bibr" target="#b12">[13]</ref> decomposes the label improvement into three stages: first detecting the errors in the initial labels, replacing the incorrect labels with new ones and refining the labels by predicting residual corrections. Likewise <ref type="bibr" target="#b21">[22]</ref> employs a parallel architecture that propagates correct labels to nearby pixels and replaces the erroneous predictions with refined ones, then fuses the intermediate results to obtain a final prediction. In contrast to these methods, our proposed refinement network is much simpler as it learns to directly predict the refined labels from the initial estimate using a simple feed-forward fully convolutional network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we describe our approach -PoseRefiner -in detail. We propose a pose refinement network which takes as input both an RGB image and a body pose estimate and aims to refine the initial prediction by jointly reasoning about the input and output space (see <ref type="figure">Figure 2</ref>). Exploiting the dependencies between the image and the predicted body pose makes it easier for the model to identify the errors in the initial estimate and how to refine them. For the network to be able to learn to correct the erroneous body joint predictions we employ a training data augmentation scheme, modeled to generate the most common failure cases of human pose estimators. This yields a model that is able to refine a human pose estimate derived from different pose estimation approaches and allows to achieve state-of-the-art results on the challenging MPII Human Pose and PoseTrack benchmarks. Approach. We approach the refinement of pose estimation as a system on its own, which can be easily used as a postprocessing step following any keypoint prediction task. Although there can be multiple estimated people poses in an image, we apply the refinement process on a per-person level. Given an estimated person pose, we initially rescale and crop around it to obtain a reference input. We then forward this obtained image I and pose estimate P input as an input to a fully convolutional neural network f , modeled to compute a refined pose prediction P output .</p><p>Formally, we refine an initial pose estimate P input as:</p><formula xml:id="formula_0">P output = f (I, P input ), where:</formula><p>• f is the PoseRefiner, the function to be learned.</p><p>Since the output of this function is a single person pose, we model it using a fully convolutional network designed for single-person pose estimation.</p><p>• I is the original image in RGB format.</p><p>• P input is the initial body pose, which needs to be refined. It is concatenated with the RGB image as n additional channels, where n is the number of body joints. <ref type="figure">Figure 3</ref>: Examples of the proposed data synthesis for training. Starting from the ground truth P output , we synthesize the initial pose estimate P input to mimic the most common errors of pose estimators. For visualization purposes, we only illustrate one transformation at a time: T1 shifts the left shoulder (yellow), T2 switches the left ankle (pink) with the right ankle (red), T3 replaces the right shoulder (fuchsia) by the left shoulder of the neighboring person and T4 removes the right knee.</p><formula xml:id="formula_1">P output P input = T1(P output ) P input = T2(P output ) P input = T3(P output ) P input = T4(P output )</formula><p>• P output is the refined pose, in the form of n channels.</p><p>Both P input and P output are encoded using one binary channel for each body joint.</p><p>Architecture. We adopt the design choices that were shown successful in architectures with strong body joint detectors. As network architecture, we employ the ResNet-101 <ref type="bibr" target="#b19">[20]</ref> backbone converted to a fully convolutional mode with stride of 8 px. Although the ResNet-101 network is designed to accept as input only 3 (RGB) channels, it can be easily extended to accept additional body joint channels by increasing the depth of the filters of the first convolutional layer (from 3 to 3 + n), where n is the number of body joints.</p><p>Following <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b23">24]</ref>, we train the network to predict two types of output: likelihood heatmaps of each body joint and offsets from the locations on the heatmap grid to the ground truth joint locations. Likelihood heatmaps for each joint type are trained using sigmoid activations and cross entropy loss function. The shape of the output heatmaps is 8 times smaller in each spatial dimension than the shape of the input, due to the 8 pixel stride of the network. Hence to recover from the lost resolution, we learn to predict offset vectors from every heatmap location to the ground truth joint coordinate by regressing displacements (∆x, ∆y) using mean squared error.</p><p>At test time, every pixel location in each likelihood heatmap indicates the probability of presence of the particular joint at that coordinate. The pixel with the highest confidence in each likelihood heatmap is selected as the rough downscaled joint coordinate. The final coordinate is obtained by adding the offset vector (∆x, ∆y) to the upscaled joint location predicted at the lower resolution.</p><p>Training Data Synthesis. To train the network f , we need to have access to ground truth triplets (I, P input , P output ). While (I, P output ) pairs are already available in large scale pose estimation datasets, we propose to synthesize P input to mimic the most common failure cases of human pose estimators. The goal is for the model to be able to refine initial estimates and become robust to the "hard" body pose cases. In essence, P input is a noisy version of P output , which we synthesize from the ground truth by applying the following transformations (visualized in <ref type="figure">Figure 3</ref>):</p><p>(T1) Shift the coordinates of each joint by a displacement vector. The angle of the displacement vector is sampled uniformly from [0, 2π]. The length of the displacement vector is sampled with 90% chance from [0px, 25px] and 10% chance from [25px, 125px] to ensure both small and large displacements. In this way, the model is able to learn to do local refinements as well as to handle larger offsets of joints in spurious body configurations.</p><p>(T2) Switch symmetric joints of the same person (e.g. replace left shoulder by right shoulder) -with probability 10% per pair of joints. Such type of noise is a usual failure case of pose estimators, which occasionally confuse similarly looking left and right limbs or whether the joints are faced from the front or from the back.</p><p>(T3) Replace joints of a person by joints of the same/symmetric type of neighboring persons (e.g left hip of person A is replaced by the neighboring left/right hip of person B). Such synthesis is possible only when the pose estimation dataset contains multiple annotated people in the same</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT Pose</head><p>Initial Pose <ref type="bibr" target="#b45">[46]</ref> Refined Pose GT Pose Initial Pose <ref type="bibr" target="#b16">[17]</ref> Refined Pose GT Pose Initial Pose <ref type="bibr" target="#b43">[44]</ref> Refined Pose GT Pose Initial Pose <ref type="bibr" target="#b5">[6]</ref> Refined Pose image. If such a neighbor joint exists in a 75px vicinity, replacement is done with 30% probability. This transformation models the pose estimation errors arising in crowded scenes, when limbs of different people are merged together.</p><p>(T4) Remove body joint with 30% chance. This transformation helps to simulate the common missing joint error of body pose estimators, which is usually introduced by thresholding of low-confident keypoint detections.</p><p>Implementation Details. We implement our system using the publicly available TensorFlow [1] framework. Following <ref type="bibr" target="#b23">[24]</ref>, we rescale the input pose and image such that the reference height of a person is 340 px. The height of a person is estimated either from the scale of the ground truth head bounding box (if available, as in the MPII Single Person Dataset), or directly from the estimated input pose. We also crop 250 px in each direction around the bounding box of the input pose. This should standardize the input and minimize the searching space of the joints, while providing enough context to the PoseRefiner.</p><p>The input body pose estimate P input is encoded using one binary channel for each body joint. Each channel contains a circular blob of radius 15 px around the joint coordinate. If there is no coordinate for the particular joint, the respective channel will be the null matrix. This encoding follows the encoding of the P output channels during training, which has been shown to work well for training strong body part detectors <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>Our training procedure contains a data augmentation step, which we employ for generating more training data. We apply random rescaling ±30% and random flipping around the vertical axis.</p><p>When no pre-training is applied, we initialize the network with the weights of models trained on ImageNet <ref type="bibr" target="#b10">[11]</ref>. For initialization of the extra convolutional filters corresponding to additional channels of P input , we reuse the weights corresponding to RGB channels of I.</p><p>Optimization is done using stochastic gradient descent with 1 image per batch, starting with learning rate lr = 0.005 for one third of an epoch and continuing with lr = 0.02 for 15 epochs, lr = 0.002 for 10 epochs and lr = 0.001 for 10 other epochs. Training on the MPII dataset (≈ 29k people) runs for 40 hours on one GPU 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We now evaluate the proposed approach on the tasks of articulated single-and multi-person pose estimation, and articulated pose tracking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>We test our refinement network on three tasks involving human body pose estimation: single-person pose estimation, multi-person pose estimation and multi-person articulated tracking. In each of these tasks, we refine the predictions of several state-of-the-art methods by postprocessing each initially estimated body pose using the PoseRefiner.</p><p>We experiment on four public challenges: MPII Human Pose <ref type="bibr" target="#b2">[3]</ref> ("Single-Person" and "Multi-Person") and PoseTrack <ref type="bibr" target="#b1">[2]</ref> ("Single-Frame Multi-Person Pose Estimation" and "Multi-Person Articulated Tracking").</p><p>Datasets. For fair comparison with the methods whose prediction we refine, we follow the most common practices in choosing the datasets for training the PoseRefiner.</p><p>For the MPII Human Pose challenges, we train on MPII Human Pose only, whose training set contains ≈ 29k poses. For evaluation, we report results on the test set of MPII Human Pose, which includes 7, 247 sufficiently separated poses used in the "Single-Person" challenge, as well as 4, 485 poses organized in groups of interacting people, used in the evaluation of the "Multi-Person" challenge. Although the protocol of the MPII Multi-Person task assumes that the location and the rough scale of each group of people is provided during test time, the PoseRefiner does not require any of this information.</p><p>In the case of the PoseTrack challenges, we pretrain on the COCO <ref type="bibr" target="#b30">[31]</ref> train2017 set (≈ 150k poses), then fine-tune on the MPII training set and afterwards on the PoseTrack training set. Pretraining is needed as the PoseTrack training set contains ≈ 61k poses, but only 2, 437 different identities, which do not cover a very high appearance variability. Since the set of joint types differs across datasets (MPII annotates 16 keypoints, PoseTrack annotates 15 and COCO annotates 17), we use the PoseTrack set of body joints as reference and map all the other types to their closest PoseTrack joint type. The COCO dataset does not provide annotations for top-head and bottom-head, so we heuristically use the top most semantic segmentation vertex as the top-head keypoint, and the midpoint between the nose and the midpoint of the shoulders as the bottom-head keypoint. Similarly, MPII does not provide annotations for the nose joint, so we use the midpoint between the bottom-head and top-head as a replacement. For evaluation, we report results on the PoseTrack validation set (50 videos, containing 18, 996 poses), which is publicly available.</p><p>Evaluation metrics. For each task, we adopt the evaluation protocol proposed by the respective challenges.</p><p>On the MPII Human Pose (Single-Person) dataset, we report the Percentage of Correct Keypoints metric calculated with the matching threshold of half the length of the head segment (P CK h @0.5), averaged across all joint types. We also report the Area Under the Curve measure (AUC), corresponding to the curve generated by P CK h measured over a range of percentages of the length of the head segment. Since none of the metrics is sensitive to false-positive joint detections, we do not remove non-confident predicted keypoints by thresholding them.</p><p>On the MPII Human Pose (Multi-Person) dataset, we report the mean Average Precision (mAP) based on the matching of body poses using P CK h @0.5, following the evaluation kit of <ref type="bibr" target="#b38">[39]</ref>. This metric requires providing a confidence score for each detected body joint in addition to its location. Since the confidence scores provided by the PoseRefiner are in fact conditional probabilities of detection, we use the initial confidence scores (before refinement) that the input pose predictions come with.</p><p>On the Single-Frame Multi-Person Pose Estimation task of PoseTrack, we report the same mAP metric as in MPII Human Pose, with the slight difference that the rough scale and location of people are not provided during test time, so the mAP evaluation in PoseTrack does not require it.</p><p>On the Multi-Person Articulated Tracking task of PoseTrack, we calculate the Multiple Object Tracking Accuracy (MOTA) for each joint, and report mMOTA, averaged across all joints. This metric requires providing a track ID for each detected body pose, but no confidence score for joint detections. Since the metric is sensitive to false positive keypoints, we threshold the low confidence joints with the aim of removing incorrect predictions. We experimentally find that removing all joint detections with confidence scores less than τ = 0.7 provides the best trade-off between the number of missed joints and the number of false positive joints, both penalized in the calculation of MOTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Single-Person Pose Estimation</head><p>The effect that the PoseRefiner has on the test set of MPII Single-Person is shown in <ref type="table">Table 1</ref>. We refine various single person pose estimates given by different methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>, including the state of the art <ref type="bibr" target="#b45">[46]</ref>.</p><p>One can notice that the performance of <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b7">8]</ref> is already quite high, which motivates using less noise in the synthesis of the input pose for training the refinement model for these methods. Hence, we decrease the level of noise used in the generation of input poses during training by switching off all noise transformations, with the exception of (T1). We do not find it necessary to change the original set of noise transformations in any other experiment.</p><p>Using PoseRefiner as a post-processing step consistently increases the performance of each method, with the average improvement of mP CK h @0.5 and AUC ranging from 0.3 to 6.8, while hurting neither metric. This shows the generality and effectiveness of our refinement method on single-person pose estimation, which already hints at its use in other more complex tasks involving keypoint detec-Input Poses <ref type="bibr" target="#b13">[14]</ref> Refined Poses Input Poses <ref type="bibr" target="#b13">[14]</ref> Refined Poses  We present the qualitative results on MPII Single Pose in <ref type="figure" target="#fig_0">Figure 4</ref>. Our refinement network is able to correct confusion between different joint types, recover from spurious or missing keypoints and provide better overall localization of joints.</p><p>2 Using a refinement model trained with only (T1) transformations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method mAP ∆mAP</head><p>Associative Embedding <ref type="bibr" target="#b32">[33]</ref> 77.5 -+ Refinement 78.0 +0.5 Part Affinity Fields <ref type="bibr" target="#b5">[6]</ref> 75.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-Person Pose Estimation</head><p>Since the output of a multi-person pose estimator is a set of body poses in an image, we can use the PoseRefiner to perform error correction on each estimated pose, independently of the others. <ref type="table" target="#tab_2">Table 2</ref> shows the quantitative effect that the refinement post processing step has on several methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b43">44]</ref> applied on the MPII Multi-Person test set. It proves to help the overall performance of each system, including the best performing method <ref type="bibr" target="#b32">[33]</ref>   2.9 mAP) can be considered significant for the localization of joints. <ref type="table" target="#tab_4">Table 3</ref> shows the results on the PoseTrack validation set. We refine the pose predictions of methods proposed for the Single-Frame Multi-Person Pose Estimation case <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b13">14]</ref>. They process images independently of each other and optimize the mAP metric. We again observe consistent improvements when employing the PoseRefiner, managing to increase the best reported performance on the dataset from 71.9 mAP [48] to 73.8 mAP.</p><p>In the case of ArtTrack <ref type="bibr" target="#b22">[23]</ref>, which does not output a nose joint, we remove the missing keypoint from the ground truth and from the evaluation procedure and report the obtained result (68.6 mAP) on the remaining subset of joints (64.0 mAP evaluated on all joints). After post processing with the PoseRefiner, the nose joint is recovered, and we report results for both evaluations: when removing the nose joint from the evaluation procedure (70.0 mAP) and when counting it into the evaluation (69.7 mAP). The fact that the difference between the two is small shows that the new nose joint is recovered and nearly as well localized as the other joints. In addition, the overall performance after the refinement step is increasing (68.6 → 69.7 mAP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Multi-Person Articulated Tracking</head><p>Multi-Person Articulated Tracking involves detecting all people in each frame of a video, estimating their pose and linking their identities over time. We can therefore apply the PoseRefiner on each estimated pose independently of the others, while keeping the original identities of the detected people.   <ref type="table" target="#tab_4">Table 3</ref> from <ref type="table" target="#tab_5">Table 4</ref>, depending on which metric the method optimizes. Although the refinement only updates the coordinates of already detected body poses and no tracklet IDs are changed, the overall mMOTA improvement obtained by our system is significant (from 2.2 to 4.9 mMOTA). We show systematic improvement on every tracking result we refine, including the predictions of the method with the highest performance <ref type="bibr" target="#b25">[26]</ref>. The state of the art is hence extended, reaching 58.4 mMOTA on this benchmark. Similar to the Multi-Person Pose Estimation case, we recover the missing nose joint on ArtTrack and manage to refine its overall tracking results by 3.9 mMOTA. Qualitative results of multi-person articulated tracking are presented in <ref type="figure" target="#fig_1">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we proposed a human pose refinement network which can be applied over a body pose estimate derived from any human pose estimation approach. In comparison to other refinement techniques, our approach provides a simpler solution by directly generating the refined body pose from the initial pose prediction in one forward pass, exploiting the dependencies between the input and output spaces. We report consistent improvement of our model applied over state-of-the-art methods across different datasets and tasks, highlighting its effectiveness and generality. Our experiments show that even top performing methods can benefit from the proposed refinement step. With our refinement network we improve the best reported results on MPII Human Pose and PoseTrack datasets for multi-person pose estimation and pose tracking tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 :</head><label>4</label><figDesc>Qualitative results on the MPII Single-Pose dataset (top) and MPII Multi-Pose dataset (bottom). The blue circles denote the areas where the PoseRefiner brings significant improvement. Our refinement method provides better localization for the challenging keypoint extremities (ankles and wrists), can remove confusions between symmetrical joint types (right ankle in top right and left ankle in bottom left figures) and can recover spurious joints (left wrist in top left figures) or missing joints (right hip in bottom right figures) by reasoning about the pose structure of the target person.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Qualitative results on the PoseTrack validation set, before and after applying the proposed refinement. The blue circles denote the areas where the proposed post-processing step brings significant improvement. The PoseRefiner recovers missing joints (e.g. right elbow and right hip in top seq. -fr.2, right wrist in bottom seq. -fr.3) and helps with confusions of symmetrical joints (left elbow in top seq. -fr.3, right hip in bottom seq. -fr.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Effect of the proposed refinement over different methods on the MPII Multi-Person<ref type="bibr" target="#b2">[3]</ref> test set. ∆mAP indicates the improvement of mAP after applying the proposed pose refinement network.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>on this dataset, setting a new state of the art of 78.0 mAP. Given that our system does not have any influence over non detected people/human body poses, the overall improvement (ranging from 0.5 mAP to</figDesc><table><row><cell>Method</cell><cell>mAP</cell><cell>∆mAP</cell></row><row><cell>ML_Lab [48]</cell><cell>71.9</cell><cell>-</cell></row><row><cell>+ Refinement</cell><cell>73.8</cell><cell>+1.9</cell></row><row><cell>ArtTrack [23] (best mAP) 3</cell><cell>68.6</cell><cell>-</cell></row><row><cell>+ Refinement (w/o nose)</cell><cell>70.0</cell><cell>+1.4</cell></row><row><cell>+ Refinement (with nose)</cell><cell>69.7</cell><cell>+1.1</cell></row><row><cell>BUTD [26] (best mAP)</cell><cell>67.8</cell><cell>-</cell></row><row><cell>+ Refinement</cell><cell>70.9</cell><cell>+3.1</cell></row><row><cell>Detect-and-Track [14]</cell><cell>60.4</cell><cell>-</cell></row><row><cell>+ Refinement</cell><cell>65.7</cell><cell>+5.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Effect of the proposed refinement on the PoseTrack [2] validation set, the Single-Frame Multi-Person Pose Estimation challenge. ∆mAP indicates the improvement of mAP after applying the pose refinement model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc>shows the quantitative effect of the proposed refinement step on the PoseTrack validation</figDesc><table><row><cell>Method</cell><cell cols="3">mAP mMOTA ∆mMOTA</cell></row><row><cell>BUTD [26] (best mMOTA)</cell><cell>62.5</cell><cell>56.0</cell><cell>-</cell></row><row><cell>+ Refinement</cell><cell>64.3</cell><cell>58.4</cell><cell>+2.4</cell></row><row><cell>Detect-and-Track [14]</cell><cell>60.4</cell><cell>55.1</cell><cell>-</cell></row><row><cell>+ Refinement</cell><cell>64.1</cell><cell>57.3</cell><cell>+2.2</cell></row><row><cell cols="2">ArtTrack [23] (best mMOTA) 4 66.7</cell><cell>50.2</cell><cell>-</cell></row><row><cell>+ Refinement (w/o nose)</cell><cell>66.5</cell><cell>53.3</cell><cell>+3.1</cell></row><row><cell>+ Refinement (with nose)</cell><cell>67.0</cell><cell>54.1</cell><cell>+3.9</cell></row><row><cell>ML_Lab [48]</cell><cell>71.9</cell><cell>48.6</cell><cell>-</cell></row><row><cell>+ Refinement</cell><cell>70.1</cell><cell>53.5</cell><cell>+4.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Effect of the proposed refinement on the PoseTrack<ref type="bibr" target="#b1">[2]</ref> validation set, the Multi-Person Articulated Tracking challenge. ∆mMOTA indicates the improvement of mMOTA after applying the pose refinement model. set. Note that there are cases in which the results of the same method differ in</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We use NVIDIA Tesla V100 GPU with 16 GB RAM</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">ArtTrack does not output a nose joint, so the evaluation before refinement is performed without considering this joint. Our refinement network can recover the missing nose joint, leading to better performance (68.6 → 69.7 mAP).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Although ArtTrack does not output a nose joint, our refinement network can recover the missing nose joint, while improving overall performance (50.2 → 54.1 mMOTA).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pictorial structures revisited: People detection and articulated pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face and Gesture Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<idno>abs/1705.00389</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07319</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human pose estimation using body parts dependent joint regressors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">RMPE: Regional multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Detect, replace, refine: Deep structured prediction for pixel wise labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Detect-and-track: Efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.09184</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Articulated pose estimation using discriminative armlet classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Using k-poselets for detecting people and localizing their keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Densepose: Dense human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A coarse-fine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Error correction for dense semantic image labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.03812</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Arttrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Posetrack: Joint multi-person pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Towards multi-person pose tracking: Bottom-up and top-down methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV PoseTrack Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Iterative instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Look into person</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01984</idno>
	</analytic>
	<monogr>
		<title level="m">Joint body parsing &amp; pose estimation network and a new benchmark</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03816</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Mass displacement networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Cascade residual learning: A two-stage convolutional neural network for stereo matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Simultaneous multi-person detection and single-person pose estimation with a single heatmap regression network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Payer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Neff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV PoseTrack Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Adaptive pose priors for pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Thin-slicing network: A deep structured model for pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Hilliges</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A greedy part assignment algorithm for real-time multi-person 2d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tickoo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.09182</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation for posetrack with enhanced part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV PoseTrack Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
