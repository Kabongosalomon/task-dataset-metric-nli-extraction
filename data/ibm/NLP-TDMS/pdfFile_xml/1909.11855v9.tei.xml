<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Universal Graph Transformer Self-Attention Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Dai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><surname>Dinh Nguyen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Phung</surname></persName>
						</author>
						<title level="a" type="main">Universal Graph Transformer Self-Attention Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T21:50+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Graph neural networks · Graph classifica- tion · Transformer · Self-attention</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The transformer self-attention network has been extensively used in research domains such as computer vision, image processing, and natural language processing. But it has not been actively used in graph neural networks (GNNs) where constructing an advanced aggregation function is essential. To this end, we present U2GNN, an effective GNN model leveraging a transformer self-attention mechanism followed by a recurrent transition, to induce a powerful aggregation function to learn graph representations. Experimental results show that the proposed U2GNN achieves state-of-the-art accuracies on well-known benchmark datasets for graph classification. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph-structured data appear in many real-world and scientific fields, e.g., knowledge graphs, recommender systems, social and citation networks, as well as telecommunication and biological networks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b55">56]</ref>. In general, we can view a graph as a network of nodes and edges, where nodes correspond to individual objects, and edges encode relationships among those objects. For example, in an online forum, each discussion thread can be constructed as a graph where nodes represent users, and edges represent commenting activities between users <ref type="bibr" target="#b48">[49]</ref>.</p><p>Learning graph representations is one of the most important topics for the graph-structured data <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52]</ref>, where a goal is to construct vector representations for graphs to predict graph labels. Early approaches measure similarities among graphs to build a graph kernel for graph classification <ref type="bibr" target="#b13">[14]</ref>, wherein graph kernel-based approaches decompose graphs into "atomic subgraphs" (such as random walks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44]</ref>, shortest paths <ref type="bibr" target="#b3">[4]</ref>, graphlets <ref type="bibr" target="#b38">[39]</ref>, and Weisfeiler-Lehman subtree patterns <ref type="bibr" target="#b37">[38]</ref>) to have a vector of frequencies of atomic subgraphs for each given graph. Besides, word embedding-based approaches utilize Word2Vec <ref type="bibr" target="#b31">[32]</ref> and Doc2Vec <ref type="bibr" target="#b25">[26]</ref> to learn embeddings for the atomic subgraphs <ref type="bibr" target="#b48">[49]</ref> and the entire graphs <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>Recently, graph neural networks (GNNs) become an essential strand to learn low-dimensional continuous embeddings of the entire graphs to predict the graph labels <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b51">52]</ref>. GNNs use an Aggregation function to update the vector representation of each node by transforming and aggregating the vector representations of its neighbors <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b41">42]</ref>. Then GNNs apply a graph-level pooling function (i.e., a ReadOut operation such as simple sum pooling <ref type="bibr" target="#b47">[48]</ref>) to obtain graph embeddings. Compared to the graph kernel and word embedding-based approaches, GNNs have been achieving state-of-the-art accuracies for the graph classification task.</p><p>To further improve the classification performance, constructing a powerful aggregation function is essential for GNNs as also discussed in <ref type="bibr" target="#b47">[48]</ref>. Nowadays, there are novel applications of the transformer self-attention network <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b8">9]</ref> recognized, published, and used successfully in research domains such as computer vision, image processing, and natural language processing. Hence we consider the use of the transformer to a new domain, i.e., graph neural networks (GNNs), as a novelty. Inspired by this advanced self-attention network, we present U2GNN, an effective GNN model, which induces a powerful Aggregation function leveraging a self-attention mechanism <ref type="bibr" target="#b40">[41]</ref> followed by a recurrent transition, to update the vector representations of nodes from their neighbors. In particular, our U2GNN is different from related work as follows:</p><p>• A "concurrent" work -Hyper-SAGNN [55] -utilizes the transformer self-attention network for hypergraphs that have diverse and different structures, hence requiring a different solution. Besides, the later and closely related work, Graph-BERT <ref type="bibr" target="#b52">[53]</ref>, is an extension of our U2GNN for semisupervised node classification.</p><p>• Graph Attention Network (GAT) <ref type="bibr" target="#b41">[42]</ref> borrows the standard attention technique from <ref type="bibr" target="#b1">[2]</ref> in using a single-layer feedforward neural network parametrized by a weight vector and then applying the non-linearity function followed by the softmax to compute the importance weights of neighbors of a given node. Note that our U2GNN adopts a scaled dot-product attention mechanism which is more robust and efficient than the attention technique used in GAT.</p><p>• Regarding the model architecture, Graph Transformer Network (GTN) <ref type="bibr" target="#b50">[51]</ref> identifies useful metapaths <ref type="bibr" target="#b44">[45]</ref> to transform graph structures and applies GCN <ref type="bibr" target="#b23">[24]</ref> to learn the node embeddings for the node classification task on heterogeneous graphs. Self-Attention Graph Pooling (SAGPool) <ref type="bibr" target="#b26">[27]</ref> uses GCN as an attention mechanism to weight the nodes, employs a node selection method <ref type="bibr" target="#b12">[13]</ref> to retain a portion of the nodes, and applies the existing graph-level ReadOut pooling layers (consisting of global pooling <ref type="bibr" target="#b53">[54]</ref> and hierarchical pooling <ref type="bibr" target="#b5">[6]</ref>) to obtain the graph embeddings.</p><p>• To this end, we note that U2GNN is entirely different from GAT <ref type="bibr" target="#b41">[42]</ref>, Graph Transformer Network <ref type="bibr" target="#b50">[51]</ref>, and Self-Attention Graph Pooling <ref type="bibr" target="#b26">[27]</ref>, except similar titles.</p><p>Contributions. Our main contributions in this paper are as follows:</p><p>• We propose U2GNN, an effective GNN model, leveraging the transformer self-attention network to construct an advanced aggregation function to learn the graph representations.</p><p>• Experimental results show that U2GNN obtains state-of-the-art accuracies on well-known benchmark datasets for the graph classification task.</p><p>2 Related work 2.1 Graph kernel-based approaches These early approaches decompose graphs into "atomic subgraphs" (such as random walks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b43">44]</ref>, shortest paths <ref type="bibr" target="#b3">[4]</ref>, graphlets <ref type="bibr" target="#b38">[39]</ref>, and Weisfeiler-Lehman subtree patterns <ref type="bibr" target="#b37">[38]</ref>) to measure the similarities among graphs <ref type="bibr" target="#b13">[14]</ref>. Therefore, we can view each atomic substructure as a word term and each graph as a text document. We then represent a collection of graphs as a document-term matrix that describes the normalized frequency of terms in documents. We use an inner product to compute the graph similarities to derive a "kernel matrix" used for the kernel-based learning algorithms such as Support Vector Machines (SVM) <ref type="bibr" target="#b18">[19]</ref> to measure the graph classification performance. We refer to an overview of the graph kernel-based approaches in <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b24">25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word embedding-based approaches</head><p>Since the introduction of word embedding models such as Word2Vec <ref type="bibr" target="#b31">[32]</ref> and Doc2Vec <ref type="bibr" target="#b25">[26]</ref>, several works have used them for the graph classification task. Deep graph kernel (DGK) <ref type="bibr" target="#b48">[49]</ref> applies Word2Vec to learn the embeddings of the atomic substructures (such as the graphlets, the WL subtree patterns, and the shortest paths) to derive the kernel matrix. Anonymous walk embedding (AWE) <ref type="bibr" target="#b19">[20]</ref> maps random walks into "anonymous walks", views each anonymous walk as a word token, and utilizes Doc2Vec to achieve the graph embeddings to compute the graph similarities to construct the kernel matrix. Graph2Vec <ref type="bibr" target="#b32">[33]</ref> employs Doc2Vec to learn the embeddings for the WL subtree patterns and to obtain the graph embeddings to train a SVM classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph neural networks</head><p>Recent works have focused on using graph neural networks (GNNs) to perform the graph classification task <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref>. In general, GNNs use an Aggregation function which aims to update the vector representation of each node by recursively propagating the vector representations of its neighbors <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b41">42]</ref>. The Aggregation function can be a neural network such as gated recurrent units (GRU) <ref type="bibr" target="#b27">[28]</ref> or multi-layer perceptrons (MLPs) <ref type="bibr" target="#b47">[48]</ref>. Besides, GCN <ref type="bibr" target="#b23">[24]</ref>, GraphSAGE <ref type="bibr" target="#b15">[16]</ref>, and GAT <ref type="bibr" target="#b41">[42]</ref> are also used as the Aggregation functions. GNNs then utilize a ReadOut pooling function to obtain the graph embeddings, which are fed to multiple fully-connected layers followed by a softmax layer to predict the graph labels.</p><p>Regarding the aggregation of node representations, GCN <ref type="bibr" target="#b23">[24]</ref> updates vector representation for a given node v ∈ V from its neighbors, using multiple GCN layers stacked on top of each other to capture k-hops neighbors, as:</p><formula xml:id="formula_0">(2.1) h (k+1) v = g   u∈Nv∪{v} a v,u W (k) h (k) u   , ∀v ∈ V</formula><p>where k is the layer index; a v,u is an edge constant between nodes v and u in the re-normalized adjacency matrix; W (k) is a weight matrix; h (0) u is a feature vector of node u; g is a non-linear activation function; and N v is the set of neighbors of node v.</p><p>GraphSAGE <ref type="bibr" target="#b15">[16]</ref> extends GCN to use a node-wise procedure of uniformly sampling a fixed number of neighbors for each node at each layer as: N v can be obtained using an element-wise max-pooling operation as:</p><formula xml:id="formula_1">(2.2) h (k+1) v = g W (k) h (k) v ; h (k) N v , ∀v ∈ V</formula><formula xml:id="formula_2">(2.3) h (k) N v = max g W pool h (k) u + b , ∀u ∈ N v</formula><p>where N v is defined as a fixed-size, uniformly sampled from N v of v. Besides, N v is sampled differently through each layer. Graph Attention Network (GAT) <ref type="bibr" target="#b41">[42]</ref> extends GCN to compute edge weights following the standard attention technique <ref type="bibr" target="#b1">[2]</ref> as:</p><formula xml:id="formula_3">(2.4) h (k+1) v = g   u∈Nv∪{v} τ (k) v,u W (k) h (k) u   , ∀v ∈ V where τ (k)</formula><p>v,u is an edge weight between nodes v and u, which is computed as: τ</p><formula xml:id="formula_4">(k) v,u = (2.5) softmax LeakyReLU a (k)T W (k) h (k) v ; W (k) h (k) u</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">U2GNN: Universal Graph Transformer Self-Attention Networks</head><p>In this section, we present the additional background of graph neural networks and detail our proposed U2GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Graph classification We represent each graph</head><formula xml:id="formula_5">G = V, E, {h (0) v } v∈V , where V is a set of nodes, E is a set of edges, and h (0) v ∈ R d represents the feature vector of node v ∈ V. Given a set of M graphs {G m } M m=1</formula><p>and their corresponding class labels {y m } M m=1 ⊆ Y, the graph classification task is to learn an embedding e Gm for each graph G m to predict its label y m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph neural networks</head><p>In general, GNNs aim to update the vector representation of each node by recursively aggregating and transforming the vector representations of its neighbors <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b41">42]</ref>. After that, GNNs use a ReadOut pooling function to obtain the vector representations of the entire graphs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref>. Mathematically, given a graph G, we can formalize GNNs as follows:</p><formula xml:id="formula_6">(3.6) h (k+1) v = Aggregation h (k) u u∈Nv∪{v} (3.7) e G = ReadOut h (k) v K k=0 v∈V where h (k) v</formula><p>is the vector representation of node v at the k-th iteration/layer, N v is the set of neighbors of v.</p><p>Many methods have been proposed to construct the Aggregation functions, e.g., GCN <ref type="bibr" target="#b23">[24]</ref>, GraphSAGE <ref type="bibr" target="#b15">[16]</ref>, and GAT <ref type="bibr" target="#b41">[42]</ref>. Recently, Graph Isomorphism Network (GIN-0) <ref type="bibr" target="#b47">[48]</ref> uses a more powerful Aggregation function based on a multi-layer perceptron (MLP) network of two fully-connected layers as:</p><formula xml:id="formula_7">(3.8) h (k+1) v = MLP (k)   u∈Nv∪{v} h (k) u   , ∀v ∈ V</formula><p>Following <ref type="bibr" target="#b47">[48]</ref>, we employ a concatenation over the vector representations of node v at the different layers to construct a vector representation e v for each node v as:</p><formula xml:id="formula_8">(3.9) e v = h (1) v ; h (2) v ; ...; h (K) v , ∀v ∈ V</formula><p>where K is the index of the last layer. The graph-level ReadOut function can be a simple sum pooling <ref type="bibr" target="#b47">[48]</ref> or a complicated pooling such as hierarchical pooling <ref type="bibr" target="#b5">[6]</ref>, and differentiable pooling <ref type="bibr" target="#b49">[50]</ref>. As the sum pooling produces competitive results <ref type="bibr" target="#b47">[48]</ref>, we use the sum pooling to obtain the embedding e G of the entire graph G as:</p><formula xml:id="formula_9">(3.10) e G = v∈V e v = v∈V h (1) v ; h (2) v ; ...; h (K) v</formula><p>After that, we can follow <ref type="bibr" target="#b47">[48]</ref> to feed the graph embeddings e G to a single fully-connected layer followed by a softmax layer to predict the graph labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">The proposed U2GNN</head><p>The transformer selfattention network <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b8">9]</ref> has widely applied as a novelty in research domains such as computer vision and NLP. Similarity, we also consider the successful use of this recent advanced technique to a new domain, i.e., graph neural networks (GNNs), as a novel application. Moreover, as also discussed in <ref type="bibr" target="#b47">[48]</ref>, constructing an powerful Aggregation mechanism is essential for GNNs. To this end, we induce an advanced Aggregation function, using the universal transformer network <ref type="bibr" target="#b8">[9]</ref> consisting of a self-attention mechanism <ref type="bibr" target="#b40">[41]</ref> followed by a recurrent transition (Trans) with adding residual connections <ref type="bibr" target="#b17">[18]</ref> and layer normalization (LNorm) <ref type="bibr" target="#b0">[1]</ref>, as illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>.</p><p>The residual connections <ref type="bibr" target="#b17">[18]</ref> are used to add useful information learned in the lower layers to the higher layers, and more importantly, to allow gradients to directly pass through the layers to avoid vanishing gradient or exploding gradient problems. The layer normalization (LNorm) <ref type="bibr" target="#b0">[1]</ref> is used to normalize the inputs across the feature dimensions to stabilize the network to enable smoother gradients and faster training. The residual connections and the layer normalization are commonly used in many architectures and thus are omitted in the paper for simplicity. Formally, given an input graph G, we uniformly sample a set N v of neighbors for each v ∈ V and then input N v ∪ {v} to the U2GNN learning process. Note that we sample a different N v for node v at each training batch. We also construct multiple layers stacked on top of each other in our U2GNN. Regarding the k-th layer, given a node v ∈ V, at each step t, we induce a transformer self-attention-based function to aggregate the vector representations for all nodes u ∈ N v ∪ {v} as:</p><formula xml:id="formula_10">(3.11) h (k) t,u = Transformer-Aggregation h (k) t−1,u</formula><p>In particular,</p><formula xml:id="formula_11">(3.12) x (k) t,u = LNorm h (k) t−1,u + ATT h (k) t−1,u then, (3.13) h (k) t,u = LNorm x (k) t,u + Trans x (k) t,u where h (k)</formula><p>t,u ∈ R d ; Trans(.) and ATT(.) denote a MLP network (i.e., two fully-connected layers) and a selfattention network respectively:</p><formula xml:id="formula_12">(3.14) Trans x (k) t,u = W (k) 2 ReLU W (k) 1 x (k) t,u + b (k) 1 + b (k) 2 where W (k) 1 ∈ R s×d and W (k) 2</formula><p>∈ R d×s are weight matrices, and b are bias parameters, and:</p><formula xml:id="formula_13">(3.15) ATT h (k) t−1,u = u ∈Nv∪{v} α (k) u,u V (k) h (k) t−1,u</formula><p>where V (k) ∈ R d×d is a value-projection weight matrix; α u,u is an attention weight, which is computed using the softmax function over scaled dot products between nodes u and u :</p><formula xml:id="formula_14">(3.16) α (k) u,u = softmax   Q (k) h (k) t−1,u · K (k) h (k) t−1,u √ d  </formula><p>where Q (k) ∈ R d×d and K (k) ∈ R d×d are queryprojection and key-projection matrices, respectively. After T steps, we feed h (k)</p><p>T,v ∈ R d to the next (k + 1)th layer as:</p><formula xml:id="formula_15">(3.17) h (k+1) 0,v = h (k) T,v , ∀v ∈ V Note that h (0) 0,v = h (0) v ∈ R d is the feature vector of node v.</formula><p>We apply the vector concatenation across the layers to obtain the vector representations e v of nodes v following Equation 3.9 as:</p><formula xml:id="formula_16">(3.18) e v = h (1) 0,v ; h (2) 0,v ; ...; h (K) 0,v , ∀v ∈ V</formula><p>where K is the number of layers. We use e v as the final embedding of node v ∈ V and then sum all the final embeddings of nodes in G to get the final embedding e G of the entire graph G. We feed e G to a single fullyconnected layer followed by a softmax layer to predict the graph label as:</p><formula xml:id="formula_17">(3.19)ŷ G = softmax (We G + b)</formula><p>Finally, we learn the model parameters by minimizing the cross-entropy loss function. To sum up, we briefly present the learning process of our proposed U2GNN in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Discussion</head><p>We discuss some findings in our proposed U2GNN as follows:</p><formula xml:id="formula_18">Algorithm 1</formula><p>The U2GNN learning process.</p><formula xml:id="formula_19">Input: G = V, E, {h (0) v } v∈V with its label y for k = 0, 1, ..., K − 1 do for v ∈ V do Sample N v for v for t = 1, 2, ..., T do ∀u ∈ N v ∪ {v} x (k) t,u ← LNorm h (k) t−1,u + ATT h (k) t−1,u h (k) t,u ← LNorm x (k) t,u + Trans x (k) t,u h (k+1) 0,v ← h (k) T,v ∈ R d e v ← h (1) 0,v ; h (2) 0,v ; ...; h (K) 0,v , ∀v ∈ V (w.r.t Equation 3.18) e G ← v∈V e v y ← softmax (We G + b)</formula><p>• If we set T to 1, α • We probably could construct a complex architecture using a complicated graph-level pooling (such as hierarchical pooling <ref type="bibr" target="#b5">[6]</ref>) followed by multiple fully-connected layers <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref> to predict the graph labels. However, we refrained from doing that, as our key purpose is to introduce a single, unified and effective model that can work well and produce competitive performances on the benchmark datasets. Therefore, using the sum pooling followed by a single fully-connected layer is reasonable for our U2GNN.</p><p>• As established empirically, our results shown in Section 5 imply that the U2GNN self-attentionbased aggregation function is a powerful computation process compared to other existing functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental setup</head><p>We use seven well-known datasets consisting of three social network datasets (COLLAB, IMDB-B, and IMDB-M) and four bioinformatics datasets (DD, MUTAG, PROTEINS, and PTC). The social network datasets do not have available node features; thus, we follow <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b53">54]</ref> to use node degrees as features. <ref type="table" target="#tab_0">Table 1</ref> reports the statistics of these datasets. • Social networks datasets: COLLAB is a scientific dataset, where each graph represents a collaboration network of a corresponding researcher with other researchers from each of 3 physics fields; each graph is labeled to a physics field that the researcher belongs to. IMDB-B and IMDB-M are movie collaboration datasets, where each graph is derived from actor/actress and genre information of different movies on IMDB; nodes correspond to actors/actresses, and each edge represents a coappearance of two actors/actresses in the same movie; each graph is assigned to a genre.</p><p>• Bioinformatics datasets: DD [10] is a collection of 1,178 protein network structures with 82 discrete node labels, where each graph is classified into enzyme or non-enzyme class. MUTAG <ref type="bibr" target="#b7">[8]</ref> is a collection of 188 nitro compound networks with 7 discrete node labels, where classes indicate a mutagenic effect on a bacterium. PROTEINS comprises . We set the batch size to 4. We apply the Adam optimizer <ref type="bibr" target="#b22">[23]</ref> to train our U2GNN and select the Adam initial learning rate lr ∈ 5e −5 , 1e −4 , 5e −4 , 1e −3 . We run up to 50 epochs to evaluate our U2GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation protocol</head><p>We follow <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b6">7]</ref> to use the same data splits and the same 10-fold cross-validation scheme to calculate the classification performance for a fair comparison.</p><p>We compare our U2GNN with up-to-date strong baselines as follows:</p><p>• Unsupervised approaches: Graphlet Kernel (GK) <ref type="bibr" target="#b38">[39]</ref> and Weisfeiler-Lehman kernel (WL) <ref type="bibr" target="#b37">[38]</ref>.</p><p>• Supervised approaches: PATCHY-SAN (PSCN) <ref type="bibr" target="#b33">[34]</ref>, Graph Convolutional Network (GCN) <ref type="bibr" target="#b23">[24]</ref>, GraphSAGE <ref type="bibr" target="#b15">[16]</ref>, Graph Attention Network (GAT) <ref type="bibr" target="#b41">[42]</ref>, Deep Graph CNN (DGCNN) <ref type="bibr" target="#b53">[54]</ref>, Graph Capsule Convolution Neural Network (GCAPS) <ref type="bibr" target="#b42">[43]</ref>, Capsule Graph Neural Network (CapsGNN) <ref type="bibr" target="#b46">[47]</ref>, Self-Attention Graph Pooling (SAGPool) <ref type="bibr" target="#b26">[27]</ref>, Graph Isomorphism Network (GIN-0) <ref type="bibr" target="#b47">[48]</ref>, Graph Feature Network (GFN) <ref type="bibr" target="#b6">[7]</ref>, Invariant-Equivariant Graph Network (IEGN) <ref type="bibr" target="#b30">[31]</ref>, Provably Powerful Graph Network (PPGN) <ref type="bibr" target="#b29">[30]</ref>, and Discriminative Structural Graph Classification (DSGC) <ref type="bibr" target="#b36">[37]</ref>.</p><p>We report the baselines taken from the original papers or published in <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37]</ref>. <ref type="table" target="#tab_1">Table 2</ref> presents the experimental results of U2GNN and other strong baseline models for the benchmark datasets, and <ref type="figure" target="#fig_5">Figure 3</ref> shows the classification accuracies of our proposed U2GNN across 10 folds for each dataset. On the social network datasets, our U2GNN produces new state-of-the-art performances on IMDB-B and IMDB-M and gains a competitive score on COL-LAB. Especially, U2GNN obtains 4+% absolute higher accuracies than all the supervised baseline models on IMDB-B and IMDB-M. Regarding COLLAB, we found the best results of GCN in <ref type="bibr" target="#b6">[7]</ref>, where, after obtaining the graph embeddings, <ref type="bibr" target="#b6">[7]</ref> utilized two fully-connected layers to predict the graph labels for GCN and GFN. This is different from our U2GNN where we used a single fully-connected layer as we discussed in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental results</head><p>On the bioinformatics datasets, our U2GNN obtains the highest accuracies on DD, PROTEINS, and PTC.  <ref type="table" target="#tab_0">-0)   0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  200  100  0  100  200   200   100   0   100   200   Node embeddings (U2GNN)   0  1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16</ref> 17 18 <ref type="figure">Figure 2</ref>: A t-SNE visualization of the node embeddings learned by GIN-0 and our U2GNN on the PTC dataset.</p><p>Moreover, U2GNN achieves a competitive accuracy compared with those of the baseline models on MUTAG.</p><p>Additionally, there are no significant differences between our U2GNN and the baselines on MUTAG as this dataset only consists of 188 graphs, which explains the high variance in the results.   Hyper-parameter analysis. We investigate the effects of the number of timesteps (T ) and the number of neighbors sampled for each node (N = |N v |) in <ref type="figure" target="#fig_7">Figure  4</ref>. In general, we find that higher T can help on most of the datasets as we may use more steps T to encode the graph structures better. Furthermore, the social network datasets are denser than the bioinformatics ones; hence we should use more sampled neighbors (i.e., using higher N ) on the social network datasets rather than on the bioinformatics ones.</p><p>Visualization. To qualitatively demonstrate the effectiveness of our U2GNN, we use t-SNE <ref type="bibr" target="#b28">[29]</ref> to visualize the node embeddings learned by GIN-0 and our U2GNN on PTC where the node labels are available. Compared to GIN-0, <ref type="figure">Figure 2</ref> shows that our U2GNN produces a higher quality of learned node embeddings wherein the nodes are well-clustered according to the node labels.</p><p>In general, the superior performance of our method over the up-to-date baselines (especially, GIN-0) indicates that the U2GNN self-attention-based aggregation function is an advanced computation process to improve the classification performance of GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We introduce U2GNN, an advantageous graph neural network model. U2GNN induces a powerful aggregation function leveraging the transformer self-attention network to improve the graph classification performance. We evaluate our U2GNN using the same data splits and the same 10-fold cross-validation scheme on well-known benchmark datasets. Experimental results demonstrate that U2GNN outperforms up-to-date models and produces state-of-the-art accuracies on these datasets. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Unsupervised Graph Neural Networks</head><p>The unsupervised learning is essential in both industry and academic applications, where expanding unsupervised GNN models is more suitable due to the limited availability of class labels. Therefore, we introduce a new unsupervised learning to train GNNs for the graph classification task.</p><p>A.1 Learning process Most of the recent approaches have focused on the supervised learning where they use the graph labels during the training process <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref>. In a situation where no graph la- <ref type="table">Table 3</ref>: Graph classification results (% accuracy) in the unsupervised learning. The best scores are in bold. uGCN denotes our unsupervised GCN. Note that we do not make any direct comparison between the unsupervised approaches and the supervised ones because of the difference in the training data.  <ref type="bibr" target="#b48">[49]</ref>, Graph2Vec <ref type="bibr" target="#b32">[33]</ref>, and AWE <ref type="bibr" target="#b19">[20]</ref>) have considered the unsupervised learning, where they can have access to all nodes from the entire dataset (i.e., additionally using all nodes in the test set during training). But they produce lower classification accuracies compared to the supervised approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 2</head><p>The unsupervised learning.</p><formula xml:id="formula_20">Input: G = (V, E, {h v } v∈V ) for k = 0, ..., K − 1 do for v ∈ V do h (k+1) v = Aggregation h (k) u u∈Nv∪{v} e v ← h (1) v ; h (2) v ; ...; h (K) v , ∀v ∈ V (with respect to Equation 3.9) o v ← e v (with respect to Equation A.1) e G ← v∈V o v</formula><p>To this end, we propose a new unsupervised learning to train GNNs for the graph classification task. We can see e v in Equation 3.9 as a vector representation encoded for the substructure around node v. The goal of our unsupervised learning is to guide GNNs to recognize and distinguish the substructures within each graph, leading to improve the classification accuracies of unsupervised models. To achieve this goal, we consider a final embedding o v for each node v, and make the similarity between e v and o v higher than that between e v and the final embeddings of the other nodes, by minimizing the sampled softmax loss function <ref type="bibr" target="#b20">[21]</ref> applied to node v as:</p><formula xml:id="formula_21">(A.1) L U2GNN (v) = − log exp(o v · e v ) v ∈V exp(o v · e v ) where V is a subset sampled from {∪V m } M m=1</formula><p>. Node embeddings o v are learned implicitly as model parameters. After that, we sum all the final embeddings o v of nodes v in G to obtain the graph embedding e G . We then use the logistic regression classifier <ref type="bibr" target="#b10">[11]</ref> with setting the termination criterion to 0.001 to evaluate our model. We outline the general process of our unsupervised learning in Algorithm 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Training protocol</head><p>We follow some unsupervised approaches such as DGK <ref type="bibr" target="#b48">[49]</ref> and AWE <ref type="bibr" target="#b19">[20]</ref> to train our unsupervised U2GNN on all nodes from the entire dataset (i.e., consisting of all nodes from the test set during training) for a fair comparison. The hyperparameters are varied as same as in Section 4.1.</p><p>We also train our GCN baseline following our unsupervised learning. We set the batch size to 4 and vary the number of GCN layers in {1, 2, 3} and the hidden layer size in {32, 64, 128, 256}. We also use the Adam optimizer <ref type="bibr" target="#b22">[23]</ref> to train this unsupervised GCN up to 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Evaluation protocol</head><p>We utilizes the logistic regression classifier <ref type="bibr" target="#b10">[11]</ref> with using the 10-fold crossvalidation scheme to evaluate our models. We compare our unsupervised GCN (denoted as uGCN) and U2GNN with the baselines: Deep Graph Kernel (DGK) <ref type="bibr" target="#b48">[49]</ref> and Anonymous Walk Embedding (AWE) <ref type="bibr" target="#b19">[20]</ref>. <ref type="table">Table 3</ref> presents the experimental results in the unsupervised learning. We encourage a re-evaluation to examine the existing GNNs from the supervised learning to our new unsupervised learning to see negative results. For example, regarding the supervised learning, as shown in <ref type="table" target="#tab_1">Table 2</ref>, our supervised U2GNN outperforms GCN on the bioinformatics datasets. However, regarding the unsupervised learning, as shown in <ref type="table">Table 3</ref>, our uGCN works better than our unsupervised U2GNN on PROTEINS, MU-TAG, and PTC. These three datasets are much sparse, and node neighbors have a similar effect on each other. Hence, without using the graph labels during training, U2GNN does not increase the similar effects on node neighbors, leading to be outperformed by our uGCN on these datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Experimental results</head><p>In general, both our unsupervised U2GNN and uGCN obtain the state-of-the-art accuracies on the benchmark datasets. The significant gains demonstrate a notable impact of our unsupervised learning. It aims to guide GNNs to identify the sub-graphs' structures for every node; hence, the models can memorize the structural differences among graphs to produce the plausible node and graph embeddings as visualized in <ref type="figure" target="#fig_9">Figure 6</ref>, leading to improve the unsupervised performance. We hope that future GNN works can consider the unsupervised learning beside the supervised one.</p><p>Hyper-parameter analysis. As shown in both Figures 4 and 5, we also see similar findings in both the supervised and unsupervised training settings. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where [;] denotes a vector concatenation, and h (k)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration of our U2GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>where</head><label></label><figDesc>to 1, V (k) to the identity matrix in Equation 3.15, and do not use both the residual connections and the layer normalization, we simplify our U2GNN aggregation function (from Equations 3.17 and 3.13) as: Trans(.) denotes the MLP network of two fully-connected layers (as defined in Equation 3.14). Thus, this implies that our U2GNN can be simplified (w.r.t Equation 3.20) to be equivalent to Graph Isomorphism Network (GIN-0) [48] (w.r.t Equation 3.8) -one of the recent state-of-the-art GNNs. Experimental results presented in Section 5 show that U2GNN outperforms GIN-0 on benchmark datasets for the graph classification task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Classification accuracies across 10 folds for each dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Effects of the number of timesteps (T ), and the number of neighbors sampled for each node (N = |N v |).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Effects of the number of timesteps (T ) and the number of neighbors sampled for each node (N = |N v |) in the unsupervised learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 :</head><label>6</label><figDesc>A visualization of the node and graph embeddings learned by our unsupervised U2GNN on the DD dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the experimental benchmark datasets. #G denotes the numbers of graphs.</figDesc><table><row><cell>Dataset</cell><cell>#G</cell><cell cols="4">#Cls Avg#N Avg#E d</cell></row><row><cell>COLLAB</cell><cell>5,000</cell><cell>3</cell><cell>74.5</cell><cell>65.9</cell><cell>-</cell></row><row><cell>IMDB-M</cell><cell>1,500</cell><cell>3</cell><cell>13.0</cell><cell>10.1</cell><cell>-</cell></row><row><cell>IMDB-B</cell><cell>1,000</cell><cell>2</cell><cell>19.8</cell><cell>9.8</cell><cell>-</cell></row><row><cell>DD</cell><cell>1,178</cell><cell>2</cell><cell>284.3</cell><cell>5.0</cell><cell>82</cell></row><row><cell cols="2">PROTEINS 1,113</cell><cell>2</cell><cell>39.1</cell><cell>3.7</cell><cell>3</cell></row><row><cell>PTC</cell><cell>344</cell><cell>2</cell><cell>25.6</cell><cell>2.0</cell><cell>19</cell></row><row><cell>MUTAG</cell><cell>188</cell><cell>2</cell><cell>17.9</cell><cell>2.2</cell><cell>7</cell></row></table><note>#Cls denotes the number of class labels. Avg#N denotes the average number of nodes per graph. Avg#E denotes the average number of neighbors per node. d is the dimension of feature vectors. Note that d is also equal to the node embedding size at each U2GNN layer.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Graph classification results (% accuracy). The best scores are in bold. Training protocol We vary the number K of U2GNN layers in {1, 2, 3}, the number of steps T in {1, 2, 3, 4}, the number of neighbors (|N v | = N ) sampled for each node in {4, 8, 16}, and the dimension s of Trans(.) (in Equation 3.14) in {128, 256, 512, 1024} (in Equation 3.14)</figDesc><table><row><cell>Model</cell><cell>COLLAB</cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>DD</cell><cell>PROTEINS</cell><cell>MUTAG</cell><cell>PTC</cell></row><row><cell>GK [39]</cell><cell>72.84 ± 0.28</cell><cell>65.87 ± 0.98</cell><cell>43.89 ± 0.38</cell><cell>78.45 ± 0.26</cell><cell>71.67 ± 0.55</cell><cell>81.58 ± 2.11</cell><cell>57.26 ± 1.41</cell></row><row><cell>WL [38]</cell><cell>79.02 ± 1.77</cell><cell>73.40 ± 4.63</cell><cell>49.33 ± 4.75</cell><cell>79.78 ± 0.36</cell><cell>74.68 ± 0.49</cell><cell>82.05 ± 0.36</cell><cell>57.97 ± 0.49</cell></row><row><cell>PSCN [34]</cell><cell>72.60 ± 2.15</cell><cell>71.00 ± 2.29</cell><cell>45.23 ± 2.84</cell><cell>77.12 ± 2.41</cell><cell>75.89 ± 2.76</cell><cell>92.63 ± 4.21</cell><cell>62.29 ± 5.68</cell></row><row><cell>GCN [24]</cell><cell>81.72 ± 1.64</cell><cell>73.30 ± 5.29</cell><cell>51.20 ± 5.13</cell><cell>79.12 ± 3.07</cell><cell>75.65 ± 3.24</cell><cell>87.20 ± 5.11</cell><cell>-</cell></row><row><cell>GFN [7]</cell><cell>81.50 ± 2.42</cell><cell>73.00 ± 4.35</cell><cell>51.80 ± 5.16</cell><cell>78.78 ± 3.49</cell><cell>76.46 ± 4.06</cell><cell>90.84 ± 7.22</cell><cell>-</cell></row><row><cell>GraphSAGE [16]</cell><cell>79.70 ± 1.70</cell><cell>72.40 ± 3.60</cell><cell>49.90 ± 5.00</cell><cell>65.80 ± 4.90</cell><cell>65.90 ± 2.70</cell><cell>79.80 ± 13.9</cell><cell>-</cell></row><row><cell>GAT [42]</cell><cell>75.80 ± 1.60</cell><cell>70.50 ± 2.30</cell><cell>47.80 ± 3.10</cell><cell>-</cell><cell>74.70 ± 2.20</cell><cell>89.40 ± 6.10</cell><cell>66.70 ± 5.10</cell></row><row><cell>DGCNN [54]</cell><cell>73.76 ± 0.49</cell><cell>70.03 ± 0.86</cell><cell>47.83 ± 0.85</cell><cell>79.37 ± 0.94</cell><cell>75.54 ± 0.94</cell><cell>85.83 ± 1.66</cell><cell>58.59 ± 2.47</cell></row><row><cell>SAGPool [27]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>76.45 ± 0.97</cell><cell>71.86 ± 0.97</cell><cell>-</cell><cell>-</cell></row><row><cell>PPGN [30]</cell><cell>81.38 ± 1.42</cell><cell>73.00 ± 5.77</cell><cell>50.46 ± 3.59</cell><cell>-</cell><cell>77.20 ± 4.73</cell><cell>90.55 ± 8.70</cell><cell>66.17 ± 6.54</cell></row><row><cell>CapsGNN [47]</cell><cell>79.62 ± 0.91</cell><cell>73.10 ± 4.83</cell><cell>50.27 ± 2.65</cell><cell>75.38 ± 4.17</cell><cell>76.28 ± 3.63</cell><cell>86.67 ± 6.88</cell><cell>-</cell></row><row><cell>DSGC [37]</cell><cell>79.20 ± 1.60</cell><cell>73.20 ± 4.90</cell><cell>48.50 ± 4.80</cell><cell>77.40 ± 6.40</cell><cell>74.20 ± 3.80</cell><cell>86.70 ± 7.60</cell><cell>-</cell></row><row><cell>GIN-0 [48]</cell><cell>80.20 ± 1.90</cell><cell>75.10 ± 5.10</cell><cell>52.30 ± 2.80</cell><cell>-</cell><cell>76.20 ± 2.80</cell><cell>89.40 ± 5.60</cell><cell>64.60 ± 7.00</cell></row><row><cell>GCAPS [43]</cell><cell>77.71 ± 2.51</cell><cell>71.69 ± 3.40</cell><cell>48.50 ± 4.10</cell><cell>77.62 ± 4.99</cell><cell>76.40 ± 4.17</cell><cell>-</cell><cell>66.01 ± 5.91</cell></row><row><cell>IEGN [31]</cell><cell>77.92 ± 1.70</cell><cell>71.27 ± 4.50</cell><cell>48.55 ± 3.90</cell><cell>-</cell><cell>75.19 ± 4.30</cell><cell>84.61 ± 10.0</cell><cell>59.47 ± 7.30</cell></row><row><cell>U2GNN</cell><cell>77.84 ± 1.48</cell><cell cols="4">77.04 ± 3.45 53.60 ± 3.53 80.23 ± 1.48 78.53 ± 4.07</cell><cell>89.97 ± 3.65</cell><cell>69.63 ± 3.60</cell></row><row><cell cols="4">1,113 graphs obtained from [5] to present secondary</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">structure elements (SSEs). PTC [40] consists of 344</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">chemical compound networks with 19 discrete node</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">labels where classes show carcinogenicity for male</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">and female rats.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>4.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>± 3.25 95.36 ± 2.64 92.67 ± 4.60 U2GNN 95.62 ± 0.92 96.41 ± 1.94 89.20 ± 2.52 95.67 ± 1.89 80.01 ± 3.21 88.47 ± 7.13 91.81 ± 6.61 bels are available during training, some works (such as DGK</figDesc><table><row><cell>Model</cell><cell>COLLAB</cell><cell>IMDB-B</cell><cell>IMDB-M</cell><cell>DD</cell><cell>PROTEINS</cell><cell>MUTAG</cell><cell>PTC</cell></row><row><cell>DGK [49]</cell><cell>73.09 ± 0.25</cell><cell>66.96 ± 0.56</cell><cell>44.55 ± 0.52</cell><cell>73.50 ± 1.01</cell><cell>75.68 ± 0.54</cell><cell>87.44 ± 2.72</cell><cell>60.08 ± 2.55</cell></row><row><cell>AWE [20]</cell><cell>73.93 ± 1.94</cell><cell>74.45 ± 5.83</cell><cell>51.54 ± 3.61</cell><cell>71.51 ± 4.02</cell><cell>-</cell><cell>87.87 ± 9.76</cell><cell>-</cell></row><row><cell>uGCN</cell><cell>93.28 ± 0.99</cell><cell>94.50 ± 2.79</cell><cell>81.66 ± 3.16</cell><cell>94.31 ± 1.71</cell><cell>89.09</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We propose a new unsupervised learning in Appendix A to train GNNs and hope that future GNN works can consider the unsupervised learning beside the supervised one.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><forename type="middle">B</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Hamrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alvaro</forename><surname>Bapst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinicius</forename><surname>Sanchez-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Zambaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tacchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Faulkner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.01261</idno>
		<title level="m">Relational inductive biases, deep learning, and graph networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Shortest-Path Kernels on Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Protein function prediction via graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soon</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schönauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Svn Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans-Peter</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kriegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="47" to="56" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
	<note>suppl</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Towards sparse hierarchical graph classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cȃtȃlina</forename><surname>Cangea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikola</forename><surname>Jovanović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01287</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Are powerful graph neural nets necessary? a dissection on graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04579</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. Correlation with molecular orbital energies and hydrophobicity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asim</forename><surname>Kumar Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosa</forename><forename type="middle">L</forename><surname>Lopez De Compadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Debnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">J</forename><surname>Shusterman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corwin</forename><surname>Hansch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of medicinal chemistry</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="786" to="797" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Universal Transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew J</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of molecular biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">LIBLINEAR: A Library for Large Linear Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Rong-En Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang-Rui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1871" to="1874" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Structured self-attention architecture for graph-level representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoguo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenlong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2083" to="2092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">On graph kernels: Hardness results and efficient alternatives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Gärtner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Flach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Wrobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning Theory and Kernel Machines</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="129" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural Message Passing for Quantum Chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Kernel methods in machine learning. The annals of statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1171" to="1220" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anonymous walk embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2191" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On using very large target vocabulary for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Marginalized kernels between labeled graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koji</forename><surname>Tsuda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akihiro</forename><surname>Inokuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="321" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">A survey on graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nils M Kriege</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fredrik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Morris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.11835</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Selfattention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<title level="m">Gated Graph Sequence Neural Networks. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2153" to="2164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben-Hamu</surname></persName>
		</author>
		<title level="m">Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph networks. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annamalai</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahinthan</forename><surname>Chandramohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajasekar</forename><surname>Venkatesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shantanu</forename><surname>Jaiswal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05005</idno>
		<title level="m">Learning distributed representations of graphs</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning Convolutional Neural Networks for Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2014" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giannis</forename><surname>Siglidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.12218</idno>
		<title level="m">Graph kernels: A survey</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Younjoo</forename><surname>Seo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13422</idno>
		<title level="m">Andreas Loukas, and Nathanael Peraudin. Discriminative structural graph classification</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient Graphlet Kernels for Large Graph Comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Petri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="488" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Statistical evaluation of the predictive toxicology challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Hannu Toivonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Helma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1183" to="1193" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Graph Attention Networks. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Li</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Graph capsule convolutional neural networks. The Joint ICML and IJCAI Workshop on Computational Biology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vichy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Nicol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risi</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1201" to="1242" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Heterogeneous graph attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houye</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2022" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Xinyi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihui</forename><surname>Chen</surname></persName>
		</author>
		<title level="m">Capsule Graph Neural Network. International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">How Powerful Are Graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks? International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Deep graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pinar</forename><surname>Yanardag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vishwanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4805" to="4815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11960" to="11970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Network representation learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daokun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Big Data</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3" to="28" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Graph-bert: Only attention is needed for learning graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Congying</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05140</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">An End-to-End Deep Learning Architecture for Graph Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Hyper-SAGNN: a self-attention based graph neural network for hypergraphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruochi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuesong</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04202</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
