<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real Image Denoising with Feature Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Data61, CSIRO</orgName>
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Barnes</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Data61, CSIRO</orgName>
								<orgName type="institution">The Australian National University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Real Image Denoising with Feature Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep convolutional neural networks perform better on images containing spatially invariant noise (synthetic noise); however, their performance is limited on real-noisy photographs and requires multiple stage network modeling. To advance the practicability of denoising algorithms, this paper proposes a novel single-stage blind real image denoising network (RIDNet) by employing a modular architecture. We use a residual on the residual structure to ease the flow of low-frequency information and apply feature attention to exploit the channel dependencies. Furthermore, the evaluation in terms of quantitative metrics and visual quality on three synthetic and four real noisy datasets against 19 state-of-the-art algorithms demonstrate the superiority of our RIDNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image denoising is a low-level vision task that is essential in a number of ways. First of all, during image acquisition, some noise corruption is inevitable and can downgrade the visual quality considerably; therefore, removing noise from the acquired image is a key step for many computer vision and image analysis applications <ref type="bibr" target="#b27">[28]</ref>. Secondly, denoising is a unique testing ground for evaluating image prior and optimization methods from a Bayesian perspective <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b66">67]</ref>. Furthermore, many image restoration tasks can be solved in the unrolled inference through variable splitting methods by a set of denoising subtasks, which further widens the applicability of image denoising <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b63">64]</ref>.</p><p>Generally, denoising algorithms can be categorized as model-based and learning-based. Model-based algorithms include non-local self-similarity (NSS) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b19">20]</ref>, sparsity <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b47">48]</ref>, gradient methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b53">54]</ref>, Markov random field models <ref type="bibr" target="#b51">[52]</ref>, and external denoising priors <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b41">42]</ref>. The model-based algorithms are computationally expensive, time-consuming, unable to suppress the spatially variant noise directly and characterize complex image textures. * : saeed.anwar@csiro.au † : nick.barnes@csiro.au Noisy CBDNet <ref type="bibr" target="#b30">[31]</ref> RIDNet (Ours) <ref type="figure">Figure 1</ref>. A real noisy face image from RNI15 dataset <ref type="bibr" target="#b37">[38]</ref>. Unlike CBDNet <ref type="bibr" target="#b30">[31]</ref>, RIDNet does not have over-smoothing or overcontrasting artifacts (Best viewed in color on high-resolution display)</p><p>On the other hand, discriminative learning aims to model the image prior from a set of noisy and ground-truth image sets. One technique is to learn the prior in steps in the context of truncated inference <ref type="bibr" target="#b16">[17]</ref> while another approach is to employ brute force learning, for example, MLP <ref type="bibr" target="#b13">[14]</ref> and CNN methods <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64]</ref>. CNN models <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b30">31]</ref> improved denoising performance, due to their modeling capacity, network training, and design. However, the performance of the current learning models is limited and tailored for a specific level of noise. A practical denoising algorithm should be efficient, flexible, perform denoising using a single model and handle both spatially variant and invariant noise when the noise standard-deviation is known or unknown. Unfortunately, the current state-of-the-art algorithms are far from achieving all of these aims. We present a CNN model which is efficient and capable of handling synthetic as well as realnoise present in images. We summarize the contributions of this work in the following paragraphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Contributions</head><p>• Present CNN based approaches for real image denoising employ two-stage models; we present the first model that provides state-of-the-art results using only one stage.</p><p>• To best of our knowledge, our model is the first to incorporate feature attention in denoising.</p><p>• Most current models connect the weight layers consecutively; and so increasing the depth will not help improve performance <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41]</ref>. Also, such networks can suffer from vanishing gradients <ref type="bibr" target="#b10">[11]</ref>. We present a modular network, where increasing the number of modules helps improve performance.</p><p>• We experiment on three synthetic image datasets and four real-image noise datasets to show that our model achieves state-of-the-art results on synthetic and real images quantitatively and qualitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>In this section, we present and discuss recent trends in the image denoising. Two notable denoising algorithms, NLM <ref type="bibr" target="#b12">[13]</ref> and BM3D <ref type="bibr" target="#b17">[18]</ref>, use self-similar patches. Due to their success, many variants were proposed, including SADCT <ref type="bibr" target="#b26">[27]</ref>, SAPCA <ref type="bibr" target="#b19">[20]</ref>, NLB <ref type="bibr" target="#b36">[37]</ref>, and INLM <ref type="bibr" target="#b28">[29]</ref> which seek self-similar patches in different transform domains. Dictionary-based methods <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b21">22]</ref> enforce sparsity by employing self-similar patches and learning overcomplete dictionaries from clean images. Many algorithms <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b58">59]</ref> investigated the maximum likelihood algorithm to learn a statistical prior, e.g. the Gaussian Mixture Model of natural patches or patch groups for patch restoration. Furthermore, Levin et al. <ref type="bibr" target="#b39">[40]</ref> and Chatterjee et al. <ref type="bibr" target="#b15">[16]</ref>, motivated external denoising <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b61">62]</ref> by showing that an image can be recovered with negligible error by selecting reference patches from a clean external database. However, all of the external algorithms are class-specific.</p><p>Recently, Schmidt et al. <ref type="bibr" target="#b52">[53]</ref> introduced a cascade of shrinkage fields (CSF) which integrated half-quadratic optimization and random-fields. Shrinkage aims to suppress smaller values (noise values) and learn mappings discriminatively. The CSF assumes the data fidelity term to be quadratic and that it has a discrete Fourier transform based closed-form solution.</p><p>Currently, due to the popularity of convolutional neural networks (CNNs), image denoising algorithms <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b7">8]</ref> have achieved a performance boost. Notable denoising neural networks, DnCNN <ref type="bibr" target="#b62">[63]</ref>, and IrCNN <ref type="bibr" target="#b63">[64]</ref> predict the residue present in the image instead of the denoised image as the input to the loss function is ground truth noise as compared to the original clean image. Both networks achieved better results despite having a simple architecture where repeated blocks of convolutional, batch normalization and ReLU activations are used. Furthermore, IrCNN <ref type="bibr" target="#b63">[64]</ref> and DnCNN <ref type="bibr" target="#b62">[63]</ref> are dependent on blindly predicted noise i.e. without taking into account the underlying structures and textures of the noisy image.</p><p>Another essential image restoration framework is Trainable Nonlinear Reaction-Diffusion (TRND) <ref type="bibr" target="#b16">[17]</ref> which uses a field-of-experts prior <ref type="bibr" target="#b51">[52]</ref> into the deep neural network for a specific number of inference steps by extending the non-linear diffusion paradigm into a profoundly trainable parametrized linear filters and the influence functions.</p><p>Although the results of TRND are favorable, the model requires a significant amount of data to learn the parameters and influence functions as well as overall fine-tuning, hyper-parameter determination, and stage-wise training. Similarly, non-local color net (NLNet) <ref type="bibr" target="#b38">[39]</ref> was motivated by non-local self-similar (NSS) priors which employ nonlocal self-similarity coupled with discriminative learning. NLNet improved upon the traditional methods; but, it lags in performance compared to most of the CNNs <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b62">63]</ref> due to the adaptaton of NSS priors, as it is unable to find the analogs for all the patches in the image.</p><p>Building upon the success of DnCNN <ref type="bibr" target="#b62">[63]</ref>, Jiao et al. proposed a network consisting of two stacked subnets, named "FormattingNet" and "DiffResNet" respectively. The architecture of both networks is similar, and the difference lies in the loss layers used. The first subnet employs total variational and perceptual loss while the second one uses 2 loss. The overall model is named as FormResNet and improves upon <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b62">63]</ref> by a small margin. Lately, Bae et al. <ref type="bibr" target="#b9">[10]</ref> employed persistent homology analysis <ref type="bibr" target="#b23">[24]</ref> via wavelet transformed domain to learn the features in CNN denoising. The performance of the model is marginally better compared to <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b34">35]</ref>, which can be attributed to a large number of feature maps employed rather than the model itself. Recently, Anwar et al. introduced CIMM, a deep denoising CNN architecture, composed of identity mapping modules <ref type="bibr" target="#b7">[8]</ref>. The network learns features in cascaded identity modules using dilated kernels and uses self-ensemble to boost performance. CIMM improved upon all the previous CNN models <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b34">35]</ref>.</p><p>Recently, many algorithms focused on blind denoising on real-noisy images <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b11">12]</ref>. The algorithms <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b34">35]</ref> benefitted from the modeling capacity of CNNs and have shown the ability to learn a single-blind denoising model; however, the denoising performance is limited, and the results are not satisfactory on real photographs. Generally speaking, real-noisy image denoising is a two-step process: the first involves noise estimation while the second addresses non-blind denoising. Noise clinic (NC) <ref type="bibr" target="#b37">[38]</ref> estimates the noise model dependent on signal and frequency followed by denoising the image using non-local Bayes (NLB). In comparison, Zhang et al. <ref type="bibr" target="#b64">[65]</ref> proposed a nonblind Gaussian denoising network, termed FFDNet that can produce satisfying results on some of the real noisy images; however, it requires manual intervention to select high noise-level.</p><p>Very recently, CBDNet <ref type="bibr" target="#b30">[31]</ref> trains a blind denoising model for real photographs. CBDNet <ref type="bibr" target="#b30">[31]</ref> is composed of two subnetworks: noise estimation and non-blind denoising. CBDNet <ref type="bibr" target="#b30">[31]</ref> also incorporated multiple losses, is engineered to train on real-synthetic noise and real-image noise and enforces a higher noise standard deviation for low noise images. Furthermore, <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b64">65]</ref> may require manual inter- vention to improve results. On the other hand, we present an end-to-end architecture that learns the noise and produces results on real noisy images without requiring separate subnets or manual intervention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CNN Denoiser</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>Our model is composed of three main modules i.e. feature extraction, feature learning residual on the residual module, and reconstruction, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Let us consider x is a noisy input image andŷ is the denoised output image. Our feature extraction module is composed of only one convolutional layer to extract initial features f 0 from the noisy input:</p><formula xml:id="formula_0">f 0 = M e (x),<label>(1)</label></formula><p>where M e (·) performs convolution on the noisy input image. Next, f 0 is passed on to the feature learning residual on the residual module, termed as M f l ,</p><formula xml:id="formula_1">f r = M f l (f 0 ),<label>(2)</label></formula><p>where f r are the learned features and M f l (·) is the main feature learning residual on the residual component, composed of enhancement attention modules (EAM) that are cascaded together as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Our network has small depth, but provides a wide receptive field through kernel dilation in each EAM initial two branch convolutions. The output features of the final layer are fed to the reconstruction module, which is again composed of one convolutional layer.</p><formula xml:id="formula_2">y = M r (f r ),<label>(3)</label></formula><p>where M r (·) denotes the reconstruction layer.</p><p>There are several choices available as loss function for optimization such as 2 <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b7">8]</ref>, perceptual loss <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b30">31]</ref>, total variation loss <ref type="bibr" target="#b34">[35]</ref> and asymmetric loss <ref type="bibr" target="#b30">[31]</ref>. Some networks <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b30">31]</ref> employs more than one loss to optimize the model, contrary to earlier networks, we only employ one loss i.e. 1 . Now, given a batch of N training pairs,</p><formula xml:id="formula_3">{x i , y i } N i=1</formula><p>, where x is the noisy input and y is the ground truth, the aim is to minimize the 1 loss function as</p><formula xml:id="formula_4">L(W) = 1 N N i=1 ||RIDNet(x i ) − y i || 1 ,<label>(4)</label></formula><p>where RIDNet(·) is our network and W denotes the set of all the network parameters learned. Our feature extraction M e and reconstruction module M r resemble the previous algorithms <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b7">8]</ref>. We now focus on the feature learning residual on the residual block, and feature attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Feature learning Residual on the Residual</head><p>In this section, we provide more details on the enhancement attention modules that uses a Residual on the Residual structure with local skip and short skip connections. Each EAM is further composed of D blocks followed by feature attention. Due to the residual on the residual architecture, very deep networks are now possible that improve denoising performance; however, we restrict our model to four EAM modules only. The first part of EAM covers the full receptive field of input features, followed by learning on the features; then the features are compressed for speed, and finally a feature attention module enhances the weights of important features from the maps. The first part of EAM is realized using a novel merge-and-run unit as shown in <ref type="figure" target="#fig_0">Figure 2</ref> second row. The input features branched and are passed through two dilated convolutions, then concatenated and passed through another convolution. Next, the features are learned using a residual block of two convolutions while compression is achieved by an enhanced residual block (ERB) of three convolutional layers. The last layer of ERB flattens the features by applying a 1×1 kernel. Finally, the output of the feature attention unit is added to the input of EAM. In image recognition, residual blocks <ref type="bibr" target="#b31">[32]</ref> are stacked together to construct a network of more than 1000 layers. Similarly, in image superresolution, EDSR <ref type="bibr" target="#b40">[41]</ref> stacked the residual blocks and used long skip connections (LSC) to form a very deep network. However, to date, very deep networks have not been investigated for denoising. Motivated by the success of <ref type="bibr" target="#b65">[66]</ref>, we introduce the residual on the residual as a basic module for our network to construct deeper systems. Now consider the m-th module of the EAM is given as</p><formula xml:id="formula_5">f m = EAM m (EAM m − 1(· · · (M 0 (f 0 )) · · · )), (5) where f m is the output of the EAM m feature learning module, in other words f m = EAM m (f m−1 ). The out- put of each EAM is added to the input of the group as f m = f m + f m−1 .</formula><p>We have observed that simply cascading the residual modules will not achieve better performance, instead we add the input of the feature extractor module to the final output of the stacked modules as</p><formula xml:id="formula_6">f g = f 0 + M f l (W w,b ),<label>(6)</label></formula><p>where W w,b are the weights and biases learned in the group. This addition i.e. LSC, eases the flow of information across groups. f g is passed to reconstruction layer to output the same number of channels as the input of the network. Furthermore, we use another long skip connection to add the input image to the network output i.e.ŷ = M r (f g ) + x, in order to learn the residual (noise) rather than the denoised image, as this technique helps in faster learning as compared to learning original image due to the sparse representation of the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Feature Attention</head><p>This section provides information about the feature attention mechanism. Attention <ref type="bibr" target="#b59">[60]</ref> has been around for some time; however, it has not been employed in image denoising. Channel features in image denoising methods are treated equally, which is not appropriate for many cases. To exploit and learn the critical content of the image, we focus attention on the relationship between the channel features; hence the name: feature attention (see <ref type="figure" target="#fig_1">Figure 3</ref>). An important question here is how to generate attention differently for each channel-wise feature. Images generally can be considered as having low-frequency regions (smooth or flat areas), and high-frequency regions (e.g., lines edges and texture). As convolutional layers exploit local information only and are unable to utilize global contextual information, we first employ global average pooling to express the statistics denoting the whole image, other options for aggregation of the features can also be explored to represent the image descriptor. Let f c be the output features of the last convolutional layer having c feature maps of size h × w; global average pooling will reduce the size from h × w × c to 1 × 1 × c as:</p><formula xml:id="formula_7">g p = 1 h × w h i=1 w i=1 f c (i, j),<label>(7)</label></formula><p>where f c (i, j) is the feature value at position (i, j) in the feature maps. Furthermore as investigated in <ref type="bibr" target="#b33">[34]</ref>, we propose a selfgating mechanism to capture the channel dependencies from the descriptor retrieved by global average pooling. According to <ref type="bibr" target="#b33">[34]</ref>, the mentioned mechanism must learn the nonlinear synergies between channels as well as mutuallyexclusive relationships. Here, we employ soft-shrinkage and sigmoid functions to implement the gating mechanism. Let us consider δ, and α are the soft-shrinkage and sigmoid operators, respectively. Then the gating mechanism is</p><formula xml:id="formula_8">r c = α(H U (δ(H D (g p )))),<label>(8)</label></formula><p>where H D and H U are the channel reduction and channel upsampling operators, respectively. The output of the global pooling layer g p is convolved with a downsampling Conv layer, activated by the soft-shrinkage function. To differentiate the channel features, the output is then fed into an upsampling Conv layer followed by sigmoid activation. Moreover, to compute the statistics, the output of the sigmoid (r c ) is adaptively rescaled by the input f c of the channel features asf</p><formula xml:id="formula_9">c = r c × f c<label>(9)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation</head><p>Our proposed model contains four EAM blocks. The kernel size for each convolutional layer is set to 3 × 3, except the last Conv layer in the enhanced residual block and those of the features attention units, where the kernel size is 1 × 1. Zero padding is used for 3 × 3 to achieve the same size outputs feature maps. The number of channels for each convolutional layer is fixed at 64, except for feature attention downscaling. A factor of 16 reduces these Conv layers; hence having only four feature maps. The final convolutional layer either outputs three or one feature maps depending on the input. As for running time, our method takes about 0.2 second to process a 512 × 512 image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Long skip connection (LSC) Short skip connection (SSC) Long connection (LC) Feature attention (FA) PSNR (in dB)</head><p>28. <ref type="bibr" target="#b44">45</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training settings</head><p>To generate noisy synthetic images, we employ BSD500 <ref type="bibr" target="#b43">[44]</ref>, DIV2K <ref type="bibr" target="#b3">[4]</ref>, and MIT-Adobe FiveK <ref type="bibr" target="#b14">[15]</ref>, resulting in 4k images while for real noisy images, we use cropped patches of 512×512 from SSID <ref type="bibr" target="#b0">[1]</ref>, Poly <ref type="bibr" target="#b54">[55]</ref>, and RENOIR <ref type="bibr" target="#b5">[6]</ref>. Data augmentation is performed on training images, which includes random rotations of 90 • , 180 • , 270 • and flipping horizontally. In each training batch, 32 patches are extracted as inputs with a size of 80 × 80. Adam <ref type="bibr" target="#b35">[36]</ref> is used as the optimizer with default parameters. The learning rate is initially set to 10 −4 and then halved after 10 5 iterations. The network is implemented in the Pytorch <ref type="bibr" target="#b46">[47]</ref> framework and trained with an Nvidia Tesla V100 GPU. Furthermore, we use PSNR as evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Influence of the skip connections</head><p>Skip connections play a crucial role in our network. Here, we demonstrate the effectiveness of the skip connections. Our model is composed of three basic types of connections which includes long skip connection (LSC), short skip connections (SSC), and local connections (LC). <ref type="table">Table 1</ref> shows the average PSNR for the BSD68 <ref type="bibr" target="#b51">[52]</ref> dataset. The highest performance is obtained when all the skip connections are available while the performance is lower when any connection is absent. We also observed that increasing the depth of the network in the absence of skip connections does not benefit performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Feature-attention</head><p>Another important aspect of our network is feature attention. <ref type="table">Table 1</ref> compares the PSNR values of the networks with and without feature attention. The results support our claim about the benefit of using feature attention. Since the inception of DnCNN <ref type="bibr" target="#b62">[63]</ref>, the CNN models have matured, and further performance improvement requires the careful design of blocks and rescaling of the feature maps. The two mentioned characteristics are present in our model in the form of feature-attention and the skip connections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparisons</head><p>We evaluate our algorithm using the Peak Signalto-Noise Ratio (PSNR) index as the error metric and compare against many state-of-the-art competitive algorithms which include traditional methods i.e. CBM3D <ref type="bibr" target="#b18">[19]</ref>, WNNM <ref type="bibr" target="#b29">[30]</ref>, EPLL <ref type="bibr" target="#b66">[67]</ref>, CSF <ref type="bibr" target="#b52">[53]</ref> and CNN-based denoisers i.e. MLP <ref type="bibr" target="#b13">[14]</ref>, TNRD <ref type="bibr" target="#b16">[17]</ref>, DnCNN <ref type="bibr" target="#b62">[63]</ref>, IrCNN <ref type="bibr" target="#b63">[64]</ref>, CNLNet <ref type="bibr" target="#b38">[39]</ref>, FFDNet <ref type="bibr" target="#b64">[65]</ref> and CBDNet <ref type="bibr" target="#b30">[31]</ref>. To be fair in comparison, we use the default setting of the traditional methods provided by the corresponding authors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Test Datasets</head><p>In the experiments, we test four noisy real-world datasets i.e. RNI15 <ref type="bibr" target="#b37">[38]</ref>, DND <ref type="bibr" target="#b48">[49]</ref>, Nam <ref type="bibr" target="#b44">[45]</ref> and SSID <ref type="bibr" target="#b0">[1]</ref>. Furthermore, we prepare three synthetic noisy datasets from the widely used 12 classical images, BSD68 <ref type="bibr" target="#b51">[52]</ref> color and gray 68 images for testing. We corrupt the clean images by additive white Gaussian noise using noise sigma of 15, 25 and 50 standard deviations.</p><p>• RNI15 <ref type="bibr" target="#b37">[38]</ref> provides 15 real-world noisy images. Unfortunately, the clean images are not given for this dataset; therefore, only the qualitative comparison is presented for this dataset.</p><p>• Nam <ref type="bibr" target="#b44">[45]</ref> comprises of 11 static scenes and the corresponding noise-free images obtained by the mean of 500 noisy images of the same scene. The size of the images are enormous; hence, we cropped the images in 512 × 512 patches and randomly selected 110 from those for testing.  not released yet. We will use the validation images for testing our algorithm and the competitive methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Grayscale noisy images</head><p>In this subsection, we evaluate our model on the noisy grayscale images corrupted by spatially invariant additive white Gaussian noise. We compare against nonlocal self-similarity representative models i.e. BM3D <ref type="bibr" target="#b17">[18]</ref> and WNNM <ref type="bibr" target="#b29">[30]</ref>, learning based methods i.e. EPLL, TNRD <ref type="bibr" target="#b16">[17]</ref>, MLP <ref type="bibr" target="#b13">[14]</ref>, DnCNN <ref type="bibr" target="#b62">[63]</ref>, IrCNN <ref type="bibr" target="#b63">[64]</ref>, and CSF <ref type="bibr" target="#b52">[53]</ref>. In <ref type="table" target="#tab_1">Tables 3 and 2</ref>, we present the PSNR values on Set12 and BSD68. It is to be remembered here that BSD500 <ref type="bibr" target="#b43">[44]</ref> and BSD68 <ref type="bibr" target="#b51">[52]</ref> are two disjoint sets. Our method outperforms all the competitive algorithms on both datasets for all noise levels; this may be due to the larger receptive field as well as better modeling capacity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Color noisy images</head><p>Next, for noisy color image denoising, we keep all the parameters of the network similar to the grayscale model, except the first and last layer are changed to input and output three channels rather than one. <ref type="figure">Figure 4</ref> presents the visual comparison and <ref type="table">Table 4</ref> reports the PSNR numbers between our methods and the alternative algorithms. Our algorithm consistently outperforms all the other techniques published in <ref type="table">Table 4</ref> for CBSD68 dataset <ref type="bibr" target="#b51">[52]</ref>. Similarly, our network produces the best perceptual quality images as shown in <ref type="figure">Figure 4</ref>. A closer inspection on the vase reveals that our 31.68dB 32.21dB Noisy BM3D <ref type="bibr" target="#b18">[19]</ref> IRCNN <ref type="bibr" target="#b63">[64]</ref> 32.33dB 32.84dB DnCNN <ref type="bibr" target="#b62">[63]</ref> Ours GT <ref type="figure">Figure 4</ref>. Denoising performance of our RIDNet versus state-ofthe-art methods on a color images from <ref type="bibr" target="#b51">[52]</ref> for σn = 50 network generates textures closest to the ground-truth with fewer artifacts and more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Real-World noisy images</head><p>To further assess the practicality of our model, we employ a real noise dataset. The evaluation is difficult because of the unknown level of noise, the various noise sources such as shot noise, quantization noise etc., imaging pipeline i.e. image resizing, lossy compression etc. Furthermore, the noise is spatially variant (non-Gaussian) and also signal dependent; hence, the assumption that noise is spatially invariant, employed by many algorithms does not hold for real image noise. Therefore, real-noisy images evaluation determines the success of the algorithms in real-world applications.</p><p>DnD: <ref type="table" target="#tab_4">Table 5</ref> presents the quantitative results (PSNR/SSIM) on the sRGB data for competitive algorithms and our method obtained from the online DnD benchmark website available publicly. The blind Gaussian denoiser DnCNN <ref type="bibr" target="#b62">[63]</ref> performs inefficiently and is unable to achieve better results than BM3D <ref type="bibr" target="#b17">[18]</ref> and WNNM <ref type="bibr" target="#b29">[30]</ref> due to the poor generalization of the noise during training. Similarly, the non-blind Gaussian traditional denoisers are able to report limited performance, although the noise standard-deviation is provided. This may be due to the fact that these denoisers <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b66">67]</ref> are tailored for AWGN only and real-noise is different in characteristics to syn-Noise Methods Levels CBM3D <ref type="bibr" target="#b18">[19]</ref> MLP <ref type="bibr" target="#b13">[14]</ref> TNRD <ref type="bibr" target="#b16">[17]</ref> DnCNN <ref type="bibr" target="#b62">[63]</ref> IrCNN <ref type="bibr" target="#b63">[64]</ref> CNLNet <ref type="bibr" target="#b38">[39]</ref>  FFDNet <ref type="bibr" target="#b64">[65]</ref> CBDNet <ref type="bibr" target="#b30">[31]</ref> RIDNet (Ours) <ref type="figure">Figure 5</ref>. A real noisy example from DND dataset <ref type="bibr" target="#b48">[49]</ref> for comparison of our method against the state-of-the-art algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Blind/Non-blind PSNR SSIM CDnCNNB <ref type="bibr" target="#b62">[63]</ref> Blind 32.43 0.7900 EPLL <ref type="bibr" target="#b66">[67]</ref> Non-blind 33.51 0.8244 TNRD <ref type="bibr" target="#b16">[17]</ref> Non-blind 33.65 0.8306 NCSR <ref type="bibr" target="#b22">[23]</ref> Non-blind 34.05 0.8351 MLP <ref type="bibr" target="#b13">[14]</ref> Non-blind 34.23 0.8331 FFDNet <ref type="bibr" target="#b64">[65]</ref> Non-blind 34.40 0.8474 BM3D <ref type="bibr" target="#b17">[18]</ref> Non-blind 34.51 0.8507 FoE <ref type="bibr" target="#b51">[52]</ref> Non-blind 34.62 0.8845 WNNM <ref type="bibr" target="#b29">[30]</ref> Non-blind 34.67 0.8646 NC <ref type="bibr" target="#b37">[38]</ref> Blind 35.43 0.8841 NI <ref type="bibr" target="#b1">[2]</ref> Blind 35.11 0.8778 CIMM <ref type="bibr" target="#b7">[8]</ref> Non-blind 36.04 0.9136 KSVD <ref type="bibr" target="#b4">[5]</ref> Non-blind 36.49 0.8978 MCWNNM <ref type="bibr" target="#b57">[58]</ref> Non-blind 37.38 0.9294 TWSC <ref type="bibr" target="#b56">[57]</ref> Non-blind 37.96 0.9416 FFDNet+ <ref type="bibr" target="#b64">[65]</ref> Non-blind 37.61 0.9415 CBDNet <ref type="bibr" target="#b30">[31]</ref> Blind 38.06 0.9421 RIDNET (Ours) Blind 39.23 0.9526 (i.e. total variation, 2 and asymmetric learning) and both real-noise as well as synthetically generated real-noise. As reported by the author of CBDNet <ref type="bibr" target="#b30">[31]</ref>, it is able to achieve 37.72 dB with real-noise images only. Noise Clinic (NC) <ref type="bibr" target="#b37">[38]</ref> and Neat Image (NI) <ref type="bibr" target="#b1">[2]</ref> are the other two state-of-the-art blind denoisers other than <ref type="bibr" target="#b30">[31]</ref>. NI <ref type="bibr" target="#b1">[2]</ref> is commercially available as a part of Photoshop and Corel PaintShop. Our network is able to achieve 3.82dB and 4.14dB more PSNR from NC <ref type="bibr" target="#b37">[38]</ref> and NI <ref type="bibr" target="#b1">[2]</ref>, respectively. Next, we visually compare the result of our method with the competing methods on the denoised images provided by the online system of Plotz et al. <ref type="bibr" target="#b48">[49]</ref> in <ref type="figure">Figure 5</ref>. The PSNR and SSIM values are also taken from the website. From <ref type="figure">Figure 5</ref>, it is clear that the methods of <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b62">63]</ref> perform poorly in removing the noise from the star and in some cases the image is over-smoothed, on the other hand, our algorithm can eliminate the noise while preserving the finer details and structures in the star image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noisy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DnCNN</head><p>FFDNet Ours <ref type="figure">Figure 7</ref>. A real high noise example from RNI15 dataset <ref type="bibr" target="#b37">[38]</ref>. Our method is able to remove the noise in textured and smooth areas without introducing artifacts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Datasets</head><p>BM3D DnCNN FFDNet CBDNet Ours Nam <ref type="bibr" target="#b44">[45]</ref> 37 RNI15: On RNI15 <ref type="bibr" target="#b37">[38]</ref>, we provide qualitative images only as the ground-truth images are not available. <ref type="figure">Figure 6</ref> presents the denoising results on a low noise intensity image. FFDNet <ref type="bibr" target="#b64">[65]</ref> and CBDNet <ref type="bibr" target="#b30">[31]</ref> are unable to remove the noise in its totality as can been seen near the bottom left of handle and body of the cup image. On the contrary, our method is able to remove the noise without the introduction of any artifacts. We present another example from the RNI15 dataset <ref type="bibr" target="#b37">[38]</ref> with high noise in <ref type="figure">Figure 7</ref>. CD-nCNN <ref type="bibr" target="#b62">[63]</ref> and FFDNet <ref type="bibr" target="#b64">[65]</ref> produce results of limited nature as some noisy elements can be seen in the near the eye and gloves of the Dog image. In comparison, our algorithm recovers the actual texture and structures without compromising on the removal of noise from the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Nam:</head><p>We present the average PSNR scores of the resultant denoised images in <ref type="table">Table 6</ref>. Unlike CBDNet <ref type="bibr" target="#b30">[31]</ref>, which is trained on Nam <ref type="bibr" target="#b44">[45]</ref> to specifically deal with the JPEG compression, we use the same network to denoise the Nam images <ref type="bibr" target="#b44">[45]</ref> and achieve favorable PSNR numbers. Our performance in terms of PSNR is higher than any of the current state-of-the-art algorithms. Furthermore, our claim is supported by the visual quality of the images produced by our model as shown in <ref type="figure">Figure 8</ref>. The amount of noise present after denoising by our method is negligible as compared to CDnCNN and other counterparts. SSID: As a last dataset, we employ the SSID real noise dataset which has the highest number of test (validation) images available. The results in terms of PSNR are shown in the second row of <ref type="table">Table 6</ref>. Again, it is clear that our method outperforms FFDNet <ref type="bibr" target="#b64">[65]</ref> and CBDNet <ref type="bibr" target="#b30">[31]</ref> by a margin of 9.5dB and 7.93dB, respectively. In <ref type="figure">Figure 9</ref>, we show the denoised results of a challenging image by different algorithms. Our technique recovers the true colors which are closer to the original pixel values while competing methods CBDNet Ours GT <ref type="figure">Figure 9</ref>. A challenging example from SSID dataset <ref type="bibr" target="#b0">[1]</ref>. Our method can remove noise and restore true colors.</p><p>are unable to restore original colors and in specific regions induce false colors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a new CNN denoising model for synthetic noise and real noisy photographs. Unlike previous algorithms, our model is a single-blind denoising network for real noisy images. We propose a novel restoration module to learn the features and to enhance the capability of the network further; we adopt feature attention to rescale the channel-wise features by taking into account the dependencies between the channels. We also use LSC, SSC, and SC to allow low-frequency information to bypass so the network can focus on residual learning. Extensive experiments on three synthetic and four real-noise datasets demonstrate the effectiveness of our proposed model. This work was supported in part by NH&amp;MRC Project grant # 1082358.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>The architecture of the proposed network. Different green colors of the conv layers denote different dilations while the smaller size of the conv layer means the kernel is 1 × 1. The second row shows the architecture of each EAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>The feature attention mechanism for selecting the essential features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>28.77 28.81 28.86 28.52 28.85 28.86 28.90 28.96Table 1. Investigation of skip connections and feature attention. The best result in PSNR (dB) on values on BSD68<ref type="bibr" target="#b51">[52]</ref> in 2×10 5 iterations is presented.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>•Table 2 .</head><label>2</label><figDesc>DnD is recently proposed by Plotz et al.<ref type="bibr" target="#b48">[49]</ref> which originally contains 50 pairs of real-world noisy and noise-free scenes. The scenes are further cropped into patches of size 512 × 512 by the providers of the dataset which resulted in 1000 smaller images. The near noise-free images are not publicly available, and the results (PSNR/SSIM) can only be obtained through the online system introduced by<ref type="bibr" target="#b48">[49]</ref>. The similarity between the denoised and the clean images of BSD68 dataset<ref type="bibr" target="#b51">[52]</ref> for our method and competing measured in terms of average PSNR for σ=15, 25, and 50 on grayscale images.</figDesc><table><row><cell>Noise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Methods</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">Level BM3D WNNM EPLL TNRD DenoiseNet DnCNN IrCNN NLNet FFDNet Ours</cell></row><row><cell>15</cell><cell>31.08</cell><cell>31.32</cell><cell>31.19</cell><cell>31.42</cell><cell>31.44</cell><cell>31.73</cell><cell>31.63</cell><cell>31.52</cell><cell>31.63</cell><cell>31.81</cell></row><row><cell>25</cell><cell>28.57</cell><cell>28.83</cell><cell>28.68</cell><cell>28.92</cell><cell>29.04</cell><cell>29.23</cell><cell>29.15</cell><cell>29.03</cell><cell>29.23</cell><cell>29.34</cell></row><row><cell>50</cell><cell>25.62</cell><cell>25.83</cell><cell>25.67</cell><cell>26.01</cell><cell>26.06</cell><cell>26.23</cell><cell>26.19</cell><cell>26.07</cell><cell>26.29</cell><cell>26.40</cell></row><row><cell cols="2">Methods</cell><cell cols="3">σ = 15 σ = 25 σ = 50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">BM3D [18]</cell><cell>32.37</cell><cell>29.97</cell><cell>26.72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">WNNM [30]</cell><cell>32.70</cell><cell>30.26</cell><cell>27.05</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">EPLL [67]</cell><cell>32.14</cell><cell>29.69</cell><cell>26.47</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">MLP [14]</cell><cell>-</cell><cell>30.03</cell><cell>26.78</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">CSF [53]</cell><cell>32.32</cell><cell>29.84</cell><cell>-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">TNRD [17]</cell><cell>32.50</cell><cell>30.06</cell><cell>26.81</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">DnCNN [63]</cell><cell>32.86</cell><cell>30.44</cell><cell>27.18</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">IrCNN [64]</cell><cell>32.77</cell><cell>30.38</cell><cell>27.14</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">FFDNet [65]</cell><cell>32.75</cell><cell>30.43</cell><cell>27.32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell>32.91</cell><cell>30.60</cell><cell>27.43</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">• SSID [1] (Smartphone Image Denoising Dataset) is re-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">cently introduced. The authors have collected 30k real</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">noisy images and their corresponding clean images;</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">however, only 320 images are released for training and</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">1280 images pairs for validation, as testing images are</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table /><note>The quantitative comparison between denoising algo- rithms on 12 classical images, (in terms of PSNR). The best results are highlighted as bold.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>The Mean PSNR and SSIM denoising results of state-ofthe-art algorithms evaluated on the DnD sRGB images<ref type="bibr" target="#b48">[49]</ref> thetic noise. Incorporating feature attention and capturing the appropriate characteristics of the noise through a novel module means our algorithm leads by large margin i.e. 1.17dB PSNR compared to the second performing method, CBDNet<ref type="bibr" target="#b30">[31]</ref>. Furthermore, our algorithm only employs real-noisy images for training using only 1 loss while CBDNet<ref type="bibr" target="#b30">[31]</ref> uses many techniques such as multiple losses</figDesc><table><row><cell>Noisy</cell><cell>FFDNet</cell><cell>CBDNet</cell><cell>RIDNet</cell></row><row><cell cols="4">Figure 6. Comparison of our method against the other methods</cell></row><row><cell cols="4">on a real image from RNI15 [38] benchmark containing spatially</cell></row><row><cell>variant noise.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A high-quality denoising dataset for smartphone cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdelrahman</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">ABSoft. Neat image</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast image recovery using variable splitting and constrained optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manya V Afonso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>José</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bioucas-Dias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mário</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An algorithm for designing overcomplete dictionaries for sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Aharon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><surname>Bruckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ksvd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Renoir-a dataset for real low-light image noise reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josue</forename><surname>Anaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Barbu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Combined internal and external category-specific image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Phouc</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.02933</idno>
		<title level="m">Chaining identity mapping modules for image denoising</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Category-specific object image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Anwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong Phuoc</forename><surname>Huynh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Beyond deep residual learning for image restoration: Persistent homologyguided manifold simplification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Woong</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaejun</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong Chul</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNN</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unprocessing images for learned raw denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Brooks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Mildenhall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dillon</forename><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A non-local algorithm for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bartomeu</forename><surname>Coll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image denoising: Can plain neural networks compete with bm3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harold</forename><forename type="middle">Christopher</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Schuler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning photographic global tonal adjustment with a database of input/output image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Bychkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédo</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Is denoising dead?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Trainable nonlinear reaction diffusion: A flexible framework for fast and effective image restoration. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Pock</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Image denoising by sparse 3-D transformdomain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Color image denoising via sparse 3-D collaborative filtering with grouping constraint in luminancechrominance space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostadin</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">BM3D image denoising with shape-adaptive principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing with Adaptive Sparse Structured Representations</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Image super-resolution using deep convolutional networks. TPAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sparsity-based image denoising via dictionary learning and structural clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisheng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Nonlocally centralized sparse representation for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weisheng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Persistent homology-a survey. Contemporary mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Edelsbrunner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Harer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Example-based regularization deployed to super-resolution reconstruction of a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Datsenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. J</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">External Patch Prior Guided Internal Clustering for Image Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pointwise shape-adaptive DCT for high-quality denoising and deblocking of grayscale and color images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Digital image processing (book)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wintz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematics and Computation</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1977" />
			<publisher>Addison-Wesley Publishing Co., Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aleksandra Pizurica, and Wilfried Philips. An improved non-local denoising algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Goossens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiêp</forename><surname>Luong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Weighted nuclear norm minimization with application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangchu</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Toward convolutional blind denoising of real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifei</forename><surname>Shi Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.04686</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Flexisp: A flexible camera image processing framework. TOG</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Heide</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Steinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Ta</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mushfiqur</forename><surname>Rouf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawid</forename><surname>Pajak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dikpal</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orazio</forename><surname>Gallo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Heidrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Egiazarian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Formresnet: Formatted residual learning for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengfeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rynson Wh</forename><surname>Lau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A nonlocal bayesian image denoising algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoni</forename><surname>Buades</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">The noise clinic: a blind image denoising algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Lebrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Colom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Non-local color image denoising with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Natural image denoising: Optimality and inherent bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Adaptive image denoising by targeted databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Truong Q Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-local sparse models for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillermo</forename><surname>Sapiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A holistic approach to cross-channel image noise modeling and its application to image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonghyeon</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngbae</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasuyuki</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">An iterative regularization method for total variation-based image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Goldfarb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rasl: Robust alignment by sparse and low-rank decomposition for linearly correlated images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yigang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Benchmarking denoising algorithms with real photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Plötz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01313</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Neural nearest neighbors networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Plötz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The little engine that could: Regularization by denoising (red)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaniv</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyman</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Shrinkage fields for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">What makes a good model of natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William T Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhetong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02603</idno>
		<title level="m">Real-world noisy image denoising: A new benchmark</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Iterative regularization and nonlinear inverse scale space applied to wavelet-based denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanley</forename><surname>Osher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A trilateral weighted sparse coding scheme for real-world image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multi-channel weighted nuclear norm minimization for real color image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangchu</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Patch Group Based Nonlocal Self-Similarity Prior Learning for Image Denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangchu</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Cid: Combined image denoising in spatial and frequency domains using web images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Image denoising by exploring external and internal correlations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunjin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Learning deep cnn denoiser prior for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Ffdnet: Toward a fast and flexible solution for cnn-based image denoising. TIP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangmeng</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.02758</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">From learning models of natural image patches to whole image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
