<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ACCEPTED TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Unsupervised Tracklet Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minxian</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaogang</forename><surname>Gong</surname></persName>
						</author>
						<title level="a" type="main">ACCEPTED TO APPEAR IN IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Unsupervised Tracklet Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-26T13:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Person Re-Identification</term>
					<term>Unsupervised Tracklet Association</term>
					<term>Trajectory Fragmentation</term>
					<term>Multi-Task Deep Learning !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Most existing person re-identification (re-id) methods rely on supervised model learning on per-camera-pair manually labelled pairwise training data. This leads to poor scalability in a practical re-id deployment, due to the lack of exhaustive identity labelling of positive and negative image pairs for every camera-pair. In this work, we present an unsupervised re-id deep learning approach. It is capable of incrementally discovering and exploiting the underlying re-id discriminative information from automatically generated person tracklet data end-to-end. We formulate an Unsupervised Tracklet Association Learning (UTAL) framework. This is by jointly learning within-camera tracklet discrimination and cross-camera tracklet association in order to maximise the discovery of tracklet identity matching both within and across camera views. Extensive experiments demonstrate the superiority of the proposed model over the state-of-the-art unsupervised learning and domain adaptation person re-id methods on eight benchmarking datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>P ERSON re-identification (re-id) aims to match the underlying identity classes of person bounding box images detected from non-overlapping camera views <ref type="bibr" target="#b0">[1]</ref>. In recent years, extensive research has been carried out on re-id <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>. Most existing person re-id methods, in particular neural network deep learning models, adopt the supervised learning approach. Supervised deep models assume the availability of a large number of manually labelled cross-view identity matching image pairs for each camera pair. This enables deriving a feature representation and/or a distance metric function optimised for each camera-pair. Such an assumption is inherently limited for generalising a person re-id model to many different camera networks. This is because, exhaustive manual identity (ID) labelling of positive and negative person image pairs for every camera-pair is prohibitively expensive, given that there are a quadratic number of camera pairs in a surveillance network.</p><p>It is no surprise that person re-id by unsupervised learning become a focus in recent research. In this setting, percamera-pair ID labelled training data is no longer required <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>. However, existing unsupervised learning re-id models are significantly inferior in re-id accuracy. This is because, lacking cross-view pairwise ID labelled data deprives a model's ability to learn strong discriminative information. This nevertheless is critical for handling significant appearance change across cameras.</p><p>An alternative approach is to leverage jointly (1) unlabelled data from a target domain which is freely available, e.g. videos of thousands of people travelling through a camera view everyday in a public scene, and (2) pairwise ID labelled datasets from independent source domains <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. The main idea is to first learn a "view-invariant" representation from ID labelled source data, then adapt the pre-learned model to a target domain by using only unlabelled target data. This approach makes an implicit assumption that, the source and target domains share some common cross-view characteristics so that a view-invariant representation can be estimated. This is not always true.</p><p>In this work, we consider a pure unsupervised person re-id deep learning problem. That is, no ID labelled training data are assumed, neither cross-view nor within-view ID labelling. Although this learning objective shares some modelling spirit with two recent domain transfer models <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b18">19]</ref>, both those models do require suitable person ID labelled source domain training data, i.e. visually similar to the target domain. Specifically, we consider unsupervised re-id model learning by jointly optimising unlabelled person tracklet data within-camera view to be more discriminative and cross-camera view to be more associative end-to-end.</p><p>Our contributions are: We formulate a novel unsupervised person re-id deep learning method using automatically generated person tracklets. This avoids the need for camera pairwise ID labelled training data, i.e. unsupervised tracklet re-id discriminative learning. Specifically, we propose a Unsupervised Tracklet Association Learning (UTAL) model with two key ideas: (1) Per-Camera Tracklet Discrimination Learning that optimises "local" within-camera tracklet label discrimination. It aims to facilitate cross-camera tracklet association given per-camera independently created tracklet label spaces. (2) Cross-Camera Tracklet Association Learning that optimises "global" cross-camera tracklet matching. It aims to find cross-view tracklet groupings that are most likely of the same person identities without ID label information. This is formulated as to jointly discriminate within-camera tracklet identity semantics and self-discover cross-camera tracklet pairwise matching in end-to-end deep learning. Critically, the proposed UTAL method does not assume any domain-specific knowledge such as camera spacetime topology and cross-camera ID overlap. Therefore, it is scalable to arbitrary surveillance camera networks with unknown viewing conditions and background clutters.</p><p>Extensive comparative experiments are conducted on arXiv:1903.00535v1 [cs.CV] 1 Mar 2019 seven existing benchmarking datasets (CUHK03 <ref type="bibr" target="#b20">[21]</ref>, Market-1501 <ref type="bibr" target="#b21">[22]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, MSMT17 <ref type="bibr" target="#b3">[4]</ref>, iLIDS-VID <ref type="bibr" target="#b24">[25]</ref>, PRID2011 <ref type="bibr" target="#b25">[26]</ref>, MARS <ref type="bibr" target="#b26">[27]</ref>) and one newly introduced tracklet person re-id dataset called DukeTracklet.</p><p>The results show the performance advantages and superiority of the proposed UTAL method over the state-of-the-art unsupervised and domain adaptation person re-id models. A preliminary version of this work was reported in <ref type="bibr" target="#b27">[28]</ref>. Compared with the earlier study, there are a few key differences introduced: (i) This study presents a more principled and scalable unsupervised tracklet learning method that learns deep neural network re-id models directly from large scale raw tracklet data. The method in <ref type="bibr" target="#b27">[28]</ref> requires a separate preprocessing for domain-specific spatio-temporal tracklet sampling for reducing the tracklet ID duplication rate per camera view. This need for pre-sampling not only makes model learning more complex, not-end-to-end therefore suboptimal, but also loses a large number of tracklets with potential rich information useful for more effective model learning. (ii) We propose in this study a new concept of soft tracklet labelling, which aims to explore any inherent space-time visual correlation of the same person ID between unlabelled tracklets within each camera view. This is designed to better address the tracklet fragmentation problem through an end-to-end model optimisation mechanism. It improves person tracking within individual camera views, which is lacking in <ref type="bibr" target="#b27">[28]</ref>. (iii) Unlike the earlier method, the current model self-discovers and exploits explicit crosscamera tracklet association in terms of person ID, improving the capability of re-id discriminative unsupervised learning and leading to superior model performances. (iv) Besides creating a new tracklet person re-id dataset, we further conduct more comprehensive evaluations and analyses for giving useful and significant insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Person Re-Identification. Most existing person re-id models are built by supervised model learning on a separate set of per-camera-pair ID labelled training data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref>. While having no class intersection, the training and testing data are often assumed to be drawn from the same camera network (domain). Their scalability is therefore significantly poor for realistic applications when no such large training sets are available for every camera-pair in a test domain. Human-in-the-loop re-id provides a means of reducing the overall amount of training label supervision by exploring the benefits of human-computer interaction <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. But, it is still labour intensive and tedious. Human labellers need to be deployed repeatedly for conducting similar screen profiling operations whenever a new target domain exhibits. It is therefore not scalable either.</p><p>Unsupervised model learning is an intuitive solution to avoiding the need of exhaustively collecting a large set of labelled training data per application domain. However, previous hand-crafted features-based unsupervised learning methods offer significantly inferior re-id matching performance <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>, when compared to the supervised learning models. A trade-off between reid model scalability and generalisation performance can be achieved by semi-supervised learning <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b33">34]</ref>. But these models still assume sufficiently large sized cross-view pairwise ID labelled data for model training.</p><p>There are attempts on unsupervised learning by domain adaptation <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. The idea is to exploit the knowledge of labelled data in "related" source domains through model adaptation on the unlabelled target domain data. One straightforward approach is to convert the source ID labelled training data into the target domain by appearance mimicry. This enables to train a model using the domain style transformed source training data via supervised learning <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b35">36]</ref>. Alternative techniques include semantic attribute knowledge transfer <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b37">38]</ref>, space-time pattern transfer <ref type="bibr" target="#b38">[39]</ref>, virtual ID synthesis <ref type="bibr" target="#b39">[40]</ref>, and progressive adaptation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. While these models perform better than the earlier generation of methods <ref type="table" target="#tab_1">(Tables 2 and 3)</ref>, they require similar data distributions and viewing conditions between the labelled source domain and the unlabelled target domain. This restricts their scalability to arbitrarily diverse (and unknown) target domains in large scale deployments.</p><p>Unlike all existing unsupervised learning re-id methods, the proposed tracklet association method in this work enables unsupervised re-id deep learning from scratch at endto-end. This is more scalable and general. Because there is no assumption on either the scene characteristic similarity between source and target domains, or the complexity of handling ID label knowledge transfer. Our method directly learns to discover the re-id discriminative knowledge from unlabelled tracklet data automatically generated.</p><p>Moreover, the proposed method does not assume any overlap of person ID classes across camera views or other domain-specific information. It is therefore scalable to the scenarios without any knowledge about camera space-time topology <ref type="bibr" target="#b38">[39]</ref>. Unlike the existing unsupervised learning method relying on extra hand-crafted features, our model learns tracklet based re-id discriminative features from an end-to-end deep learning process. To our best knowledge, this is the first attempt at unsupervised tracklet association based person re-id deep learning model without relying on any ID labelled training video or imagery data. Multi-Task Learning in Neural Networks. Multi-task learning (MTL) is a machine learning strategy that learns several related tasks simultaneously for their mutual benefits <ref type="bibr" target="#b40">[41]</ref>. A good MTL survey with focus on neural networks is provided in <ref type="bibr" target="#b41">[42]</ref>. Deep CNNs are well suited for performing MTL. As they are inherently designed to learn joint feature representations subject to multiple label objectives concurrently in multi-branch architectures. Joint learning of multiple related tasks has been proven to be effective in solving computer vision problems <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b43">44]</ref>.</p><p>In contrast to all the existing methods aiming for supervised learning problems, the proposed UTAL method exploits differently the MTL principle to solve an unsupervised learning task. Critically, our method is uniquely designed to explore the potential of MTL in correlating the underlying group level semantic relationships between different individual learning tasks 1 . This dramatically differs from existing MTL based methods focusing on mining the shared knowledge among tasks at the sample level. <ref type="bibr" target="#b0">1</ref>. In the unsupervised tracklet person re-id context, a group corresponds to a set of categorical labels each associated with an individual person tracklet drawn from a specific camera view.</p><p>Critically, it avoids the simultaneous labelling of multi-tasks on each training sample. Sample-wise multi-task labels are not available in the unsupervised tracklet re-id problem.</p><p>Besides, unsupervised tracklet labels in each task (camera view) are noisy. As they are obtained without manual verification. Hence, the proposed UTAL model is in effect performing weakly supervised multi-task learning with noisy per-task labels. This makes our method fundamentally different from existing MTL approaches that are only interested in discovering discriminative cross-task common representations by strongly supervised learning of clean and exhaustive sample-wise multi-task labelling. Unsupervised Deep Learning. Unsupervised learning of visual data is a long standing research problem starting from the auto-encoder models <ref type="bibr" target="#b44">[45]</ref> or earlier. Recently, this problem has regained attention in deep learning. One common approach is by incorporating with data clustering <ref type="bibr" target="#b45">[46]</ref> that jointly learns deep feature representations and image clusters. Alternative unsupervised learning techniques include formulating generative models <ref type="bibr" target="#b34">[35]</ref>, devising a loss function that preserves information flowing <ref type="bibr" target="#b46">[47]</ref> or discriminates instance classes <ref type="bibr" target="#b47">[48]</ref>, exploiting the object tracking continuity cue <ref type="bibr" target="#b48">[49]</ref> in unlabelled videos, and so forth.</p><p>As opposite to all these methods focusing on uni-domain data distributions, our method is designed particularly to learn visual data sampled from different camera domains with unconstrained viewing settings. Conceptually, the proposed idea of soft tracklet labels is related to data clustering. As the per-camera affinity matrix used for soft label inference is related to the underlying data cluster structure. However, our affinity based method has the unique merits of avoiding per-domain hard clustering and having fewer parameters to tune (e.g. the per-camera cluster number).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD FORMULATION</head><p>To overcome the limitation of supervised model learning algorithms for exhaustive within-camera and cross-camera ID labelling, we propose a novel Unsupervised Tracklet Association Learning (UTAL) method to person re-id in videos (or multi-shot images in general). This is achieved by exploiting person tracklet labelling obtained from existing trackers 2 without any ID labelling either cross-camera or withincamera. The UTAL learns a person re-id model end-to-end therefore benefiting from joint overall model optimisation in deep learning. In the follows, we first present unsupervised per-camera tracklet labelling (Sec. 3.1), then describe our model design for within-camera and cross-camera tracklet association by joint unsupervised deep learning (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Unsupervised Per-Camera Tracklet Formation</head><p>Given a large quantity of video data captured by disjoint surveillance cameras, we first deploy the off-the-shelf pedestrian detection and tracking models <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref> to automatically extract person tracklets. We then annotate each tracklet S with a unique class (one-hot) label y in an unsupervised 2. Although object tracklets can be generated by any independent single-camera-view multi-object tracking (MOT) models widely available currently, a conventional MOT model is not end-to-end optimised for cross-camera tracklet association. and camera-independent manner. This does not involve any manual ID verification on tracklets. By applying this tracklet labelling method in each camera view separately, we can obtain an independent set of labelled tracklets {S i , y i } per camera, where each tracklet S contains a varying number of person bounding box images I as S = {I 1 , I 2 , · · · }. Challenges. To effectively learn a person re-id model from such automatically labelled tracklet training data, we need to deal with two modelling challenges centred around the supervision of person ID class labels: (1) Due to frequent trajectory fragmentation, multiple tracklets (unknown due to no manual verification) are often generated during the appearing period of a person under one camera view. However, they are unsupervisedly assigned with different onehot categorical labels. This may significantly mislead the discriminative learning process of a re-id model. <ref type="bibr" target="#b1">(2)</ref> There are no access to positive and negative pairwise ID correspondence between tracklet labels across disjoint camera views. Lacking cross-camera person ID supervision underpins one of the key challenges in unsupervised tracklet person re-id.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unsupervised Tracklet Association</head><p>Given per-camera independent tracklets {S i , y i }, we explore tracklet label re-id discriminative learning without person ID labels in a deep learning classification framework. We formulate an Unsupervised Tracklet Association Learning (UTAL) method, with the overall architecture design illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>. The UTAL contains two model components: (I) Per-Camera Tracklet Discrimination (PCTD) learning for optimising "local" within-camera tracklet label discrimination. This facilitates "global" cross-camera tracklet association, given independent tracklet label spaces in different camera views. (II) Cross-Camera Tracklet Association (CCTA) learning for discovering "global" cross-camera tracklet identity matching without ID labels.</p><p>For accurate cross-camera tracklet association, it is important to formulate a robust image feature representation to characterise the person appearance of each tracklet. However, it is sub-optimal to achieve "local" per-camera tracklet discriminative learning using only per-camera independent tracklet labels without "global" cross-camera tracklet correlations. We therefore propose to optimise jointly both PCTD and CCTA. The two components integrate as a whole in a single deep learning architecture, learn jointly and mutually benefit each other in incremental end-to-end model optimisation. Our overall idea for unsupervised learning of tracklet person re-id is to maximise coarse-grained latent group-level cross-camera tracklet association. This is based on exploring an notion of tracklet set correlation learning ( <ref type="figure" target="#fig_2">Fig.  2(b)</ref>). It differs significantly from supervised re-id learning that relies heavily on the fine-grained explicit instance-level cross-camera ID pairwise supervision ( <ref type="figure" target="#fig_2">Fig. 2(a)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Per-Camera Tracklet Discrimination Learning</head><p>In PCTD learning, we treat each individual camera view separately. That is, optimising per-camera labelled tracklet discrimination as a classification task with the unsupervised per-camera tracklet labels (not person ID labels) ( <ref type="figure" target="#fig_1">Fig. 1(a)</ref>). Given a surveillance network with T cameras, we hence have a total of T different tracklet classification tasks each corresponding to a specific camera view.  The PCTD aims to derive the "local" discrimination of per-camera tracklets in the respective tracklet label space (represented by soft labels (e)) by a multi-task inference process (one task for a specific camera view), whilst the CCTA to learn the "global" crosscamera tracklet association across independently formed tracklet label spaces. In UTAL design, the PCTD and CCTA jointly learn to optimise a re-id model for maximising their complementary contributions and advantages in a synergistic interaction and integration. Best viewed in colour.</p><p>Importantly, we further formulate these T classification tasks in a multi-branch network architecture design. All the tasks share the same feature representation space ( <ref type="figure" target="#fig_1">Fig.  1(b)</ref>) whilst enjoying an individual classification branch ( <ref type="figure" target="#fig_1">Fig.  1(c)</ref>). This is a multi-task learning <ref type="bibr" target="#b41">[42]</ref>.</p><p>Formally, we assume M t different tracklet labels {y} with the training tracklet image frames {I} from a camera view t ∈ {1, · · · , T } (Sec. 3.1). We adopt the softmax Cross-Entropy (CE) loss function to optimise the corresponding classification task (the t-th branch). The CE loss on a training image frame (I, y) is computed as:</p><formula xml:id="formula_0">L ce = − Mt j=1 1(j = y) · log exp(W j x) Mt k=1 exp(W k x)<label>(1)</label></formula><p>where x specifies the feature vector of I extracted from the task-shared representation space and W y the y-th class prediction parameters. 1(·) denotes an indicator function that returns 1/0 for true/false arguments. Given a training mini-batch, we compute the CE loss for each such training sample with the respective tracklet label space and utilise their average to form the PCTD learning objective as:</p><formula xml:id="formula_1">L pctd = 1 N bs T t=1 L t ce<label>(2)</label></formula><p>where L t ce denotes the CE loss of all training tracklet frames from the t-th camera, and N bs specifies the batch size.</p><p>Recall that, one of the main challenges in unsupervised tracklet re-id learning arises from within-camera trajectory fragmentation. This causes the tracklet ID duplication issue, i.e. the same-ID tracklets are assigned with distinct labels. By treating every single tracklet label as a unique class (Eq <ref type="formula" target="#formula_1">(2)</ref>), misleading supervision can be resulted potentially hampering the model learning performance. Soft Labelling. For gaining learning robustness against unconstrained trajectory fragmentation, we exploit the pairwise appearance affinity (similarity) information between within-camera tracklets. To this end, we propose soft tracklet labels to replace the hard counterpart (one-hot labels). This scheme uniquely takes into account the underlying ID correlation between tracklets in the PCTD learning (Eq <ref type="formula" target="#formula_1">(2)</ref>). It is based on the intuition that, multiple fragmented tracklets of the same person are more likely to share higher visual affinity with each other than those describing different people. Therefore, using tracklet labels involving the appearance affinity (i.e. soft labels) means imposing person ID relevant information into model training, from the manifold structure learning perspective <ref type="bibr" target="#b52">[53]</ref>.</p><p>Formally, we start the computation of soft tracklet labels by constructing an affinity matrix of person appearance A t ∈ R Mt×Mt on all M t tracklets for each camera t ∈ {1, · · · , T }, where each element A t (i, j) specifies the visual appearance similarity between the tracklets i and j. This requires a tracklet feature representation space. We achieve this by cumulatively updating an external feature vector s for every single tracklet S with the image features of the constituent frames in a batch-wise manner.</p><p>More specifically, given a mini-batch including n t i image frames from the i-th tracklet S t i of t-th camera view, the corresponding tracklet representation s t i is progressively updated across the training iterations as:</p><formula xml:id="formula_2">s t i = 1 1 + α s t i + α 1 n t i n t i k=1 x k (3)</formula><p>where x k is the feature vector of the k-th in-batch image frame of S t i , extracted by the up-to-date model. The learning rate parameter α controls how fast s t i updates. This method avoids the need of forwarding all tracklet data in each iteration therefore computationally efficient and scalable.</p><p>Given the tracklet feature representations, we subsequently sparsify the affinity matrix as:</p><formula xml:id="formula_3">A t (i, j) = exp(− s t i −s t j 2 2 σ 2 ), if s t j ∈ N (s t i ) 0, otherwise<label>(4)</label></formula><p>where N (s t i ) are the K nearest neighbours (NN) of s t i defined by the Euclidean distance in the feature space. Using the sparse NN idea on the affinity structure is for suppressing the distracting effect of visually similar tracklets from unmatched ID classes. Computationally, each A has a quadratic complexity, but only to the number of per-camera tracklet and linear to the total number of cameras, rather than quadratic to all tracklets from all the cameras. The use of sparse similarity matrices significantly reduces the memory demand.</p><p>To incorporate the local density structure <ref type="bibr" target="#b53">[54]</ref>, we deploy a neighbourhood structure-aware scale defined as:</p><formula xml:id="formula_4">σ 2 = 1 M t · K Mt i=1 K j=1 s t i − s t j 2 2 , s.t. s t j ∈ N (s t i )<label>(5)</label></formula><p>Based on the estimated neighbourhood structures, we finally compute the soft label ( <ref type="figure" target="#fig_1">Fig. 1(e)</ref>) for each tracklet S t i as the L 1 normalised affinity measurement:</p><formula xml:id="formula_5">y t i = A(i, 1 : M t ) Mt j=1 A(i, j)<label>(6)</label></formula><p>Given the proposed soft tracklet labels, the CE loss function (Eq (2)) is then reformulated as:</p><formula xml:id="formula_6">L sce = − Mt j=1ŷ t i (j) · log exp(W j x) Mt k=1 exp(W k x)<label>(7)</label></formula><p>We accordingly update the PCTD learning loss (Eq (2)) as:</p><formula xml:id="formula_7">L pctd = 1 N bs T t=1 L t sce .<label>(8)</label></formula><p>Remarks. In PCTD, the objective function (Eq <ref type="formula" target="#formula_7">(8)</ref>) optimises by supervised learning person tracklet discrimination within each camera view alone. It does not explicitly consider supervision in cross-camera tracklet association. Interestingly, when jointly learning all the per-camera tracklet discrimination tasks together, the learned representation model is implicitly and collectively cross-view tracklet discriminative in a latent manner. This is due to the existence of crosscamera tracklet ID class correlation. That being said, the shared feature representation is optimised to be tracklet discriminative concurrently for all camera views, latently expanding model discriminative learning from per-camera (locally) to cross-camera (globally). Apart from multi-camera multi-task learning, we exploit the idea of soft tracklet labels to further improve the model ID discrimination learning capability. This is for better robustness against trajectory fragmentation. Fundamentally, this is an indirect strategy of refining fragmented tracklets. It is based on the visual appearance affinity without the need of explicitly stitching tracklets. The intuition is that, the tracklets of the same person are possible to be assigned with more similar soft labels (i.e. signatures). Consequently, this renders the unsupervised tracklet labels closer to supervised ID class labels in terms of discrimination power, therefore helping re-id model optimisation.</p><p>In equation formulation, our PCTD objective is related to the Knowledge Distillation (KD) technique <ref type="bibr" target="#b54">[55]</ref>. KD also utilises soft class probability labels inferred by an independent teacher model. Nevertheless, our method conceptually differs from KD, since we primarily aim to unveil the hidden fine-grained discriminative information of the same class (ID) distributed across unconstrained tracklet fragments, Besides, our model retains the KD's merit of modelling the inter-class similarity geometric manifold information. Also, our method has no need for learning a heavy source knowledge teacher model, therefore, computationally more efficient. We will evaluate the PCTD model design <ref type="table" target="#tab_3">(Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Cross-Camera Tracklet Association Learning</head><p>The PCTD achieves somewhat global (all the camera views) tracklet discrimination capability implicitly. But the resulting representation remains sub-optimal, due to the lack of explicitly optimising cross-camera tracklet association at the fine-grained instance level. It is non-trivial to impose cross-camera re-id discriminative learning constraints at the absence of ID labels. To address this problem, we introduce a Cross-Camera Tracklet Association (CCTA) learning algorithm for enabling tracklet association between cameras ( <ref type="figure" target="#fig_1">Fig. 1(d)</ref>). Conceptually, the CCTA is based on adaptively and incrementally self-discovering cross-view tracklet association in the multi-task camera-shared feature space ( <ref type="figure" target="#fig_1">Fig. 1(b)</ref>).</p><p>Specifically, we ground the CCTA learning on crosscamera nearest neighbourhoods. In re-id, the vast majority of cross-camera tracklet pairs are negative associations from unmatched ID classes. They provide no desired information about how a person's appearance varies under different camera viewing conditions. The key for designing an informative CCTA loss is therefore to self-discover the crosscamera positive matching pairs. This requires to search similar samples (i.e. neighbours) which however is a challenging task because: (1) Tracklet feature representations s can be unreliable and error-prone due to the lack of cross-camera pair supervision (hence a catch-22 problem). (2) False positive pairs may easily propagate the erroneous supervision cumulatively through the learning process, guiding the optimisation towards poor local optima; Deep neural networks possess the capacity to fit any supervision labels <ref type="bibr" target="#b55">[56]</ref>.</p><p>To overcome these challenges, we introduce a model matureness adaptive matching pair search mechanism. It progressively finds an increasing number of plausible true matches across cameras as a reliable basis for the CCTA loss formulation. In particular, in each training epoch, we first retrieve the reciprocal cross-camera nearest neighbour R(s t i ) for each tracklet s t i . The R is obtained based on the mutual nearest neighbour notion <ref type="bibr" target="#b56">[57]</ref>. Formally, let N 1 (s t i ) be the cross-camera NN of s t i . The R(s t i ) is then defined as:</p><formula xml:id="formula_8">R(s t i ) = {s|s ∈ N 1 (s t i ) &amp;&amp; s t i ∈ N 1 (s)}<label>(9)</label></formula><p>Given such self-discovered cross-camera matching pairs, we then formulate a CCTA objective loss for a tracklet s t i as:</p><formula xml:id="formula_9">L ccta = s∈R(s t i ) s t i − s 2<label>(10)</label></formula><p>With Eq <ref type="formula" target="#formula_0">(10)</ref>, we impose a cross-camera discriminative learning constraint by encouraging the model to pull the neighbour tracklets in R t i close to s t i . This CCTA loss applies only to those tracklets s with cross-camera matches, i.e. R(s) is non-empty, so that it is model matureness adaptive. As the training proceeds, the model is supposed to become more mature, leading to more cross-camera tracklet matches discovered. We will evaluate the CCTA loss in Sec. 4.3.</p><p>Remarks. Under the mutual nearest neighbour constraint, R(s t i ) are considered to be more strictly similar to s t i than the conventional nearest neighbours N (s t i ). With uncontrolled viewing condition variations across cameras, matching tracklets with dramatic appearance changes may be excluded from the R(s t i ) particularly in the beginning, representing a conservative search strategy. This is designed so to minimise the negative effect of error propagation from false matching pairs. More matching pairs are likely unveiled as the training proceeds. Many previously missing pairs can be gradually discovered when the model becomes more mature and discriminative. Intuitively, easy matching pairs are found before hard ones. Hence, the CCTA loss is in a curriculum leaning spirit <ref type="bibr" target="#b57">[58]</ref>. In Eq (10) we consider only the positive pairs whilst ignoring the negative matches. This is conceptually analogue to the formulation of Canonical Correlation Analysis (CCA) <ref type="bibr" target="#b58">[59]</ref>, and results in a simpler objective function without the need to tune a margin hyperparameter as required by the ranking losses <ref type="bibr" target="#b59">[60]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Joint Unsupervised Tracklet Association Learning</head><p>By combining the CCTA and PCTD learning constraints, we obtain the final model objective loss function of UTAL as:</p><formula xml:id="formula_10">L utal = L pctd + λL ccta<label>(11)</label></formula><p>where λ is a balance weight. Note that L pctd is an average loss term at the individual tracklet image level whilst L ccta at the tracklet group (set) level. Both are derived from the same mini-batch of training data concurrently. Remarks. By design, the CCTA enhances model representation learning. It imposes discriminative constraints derived from self-discovered cross-camera tracklet association. This is based on the PCTD learning of unsupervised and percamera independent tracklet label spaces. With more discriminative representation in the subsequent training iterations, the PCTD is then able to deploy more accurate soft tracklet labels. This in turn facilitates not only the following representation learning of per-camera tracklets, but also the discovery of higher quality and more informative crosscamera tracklet matching pairs. In doing so, the two learning components integrate seamlessly and optimise a person re-id model concurrently in an end-to-end batch-wise learning process. Consequently, the overall UTAL method formulates a benefit-each-other closed-loop model design. This eventually leads to cumulative and complementary advantages throughout training.</p><formula xml:id="formula_11">(a) (b) (c) (d) (e) (f) (g) (h)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Model Training and Testing</head><p>Model Training. To minimise the negative effect of inaccurate cross-camera tracklet matching pairs, we deploy the CCTA loss only during the second half training process. Specifically, UTAL begins the model training with the soft tracklet label based PCTD loss (Eq (8)) for the first half epochs. We then deploy the full UTAL loss (Eq (11)) for the remaining epochs. To improve the training efficiency, we update the per-camera tracklet soft labels (Eq (6)) and crosscamera tracklet matches (Eq (9)) per epoch. These updates progressively enhance the re-id discrimination power of the UTAL objective throughout training, as we will show in our model component analysis and diagnosis in Sec. 4.3. Model Testing. Once a deep person re-id model is trained by the UTAL unsupervised learning method, we deploy the camera-shared feature representations ( <ref type="figure" target="#fig_1">Fig. 1(b)</ref>) for re-id matching under the Euclidean distance metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setting</head><p>Datasets. To evaluate the proposed UTAL model, we tested both video (iLIDS-VID <ref type="bibr" target="#b24">[25]</ref>, PRID2011 <ref type="bibr" target="#b25">[26]</ref>, MARS <ref type="bibr" target="#b26">[27]</ref>) and image (CUHK03 <ref type="bibr" target="#b20">[21]</ref>, Market-1501 <ref type="bibr" target="#b21">[22]</ref>, DukeMTMC-ReID <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>, MSMT17 <ref type="bibr" target="#b3">[4]</ref>) person re-id datasets. In previous studies, these two sets of benchmarks were usually evaluated separately. We consider both sets because recent large image re-id datasets were typically constructed by sampling person bounding boxes from videos, so they share similar characteristics as the video re-id datasets. We adopted the standard test protocols as summarised in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>To further test realistic model performances, we introduced a new tracklet person re-id benchmark based on For benchmarking DukeTracklet, we need the groundtruth person ID labels of tracklets. To this end, we used the criterion of spatio-temporal average Intersection over Union (IoU) between detected tracklets and ground-truth trajectories available in DukeMTMC. In particular, we labelled an auto-generated tracklet by the ground-truth person ID associated with a manually-generated trajectory if their average IoU is over 50%. Otherwise, we labelled the auto-generated tracklet as "unknown ID". To maximise the comparability with existing DukeMTMC variants, we threw away those tracklets labelled with unknown IDs. We finally obtained 12,647 person tracklets from 1,788 unique IDs. The average tracklet duration is 65.9 frames. To match DukeMTMC-ReID <ref type="bibr" target="#b23">[24]</ref>, we set the same 702 training IDs with the remaining 1,086 people for performance test (missing 14 test IDs against DukeMTMC-ReID due to tracking failures). Tracklet Label Assignment. For each video re-id dataset, we simply assigned each tracklet with a unique label in a camera-independent manner (Sec. 3.1). For each multi-shot image datasets, we assumed all person images per ID per camera were drawn from a single pedestrian tracklet, and similarly labelled them as the video datasets. Performance Metrics. We adopted the common Cumulative Matching Characteristic (CMC) and mean Average Precision (mAP) metrics <ref type="bibr" target="#b21">[22]</ref> for model performance measurement. Implementation Details. We used an ImageNet pre-trained ResNet-50 <ref type="bibr" target="#b68">[69]</ref> as the backbone net for UTAL, along with an additional 2,048-D fully-connected (FC) layer for deriving the camera-shared representations. Every camera-specific branch was formed by one FC classification layer. Person bounding box images were resized to 256×128. To ensure each training mini-batch has person images from all cameras, we set the batch size to 128 for PRID2011, iLIDS-VID and CUHK03, and 384 for MSMT17, Market-1501, MARS, DukeMTMC-ReID, and DukeTracklet. In order to balance the model training speed across cameras, we randomly selected the same number of tracklets per camera and the same number of frame images (4 images) per chosen tracklet when sampling each mini-batch. We adopted the Adam optimiser <ref type="bibr" target="#b69">[70]</ref> with the learning rate of 3.5 × 10 −4 and the epoch of 200. By default, we set λ = 10 for Eq <ref type="bibr" target="#b10">(11)</ref>, α = 1 for Eq (3), and K = 4 for Eq <ref type="bibr" target="#b4">(5)</ref> in the following experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons to the State-Of-The-Art Methods</head><p>We compared two different sets of state-of-the-art methods on image and video re-id datasets, due to the independent studies on them in the literature. Evaluation on Image Datasets. <ref type="table" target="#tab_1">Table 2</ref> shows the unsupervised re-id performance of the proposed UTAL and 15 state-of-the-art methods including 3 hand-crafted feature based methods (Dic <ref type="bibr" target="#b8">[9]</ref>, ISR <ref type="bibr" target="#b9">[10]</ref>, RKSL <ref type="bibr" target="#b12">[13]</ref>) and 12 auxiliary knowledge (identity/attribute) transfer based models (AE <ref type="bibr" target="#b60">[61]</ref>, AML <ref type="bibr" target="#b62">[63]</ref>, UsNCA <ref type="bibr" target="#b63">[64]</ref>, CAMEL <ref type="bibr" target="#b19">[20]</ref>, JSTL <ref type="bibr" target="#b61">[62]</ref>, PUL <ref type="bibr" target="#b18">[19]</ref>, TJ-AIDL <ref type="bibr" target="#b16">[17]</ref>, CycleGAN <ref type="bibr" target="#b34">[35]</ref>, SPGAN <ref type="bibr" target="#b35">[36]</ref>, HHL <ref type="bibr" target="#b36">[37]</ref>, DASy <ref type="bibr" target="#b39">[40]</ref>). The results show four observations as follows.</p><p>(1) Among the existing methods, the knowledge transfer based models are often superior due to the use of additional label information, e.g. Rank-1 39.4% by CAMEL vs 36.5% by Dic on CUHK03; 65.7% by DASy vs 50.2% by Dic on Market-1501. To that end, CAMEL needs to benefit from learning on 7 different person re-id datasets of diverse domains (CUHK03 <ref type="bibr" target="#b20">[21]</ref>, CUHK01 <ref type="bibr" target="#b73">[74]</ref>, PRID <ref type="bibr" target="#b25">[26]</ref>, VIPeR <ref type="bibr" target="#b74">[75]</ref>, i-LIDS <ref type="bibr" target="#b75">[76]</ref>) including a total of 44,685 images and 3,791 IDs; HHL requires to utilise labelled Market-1501 (750 IDs) or DukeMTMC-ReID (702 IDs) as the source training data. DASy needs elaborative ID synthesis and adaptation.</p><p>(2) The proposed UTAL outperforms all competitors with significant margins. For example, the Rank-1 margin by UTAL over HHL is 7.0% (69.2-62.2) on Market-1501 and 15.4% (62.3-46.9) on DukeMTMC-ReID. Also, our preliminary method TAUDL already surpasses all previous methods. It is worth pointing out that UTAL dose not benefit from any additional labelled source domain training data as compared to the strong alternative HHL. Importantly, UTAL is potentially more scalable due to no reliance at all on the similarity constraint between source and target domains.</p><p>(3) The UTAL is simpler to train with a simple end-to-end model learning, vs the alternated deep CNN training and data clustering required by PUL, a two-stage model training of TJ-AIDL, high GAN training difficulty of HHL, and elaborative ID synthesis of DASy. These results show both the performance advantage and model design superiority of UTAL over state-of-the-art re-id methods. (4) A large performance gap exists between unsupervised and supervised learning models. Further improvement is required on unsupervised learning algorithms. Evaluation on Video Datasets. In <ref type="table" target="#tab_2">Table 3</ref>, we compared the UTAL with 8 state-of-the-art unsupervised video re-id models (GRDL <ref type="bibr" target="#b10">[11]</ref>, UnKISS <ref type="bibr" target="#b11">[12]</ref>, SMP <ref type="bibr" target="#b15">[16]</ref>, DGM+MLAPG/IDE <ref type="bibr" target="#b14">[15]</ref>, DAL <ref type="bibr" target="#b71">[72]</ref>, RACE <ref type="bibr" target="#b70">[71]</ref>, DASy <ref type="bibr" target="#b39">[40]</ref>) on the video benchmarks. Unlike UTAL, all these existing models except DAL are not end-to-end deep learning methods using handcrafted or independently trained deep features as input.</p><p>The comparisons show that, our UTAL outperforms all existing video person re-id models on the large scale video  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Component Analysis and Discussion</head><p>We conducted detailed UTAL model component analysis on two large tracklet re-id datasets, MARS and DukeTracklet. Per-Camera Tracklet Discrimination Learning. We started by testing the performance impact of the PCTD component. This is achieved by designing a baseline that treats all cameras together, that is, concatenating the per-camera tracklet label spaces and deploying the Cross-Entropy loss for learning a Single-Task Classification (STC). In this analysis, we did not consider the cross-camera tracklet association component for a more focused evaluation. <ref type="table" target="#tab_3">Table 4</ref> shows that, the proposed PCTD design is significantly superior over the STC learning algorithm, e.g.</p><p>achieving Rank-1 gain of 27.9% <ref type="bibr">(43.8-15.9)</ref>, and 27.8% (31.7-3.9) on MARS and DukeTracklet, respectively. The results demonstrate modelling advantages of PCTD in exploiting unsupervised tracklet labels for learning cross-view re-id discriminative features. This validates the proposed idea of implicitly deriving a cross-camera shared feature representation through a multi-camera multi-task learning strategy. Recall that we propose a soft label (Eq (6)) based Cross-Entropy loss (Eq (7)) for tackling the notorious trajectory fragmentation challenge. To test how much benefit our soft labelling strategy brings to unsupervised tracklet re-id, we compared it against the one-hot class hard-labelling counterpart (Eq (1)). <ref type="table" target="#tab_4">Table 5</ref> shows that the proposed soft-labelling is significantly superior, suggesting a clear benefit in mitigating the negative impact of trajectory fragmentation. This is due to the intrinsic capability of exploiting the appearance pairwise affinity knowledge among tracklets per camera.</p><p>We further examined the ID discrimination capability of tracklet soft labels that underpins its outperforming over the corresponding hard labels. To this end, we measured the Mean Pairwise Similarity (MPS) of soft label vectors assigned to per-camera same-ID tracklets. <ref type="figure" target="#fig_4">Figure 4</ref> shows that, the MPS metric goes higher as the training epoch increases, particularly after the CCTA loss is further exploited in the middle of training (at 100 th epoch). This indicates explicitly the evolving process of mining the discriminative knowledge among fragmented tracklets with the same person ID labels in a self-supervising fashion.</p><p>In effect, the affinity measurement (Eq (4)) used for computing soft labels can be useful for automatically merging short fragmented tracklets into long trajectories per camera. We tested this tracking refinement capability of our method. In particular, we built a sparse connection graph by thresholding the pairwise affinity scores at 0.5, analysed the connected tracklet components <ref type="bibr" target="#b76">[77]</ref>, and merged all tracklets in each component into a long trajectory.  <ref type="table" target="#tab_5">Table 6</ref> shows that with our per-camera tracklet affinity measurement, even such a simple strategy can merge 4,389/2,527 out of 8,298/5,803 short tracklets into 1,532/982 long trajectories at the NMI (Normalised Mutual Information) rate of 0.896/0.934 on MARS/DukeTracklet. This not only suggests the usefulness of UTAL in tracklet refinement, but also reflects the underlying correlation between tracking and person re-id. For visual quality examination, we gave example cases for tracking refinement in <ref type="figure">Fig. 5</ref>. Algorithmically, our soft label PCTD naturally inherits the cross-class (ID) knowledge transfer capability from Knowledge Distillation <ref type="bibr" target="#b54">[55]</ref>. It is interesting to see how much performance benefit this can bring to unsupervised tracklet re-id. To this end, we conducted a controlled experiment with only one randomly selected training tracklet per ID per camera. Doing so enforces that no multiple per-camera tracklets share the same person ID, which more explicitly evaluates the impact of cross-ID knowledge transfer. Note, this leads to probably inferior re-id model generalisation capability due to less training data used in optimisation. <ref type="table" target="#tab_6">Table 7</ref> shows that the cross-ID knowledge transfer gives notable performance improvements.  To further examine why CCTA enables more discriminative re-id model learning, we tracked the self-discovered cross-camera tracklet matching pairs throughout the training. <ref type="figure" target="#fig_6">Figure 6</ref> shows that both the number and precision of self-discovered cross-camera tracklet pairs increase. This echoes the model performance superiority of UTAL.  Model Parameters. We evaluated the performance impact of three UTAL hyper-parameters: (1) the tracklet feature update learning rate α (Eq (3)), (2) the sparsity K of the tracklet affinity matrix used in computing the soft labels (Eq (4) and <ref type="formula" target="#formula_4">(5)</ref>); (3) the loss balance weight λ (Eq <ref type="formula" target="#formula_0">(11)</ref>). <ref type="figure">Figure 7</ref> shows that: (1) α is not sensitive with a wide satisfactory range. This suggests a stable model learning procedure.</p><p>(2) K has an optimal value at "4". Too small values lose the opportunities of incorporating same-ID tracklets into the soft labels whilst the opposite instead introduces distracting/noisy neighbour information. (3) λ is found more domain dependent, with the preference values around "10". This indicates a higher importance of cross-camera tracklet association and matching.</p><p>(a) The sensitive of α (b) The sensitive of K (c) The sensitive of λ <ref type="figure">Fig. 7</ref>. Analysis of the UTAL model parameters. <ref type="figure">Neighbours (CCNN)</ref>. We evaluated the effect of CCNN R (Eq <ref type="formula" target="#formula_8">(9)</ref>) used in the CCTA loss. We compared two designs: Our reciprocal 2-way 1-NN vs. common 1-way 1-NN. <ref type="table" target="#tab_8">Table 9</ref> shows that the more strict 2way 1-NN gives better overall performance whilst the 1-way 1-NN has a slight advantage in Rank-1 on MARS (50.5% vs. 49.9%). By 2-way, we found using more neighbours (5/10-NN) degrades model performance. This is due to the introduction of more false cross-camera matches. Tracklet Sampling versus All Tracklets. In our preliminary solution TAUDL <ref type="bibr" target="#b27">[28]</ref>, we considered a Sparse Space-Time Tracklet (SSTT) sampling strategy instead of unsupervised learning on all tracklet data. It is useful in minimising the person ID duplication rate in tracklets. However, such a data sampling throws away a large number of tracklets with rich information of person appearance exhibited continuously and dynamically over space and time. To examine this, we compared the SSTT sampling with using all tracklets. <ref type="table" target="#tab_0">Table 10</ref> shows two observations: <ref type="bibr" target="#b0">(1)</ref> In overall re-id performance, our preliminary method TAUDL <ref type="bibr" target="#b27">[28]</ref> is outperformed significantly by UTAL. For example, the Rank-1 results are improved by 6.1% (49.9-43.8) on MARS and by 17.7% (43.8-26.1) on DukeTracklet. (2) When using the same UTAL model, the SSTT strategy leads to inferior re-id rates as compared with using all tracklets. For instance, the Rank-1 performance drop is 4.8% (49.9-45.1) on MARS, and 12.1% (43.8-31.7) on DukeTracklet. These performance gains are due to the proposed soft label learning idea that effectively handles the trajectory fragmentation problem. Overall, this validates the efficacy of our model design in solving the SSTT's limitation whilst more effectively tackling the ID duplication (due to trajectory fragmentation) problem. Effect of Neural Network Architecture. The model generalisation performance of UTAL may depend on the selection of neural network architecture. To assess this aspect, we evaluated one more UTAL variant using a more recent DenseNet-121 <ref type="bibr" target="#b77">[78]</ref> as the backbone network, versus the default choice ResNet-50 <ref type="bibr" target="#b68">[69]</ref>. <ref type="table" target="#tab_0">Table 11</ref> shows that even superior re-id performances can be obtained when using a stronger network architecture. This suggests that UTAL can readily benefit from the advancement of network designs. Weakly Supervised Tracklet Association Learning. For training data labelling in person re-id, the most costly procedure is on exhaustive manual search of cross-camera image/tracklet matching pairs. It is often unknown where and when a specific person will appear given complex camera spatio-temporal topology and unconstrained people's behaviours in the public spaces. Therefore, per-camera independent ID labelling is more affordable. Such labelled data are much weaker and less informative, due to the lack of cross-camera positive and negative ID pairs information. We call the setting Weakly Supervised Learning (WSL). The proposed UTAL model can be flexibly applied in the WSL setting. Interestingly, this allows to test how much re-id performance benefit such labels can provide. Unlike in the unsupervised learning setting, the soft label based PCTD loss is no longer necessary in WSL given the withincamera ID information. Hence, we instead deployed the hard one-hot label (Eq (1)) based PCTD loss (Eq (2)). <ref type="table" target="#tab_0">Table  12</ref> shows that such weak labels are informative and useful for person re-id by the UTAL method. This test indicates a wide suitability and usability of our method in practical deployments under various labelling budgets. Manual Tracking. DukeMTMC-VideoReID provides manually labelled trajectories, originally introduced for one-shot person re-id <ref type="bibr" target="#b65">[66]</ref>. We tested UTAL on this dataset without the assumed one-shot labelled trajectory per ID. We set K = 0 for Eq (5) due to no trajectory fragmentation. <ref type="table" target="#tab_0">Table 13</ref> shows that UTAL outperforms EUG <ref type="bibr" target="#b65">[66]</ref> even without one-shot ID labelling. This indicates the efficacy of our unsupervised learning strategy in discovering re-id information. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-Camera Nearest</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>We presented a novel Unsupervised Tracklet Association Learning (UTAL) model for unsupervised tracklet person reidentification. This model learns from person tracklet data automatically extracted from videos, eliminating the expensive and exhaustive manual ID labelling. This enables UTAL to be more scalable to real-world applications. In contrast to existing re-id methods that require exhaustively pairwise labelled training data for every camera-pair or assume labelled source domain training data, the proposed UTAL model performs end-to-end deep learning of a person re-id model from scratch using totally unlabelled tracklet data. This is achieved by optimising jointly both a Per-Camera Tracklet Discrimination loss function and a Cross-Camera Tracklet Association loss function in a unified architecture. Extensive evaluations were conducted on eight image and video person re-id benchmarks to validate the advantages of the proposed UTAL model over state-of-the-art unsupervised and domain adaptation re-id methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 1 .</head><label>1</label><figDesc>Soft label ෝ of a tracklet itself An overview of the proposed Unsupervised Tracklet Association Learning (UTAL) person re-identification model. The UTAL takes as input (a) auto-detected tracklets from all the camera views without any person ID class labelling either within-camera or cross-camera. The objective is to derive (b) a person re-id discriminative feature representation model by unsupervised learning. To this end, we formulate the UTAL model for simultaneous (c) Per-Camera Tracklet Discrimination (PCTD) learning and (d) Cross-Camera Tracklet Association (CCTA) learning in an end-toend neural network architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>…Fig. 2 .</head><label>2</label><figDesc>Comparing (a) Fine-grained explicit instance-level cross-camera ID labelled image pairs for supervised person re-id model learning and (b) Coarse-grained latent group-level cross-camera tracklet (a multi-shot group) label correlation for ID label-free (unsupervised) person re-id learning using the proposed UTAL method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Example cross-camera matching image/tracklet pairs from (a) CUHK03, (b) Market-1501, (c) DukeMTMC-ReID, (d) MSMT17, (e) PRID2011, (f) iLIDS-VID, (g) MARS, (h) DukeMTMC-SI-Tracklet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The evolving process of tracklet soft label quality over the model training epochs on MARS and DukeTracklet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Number of tracklet pairs. (b) Precision of matching pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>The evolving process of self-discovered cross-camera tracklet matching pairs in (a) number and (b) precision throughout the training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Dataset statistics and evaluation setting.</figDesc><table><row><cell>Dataset</cell><cell cols="5"># ID # Train # Test # Image # Tracklet</cell></row><row><cell>iLIDS-VID [25]</cell><cell>300</cell><cell>150</cell><cell>150</cell><cell>43,800</cell><cell>600</cell></row><row><cell>PRID2011 [26]</cell><cell>178</cell><cell>89</cell><cell>89</cell><cell>38,466</cell><cell>354</cell></row><row><cell>MARS [27]</cell><cell>1,261</cell><cell>625</cell><cell cols="3">636 1,191,003 20,478</cell></row><row><cell cols="2">DukeMTMC-SI-Tracklet 1,788</cell><cell>702</cell><cell cols="2">1,086 833,984</cell><cell>12,647</cell></row><row><cell>CUHK03 [21]</cell><cell>1,467</cell><cell>767</cell><cell>700</cell><cell>14,097</cell><cell>0</cell></row><row><cell>Market-1501 [22]</cell><cell>1,501</cell><cell>751</cell><cell>750</cell><cell>32,668</cell><cell>0</cell></row><row><cell cols="2">DukeMTMC-ReID [24] 1,812</cell><cell>702</cell><cell cols="2">1,110 36,411</cell><cell>0</cell></row><row><cell>MSMT17 [4]</cell><cell cols="4">4,101 1,041 3,060 126,441</cell><cell>0</cell></row><row><cell cols="6">DukeMTMC [23]. It differs from all the existing DukeMTMC</cell></row><row><cell cols="6">variants [24,66,67] by uniquely providing automatically gen-</cell></row><row><cell cols="6">erated tracklets. We built this new tracklet person re-id</cell></row><row><cell cols="6">dataset as follows. We first deployed an efficient deep</cell></row><row><cell cols="6">learning tracker that leverages a COCO+PASCAL trained</cell></row><row><cell cols="6">SSD [50] for pedestrian detection and an ImageNet trained</cell></row><row><cell cols="6">Inception [68] for person appearance matching. Applying</cell></row><row><cell cols="6">this tracker to all DukeMTMC raw videos, we generated</cell></row><row><cell cols="6">19,135 person tracklets. Due to the inevitable detection and</cell></row><row><cell cols="6">tracking errors caused by background clutters and visual</cell></row><row><cell cols="6">ambiguity, these tracklets may present typical mistakes (e.g.</cell></row></table><note>ID switch) and corruptions (e.g. occlusion). We name this test DukeMTMC-SI-Tracklet, abbreviated as DukeTracklet. The DukeMTMC-SI-Tracklet dataset is publicly released at: https://github.com/liminxian/DukeMTMC-SI-Tracklet.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Unsupervised person re-id on image based datasets. : Benefited from extra labelled auxiliary training data. "-": No reported result.</figDesc><table><row><cell>Dataset</cell><cell cols="2">CUHK03 [21]</cell><cell cols="2">Market-1501 [22]</cell><cell cols="2">DukeMTMC-ReID [24]</cell><cell cols="2">MSMT17 [4]</cell></row><row><cell>Metric (%)</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>Dic [9]</cell><cell>36.5</cell><cell>-</cell><cell>50.2</cell><cell>22.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ISR [10]</cell><cell>38.5</cell><cell>-</cell><cell>40.3</cell><cell>14.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RKSL [13]</cell><cell>34.8</cell><cell>-</cell><cell>34.0</cell><cell>11.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SAE  *  [61] JSTL  *  [62] AML  *  [63] UsNCA  *  [64] CAMEL  *  [20] PUL  *  [19] TJ-AIDL  *  [17] CycleGAN  *  [35] SPGAN  *  [36] SPGAN+LMP  *  [36] HHL  *  [37] DASy  *  [40]</cell><cell>30.5 33.2 31.4 29.6 39.4 -------</cell><cell>------------</cell><cell>42.4 44.7 44.7 45.2 54.5 44.7 58.2 48.1 51.5 57.7 62.2 65.7</cell><cell>16.2 18.4 18.4 18.9 26.3 20.1 26.5 20.7 22.8 26.7 31.4 -</cell><cell>-----30.4 44.3 38.5 41.1 46.4 46.9 -</cell><cell>-----16.4 23.0 19.9 22.3 26.2 27.2 -</cell><cell>------------</cell><cell>------------</cell></row><row><cell>TAUDL [28]</cell><cell>44.7</cell><cell>31.2</cell><cell>63.7</cell><cell>41.2</cell><cell>61.7</cell><cell>43.5</cell><cell>28.4</cell><cell>12.5</cell></row><row><cell>UTAL</cell><cell>56.3</cell><cell>42.3</cell><cell>69.2</cell><cell>46.2</cell><cell>62.3</cell><cell>44.6</cell><cell>31.4</cell><cell>13.1</cell></row><row><cell>GCS [65](Supervised)</cell><cell>88.8</cell><cell>97.2</cell><cell>93.5</cell><cell>81.6</cell><cell>84.9</cell><cell>69.5</cell><cell>-</cell><cell>-</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc>Unsupervised person re-id on video based datasets. : Use some ID labels for model initialisation.</figDesc><table><row><cell>Dataset</cell><cell cols="3">PRID2011 [26]</cell><cell cols="3">iLIDS-VID [25]</cell><cell></cell><cell cols="2">MARS [27]</cell><cell></cell><cell></cell><cell cols="2">DukeTracklet</cell><cell></cell></row><row><cell>Metric (%)</cell><cell cols="14">Rank-1 Rank-5 Rank-20 Rank-1 Rank-5 Rank-20 Rank-1 Rank-5 Rank-20 mAP Rank-1 Rank-5 Rank-20 mAP</cell></row><row><cell>GRDL [11]</cell><cell>41.6</cell><cell>76.4</cell><cell>89.9</cell><cell>25.7</cell><cell>49.9</cell><cell>77.6</cell><cell>19.3</cell><cell>33.2</cell><cell>46.5</cell><cell>9.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>UnKISS [12]</cell><cell>58.1</cell><cell>81.9</cell><cell>96.0</cell><cell>35.9</cell><cell>63.3</cell><cell>83.4</cell><cell>22.3</cell><cell>37.4</cell><cell>53.6</cell><cell>10.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SMP  *  [16]</cell><cell>80.9</cell><cell>95.6</cell><cell>99.4</cell><cell>41.7</cell><cell>66.3</cell><cell>80.7</cell><cell>23.9</cell><cell>35.8</cell><cell>44.9</cell><cell>10.5</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DGM+MLAPG  † [15]</cell><cell>73.1</cell><cell>92.5</cell><cell>99.0</cell><cell>37.1</cell><cell>61.3</cell><cell>82.0</cell><cell>24.6</cell><cell>42.6</cell><cell>57.2</cell><cell>11.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DGM+IDE  † [15]</cell><cell>56.4</cell><cell>81.3</cell><cell>96.4</cell><cell>36.2</cell><cell>62.8</cell><cell>82.7</cell><cell>36.8</cell><cell>54.0</cell><cell>68.5</cell><cell>21.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>RACE  † [71] DASy  *  [40]</cell><cell>50.6 43.0</cell><cell>79.4 -</cell><cell>91.8 -</cell><cell>19.3 56.5</cell><cell>39.3 -</cell><cell>68.7 -</cell><cell>43.2 -</cell><cell>57.1 -</cell><cell>67.6 -</cell><cell>24.5 -</cell><cell>--</cell><cell>--</cell><cell>--</cell><cell>--</cell></row><row><cell>DAL [72]</cell><cell>85.3</cell><cell>97.0</cell><cell>99.6</cell><cell>56.9</cell><cell>80.6</cell><cell>91.9</cell><cell>46.8</cell><cell>63.9</cell><cell>77.5</cell><cell>21.4</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TAUDL [28]</cell><cell>49.4</cell><cell>78.7</cell><cell>98.9</cell><cell>26.7</cell><cell>51.3</cell><cell>82.0</cell><cell>43.8</cell><cell>59.9</cell><cell>72.8</cell><cell>29.1</cell><cell>26.1</cell><cell>42.0</cell><cell>57.2</cell><cell>20.8</cell></row><row><cell>UTAL</cell><cell>54.7</cell><cell>83.1</cell><cell>96.2</cell><cell>35.1</cell><cell>59.0</cell><cell>83.8</cell><cell>49.9</cell><cell>66.4</cell><cell>77.8</cell><cell>35.2</cell><cell>43.8</cell><cell>62.8</cell><cell>76.5</cell><cell>36.6</cell></row><row><cell>Snippet [73](Supervised)</cell><cell>93.0</cell><cell>99.3</cell><cell>100.0</cell><cell>85.4</cell><cell>96.7</cell><cell>99.5</cell><cell>86.3</cell><cell>94.7</cell><cell>98.2</cell><cell>76.1</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="6">dataset MARS, e.g. by a Rank-1 margin of 3.1% (49.9-46.8)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">and a mAP margin of 13.8% (35.2-21.4) over the best com-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">petitor DAL. However, UTAL is inferior than top existing</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">models on the two small benchmarks iLIDS-VID (300 train-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">ing tracklets) and PRID2011 (178 training tracklets), vs 8,298</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">training tracklets in MARS. This shows that UTAL does</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">need sufficient tracklet data in order to have its performance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">advantage. As the required tracklet data are not manually</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">labelled, this requirement is not a hindrance to its scalability</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">on large scale deployments. Quite the contrary, UTAL works</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">the best when large unlabelled video data are available. A</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">model would benefit from pre-training using UTAL on large</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">auxiliary unlabelled videos with similar viewing conditions.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>* : Assume no tracking fragmentation.†</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Effect of Per-Camera Tracklet Discrimination (PCTD) learning.</figDesc><table><row><cell>Dataset</cell><cell cols="2">MARS [27]</cell><cell cols="2">DukeTracklet</cell></row><row><cell>Metric (%)</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>STC</cell><cell>15.9</cell><cell>10.0</cell><cell>3.9</cell><cell>4.7</cell></row><row><cell>PCTD</cell><cell>43.8</cell><cell>31.4</cell><cell>31.7</cell><cell>26.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5</head><label>5</label><figDesc>Soft-labelling versus hard-labelling.</figDesc><table><row><cell>Dataset</cell><cell cols="2">MARS [27]</cell><cell cols="2">DukeTracklet</cell></row><row><cell>Metric (%)</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>Hard-Labelling</cell><cell>35.5</cell><cell>20.5</cell><cell>24.5</cell><cell>17.3</cell></row><row><cell>Soft-Labelling</cell><cell>49.9</cell><cell>35.2</cell><cell>43.8</cell><cell>36.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6</head><label>6</label><figDesc>Evaluating the tracking refinement capability of soft labels.</figDesc><table><row><cell>Dataset</cell><cell>MARS [27]</cell><cell>DukeTracklet</cell></row><row><cell>Original Tracklets</cell><cell>8,298</cell><cell>5,803</cell></row><row><cell>Mergable Tracklets</cell><cell>4,389</cell><cell>2,527</cell></row><row><cell>Long Trajectories</cell><cell>1,532</cell><cell>928</cell></row><row><cell>NMI</cell><cell>0.896</cell><cell>0.934</cell></row></table><note>Fig. 5. Example long trajectories discovered by UTAL among unlabelled short fragmented tracklets. Each row denotes a case. The tracklets in green/red bounding box denote the true/false matches, respectively. Failure tracklet merging may be due to detection and tracking errors.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 7</head><label>7</label><figDesc>Evaluating the cross-ID knowledge transfer effect of soft labels.</figDesc><table><row><cell>Dataset</cell><cell cols="2">MARS [27]</cell><cell cols="2">DukeTracklet</cell></row><row><cell>Metric (%)</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>Hard Label</cell><cell>45.1</cell><cell>31.1</cell><cell>28.6</cell><cell>20.8</cell></row><row><cell>Soft Label</cell><cell>46.5</cell><cell>31.2</cell><cell>31.7</cell><cell>24.8</cell></row><row><cell cols="5">Cross-Camera Tracklet Association Learning. We evalu-</cell></row><row><cell cols="5">ated the CCTA component by measuring the performance</cell></row><row><cell cols="5">drop once eliminating it. Table 8 shows that CCTA brings</cell></row><row><cell cols="5">a significant re-id accuracy benefit, e.g. a Rank-1 boost of</cell></row><row><cell cols="5">6.1% (49.9-43.8) and 12.1% (43.8-31.7) on MARS and Duke-</cell></row><row><cell cols="5">Tracklet, respectively. This suggests the importance of cross-</cell></row><row><cell cols="5">camera ID class correlation modelling and the capability</cell></row><row><cell cols="5">of our CCTA formulation in reliably associating tracklets</cell></row><row><cell cols="5">across cameras for unsupervised re-id model learning.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE 8</head><label>8</label><figDesc>Effect of Cross-Camera Tracklet Association (CCTA) learning.</figDesc><table><row><cell>Dataset</cell><cell cols="2">MARS [27]</cell><cell cols="2">DukeTracklet</cell></row><row><cell>CCTA</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell></cell><cell>43.8</cell><cell>31.4</cell><cell>31.7</cell><cell>26.4</cell></row><row><cell></cell><cell>49.9</cell><cell>35.2</cell><cell>43.8</cell><cell>36.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 9</head><label>9</label><figDesc>Effect of cross-camera nearest neighbours.</figDesc><table><row><cell>Dataset</cell><cell cols="2">MARS [27]</cell><cell cols="2">DukeTracklet</cell></row><row><cell>Metric (%)</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>1-way 1-NN</cell><cell>50.5</cell><cell>33.2</cell><cell>41.9</cell><cell>34.5</cell></row><row><cell>2-way 1-NN</cell><cell>49.9</cell><cell>35.2</cell><cell>43.8</cell><cell>36.6</cell></row><row><cell>2-way 5-NN</cell><cell>47.6</cell><cell>34.5</cell><cell>38.0</cell><cell>31.6</cell></row><row><cell>2-way 10-NN</cell><cell>45.5</cell><cell>32.5</cell><cell>33.9</cell><cell>27.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 10</head><label>10</label><figDesc>Tracklet sampling versus using all tracklets.</figDesc><table><row><cell>Dataset</cell><cell cols="2">MARS [27]</cell><cell cols="2">DukeTracklet</cell></row><row><cell>Metric (%)</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>TAUDL [28]</cell><cell>43.8</cell><cell>29.1</cell><cell>26.1</cell><cell>20.8</cell></row><row><cell>UTAL(SSTT)</cell><cell>45.1</cell><cell>31.1</cell><cell>31.7</cell><cell>24.8</cell></row><row><cell>UTAL(All Tracklets)</cell><cell>49.9</cell><cell>35.2</cell><cell>43.8</cell><cell>36.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 11</head><label>11</label><figDesc>Effect of backbone neural network in UTAL.</figDesc><table><row><cell>Dataset</cell><cell cols="2">MARS [27]</cell><cell cols="2">DukeTracklet</cell></row><row><cell>Metric (%)</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>ResNet-50 [69]</cell><cell>49.9</cell><cell>35.2</cell><cell>43.8</cell><cell>36.6</cell></row><row><cell>DenseNet-121 [78]</cell><cell>51.6</cell><cell>35.9</cell><cell>44.3</cell><cell>36.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 12</head><label>12</label><figDesc>Evaluation of weakly supervised tracklet association learning.</figDesc><table><row><cell>Dataset</cell><cell cols="2">MARS [27]</cell><cell cols="2">DukeTracklet</cell></row><row><cell>Metric (%)</cell><cell>Rank-1</cell><cell>mAP</cell><cell>Rank-1</cell><cell>mAP</cell></row><row><cell>Unsupervised</cell><cell>49.9</cell><cell>35.2</cell><cell>43.8</cell><cell>36.6</cell></row><row><cell>Weakly Supervised</cell><cell>59.5</cell><cell>51.7</cell><cell>46.4</cell><cell>39.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 13</head><label>13</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">Evaluation on DukeMTMC-VideoReID.</cell><cell></cell></row><row><cell>Metric (%)</cell><cell>Rank-1</cell><cell>Rank-5</cell><cell>Rank-20</cell><cell>mAP</cell></row><row><cell>EUG [66]</cell><cell>72.8</cell><cell>84.2</cell><cell>91.5</cell><cell>63.2</cell></row><row><cell>UTAL</cell><cell>74.5</cell><cell>88.7</cell><cell>96.3</cell><cell>72.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This work is partially supported by Vision Semantics Limited, National Natural Science Foundation of China (Project No. 61401212), Royal Society Newton Advanced Fellowship Programme (NA150459), and Innovate UK Industrial Challenge Project on Developing and Commercialising Intelligent Video Analytics Solutions for Public Safety (98111-571149).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Person re-identification by deep joint learning of multi-loss classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Jo. Conf. of Artif. Intell</title>
		<meeting>Int. Jo. Conf. of Artif. Intell</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Harmonious attention network for person reidentification</title>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Person transfer gan to bridge domain gap for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="79" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mask-guided contrastive attention model for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1179" to="1188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multi-level factorisation net for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2109" to="2118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep groupshuffling random walk for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2265" to="2274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Unsupervised learning of generative topic saliency for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Bri. Mach. Vis. Conf</title>
		<meeting>Bri. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dictionary learning with iterative laplacian regularisation for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Bri. Mach. Vis. Conf</title>
		<meeting>Bri. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Person reidentification by iterative re-weighted sparse ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Del</forename><surname>Bimbo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1629" to="1642" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Person re-identification by unsupervised l 1 graph learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="178" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised data association for metric learning in the context of multi-shot person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bremond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Adv. Vid. Sig. Surv</title>
		<meeting>IEEE Conf. Adv. Vid. Sig. Surv</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="256" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards unsupervised open-set person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Int. Conf. on Img. Proc</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="769" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Person re-identification by unsupervised video matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-M</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="197" to="210" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dynamic label graph matching for unsupervised video re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5142" to="5150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stepwise metric promotion for unsupervised video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2429" to="2438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transferable joint attributeidentity deep learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2275" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint semantic and latent attribute modelling for cross-class transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1625" to="1638" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised person reidentification: Clustering and fine-tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-view asymmetric metric learning for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="994" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Performance measures and a data set for multi-target, multi-camera tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Solera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="17" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3754" to="3762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Person re-identification by video ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="688" to="703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Person reidentification by descriptive and discriminative classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Beleznai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scand. Conf. Img. Anal</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="91" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mars: A video benchmark for large-scale person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="868" to="884" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification by deep learning tracklet association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="737" to="753" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Person reidentification by camera correlation aware feature augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="392" to="408" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human-in-the-loop person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="405" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pop: Person re-identification post-rank optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="441" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Person re-identification by symmetry-driven accumulation of local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Farenzena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bazzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Perina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2360" to="2367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unsupervised salience learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3586" to="3593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semi-supervised coupled dictionary learning for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3550" to="3557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unpaired image-toimage translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2223" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="994" to="1003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generalizing a person retrieval model hetero-and homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="172" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset transfer learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1306" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7948" to="7956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Domain adaptation through synthesis for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Lalonde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="189" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neur. Info. Proc. Sys</title>
		<meeting>Neur. Info. . Sys</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="75" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-task curriculum transfer deep learning of clothing attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Win. Conf. App. of Comp. Vis</title>
		<meeting>IEEE Win. Conf. App. of Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="520" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning deep representation for face alignment with auxiliary attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="918" to="930" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sparse coding with an overcomplete basis set: A strategy employed by v1?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Olshausen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page" from="3311" to="3325" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised deep embedding for clustering analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unsupervised learning by predicting noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="517" to="526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis</title>
		<meeting>IEEE Int. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">How far are we from solving pedestrian detection?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1259" to="1267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01942</idno>
		<title level="m">Motchallenge 2015: Towards a benchmark for multi-target tracking</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journ. of Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Self-tuning spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neur. Info. Proc. Sys</title>
		<meeting>Neur. Info. . Sys</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1601" to="1608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. on Learn</title>
		<meeting>Int. Conf. on Learn</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Hello neighbor: Accurate object retrieval with k-reciprocal nearest neighbors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gammeter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Quack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="777" to="784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis: An overview with application to learning methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Hardoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Szedmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2639" to="2664" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Sparse deep belief net model for visual area v2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ekanadham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Neur. Info. Proc. Sys</title>
		<meeting>Neur. Info. . Sys</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="873" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning deep feature representations with domain guided dropout for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1249" to="1258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Adaptive distance metric learning for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unsupervised neighborhood component analysis for clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="609" to="617" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Group consistent similarity learning via deep crf for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8649" to="8658" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Exploit the unknown gradually: One-shot video-based person reidentification by stepwise learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5177" to="5186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Dukemtmc4reid: A large-scale multi-camera person reidentification dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Radke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop of IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Robust anchor embedding for unsupervised video person re-identification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="170" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deep association learning for unsupervised video person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Bri. Mach. Vis. Conf</title>
		<meeting>Bri. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Video person reidentification with competitive snippet-similarity aggregation and co-attentive snippet embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis</title>
		<meeting>IEEE Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1169" to="1178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Human reidentification with transferred metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Asi. Conf. Comp. Vis</title>
		<meeting>Asi. Conf. Comp. Vis</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="31" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Viewpoint invariant pedestrian recognition with an ensemble of localized features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Eur. Conf. Comput. Vis</title>
		<meeting>Eur. Conf. Comput. Vis</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="262" to="275" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Person reidentification by support vector ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Prosser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Bri. Mach. Vis. Conf</title>
		<meeting>Bri. Mach. Vis. Conf</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">An improved algorithm for finding the strongly connected components of a directed graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Pearce</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Wellington, NZ, Tech. Rep</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Victoria University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comput. Vis. Pattern Recognit</title>
		<meeting>IEEE Conf. Comput. Vis. Pattern Recognit</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4700" to="4708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">United Kindom and also an assistant professor of Nanjing University of Science and Technology, China. He received his Ph.D. in Pattern Recognition and Intelligent System from Nanjing University of Science and Technology, China. His research interests include computer vision</title>
		<imprint/>
		<respStmt>
			<orgName>Minxian Li is a postdoctoral researcher at Queen Mary University of London</orgName>
		</respStmt>
	</monogr>
	<note>pattern recognition and deep learning</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
