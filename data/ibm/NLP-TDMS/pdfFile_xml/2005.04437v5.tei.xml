<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Understanding Dynamic Scenes using Graph Convolution Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sravan</forename><surname>Mylavarapu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahtab</forename><surname>Sandhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priyesh</forename><surname>Vijayan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Madhava Krishna</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balaraman</forename><surname>Ravindran</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Namboodiri</surname></persName>
						</author>
						<title level="a" type="main">Understanding Dynamic Scenes using Graph Convolution Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2021-06-25T22:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a novel Multi-Relational Graph Convolutional Network (MRGCN) based framework to model onroad vehicle behaviors from a sequence of temporally ordered frames as grabbed by a moving monocular camera. The input to MRGCN is a multi-relational graph where the graph's nodes represent the active and passive agents/objects in the scene, and the bidirectional edges that connect every pair of nodes are encodings of their Spatio-temporal relations.</p><p>We show that this proposed explicit encoding and usage of an intermediate spatio-temporal interaction graph to be well suited for our tasks over learning end-end directly on a set of temporally ordered spatial relations. We also propose an attention mechanism for MRGCNs that conditioned on the scene dynamically scores the importance of information from different interaction types.</p><p>The proposed framework achieves significant performance gain over prior methods on vehicle-behavior classification tasks on four datasets. We also show a seamless transfer of learning to multiple datasets without resorting to fine-tuning. Such behavior prediction methods find immediate relevance in a variety of navigation tasks such as behavior planning, state estimation, and applications relating to the detection of traffic violations over videos.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>We consider dynamic traffic scenes consisting of potentially active participants/agents such as cars and other vehicles that constitute the traffic and passive objects such as lane markings and poles (see example in <ref type="figure" target="#fig_0">Fig. 1</ref>). In this work, we propose a framework to model the behavior of each such active agents by analyzing the Spatio-temporal evolution of their relations with other active and passive objects in the scene. By relation, we refer to the spatial relations an agent/object possesses and enjoys with other agents/objects, such as between the vehicle and lane markings, as shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>.</p><p>Here, we model both objects and agents, and thus for convenience, we commonly refer them as objects and specifically as agents when referring to active vehicles. The evolution of the spatial relationship between all pairs of objects in a scene is essential in understanding their behaviors. To this end, we propose an Interaction graph that models different agents and objects in the scene as nodes. This graph captures the Spatio-temporal evolution of relations between all-pair of objects in the scene with appropriate bi-directional <ref type="bibr" target="#b0">1</ref> Authors are with the Center for Visual Information Technology, KCIS, IIIT Hyderabad 2 Authors are with the Robotics Research Center, KCIS, IIIT Hyderabad <ref type="bibr" target="#b3">3</ref>   asymmetric edges with annotations reflecting their evolution (see <ref type="figure" target="#fig_0">Fig. 1(c)</ref>).</p><p>The dynamic traffic scene modeled as an Interaction graph is then inputted to a Multi-Relational Graph Convolutional Network (MRGCN), which outputs the overall behavior exhibited by all the agents and objects in the scene. While the MRGCN maps the input graph to an output behavior for every graph node, we are only interested in active and not the passive objects. Hitherto by behavior, we denote the overall behavior of an active agent in the scene. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, the behavior of the car on the left is Overtaking, and the car on the right is moving ahead.</p><p>The choice of Graph Convolutional Networks (GCN) <ref type="bibr" target="#b0">[1]</ref> and the use of its Multi-Relational Variant as a choice for this problem stems from the recent success of such models in learning over data that does not present itself in a regular grid like structure and yet can be modeled as a graph such as in social and biological networks. Since a road scene can also be represented as a graph with nodes sharing multiple relations with other nodes, a GCN based model is apt for inferring overall node (object) behavior from a graph of interconnected relationships.</p><p>The decomposition of a dynamic on-road scene into its associated Interaction graph and the classification of the agent behavior by the MRGCN supervised over labels that are human-understandable (Lane Change, Overtake, etc.) form the main thesis of this effort. Such behavior classification of agents in the scene finds immediate utility in downstream modules and applications. Recent research showcase results that understanding on-road vehicle behavior leads to better behavior planners for the ego vehicle <ref type="bibr" target="#b1">[2]</ref>. In <ref type="bibr" target="#b1">[2]</ref>, belief states over driver intents of the other vehicles where the intents take the labels "Left Lane Change", "Right Lane Change", "Lane Keep" are used for a high-level POMDP based behavior planner for the ego vehicle. For example, the understanding that a car on the ego-vehicle's right lane is executing a lane change behavior into the current lane of the ego-vehicle can activate its "Change to Right Lane" behavior operation option for path planning. Similarly, modeling an agent's behavior such as a car in a parked state can make the ego vehicle use feature descriptors of the parked car to update its state accordingly, which would not be possible if the active object was engaged in any other behavior. Understanding on-road vehicle behaviors also lend itself to very pertinent applications such as detecting and classifying traffic scene violations such as "Overtaking Prohibited", "Lane Change Prohibited."</p><p>Our contributions are as follows: 1) We propose a novel yet simple scheme for spatial behavior encoding from a sequence of single-camera observations using straight forward projective geometry techniques. This method of encoding spatial behaviors for agents is better than previous efforts that have used end-to-end learning of spatial behaviors <ref type="bibr" target="#b3">[3]</ref>.</p><p>2) We demonstrate the aptness of the proposed pipeline that directly encodes Spatio-temporal behaviors as an intermediate representation into the scene graph G, followed by the MRGCN based behavioral classifier. We do this by comparing with two previous methods <ref type="bibr" target="#b3">[3]</ref>, <ref type="bibr" target="#b4">[4]</ref> that are devoid of such intermediate Spatio-temporal representations but activate the behavior classifier on per frame spatial representations sequenced temporally. Specifically we tabulate significant performance gain of at-least 25% on an average vis a vis <ref type="bibr" target="#b4">[4]</ref> and 10% over <ref type="bibr" target="#b3">[3]</ref> on a variety of datasets collected in various parts of the world <ref type="bibr" target="#b5">[5]</ref>, <ref type="bibr" target="#b6">[6]</ref>, <ref type="bibr" target="#b7">[7]</ref> and our own native dataset (refer Table IV-B). 3) We signify through a label deficient setup, the need for a neural-attention component that integrates with the MRGCN and further boosts its performance to nearly perfect predictions, as seen in Tables III and V. Critically incorporating the attention function leads to high performance even in a limited training set, which the MRGCN without attention function cannot replicate. 4) We also show seamless transfer of learning without further need to fine-tune across various combinations of datasets for the train and test split. Here again, we show better transfer capability of the current model vis a vis prior work, as shown in <ref type="table" target="#tab_3">Table IV</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Vehicle Behavior and Scene Understanding:</head><p>The problem of on-road vehicle scene understanding is an important problem within autonomous driving. Most earlier works relied on multiple sensor-based data to solve this task. Rulebased <ref type="bibr" target="#b8">[8]</ref> and probabilistic modeling <ref type="bibr" target="#b9">[9]</ref>, <ref type="bibr" target="#b10">[10]</ref>, <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b12">[12]</ref>, <ref type="bibr" target="#b13">[13]</ref>, <ref type="bibr" target="#b14">[14]</ref>, <ref type="bibr" target="#b15">[15]</ref> where the goto approaches for classification of driver behavior with sensor data. Also, many of these works <ref type="bibr" target="#b11">[11]</ref>, <ref type="bibr" target="#b14">[14]</ref> were concerned only with predicting future trajectories rather than classification. Herein, we chose a simpler and challenging set up to understand the scene and predict vehicle behaviors with observations from a singlecamera in this work. While there are few works based on a single-camera data feed, they only focus on ego-centered predictions <ref type="bibr" target="#b16">[16]</ref>. Here we focus on classifying other vehicles' behavior from an ego-vehicle perspective. Learning other vehicle behaviors can be helpful in behavior planning, state estimation, and applications relating to the detection of traffic violations over videos 2) Graph based reasoning: Graphs are a popular choice of data structures to model numerous irregular domains. With the recent advent of Graph Convolutional Networks (GCNs) <ref type="bibr" target="#b0">[1]</ref> that can obtain relevant node-level features for graphs, there is a widespread adaption of graph-based modeling of numerous computer vision problems such as in situationrecognition tasks <ref type="bibr" target="#b17">[17]</ref>. <ref type="bibr" target="#b18">[18]</ref> encodes object-centric relations in an image using a GCNS to learn object-centric policies for autonomous driving. <ref type="bibr" target="#b4">[4]</ref> and <ref type="bibr" target="#b3">[3]</ref> models objects in a video as Spatio-temporal graphs to make predictions of Spatiotemporal nature. It is common to model the temporal context of objects with recurrent neural nets and spatial context with a graph-based neural net.</p><p>In this work, we focus on the task of on-road vehicle behavior and show that a proposed intermediate representation, called an Interaction graph, can yield better performance over working with a raw set of spatial graphs as done traditionally. This also portrays current models' incapacity to learn endend and derive such useful features as with the Interaction graph from the raw spatial graphs. The closest works to ours are <ref type="bibr" target="#b16">[16]</ref> and <ref type="bibr" target="#b19">[19]</ref>. They generate an affinity graph that captures actor-objects relationships. A simple GCN is then used to reason over this graph to classify ego-car action and not other vehicles. In our work, we use a richer multirelational graph and a corresponding multi-relational GCN to work on the same. Further, we propose an attention based model that can leverage different relation types depending on the scene context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHODOLOGY</head><p>Dynamic scene understanding requires well modeling of the different Spatio-temporal relations that may exist between various active objects in a scene. Towards this goal, we propose a pipeline that first computes a time-based ordered set of spatial relations for each object in the video scene. Secondly, it generates a multi-relational interaction graph representing the temporal evolution of the spatial relations between entities obtained from the previous step. Finally, it leverages a graph-based behavior learning model to predict behaviors of vehicles in the scene. <ref type="figure">Fig 2 provides</ref> an overview of our proposed framework.</p><p>Our proposed pipeline leverages and improves the data modeling pipeline introduced in [3] (MRGCN-LSTM). Our pipeline's performance gains primarily stem from two of our contributions: (i) the interaction graph that provides useful and explicit temporal evolution information of spatial  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Spatial Graph Generation</head><p>For dynamic scene understanding, we need to identify different objects in the scene and determine the atomic spatial relationships between them at each time-step. This phase of the pipeline closely follows the spatial scene graph generation step of <ref type="bibr" target="#b3">[3]</ref>. refer to the same for mode details.</p><p>1) Object Detection and Tracking: Different vehicles in the video frames are detected and tracked through instance segmentation <ref type="bibr" target="#b20">[20]</ref> and per-pixel optical flow <ref type="bibr" target="#b21">[21]</ref> respectively. The instance-level segmentation of an object obtained from MaskRCNN are projected to the next frame and are associated with the highest overlapping instance using optical flows. Apart from tracking vehicles in the scene, static objects such as lane markings are also tracked with semantic segmentation <ref type="bibr" target="#b22">[22]</ref> to better understand changing relations among static and non-static objects. <ref type="bibr" target="#b3">[3]</ref> has shown that performance improvement can be obtained by leveraging a higher number of static objects in the scene.</p><p>2) Bird's Eye View: The tracklets of objects obtained in the image space are re-oriented in the Bird's eye view (Top View) by projecting the image coordinates into 3D coordinates as described in <ref type="bibr" target="#b23">[23]</ref>. This reorientation facilitates determining spatial relations between different entities. Each object is assigned a reference point to account for the difference in heights. The reference is at the center for lane markings, and for vehicles, it is the point adjacent to the road.</p><p>3) Spatial relations: At all T frames of the video, the spatial relations between different entities are determined using their 3D positional information in the Bird's eye view. Specifically, the spatial relations are the four quadrants, {top left, top right, bottom left, and bottom right}. For a subject entity, i, its spatial relation with an object entity, j at timestep, t is denoted as S t i,j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Temporal Interaction Graph Generation</head><p>Modeling object interactions as a time-based ordered set of spatial graphs is a popular approach in many Spatio-temporal problems. such as <ref type="bibr" target="#b24">[24]</ref>, <ref type="bibr" target="#b25">[25]</ref>, <ref type="bibr" target="#b26">[26]</ref>, <ref type="bibr" target="#b27">[27]</ref> where spatial graphs are constructed over object interactions to classify actions in a video stream. <ref type="bibr" target="#b4">[4]</ref>, <ref type="bibr" target="#b3">[3]</ref> use a similar approach for predicting on road object behaviors. We found that it is harder for models learned with such data to learn some of the simple temporal-evolution behaviors needed for the end task, specifically in our problem of interest. In our problem of focus, behavior prediction, it is important for the model to learn the nature of some simple temporal evolution of interaction between entities such as move-forward, move-backward, moved left to the right, moved right to the left, no-change. However, we found that explicitly modeling such information was highly beneficial. A simple rule-based model with such interaction information outperformed learned models on the primitive information at the level of spatial relations. Having motivated by this insight, we propose a way to define an Interaction graph with temporal evolution information and a new model that can benefit from such information.</p><p>The Interaction graph summarizes the temporal evolution of spatial relations from T frames into a single multirelational graph with temporal relations, R d = {moveforward, move-backward, moved left-to-right, moved rightto-left, no-change}. The edge, E i,j ∈ R d , denotes the temporal relation between subject entity, i and object entity, j. E i,j is computed by deterministic rule based pipeline, that takes in their spatial relations over T frames, i.e, {S 1 i,j , S 2 i,j , ..., S T i,j }. For example, an object entity, j that is initially in the bottom-left quadrant with respect to subject entity i, changes it's atomic spatial relation to top-left with respect to i at some time t, will have its temporal relation E i,j as move-forward. Similarly, an object entity, j that is initially in bottom-left quadrant changes to bottom-right quadrant with respect to subject i at some time t, will have temporal relation E i,j as moved left-to-right. Since these temporal relation annotated edges have a proper direction semantics, we can't treat the graph is undirected. Thus, we also introduce inverse edges for complementary relations suchs as moved forward and backward and moved left-to-right and moved right-to-left. An overview of the temporal interaction graph generation is presented in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Behavior Prediction Model</head><p>We propose a Multi-relational Graph Convolution Network (MRGCN) with a relation-attention module that conditioned on the scene, automatically learns relevant information from different temporal relations necessary to predict vehicle behaviors.</p><p>1) Multi-Relational Graph Convolution Networks: Recently Graph convolution Networks <ref type="bibr" target="#b0">[1]</ref> has become the popular choice to model graph-structured data. We model our task of maneuver prediction by using a variant of Graph Convolutional Networks, Multi-Relational Graph Convolutional Networks (MRGCN) <ref type="bibr" target="#b28">[28]</ref> originally proposed for knowledge graphs with multiple relation types. MRGCN is composed of multiple graph convolutional layers, one for each relation between nodes. A Graph Convolution operation for a relation r here is a simple neighborhood-based information aggregation function. In the MRGCN, information obtained from convolving over different relations is combined by summation.</p><p>Let us formally define the temporal Interaction Graph as G = (V, E) with vertex set V and edge set, E, where E i,j ∈ R d is an edge between node i and j. The i th node feature obtained from a graph convolution over relation, r in l th layer is defined as follows:</p><formula xml:id="formula_0">h l r [i] = j∈Nr[i] 1 c r [i] W l r h l−1 [j]<label>(1)</label></formula><p>where, N r [i] denotes set of neighbour nodes for v i under relation r, N r [i] = {j ∈ V | E j,i = r} and c r [i] = |N r [i]| is a normalization factor. Here, W l r ∈ R d * d is the weights associated with relation r in the l th layer of MR-GCN; d , d are dimensions of (l − 1) th and l th layers of MRGCN.</p><p>Neighborhood information aggregated from all the relations are then combined by a simple summation to obtain the node representation as follows:</p><formula xml:id="formula_1">h l [i] = ReLU (W l s h l−1 [i] + r∈R d h l r [i])<label>(2)</label></formula><p>where, the first terms correspond to the node information (self-loop) and W s ∈ R d * d is the weight associated with self-loop. To account for the nature of the entity (active or passive), we learn entity embeddings,</p><formula xml:id="formula_2">E i ∈ R |O| * d for node v i , where O = {V ehicles, Lane M arkings}.</formula><p>The input to the first layer of the MRGCN, h 0 [i], is the embedding E i based on type of node i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Relation-Attention MRGCN (Rel-Att-GCN):</head><p>The MRGCN defined in Eqn: 2 treats information from all the relations equally, which might be a sub-optimal choice to learn discriminative features for certain classes. Motivated by this, we propose a simple attention mechanism that scores the node information's importance along with individual neighborhood information from each relation.</p><p>The attention scores, α are computed by concatenating the information from the node (h l−1 ) and its relational neighbors (h l r ) and transforming it with a linear layer to predict scores for each component. The predicted scores are softmax normalized. The attention scores are computed as defined below.</p><formula xml:id="formula_3">α l [i] = sof tmax([h l−1 [i] h l 1 [i] h l 2 [i]... h l |R d | ]W l u )<label>(3)</label></formula><p>where, represents concatenation and α l [i] ∈ R |R d +1| with W l u being the linear attention layer weights. These probabilities depict the importance of a specific relation conditioned on that node and its neighborhood.</p><p>The attention scores are used to scale the node and neighbor information accordingly to obtain the node representation as follows.</p><formula xml:id="formula_4">h l [i] = ReLU (α l node [i]h l−1 [i] + r∈R d α l r [i]h l r [i])<label>(4)</label></formula><p>where, α l node is the attention score for self-loop. The node representation obtained at the last layer is used to predict labels, and the model is trained by minimizing the crossentropy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT AND ANALYSIS A. Dataset</head><p>Numerous datasets have been released in the interest of solving problems related to autonomous driving. We choose four datasets for evaluating our framework, of which three are publicly available: ApolloScapes <ref type="bibr" target="#b6">[6]</ref>, KITTI <ref type="bibr" target="#b29">[29]</ref>, Honda Driving dataset <ref type="bibr" target="#b7">[7]</ref> and one is a proprietary Indian dataset. These datasets provide hours of driving data with monocular image feed in various driving conditions. We use the same dataset Train/Test/Val splits from <ref type="bibr" target="#b3">[3]</ref> for Apollo Scape, KITTI, and Indian dataset and extend the setup to Honda dataset and manually annotated accordingly for our task. 1) Apollo Dataset: We choose Apollo-scapes as our primary dataset as it contains a large number of driving scenarios that are of interest. It includes vehicles depicting overtake and lane-change behaviors. The dataset consists of image feed collected from urban areas and contains various objects such as Cars, Buses, etc. The final dataset used here contains a total of 4K frames with multiple behaviors. 2) KITTI Dataset: It consists of images collected majorly from highways and wide open-roads, unlike Apollo-scapes. We select 700 frames from Tracking Sequences 4, 5, and 10 for our purpose. These chosen sequences contained a variety of driving behaviors compared to the rest. 3) Honda Driving Dataset: This dataset consists of multiple datasets in itself. From among them, we choose the H3D dataset for our task as it contained lane change behaviors. H3D comprises driving in urban city conditions where lane changes are prominent. We excluded overtaking behavior here due to its fewer occurrences in the dataset. It consists of a total of 1.5K frames. 4) Indian Dataset Although the datasets mentioned above are widely used and include wide vehicle behaviors, they mostly contain standard vehicle frames only. To showcase models' transfer learning capabilities on less standard vehicles, we also use an Indian driving dataset that includes vehicles such as auto-rickshaw, trucks, tankers, etc. This dataset contains 600 frames.</p><p>Class labels: The vehicle behaviors predicted in these datasets are: (i) Moving Away from Us (MAU), (ii) Moving Towards Us (MTU), (iii) Parked (PRK), (iv) Lane Change from Left-right (LCL), (v) Lane Change from left-Right (LCR) and (vi) Overtake (OVT).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimentation details</head><p>All the models, both learning and rule-based, use the same pre-processing steps to identify and track objects in the scene, as explained in section: III-A.1 for T = 10 timesteps (frames). The Spatial graphs at each time frame are constructed by considering a maximum of 10 vehicles in the scene nearest to the ego-vehicle for classifying them. Then, an Interaction graph is independently generated from the set of T temporally ordered spatial graphs. The MRGCN used is identical in both models MRGCN and Rel-Att-GCN. Note that the input and output dimensions of attention are equal. We empirically found that using 3 layers of MRGCN with dimensions 64, 32, and 6 (number of classes) respectively works best for our task. In the case of Rel-Att-GCN, simple attention is applied over the output of MRGCN for each node individually with 2 heads. Outputs of the heads are concatenated across relations and projected back to the MRGCN layer's output dimension with a linear transformation. We  found adding skip connection from every layer l to (l + 2) th layer to be beneficial. The inference time for the models: MRGCN-LSTM, MRGCN, and Rel-Att-GCN averaged over 1K graphs are 0.02, 0.03, and 0.04 seconds respectively. Note that the latter two models inference time also includes the creation of the Interaction graph. All the training and testing was done on a single Nvidia Geforce Gtx 1080 GPU. More details regarding the implementation can be found on our project website, 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph used Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline Comparisons</head><p>In <ref type="table">Table:</ref> IV-B, we compare our models with Spatiotemporal approaches as well as a rule-based method on the Interaction graph. Table IV-B reports class-wise Recall scores of these methods. The results reported here in all the tables are averaged over 5 runs.</p><p>1) Spatio-Temporal approaches: To depict the importance of encoding Spatio-temporal information as an Interaction graph, we compare our model with Structural-RNN <ref type="bibr" target="#b4">[4]</ref> and MRCGN-LSTM <ref type="bibr" target="#b3">[3]</ref> that processes a time based ordered set of spatial graphs. Structural-RNN (St-RNN) encodes the spatial representation for each frame in a graph and then reasons over the temporal evolution of these graphs by feeding it to a Recurrent Neural Network. We adapt St-RNN's pipeline to our problem by replacing humans and objects in their model with vehicles and stationary landmarks, respectively. A similar methodology is employed for MRGCN-LSTM <ref type="bibr" target="#b3">[3]</ref>.</p><p>We show a quantitative comparison with our pipeline/model variations in <ref type="table" target="#tab_3">Table IV</ref>-B. St-RNN that doesn't have any GCN components fare the worst. In <ref type="table" target="#tab_3">Table IV</ref>-B, we observe that our method outperforms the traditional temporal based approach, St-RNN, by a significant margin. The gap is even more prominent when comparing the harder classes such as lane changing and overtaking, where we observe an average difference of 40% and 26%, respectively. A comparison between MRGCN-LSTM that uses a set of spatial graphs vs. MRGCN that uses the proposed interaction graph clearly shows the benefit of the proposed interaction graph. MRGCN outperforms its counter that learns in an end-end manner. This shows how such simple inherent behaviors are still hard for GCNs to learn. Further, with the addition of the attention mechanism, the Rel-Att-GCN model achieves an additional absolute 3-4% improvement on a few hard classes.</p><p>2) Rule Based Baseline: To showcase the effectiveness of an information-rich Interaction graph over traditional Spatio-  The Rel-Att-GCN model is the MRGCN model with an additional attention component. The proposed Attention function factors into account that different types of relations in the interaction graph may have different relevancy to predict different classes. The varying importance of the relations in classifying vehicle behaviors can be visualized by analyzing normalized attention scores across relations for each class. One such visualization for the Apollo Scape dataset is depicted in <ref type="figure" target="#fig_3">Fig. 4</ref>. Higher values in each class (row) denote higher importance given by the attention function to that particular relation (column) to predict that class. The attention map clearly shows how classes such as overtake and lane changes depend on moving forward and moved left to right or right to left respectively. Despite how both MVA and OVT classes have high probability mass on moveforward relation, they can distinguish themselves based on the attention score spread over other relations. Such reasoning helps the network to learn effectively under label scarcity, as given in <ref type="table" target="#tab_3">Table III</ref>. Herein, we report results for models trained with 5% , 10% and 20% of training data. Rel-Att-GCN is able to show fidelity even when only 5% percent of data is present in contrast to a normal MR-GCN trained on the same interaction graph, which finds it difficult to learn from the smaller dataset. A similar trend is observed as we increase the size of training data to 10% and 20% of the actual dataset. In <ref type="table">Table V</ref>, where the model is learned with 70% data, we observe that Rel-Att-GCN achieves superior performance across all datasets when compared with plain MR-GCN as well as temporal based methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Transfer Learning</head><p>To showcase our proposed pipeline's generality, we trained the model only on the Apollo dataset and tested it on validation sets of Honda, KITTI, and Indian datasets. At the testing phase, we removed the classes which were not present in corresponding datasets. As the proposed pipeline does not rely upon any visually learned features, we can achieve road on the right and two cars coming towards us, in (d) we see a car changing a lane (on the right), a car parked on the left and two pickup trucks moving away from us. The qualitative results validates the proposed model's near-perfect classification and generalizability across datasets even in the presence of less (or not) observed test vehicles. V. CONCLUSIONS This paper proposed a novel pipeline for on-road vehicle behavior understanding and classification. It decomposed an evolving dynamic scene into a multi-relational Interaction graph whose nodes are the agents/actors in the scene, and edges are Spatio-temporal encodings that signify the agents' spatial behaviors. The interaction graph was further acted upon by a Multi-Relational Graph Convolution Network (MRGCN) to learn and classify the vehicle's overall behavior. The key takeaway is this two-stage classification that showed much-improved performance over end-end learning frameworks. The improved performance is attributed to edge encodings of the interaction graph being an accurate intermediate representation of spatial behaviors between agents that are difficult to characterize in an end-end learning framework. The MRGCN is integrated with an attention layer that further improved the performance, often nearperfect performance. Significant performance gain on various datasets that are consistent across several metrics confirms the efficacy of the proposed framework. Seamless data transfer across datasets further showcases its reliability. Future directions include integrating the proposed framework with a behavior planner. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The figures (a) and (b) show two cars and three lane-markings from two different time frames. The evolution of the whole scene is captured in the Interaction graph (c). Our proposed model infers over such Interaction graphs to classify objects' behaviors. Here, the car on the left is moving ahead of lane markings and the car on the right. With the Interaction graph (c), our model can predict that the car on the left is overtaking the right-side car.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Overall pipeline of our framework: The input (a) to the pipeline is monocular image frames. Various object tracking pipelines are used to detect and track objects, as shown in (b). (c) denotes the tracked objects. Tracklets for each object are projected to 3D space, Bird's eye-view at each time step, as shown in (d). Spatial relations from Bird's eye view are used to generate Interaction graphs (e). This graph is passed through a Rel-Att-RGCN (f) to classify objects in the scene as shown in (g). Temporal Interaction Graph Generation: The top row contains image frames at t=0, t=5, t=10 time step with tracked objects. The bottom row shows the corresponding bird's eye view. In the scene, a car and two lane-markings are tracked at each time step. The thick red, blue edges between car, L1 in first, and the final frame denote their spatial relations. The car moves forward with respect to L1 and has no relational change with L2, as shown with thick edges in the Interaction graph. Temporal relations are represented with dotted lines, while spatial relations are represented with thick edges. Corresponding color coding is shown in legends. relations. (ii) the proposed Multi-Relational Graph Convolutional Network (MRGCN) with our novel multi-head relation attention function, which we name Relation-Attentive-GCN (Rel-Att-GCN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>D</head><label></label><figDesc>. Analysis of Relation-Attention MRGCN (Rel-Att-GCN)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Figure showsattention scores between class labels (rows) and types of spatio-temporal interactions (columns). Higher scores for a particular relation indicates a higher dependence of the class on that particular relation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .Fig. 6 .</head><label>56</label><figDesc>Sub-figures (a) and (b) showcases prediction on standard vehicles from KITTI dataset. Whereas in (c), we can see Petrol Tanker and Bus's behavior predictions from the Indian dataset that shows object class-agnosticism. In (d), we can see a complex lane changes behavior prediction. The figure shows multiple scenarios depicting various behaviors. (a), (b) are samples from ApolloScapes dataset while (c) and (d) are from Honda dataset. In (a), (c), and (d), we observe the model accurately classifying lane change, both left to right and right to left. (b) depicts a case where the magenta car overtakes the red car.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE I VEHICLE</head><label>I</label><figDesc>BEHAVIOR PREDICTION ON APOLLO SCAPE DATASET.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE III RECALL</head><label>III</label><figDesc></figDesc><table /><note>ON APOLLOSCAPE DATASET FOR DIFFERENT AMOUNT OF TRAINING DATA</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">code:https://github.com/ma8sa/Undersrtanding-Dynamic-Scenesusing-MR-GCN.git</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Video : https://youtu.be/TT4J-uH4xqI</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>temporal modeling with a set of spatial graphs, we propose a rule-based approach to infer over the Interaction graph as one of our baselines. We use the Interaction Graphs generated by our pipeline (ref section:III-B) and employ an expert set of rules carefully framed to classify between behaviors. The deterministic classification is a simple max function over different relations a vehicle is associated with. Relational behavior with majority count decides to which class the object belongs to from the following, {moving away, moving towards us, and lane changes}. For example, a node having the highest count for behavioral relation moved left to right would have its class as Lane change (Left to right). To obtain classification for overtake behavior, we iterate over all pair of vehicles, i and j, that are not classified as parked or moving towards us in the first iteration and then we observe if there exists e i,j = move-forward, in which</p><p>The quantitative comparison in <ref type="table">Table IV</ref>   fidelity across all datasets, as seen in <ref type="table">Table IV</ref>. Evaluation results for the model trained and tested on validation sets of the same dataset can be found in <ref type="table">Table V</ref>. From a comparison between <ref type="table">Table IV and Table V</ref>, the transfer learning model though not better than models that are trained and tested on the same dataset, is on par with them. Notably, in the Honda dataset, the Rel-Att-MRGCN performs better in transfer for all classes than the model trained and tested on Honda. We attribute this behavior to high variation present in the Apollo dataset, which the other datasets lack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Qualitative</head><p>A video demonstration of the qualitative performance of our model on different datasets can be found here 2 . In <ref type="figure">Fig.  5</ref>   <ref type="figure">Fig. 5</ref>, sub-figures (c) and (d), showcase results from the Indian Dataset, wherein image (c), we see a bus and truck parked and in (d) we see Lane change behavior depicted by the car on the right.</p><p>In <ref type="figure">Fig. 6 (a)</ref> we see a car changing lane and in <ref type="figure">Fig. 6  (b)</ref> we observe a car classified as overtaking. <ref type="figure">Fig. 6</ref> (c) and (d) show fidelity of our pipeline in traffic scenarios. In <ref type="figure">Fig.6</ref> (c) we observe a car changing lane and merging into the</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Context and intention aware planning for urban driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Meghjani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">H</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hsu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2891" to="2898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Towards accurate vehicle behaviour classification with multi-relational graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mylavarapu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sandhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vijayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ravindran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Namboodiri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00786</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The apolloscape dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="954" to="960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The h3d dataset for full-surround 3d multi-object detection and tracking in crowded urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Malla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A scenarioadaptive driving behavior prediction approach to urban autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">426</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning category-specific mesh reconstruction from image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<idno>abs/1803.07549</idno>
		<ptr target="http://arxiv.org/abs/1803.07549" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Looking at vehicles on the road: A survey of vision-based vehicle detection, tracking, and behavior analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sivaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1773" to="1795" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Interactionaware probabilistic behavior prediction in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hubmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Löchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Burschka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3999" to="4006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Car that knows before you do: Anticipating maneuvers via learning temporal driving models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3182" to="3190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Reliable method for driving events recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mitrovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on intelligent transportation systems</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="198" to="205" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-body motion estimation from monocular vehicle-mounted cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sabzevari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scaramuzza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="638" to="651" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Dynamic traffic scene classification with space-time coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dariush</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning actor relation graphs for group activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9964" to="9974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Situation recognition with graph neural networks supplementary material</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep objectcentric policies for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8853" to="8859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning 3d-aware egocentric spatial-temporal interaction via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.09272</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">In-place activated batchnorm for memory-optimized training of dnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5639" to="5647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Robust scale estimation in real-time monocular SFM for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2014.203</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2014.203" />
	</analytic>
	<monogr>
		<title level="m">2014 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2014-06-23" />
			<biblScope unit="page" from="1566" to="1573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Spatio-temporal scene graphs for video dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03848</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action genome: Actions as compositions of spatio-temporal scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spatio-temporal action graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
